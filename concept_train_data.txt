sparse O
bayesian O
learning B
and O
the O
relevance B
vector I
machine I
. O
adding O
a O
regularization B
term O
to O
the O
log O
likelihood O
function O
means O
the O
effective O
model O
complexity O
can O
then O
be O
controlled O
by O
the O
value O
of O
the O
regularization B
coefﬁ- O
cient O
, O
although O
the O
choice O
of O
the O
number O
and O
form O
of O
the O
basis O
functions O
is O
of O
course O
still O
important O
in O
determining O
the O
overall O
behaviour O
of O
the O
model O
. O
3.22 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
starting O
from O
( O
3.86 O
) O
verify O
all O
of O
the O
steps O
needed O
to O
show O
that O
maximiza- O
tion O
of O
the O
log O
marginal O
likelihood B
function I
( O
3.86 O
) O
with O
respect O
to O
β O
leads O
to O
the O
re-estimation O
equation O
( O
3.95 O
) O
. O
if O
we O
consider O
a O
sum-of-squares B
error I
function O
for B
regression I
, O
then O
sequential O
minimization O
of O
an O
additive O
model O
of O
the O
form O
( O
14.21 O
) O
simply O
involves O
ﬁtting O
each O
new O
base O
classiﬁer O
to O
the O
residual O
errors O
tn−fm−1 O
( O
xn O
) O
from O
the O
previous O
model O
. O
for O
qµ O
( O
µ O
) O
we O
have O
ln O
q O
( O
cid:1 O
) O
( O
cid:24 O
) O
µ O
( O
µ O
) O
= O
eτ O
[ O
ln O
p O
( O
d|µ O
, O
τ O
) O
+ O
ln O
p O
( O
µ|τ O
) O
] O
+ O
const O
( O
xn O
− O
µ O
) O
2 O
( O
cid:25 O
) O
completing B
the I
square I
over O
µ O
we O
see O
that O
qµ O
( O
µ O
) O
is O
a O
gaussian O
n O
( O
cid:10 O
) O
λ0 O
( O
µ O
− O
µ0 O
) O
2 O
+ O
= O
− O
e O
[ O
τ O
] O
2 O
n O
( O
cid:2 O
) O
n=1 O
( O
cid:11 O
) O
+ O
const O
. O
the O
plot O
on O
the O
right O
shows O
the O
result O
of O
whitening B
of O
the O
data O
to O
give O
it O
zero O
mean B
and O
unit O
covariance B
. O
if O
it O
is O
not O
in O
the O
evidence O
set O
, O
then O
it O
is O
sampled O
from O
the O
conditional B
distribution O
p O
( O
zi|pai O
) O
in O
which O
the O
conditioning O
variables O
are O
set O
to O
their O
currently O
sampled O
values O
. O
using O
the O
deﬁnition O
( O
13.46 O
) O
to O
substitute O
for O
fn+1 O
( O
zn O
, O
zn+1 O
) O
, O
and O
deﬁning O
β O
( O
zn O
) O
= O
µfn+1→zn O
( O
zn O
) O
( O
13.52 O
) O
13.2. O
hidden O
markov O
models O
627 O
we O
obtain O
the O
beta B
recursion I
given O
by O
( O
13.38 O
) O
. O
( O
2.106 O
) O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
t O
( O
cid:15 O
) O
x O
y O
( O
cid:16 O
) O
λµ O
− O
atlb O
lb O
( O
cid:15 O
) O
( O
cid:15 O
) O
using O
our O
earlier O
result O
( O
2.71 O
) O
obtained O
by O
completing B
the I
square I
over O
the O
quadratic O
form O
of O
a O
multivariate O
gaussian O
, O
we O
ﬁnd O
that O
the O
mean B
of O
z O
is O
given O
by O
e O
[ O
z O
] O
= O
r−1 O
exercise O
2.30 O
making O
use O
of O
( O
2.105 O
) O
, O
we O
then O
obtain O
e O
[ O
z O
] O
= O
( O
cid:16 O
) O
. O
in O
this O
case O
, O
a O
linear O
approximation O
for O
the O
network O
outputs O
, O
as O
was O
used O
in O
the O
case O
of O
regression B
, O
would O
be O
inappropriate O
due O
to O
the O
logistic B
sigmoid I
output- O
unit O
activation B
function I
that O
constrains O
the O
output O
to O
lie O
in O
the O
range O
( O
0 O
, O
1 O
) O
. O
3.1. O
linear O
basis O
function O
models O
the O
simplest O
linear O
model O
for B
regression I
is O
one O
that O
involves O
a O
linear O
combination O
of O
the O
input O
variables O
y O
( O
x O
, O
w O
) O
= O
w0 O
+ O
w1x1 O
+ O
. O
, O
tn O
) O
t. O
figure O
1.2 O
shows O
a O
plot O
of O
a O
training B
set I
comprising O
n O
= O
10 O
data O
points O
. O
in O
order O
that O
the O
discrete O
leapfrog O
integration O
be O
a O
reasonably O
554 O
11. O
sampling B
methods I
good O
approximation O
to O
the O
true O
continuous-time O
dynamics O
, O
it O
is O
necessary O
for O
the O
leapfrog O
integration O
scale O
 O
to O
be O
smaller O
than O
the O
shortest O
length-scale O
over O
which O
the O
potential O
is O
varying O
signiﬁcantly O
. O
( O
2.156 O
) O
again O
, O
it O
is O
also O
possible O
to O
deﬁne O
a O
conjugate B
prior I
over O
the O
covariance B
matrix I
itself O
, O
rather O
than O
over O
the O
precision B
matrix I
, O
which O
leads O
to O
the O
inverse B
wishart O
distribu- O
tion O
, O
although O
we O
shall O
not O
discuss O
this O
further O
. O
the O
result O
is O
the O
graph O
shown O
in O
figure O
8.1. O
if O
there O
is O
a O
link B
going O
from O
a O
node B
a O
to O
a O
node B
b O
, O
then O
we O
say O
that O
node B
a O
is O
the O
parent O
of O
node B
b O
, O
and O
we O
say O
that O
node B
b O
is O
the O
child O
of O
node B
a. O
note O
that O
we O
shall O
not O
make O
any O
formal O
distinction O
between O
a O
node B
and O
the O
variable O
to O
which O
it O
corresponds O
but O
will O
simply O
use O
the O
same O
symbol O
to O
refer O
to O
both O
. O
the O
evidence B
approximation I
. O
before O
doing O
so O
, O
however O
, O
it O
is O
instructive O
to O
consider O
a O
frequentist B
viewpoint O
of O
the O
model O
complexity O
issue O
, O
known O
as O
the O
bias- O
variance B
trade-off O
. O
the O
particular O
case O
of O
k O
= O
1 O
is O
called O
the O
nearest-neighbour O
rule O
, O
because O
a O
test O
point O
is O
simply O
assigned O
to O
the O
same O
class O
as O
the O
nearest O
point O
from O
the O
training B
set I
. O
a O
hier- O
archical O
latent B
variable I
model O
for O
data O
visualiza- O
tion O
. O
, O
xn O
} O
are O
independent B
given O
µ O
, O
so O
that O
figure O
8.23 O
( O
a O
) O
directed B
graph O
corre- O
sponding O
to O
the O
problem O
of O
inferring O
the O
mean B
µ O
of O
a O
univariate O
gaussian O
dis- O
tribution O
from O
observations O
x1 O
, O
. O
x1 O
x2 O
x3 O
x4 O
figure O
13.5 O
we O
can O
represent O
sequen- O
tial O
data O
using O
a O
markov O
chain O
of O
latent O
variables O
, O
with O
each O
observation O
condi- O
tioned O
on O
the O
state O
of O
the O
corresponding O
latent B
variable I
. O
l O
( O
q O
, O
θnew O
) O
ln O
p O
( O
x|θnew O
) O
figure O
9.14 O
the O
em O
algorithm O
involves O
alter- O
nately O
computing O
a O
lower B
bound I
on O
the O
log O
likelihood O
for O
the O
cur- O
rent O
parameter O
values O
and O
then O
maximizing O
this O
bound O
to O
obtain O
the O
new O
parameter O
values O
. O
indeed O
, O
from O
( O
2.141 O
) O
and O
( O
2.142 O
) O
noninformative B
prior I
by O
taking O
the O
limit O
σ2 O
we O
see O
that O
this O
gives O
a O
posterior O
distribution O
over O
µ O
in O
which O
the O
contributions O
from O
( O
cid:17 O
) O
the O
prior B
vanish O
. O
we O
therefore O
seek O
a O
general O
framework O
for O
modelling O
conditional B
probability I
distributions O
. O
this O
highlights O
the O
key O
difference O
between O
bayesian O
model B
averaging I
and O
model O
combination O
, O
because O
in O
bayesian O
model B
averaging I
the O
whole O
data O
set O
is O
generated O
by O
a O
single O
model O
. O
in O
fact O
, O
the O
log O
of O
the O
pos- O
terior O
distribution O
will O
be O
nonconvex O
, O
corresponding O
to O
the O
multiple O
local B
minima O
in O
the O
error B
function I
. O
( O
1.31 O
) O
( O
1.32 O
) O
a O
formal O
justiﬁcation O
of O
the O
sum O
and O
product O
rules O
for O
continuous O
variables O
( O
feller O
, O
1966 O
) O
requires O
a O
branch O
of O
mathematics O
called O
measure B
theory I
and O
lies O
outside O
the O
scope O
of O
this O
book O
. O
if O
we O
have O
a O
marginal B
gaussian O
distribution O
for O
x O
and O
a O
conditional B
gaussian O
distribution O
for O
y O
given O
x O
in O
the O
form O
p O
( O
x O
) O
= O
n O
( O
x|µ O
, O
λ O
p O
( O
y|x O
) O
= O
n O
( O
y|ax O
+ O
b O
, O
l−1 O
) O
−1 O
) O
( O
b.37 O
) O
( O
b.38 O
) O
( O
b.39 O
) O
( O
b.40 O
) O
( O
b.42 O
) O
( O
b.43 O
) O
( O
b.44 O
) O
( O
b.45 O
) O
then O
the O
marginal B
distribution O
of O
y O
, O
and O
the O
conditional B
distribution O
of O
x O
given O
y O
, O
are O
given O
by O
p O
( O
y O
) O
= O
n O
( O
y|aµ O
+ O
b O
, O
l−1 O
+ O
aλ O
−1at O
) O
p O
( O
x|y O
) O
= O
n O
( O
x|σ O
{ O
atl O
( O
y O
− O
b O
) O
+ O
λµ O
} O
, O
σ O
) O
where O
if O
we O
have O
a O
joint O
gaussian O
distribution O
n O
( O
x|µ O
, O
σ O
) O
with O
λ O
≡ O
σ O
( O
b.46 O
) O
−1 O
and O
we O
σ O
= O
( O
λ O
+ O
atla O
) O
−1 O
. O
however O
, O
we O
have O
not O
made O
any O
formal O
connection O
between O
conditional B
independence I
and O
factorization B
for O
undirected B
graphs O
. O
each O
such O
segment O
being O
a O
( O
noisy O
) O
continuous O
two-dimensional O
manifold B
. O
( O
8.15 O
) O
( O
cid:2 O
) O
j∈pai O
( O
cid:2 O
) O
j∈pai O
figure O
8.14 O
a O
directed B
graph O
over O
three O
gaussian O
variables O
, O
with O
one O
missing O
link O
. O
note O
that O
here O
we O
are O
applying O
the O
assumption O
of O
exponential B
family I
distribu- O
tion O
to O
the O
target O
variable O
t O
, O
in O
contrast O
to O
section O
4.2.4 O
where O
we O
applied O
it O
to O
the O
input O
vector O
x. O
we O
therefore O
consider O
conditional B
distributions O
of O
the O
target O
variable O
of O
the O
form O
( O
cid:19 O
) O
( O
cid:20 O
) O
( O
cid:17 O
) O
( O
cid:18 O
) O
p O
( O
t|η O
, O
s O
) O
= O
1 O
s O
h O
t O
s O
ηt O
s O
g O
( O
η O
) O
exp O
. O
however O
, O
the O
evaluation O
of O
the O
covariance B
matrix I
itself O
takes O
0 O
( O
nd O
2 O
) O
computations O
, O
where O
n O
is O
the O
number O
of O
data O
points O
. O
this O
corresponds O
to O
posterior O
probabilities O
p O
( O
ck|x O
) O
, O
which O
, O
for O
at O
least O
some O
values O
of O
x O
, O
are O
not O
0 O
or O
1. O
in O
such O
cases O
, O
the O
opti- O
mal O
solution O
is O
obtained O
by O
modelling O
the O
posterior O
probabilities O
accurately O
and O
then O
applying O
standard O
decision O
theory B
, O
as O
discussed O
in O
chapter O
1. O
note O
that O
nonlinear O
transformations O
φ O
( O
x O
) O
can O
not O
remove O
such O
class O
overlap O
. O
consider O
the O
class O
of O
distributions O
represented O
by O
the O
graphical B
model I
in O
fig- O
ure O
13.5 O
, O
and O
suppose O
we O
are O
given O
the O
observed O
values O
xn O
= O
( O
x1 O
, O
. O
9.3.1 O
gaussian O
mixtures O
revisited O
we O
now O
consider O
the O
application O
of O
this O
latent B
variable I
view O
of O
em O
to O
the O
spe- O
ciﬁc O
case O
of O
a O
gaussian O
mixture B
model I
. O
to O
do O
this O
, O
note O
that O
both O
models O
have O
gaussian O
predictive O
distributions O
, O
and O
so O
it O
is O
only O
necessary O
to O
show O
that O
the O
conditional B
mean O
and O
variance B
are O
the O
same O
. O
the O
perceptron B
also O
had O
a O
patch O
board O
, O
shown O
in O
the O
middle O
photograph O
, O
which O
allowed O
different O
conﬁgurations O
of O
input O
features O
to O
be O
tried O
. O
as O
our O
distortion B
measure I
, O
we O
shall O
use O
the O
squared O
distance O
between O
the O
original O
data O
point O
x O
n O
and O
its O
approximation O
xn O
, O
averaged O
over O
the O
data O
set O
, O
so O
that O
our O
goal O
is O
to O
minimize O
n O
j O
= O
~ O
l O
ilxn O
- O
xn O
11 O
2 O
. O
5.16 O
( O
( O
cid:12 O
) O
) O
the O
outer B
product I
approximation I
to O
the O
hessian O
matrix O
for O
a O
neural B
network I
using O
a O
sum-of-squares B
error I
function O
is O
given O
by O
( O
5.84 O
) O
. O
the O
structure O
of O
this O
calculation O
is O
identical O
to O
that O
of O
the O
sum-product B
algorithm I
, O
and O
so O
we O
can O
simply O
translate O
those O
results O
into O
the O
present O
context O
. O
the O
model O
may O
also O
have O
latent O
variables O
as O
well O
as O
parameters O
, O
and O
we O
shall O
denote O
the O
set O
of O
all O
latent O
variables O
and O
parameters O
by O
z. O
similarly O
, O
we O
denote O
the O
set O
of O
all O
observed O
variables O
by O
x. O
for O
example O
, O
we O
might O
have O
a O
set O
of O
n O
independent B
, O
identically O
distributed O
data O
, O
for O
which O
x O
= O
{ O
x1 O
, O
. O
the O
total O
covariance B
matrix I
can O
be O
decomposed O
into O
the O
sum O
of O
the O
within-class B
covariance I
matrix O
, O
given O
by O
( O
4.40 O
) O
and O
( O
4.41 O
) O
, O
plus O
an O
additional O
matrix O
sb O
, O
which O
we O
identify O
as O
a O
measure O
of O
the O
between-class B
covariance I
st O
= O
sw O
+ O
sb O
where O
sb O
= O
nk O
( O
mk O
− O
m O
) O
( O
mk O
− O
m O
) O
t. O
( O
4.40 O
) O
( O
4.41 O
) O
( O
4.42 O
) O
( O
4.45 O
) O
( O
4.46 O
) O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
n=1 O
k O
( O
cid:2 O
) O
k=1 O
n∈ck O
k=1 O
k O
( O
cid:2 O
) O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
( O
cid:2 O
) O
k=1 O
n∈ck O
( O
4.47 O
) O
( O
4.48 O
) O
( O
4.49 O
) O
( O
4.50 O
) O
192 O
4. O
linear O
models O
for O
classification O
these O
covariance B
matrices O
have O
been O
deﬁned O
in O
the O
original O
x-space O
. O
using O
the O
results O
( O
2.81 O
) O
and O
( O
2.82 O
) O
, O
we O
see O
that O
the O
con- O
c O
= O
k O
( O
xn O
+1 O
, O
xn O
+1 O
) O
+ O
β O
ditional O
distribution O
p O
( O
tn O
+1|t O
) O
is O
a O
gaussian O
distribution O
with O
mean B
and O
covariance B
given O
by O
−1 O
m O
( O
xn O
+1 O
) O
= O
ktc O
n O
t O
σ2 O
( O
xn O
+1 O
) O
= O
c O
− O
ktc O
−1 O
n O
k. O
( O
6.66 O
) O
( O
6.67 O
) O
these O
are O
the O
key O
results O
that O
deﬁne O
gaussian O
process O
regression B
. O
finally O
, O
when O
the O
desired O
number O
of O
base O
classiﬁers O
have O
been O
trained O
, O
they O
are O
combined O
to O
form O
a O
committee B
using O
coefﬁcients O
that O
give O
different O
weight O
to O
different O
base O
classiﬁers O
. O
( O
1.20 O
) O
( O
1.21 O
) O
we O
can O
now O
use O
the O
sum O
and O
product O
rules O
of O
probability B
to O
evaluate O
the O
overall O
probability B
of O
choosing O
an O
apple O
p O
( O
f O
= O
a O
) O
= O
p O
( O
f O
= O
a|b O
= O
r O
) O
p O
( O
b O
= O
r O
) O
+ O
p O
( O
f O
= O
a|b O
= O
b O
) O
p O
( O
b O
= O
b O
) O
= O
= O
11 O
20 O
from O
which O
it O
follows O
, O
using O
the O
sum B
rule I
, O
that O
p O
( O
f O
= O
o O
) O
= O
1 O
− O
11/20 O
= O
9/20 O
. O
also O
, O
even O
a O
large O
data O
set O
will O
contain O
very O
few O
examples O
of O
x-ray O
images O
corre- O
sponding O
to O
cancer O
, O
and O
so O
the O
learning B
algorithm O
will O
not O
be O
exposed O
to O
a O
broad O
range O
of O
examples O
of O
such O
images O
and O
hence O
is O
not O
likely O
to O
generalize O
well O
. O
to O
illustrate O
this O
, O
consider O
a O
multilayer B
perceptron I
network O
having O
two O
layers O
of O
weights O
and O
linear O
output O
units O
, O
which O
performs O
a O
mapping O
from O
a O
set O
of O
input O
variables O
{ O
xi O
} O
to O
a O
set O
of O
output O
variables O
{ O
yk O
} O
. O
8.4.6 O
exact O
inference O
in O
general O
graphs O
8.4.7 O
loopy B
belief I
propagation I
. O
( O
7.85 O
) O
( O
7.86 O
) O
section O
3.5 O
exercise O
7.10 O
our O
goal O
is O
now O
to O
maximize O
( O
7.85 O
) O
with O
respect O
to O
the O
hyperparameters O
α O
and O
β. O
this O
requires O
only O
a O
small O
modiﬁcation O
to O
the O
results O
obtained O
in O
section O
3.5 O
for O
the O
evidence B
approximation I
in O
the O
linear B
regression I
model O
. O
, O
mk O
) O
t. O
as O
for O
the O
case O
of O
the O
binomial B
distribution I
with O
its O
beta O
prior O
, O
we O
can O
interpret O
the O
parameters O
αk O
of O
the O
dirichlet O
prior B
as O
an O
effective B
number I
of I
observations I
of O
xk O
= O
1. O
note O
that O
two-state O
quantities O
can O
either O
be O
represented O
as O
binary O
variables O
and O
lejeune O
dirichlet O
1805–1859 O
johann O
peter O
gustav O
lejeune O
dirichlet O
was O
a O
modest O
and O
re- O
served O
mathematician O
who O
made O
contributions O
in O
number O
theory B
, O
me- O
chanics O
, O
and O
astronomy O
, O
and O
who O
gave O
the O
ﬁrst O
rigorous O
analysis O
of O
fourier O
series O
. O
it O
has O
a O
sigmoidal O
shape O
and O
is O
compared O
with O
the O
logistic B
sigmoid I
function O
in O
figure O
4.9. O
note O
that O
the O
use O
of O
a O
more O
gen- O
eral O
gaussian O
distribution O
does O
not O
change O
the O
model O
because O
this O
is O
equivalent O
to O
a O
re-scaling O
of O
the O
linear O
coefﬁcients O
w. O
many O
numerical O
packages O
provide O
for O
the O
evaluation O
of O
a O
closely O
related O
function O
deﬁned O
by O
erf O
( O
a O
) O
= O
2√ O
π O
exp O
( O
−θ2/2 O
) O
dθ O
( O
4.115 O
) O
( O
cid:6 O
) O
a O
0 O
( O
cid:12 O
) O
( O
cid:13 O
) O
exercise O
4.21 O
and O
known O
as O
the O
erf B
function I
or O
error B
function I
( O
not O
to O
be O
confused O
with O
the O
error B
function I
of O
a O
machine O
learning O
model O
) O
. O
one O
choice O
that O
has O
been O
widely O
used O
is O
that O
of O
radial O
basis O
functions O
, O
which O
have O
the O
property O
that O
each O
basis B
function I
depends O
only O
on O
the O
radial O
distance O
( O
typically O
euclidean O
) O
from O
a O
centre O
µj O
, O
so O
that O
φj O
( O
x O
) O
= O
h O
( O
( O
cid:5 O
) O
x O
− O
µj O
( O
cid:5 O
) O
) O
. O
( O
5.11 O
) O
however O
, O
we O
can O
provide O
a O
much O
more O
general O
view O
of O
network O
training B
by O
ﬁrst O
giving O
a O
probabilistic O
interpretation O
to O
the O
network O
outputs O
. O
the O
idea O
is O
simply O
to O
apply O
the O
sum-product B
algorithm I
even O
though O
there O
is O
no O
guar- O
antee O
that O
it O
will O
yield O
good O
results O
. O
here O
there O
is O
a O
continuum O
of O
parameters O
all O
of O
which O
lead O
to O
the O
same O
predictive O
density O
, O
in O
contrast O
to O
the O
discrete O
nonidentifiability O
associated O
with O
component O
re-labelling O
in O
the O
mixture B
setting O
. O
here O
we O
introduce O
a O
simple O
, O
but O
widely O
used O
, O
framework O
called O
the O
laplace O
ap- O
proximation B
, O
that O
aims O
to O
ﬁnd O
a O
gaussian O
approximation O
to O
a O
probability B
density O
deﬁned O
over O
a O
set O
of O
continuous O
variables O
. O
2.39 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
starting O
from O
the O
results O
( O
2.141 O
) O
and O
( O
2.142 O
) O
for O
the O
posterior O
distribution O
of O
the O
mean B
of O
a O
gaussian O
random O
variable O
, O
dissect O
out O
the O
contributions O
from O
the O
ﬁrst O
n O
− O
1 O
data O
points O
and O
hence O
obtain O
expressions O
for O
the O
sequential O
update O
of O
µn O
and O
σ2 O
n O
. O
14.15 O
( O
( O
cid:12 O
) O
) O
www O
we O
have O
already O
noted O
that O
if O
we O
use O
a O
squared O
loss B
function I
in O
a O
regres- O
sion B
problem O
, O
the O
corresponding O
optimal O
prediction O
of O
the O
target O
variable O
for O
a O
new O
input O
vector O
is O
given O
by O
the O
conditional B
mean O
of O
the O
predictive B
distribution I
. O
provided O
we O
consider O
a O
sufﬁciently O
ﬂexible O
network O
, O
we O
then O
have O
a O
framework O
for O
approximating O
arbitrary O
conditional B
distri- O
butions O
. O
b. O
probability B
distributions O
687 O
dirichlet O
the O
dirichlet O
is O
a O
multivariate O
distribution O
over O
k O
random O
variables O
0 O
( O
cid:1 O
) O
µk O
( O
cid:1 O
) O
1 O
, O
where O
k O
= O
1 O
, O
. O
( O
3.97 O
) O
we O
shall O
see O
in O
section O
10.1.3 O
that O
this O
result O
can O
be O
obtained O
from O
a O
bayesian O
treat- O
ment O
in O
which O
we O
marginalize O
over O
the O
unknown O
mean B
. O
similarly O
, O
maximizing O
( O
1.54 O
) O
with O
respect O
to O
σ2 O
, O
we O
obtain O
the O
maximum B
likelihood I
solution O
for O
the O
variance B
in O
the O
form O
xn O
n=1 O
σ2 O
ml O
= O
1 O
n O
( O
xn O
− O
µml O
) O
2 O
( O
1.56 O
) O
n O
( O
cid:2 O
) O
1 O
n O
n O
( O
cid:2 O
) O
n=1 O
which O
is O
the O
sample B
variance I
measured O
with O
respect O
to O
the O
sample B
mean I
µml O
. O
6.24 O
( O
( O
cid:12 O
) O
) O
show O
that O
a O
diagonal B
matrix O
w O
whose O
elements O
satisfy O
0 O
< O
wii O
< O
1 O
is O
positive B
deﬁnite I
. O
6.3.1 O
nadaraya-watson O
model O
in O
section O
3.3.3 O
, O
we O
saw O
that O
the O
prediction O
of O
a O
linear B
regression I
model O
for O
a O
new O
input O
x O
takes O
the O
form O
of O
a O
linear O
combination O
of O
the O
training B
set I
target O
values O
with O
coefﬁcients O
given O
by O
the O
‘ O
equivalent B
kernel I
’ O
( O
3.62 O
) O
where O
the O
equivalent B
kernel I
satisﬁes O
the O
summation O
constraint O
( O
3.64 O
) O
. O
if O
this O
requirement O
is O
not O
satisﬁed O
, O
so O
that O
some O
of O
the O
conditional B
distributions O
have O
zeros O
, O
then O
ergodicity O
, O
if O
it O
applies O
, O
must O
be O
proven O
explicitly O
. O
for O
instance O
, O
independent B
factor I
analysis I
( O
attias O
, O
1999a O
) O
considers O
a O
model O
in O
which O
the O
number O
of O
latent O
and O
observed O
variables O
can O
differ O
, O
the O
observed O
variables O
are O
noisy O
, O
and O
the O
individual O
latent O
variables O
have O
flex O
( O
cid:173 O
) O
ible O
distributions O
modelled O
by O
mixtures O
of O
gaussians O
. O
, O
n O
, O
as O
illustrated O
in O
figure O
2.17. O
we O
can O
average O
the O
vectors O
{ O
xn O
} O
and O
the O
standard B
deviation I
will O
be O
1◦ O
106 O
2. O
probability B
distributions O
figure O
2.17 O
illustration O
of O
the O
representation O
of O
val- O
ues O
θn O
of O
a O
periodic B
variable I
as O
two- O
dimensional O
vectors O
xn O
living O
on O
the O
unit O
circle O
. O
here O
we O
consider O
the O
application O
of O
the O
laplace O
approximation O
to O
the O
problem O
of O
bayesian O
logistic B
regression I
( O
spiegelhalter O
and O
lauritzen O
, O
1990 O
; O
mackay O
, O
1992b O
) O
. O
1.4 O
1.5 O
decision B
theory I
. O
to O
begin O
with O
, O
we O
shall O
deﬁne O
the O
probability B
of O
an O
event O
to O
be O
the O
fraction O
of O
times O
that O
event O
occurs O
out O
of O
the O
total O
number O
of O
trials O
, O
in O
the O
limit O
that O
the O
total O
number O
of O
trials O
goes O
to O
inﬁnity O
. O
it O
should O
be O
emphasized O
that O
the O
problem O
of O
density B
estimation I
is O
fun- O
67 O
68 O
2. O
probability B
distributions O
damentally O
ill-posed O
, O
because O
there O
are O
inﬁnitely O
many O
probability B
distributions O
that O
could O
have O
given O
rise O
to O
the O
observed O
ﬁnite O
data O
set O
. O
the O
impact O
of O
the O
regularization B
term O
on O
the O
generalization B
error O
can O
be O
seen O
by O
plotting O
the O
value O
of O
the O
rms O
error B
( O
1.3 O
) O
for O
both O
training B
and O
test O
sets O
against O
ln O
λ O
, O
as O
shown O
in O
figure O
1.8. O
we O
see O
that O
in O
effect O
λ O
now O
controls O
the O
effective O
complexity O
of O
the O
model O
and O
hence O
determines O
the O
degree O
of O
over-ﬁtting B
. O
these O
coefficients O
are O
arranged O
to O
be O
invariant O
to O
rota O
( O
cid:173 O
) O
tions O
, O
translations O
, O
and O
scalings O
of O
that O
data O
point O
and O
its O
neighbours O
, O
and O
hence O
they O
characterize O
the O
local B
geometrical O
properties O
of O
the O
neighbourhood O
. O
a O
further O
issue O
in O
ﬁnding O
maximum B
likelihood I
solutions O
arises O
from O
the O
fact O
that O
for O
any O
given O
maximum B
likelihood I
solution O
, O
a O
k-component O
mixture B
will O
have O
a O
total O
of O
k O
! O
equivalent O
solutions O
corresponding O
to O
the O
k O
! O
ways O
of O
assigning O
k O
sets O
of O
parameters O
to O
k O
components O
. O
in O
principle O
, O
a O
network O
with O
sigmoidal O
hidden O
units O
can O
always O
mimic O
skip O
layer O
con- O
nections O
( O
for O
bounded O
input O
values O
) O
by O
using O
a O
sufﬁciently O
small O
ﬁrst-layer O
weight O
that O
, O
over O
its O
operating O
range O
, O
the O
hidden B
unit I
is O
effectively O
linear O
, O
and O
then O
com- O
pensating O
with O
a O
large O
weight O
value O
from O
the O
hidden B
unit I
to O
the O
output O
. O
as O
a O
result O
of O
the O
optimization O
, O
we O
ﬁnd O
that O
a O
proportion O
of O
the O
hyperparameters O
{ O
αi O
} O
are O
driven O
to O
large O
( O
in O
principle O
inﬁnite O
) O
values O
, O
and O
so O
the O
weight O
parameters O
wi O
corresponding O
to O
these O
hyperparameters O
have O
posterior O
distributions O
with O
mean B
and O
variance B
both O
zero O
. O
assumed B
density I
ﬁltering I
can O
be O
appropriate O
for O
on-line O
learning B
in O
which O
data O
points O
are O
arriving O
in O
a O
sequence O
and O
we O
need O
to O
learn O
from O
each O
data O
point O
and O
then O
discard O
it O
before O
considering O
the O
next O
point O
. O
by O
contrast O
, O
at O
the O
microscopic O
level O
the O
classical B
newtonian O
equa- O
tions O
of O
physics O
are O
reversible O
, O
and O
so O
they O
found O
it O
difﬁcult O
to O
see O
how O
the O
latter O
could O
explain O
the O
for- O
mer O
. O
8.14 O
( O
( O
cid:12 O
) O
) O
consider O
a O
particular O
case O
of O
the O
energy B
function I
given O
by O
( O
8.42 O
) O
in O
which O
the O
coefﬁcients O
β O
= O
h O
= O
0. O
show O
that O
the O
most O
probable O
conﬁguration O
of O
the O
latent O
variables O
is O
given O
by O
xi O
= O
yi O
for O
all O
i O
. O
in O
the O
case O
of O
undirected B
graphs O
, O
it O
is O
convenient O
to O
begin O
with O
a O
discussion O
of O
conditional B
independence I
properties O
. O
( O
7.94 O
) O
( O
7.95 O
) O
using O
these O
results O
, O
we O
can O
then O
write O
the O
log O
marginal O
likelihood B
function I
( O
7.85 O
) O
in O
the O
form O
( O
7.96 O
) O
where O
l O
( O
α−i O
) O
is O
simply O
the O
log O
marginal O
likelihood O
with O
basis B
function I
ϕi O
omitted O
, O
and O
the O
quantity O
λ O
( O
αi O
) O
is O
deﬁned O
by O
l O
( O
α O
) O
= O
l O
( O
α−i O
) O
+ O
λ O
( O
αi O
) O
( O
cid:29 O
) O
( O
cid:30 O
) O
λ O
( O
αi O
) O
= O
1 O
2 O
ln O
αi O
− O
ln O
( O
αi O
+ O
si O
) O
+ O
q2 O
i O
αi O
+ O
si O
( O
7.97 O
) O
and O
contains O
all O
of O
the O
dependence O
on O
αi O
. O
( O
2.64 O
) O
because O
the O
parameter O
matrix O
σ O
governs O
the O
covariance B
of O
x O
under O
the O
gaussian O
distribution O
, O
it O
is O
called O
the O
covariance B
matrix I
. O
this O
em O
procedure O
can O
also O
be O
extended B
to O
the O
factor B
analysis I
model O
, O
for O
which O
there O
is O
no O
closed-form O
solution O
. O
in O
practice O
, O
improper B
priors O
can O
often O
be O
used O
provided O
the O
corresponding O
posterior O
distribution O
is O
proper O
, O
i.e. O
, O
that O
it O
can O
be O
correctly O
normalized O
. O
\n O
) O
−1 O
( O
mnew O
− O
m\n O
) O
( O
10.220 O
) O
( O
10.221 O
) O
( O
10.222 O
) O
this O
reﬁnement O
process O
is O
repeated O
until O
a O
suitable O
termination O
criterion O
is O
satisﬁed O
, O
for O
instance O
that O
the O
maximum O
change O
in O
parameter O
values O
resulting O
from O
a O
complete O
10.7. O
expectation B
propagation I
513 O
−5 O
0 O
5 O
θ O
10 O
−5 O
0 O
5 O
θ O
10 O
figure O
10.16 O
examples O
of O
the O
approximation O
of O
speciﬁc O
factors O
for O
a O
one-dimensional O
version O
of O
the O
clutter B
problem I
, O
showing O
fn O
( O
θ O
) O
in O
blue O
, O
efn O
( O
θ O
) O
in O
red O
, O
and O
q\n O
( O
θ O
) O
in O
green O
. O
( O
7.112 O
) O
( O
7.113 O
) O
we O
can O
now O
use O
this O
laplace O
approximation O
to O
evaluate O
the O
marginal B
likelihood I
. O
for B
regression I
problems O
, O
the O
target O
variable O
t O
was O
simply O
the O
vector O
of O
real O
num- O
bers O
whose O
values O
we O
wish O
to O
predict O
. O
this O
is O
a O
general O
result O
and O
is O
a O
consequence O
of O
the O
choice O
of O
conjugate B
distributions O
. O
thus O
the O
path O
is O
not O
blocked O
and O
so O
the O
conditional B
independence I
property O
( O
13.5 O
) O
does O
not O
hold O
for O
the O
individual O
la- O
tent O
chains O
of O
the O
factorial B
hmm O
model O
. O
thus O
the O
covariance B
matrix I
cn O
+1 O
has O
elements O
given O
by O
c O
( O
xn O
, O
xm O
) O
= O
k O
( O
xn O
, O
xm O
) O
+ O
νδnm O
( O
6.75 O
) O
where O
k O
( O
xn O
, O
xm O
) O
is O
any O
positive O
semideﬁnite O
kernel O
function O
of O
the O
kind O
considered O
in O
section O
6.2 O
, O
and O
the O
value O
of O
ν O
is O
typically O
ﬁxed O
in O
advance O
. O
most O
techniques O
involve O
choosing O
some O
initial O
value O
w O
( O
0 O
) O
for O
the O
weight B
vector I
and O
then O
moving O
through O
weight O
space O
in O
a O
succession O
of O
steps O
of O
the O
form O
w O
( O
τ O
+1 O
) O
= O
w O
( O
τ O
) O
+ O
∆w O
( O
τ O
) O
( O
5.27 O
) O
where O
τ O
labels O
the O
iteration O
step O
. O
this O
technique O
can O
usefully O
be O
extended B
by O
incorporating O
a O
separate O
parameter O
for O
each O
input O
variable O
( O
rasmussen O
and O
williams O
, O
2006 O
) O
. O
6.4.6 O
laplace O
approximation O
the O
third O
approach O
to O
gaussian O
process O
classiﬁcation B
is O
based O
on O
the O
laplace O
approximation O
, O
which O
we O
now O
consider O
in O
detail O
. O
we O
shall O
see O
in O
later O
chapters O
how O
to O
construct O
models O
for O
combining O
data O
that O
do O
not O
require O
the O
conditional B
independence I
assumption O
( O
1.84 O
) O
. O
for O
230 O
5. O
neural O
networks O
figure O
5.2 O
example O
of O
a O
neural B
network I
having O
a O
general O
feed-forward O
topology O
. O
if O
we O
substitute O
the O
solution O
for O
w O
into O
the O
expression O
for O
c O
, O
and O
make O
use O
of O
the O
orthogonality O
property O
rrt O
= O
i O
, O
we O
see O
that O
c O
is O
independent B
of O
r. O
this O
simply O
says O
that O
the O
predictive O
density O
is O
unchanged O
by O
rotations O
in O
the O
latent O
space O
as O
discussed O
earlier O
. O
only O
pending O
messages O
need O
to O
be O
transmitted O
because O
418 O
8. O
graphical O
models O
exercise O
8.29 O
other O
messages O
would O
simply O
duplicate O
the O
previous O
message O
on O
the O
same O
link B
. O
if O
, O
however O
, O
we O
use O
too O
large O
a O
value O
for O
λ O
then O
we O
again O
obtain O
a O
poor O
ﬁt O
, O
as O
shown O
in O
figure O
1.7 O
for O
ln O
λ O
= O
0. O
the O
corresponding O
coefﬁcients O
from O
the O
ﬁtted O
polynomials O
are O
given O
in O
table O
1.2 O
, O
showing O
that O
regularization B
has O
the O
desired O
effect O
of O
reducing O
exercise O
1.2 O
1.1. O
example O
: O
polynomial O
curve O
fitting O
11 O
table O
1.2 O
table O
of O
the O
coefﬁcients O
w O
( O
cid:1 O
) O
for O
m O
= O
9 O
polynomials O
with O
various O
values O
for O
the O
regularization B
parameter O
λ. O
note O
that O
ln O
λ O
= O
−∞ O
corresponds O
to O
a O
model O
with O
no O
regularization B
, O
i.e. O
, O
to O
the O
graph O
at O
the O
bottom O
right O
in O
fig- O
ure O
1.4. O
we O
see O
that O
, O
as O
the O
value O
of O
λ O
increases O
, O
the O
typical O
magnitude O
of O
the O
coefﬁcients O
gets O
smaller O
. O
hint O
: O
refer O
to O
sections O
2.1 O
and O
2.2 O
for O
a O
discussion O
of O
the O
corresponding O
maximum B
likelihood I
solutions O
for O
i.i.d O
. O
we O
can O
deﬁne O
the O
total O
within-class B
variance O
for O
the O
whole O
data O
set O
to O
be O
simply O
s2 O
2. O
the O
fisher O
criterion O
is O
deﬁned O
to O
be O
the O
ratio O
of O
the O
between-class B
variance O
to O
the O
within-class B
variance O
and O
is O
given O
by O
1 O
+ O
s2 O
j O
( O
w O
) O
= O
( O
m2 O
− O
m1 O
) O
2 O
1 O
+ O
s2 O
s2 O
2 O
. O
thus O
, O
the O
ﬁrst O
stage O
, O
namely O
the O
propagation O
of O
er- O
rors O
backwards O
through O
the O
network O
in O
order O
to O
evaluate O
derivatives O
, O
can O
be O
applied O
to O
many O
other O
kinds O
of O
network O
and O
not O
just O
the O
multilayer B
perceptron I
. O
note O
that O
the O
error B
function I
is O
deﬁned O
with O
respect O
to O
a O
training B
set I
, O
and O
so O
each O
step O
requires O
that O
the O
entire O
training B
set I
be O
processed O
in O
order O
to O
evaluate O
∇e O
. O
x1 O
xm O
y O
370 O
8. O
graphical O
models O
8.1.4 O
linear-gaussian O
models O
in O
the O
previous O
section O
, O
we O
saw O
how O
to O
construct O
joint O
probability B
distributions O
over O
a O
set O
of O
discrete O
variables O
by O
expressing O
the O
variables O
as O
nodes O
in O
a O
directed B
acyclic I
graph I
. O
thus O
a O
data O
point O
that O
is O
on O
the O
decision B
boundary I
y O
( O
xn O
) O
= O
0 O
will O
have O
ξn O
= O
1 O
, O
and O
points O
332 O
7. O
sparse O
kernel O
machines O
figure O
7.3 O
illustration O
of O
the O
slack O
variables O
ξn O
( O
cid:2 O
) O
0. O
data O
points O
with O
circles O
around O
them O
are O
support O
vectors O
. O
in O
practice O
, O
the O
histogram O
technique O
can O
be O
useful O
for O
obtaining O
a O
quick O
visual- O
ization O
of O
data O
in O
one O
or O
two O
dimensions O
but O
is O
unsuited O
to O
most O
density B
estimation I
applications O
. O
this O
requires O
the O
evaluation O
of O
derivatives O
of O
the O
log O
likelihood O
function O
with O
respect O
to O
the O
net- O
work O
parameters O
, O
and O
we O
shall O
see O
how O
these O
can O
be O
obtained O
efﬁciently O
using O
the O
technique O
of O
error B
backpropagation I
. O
exercise O
2.19 O
the O
covariance B
matrix I
σ O
can O
be O
expressed O
as O
an O
expansion O
in O
terms O
of O
its O
eigenvec- O
tors O
in O
the O
form O
( O
2.46 O
) O
( O
2.47 O
) O
( O
2.48 O
) O
( O
2.49 O
) O
( O
2.50 O
) O
d O
( O
cid:2 O
) O
i=1 O
d O
( O
cid:2 O
) O
i=1 O
σ O
= O
λiuiut O
i O
−1 O
= O
σ O
1 O
λi O
uiut O
i O
. O
exercises O
603 O
12.29 O
( O
** O
) O
em O
suppose O
that O
two O
variables O
zl O
and O
z2 O
are O
independent B
so O
thatp O
( O
zl O
' O
z2 O
) O
= O
p O
( O
zl O
) O
p O
( O
z2 O
) O
' O
show O
that O
the O
covariance B
matrix I
between O
these O
variables O
is O
diagonal B
. O
by O
a O
similar O
argument O
to O
that O
used O
above O
, O
it O
is O
easy O
to O
see O
that O
the O
marginal B
associated O
with O
a O
factor O
is O
given O
by O
the O
product O
of O
messages O
arriving O
at O
the O
factor O
node O
and O
the O
local B
factor O
at O
that O
node B
p O
( O
xs O
) O
= O
fs O
( O
xs O
) O
µxi→fs O
( O
xi O
) O
( O
8.72 O
) O
( O
cid:14 O
) O
i∈ne O
( O
fs O
) O
in O
complete O
analogy O
with O
the O
marginals O
at O
the O
variable O
nodes O
. O
170 O
3. O
linear O
models O
for B
regression I
figure O
3.15 O
contours O
of O
the O
likelihood B
function I
( O
red O
) O
and O
the O
prior B
( O
green O
) O
in O
which O
the O
axes O
in O
parameter O
space O
have O
been O
rotated O
to O
align O
with O
the O
eigenvectors O
ui O
of O
the O
hessian O
. O
we O
now O
determine O
the O
parameter O
matrix O
, O
w O
by O
minimizing O
a O
sum-of-squares O
together O
with O
a O
matrix O
( O
cid:4 O
) O
x O
whose O
nth O
row O
is O
( O
cid:4 O
) O
xt O
( O
cid:19 O
) O
( O
cid:20 O
) O
( O
( O
cid:4 O
) O
x O
, O
w O
− O
t O
) O
t O
( O
( O
cid:4 O
) O
x O
, O
w O
− O
t O
) O
setting O
the O
derivative B
with O
respect O
to O
, O
w O
to O
zero O
, O
and O
rearranging O
, O
we O
then O
obtain O
the O
solution O
for O
, O
w O
in O
the O
form O
, O
w O
= O
( O
( O
cid:4 O
) O
xt O
( O
cid:4 O
) O
x O
) O
−1 O
( O
cid:4 O
) O
xtt O
= O
( O
cid:4 O
) O
x†t O
is O
the O
pseudo-inverse B
of O
the O
matrix O
( O
cid:4 O
) O
x O
, O
as O
discussed O
in O
section O
3.1.1. O
we O
where O
( O
cid:4 O
) O
x† O
( O
cid:18 O
) O
t O
( O
cid:4 O
) O
x O
. O
the O
particular O
case O
of O
a O
quadratic O
regularizer O
is O
called O
ridge O
regres- O
sion B
( O
hoerl O
and O
kennard O
, O
1970 O
) O
. O
( O
4.112 O
) O
4.3. O
probabilistic O
discriminative O
models O
211 O
figure O
4.13 O
schematic O
example O
of O
a O
probability B
density O
p O
( O
θ O
) O
shown O
by O
the O
blue O
curve O
, O
given O
in O
this O
example O
by O
a O
mixture O
of O
two O
gaussians O
, O
along O
with O
its O
cumulative B
distribution I
function I
f O
( O
a O
) O
, O
shown O
by O
the O
red O
curve O
. O
if O
we O
present O
all O
possible O
distributions O
p O
( O
x O
) O
to O
this O
second O
kind O
of O
ﬁlter O
, O
then O
the O
d-separation B
theorem O
tells O
us O
that O
the O
set O
of O
distributions O
that O
will O
be O
allowed O
through O
is O
precisely O
the O
set O
df O
. O
we O
shall O
begin O
by O
dis- O
cussing O
bayesian O
networks O
, O
also O
known O
as O
directed B
graphical O
models O
, O
in O
which O
the O
links O
of O
the O
graphs O
have O
a O
particular O
directionality O
indicated O
by O
arrows O
. O
the O
chain B
graph I
markov O
property O
. O
12,1.1 O
mllximllm O
variance B
lormulation O
con O
, O
ider O
a O
dala O
set O
< O
if O
obser O
'' O
\lations O
{ O
x O
, O
, O
} O
where O
'' O
= O
1 O
... O
.. O
s O
, O
and O
x O
'' O
i O
, O
a O
euclidean O
variable O
`` O
'ilh O
dimen O
, O
ionality O
d. O
our O
goal O
is O
to O
project O
if O
> O
/ O
: O
: O
data O
onto O
a O
'pace O
ha O
'' O
ing O
dimen O
, O
ionality O
m O
< O
d O
'' O
hile O
ill3jli O
'' O
, O
i O
, O
illg O
the O
`` O
ariallce O
of O
the O
projttted O
data O
. O
once O
a O
mixture B
density I
network I
has O
been O
trained O
, O
it O
can O
predict O
the O
conditional B
density O
function O
of O
the O
target O
data O
for O
any O
given O
value O
of O
the O
input O
vector O
. O
the O
model O
then O
emits O
t O
values O
of O
the O
observed B
variable I
xt O
, O
which O
are O
generally O
assumed O
to O
be O
independent B
so O
that O
the O
corresponding O
emis- O
t=1 O
p O
( O
xt|k O
) O
. O
values O
of O
m O
in O
the O
range O
3 O
( O
cid:1 O
) O
m O
( O
cid:1 O
) O
8 O
give O
small O
values O
for O
the O
test B
set I
error O
, O
and O
these O
also O
give O
reasonable O
representations O
of O
the O
generating O
function O
sin O
( O
2πx O
) O
, O
as O
can O
be O
seen O
, O
for O
the O
case O
of O
m O
= O
3 O
, O
from O
figure O
1.4 O
. O
suppose O
we O
now O
observe O
the O
variable O
d. O
show O
that O
in O
general O
a O
( O
cid:9 O
) O
⊥⊥ O
b O
| O
d. O
8.11 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
example O
of O
the O
car O
fuel B
system I
shown O
in O
figure O
8.21 O
, O
and O
suppose O
that O
instead O
of O
observing O
the O
state O
of O
the O
fuel O
gauge O
g O
directly O
, O
the O
gauge O
is O
seen O
by O
the O
driver O
d O
who O
reports O
to O
us O
the O
reading O
on O
the O
gauge O
. O
this O
is O
similar O
to O
the O
evaluation O
of O
the O
marginal B
for O
a O
single O
node B
, O
except O
that O
there O
are O
now O
two O
variables O
that O
are O
not O
summed O
out O
. O
we O
therefore O
see O
that O
the O
bias B
parameter I
w0 O
determines O
the O
location O
of O
the O
decision B
surface I
. O
graphical O
models O
for O
ma- O
chine O
learning B
and O
digital O
communication O
. O
noting O
that O
z O
and O
r O
are O
independent B
in O
the O
distribution O
p O
( O
z O
, O
r O
) O
, O
we O
see O
that O
the O
conditional B
distribution O
p O
( O
r|z O
) O
is O
a O
gaussian O
from O
which O
it O
is O
straightforward O
to O
sample O
. O
the O
histogram O
approach O
to O
density B
estimation I
does O
, O
however O
, O
teach O
us O
two O
im- O
portant O
lessons O
. O
recall O
from O
section O
1.5.5 O
that O
the O
optimal O
function O
that O
minimizes O
a O
sum-of-squares O
loss O
is O
the O
conditional B
average O
of O
the O
target O
data O
. O
for O
n O
→ O
∞ O
, O
the O
posterior O
mean O
is O
given O
by O
the O
maximum B
likelihood I
solution O
. O
however O
, O
in O
many O
machine O
learning O
problems O
the O
presence O
of O
multimodality B
, O
particularly O
in O
problems O
involving O
spaces O
of O
high O
di- O
mensionality O
, O
can O
be O
less O
obvious O
. O
we O
can O
see O
immediately O
that O
the O
variational B
posterior O
distribution O
over O
the O
parameters O
must O
factorize O
between O
π O
and O
the O
remaining O
param- O
eters O
µ O
and O
λ O
because O
all O
paths O
connecting O
π O
to O
either O
µ O
or O
λ O
must O
pass O
through O
one O
of O
the O
nodes O
zn O
all O
of O
which O
are O
in O
the O
conditioning O
set O
for O
our O
conditional B
inde- O
pendence O
test O
and O
all O
of O
which O
are O
head-to-tail O
with O
respect O
to O
such O
paths O
. O
the O
partition B
function I
is O
needed O
for O
parameter O
learning B
because O
it O
will O
be O
a O
function O
of O
any O
parameters O
that O
govern O
the O
potential O
functions O
ψc O
( O
xc O
) O
. O
( O
1.98 O
) O
( O
cid:2 O
) O
i O
appendix O
e O
distributions O
p O
( O
xi O
) O
that O
are O
sharply O
peaked O
around O
a O
few O
values O
will O
have O
a O
relatively O
low O
entropy B
, O
whereas O
those O
that O
are O
spread O
more O
evenly O
across O
many O
values O
will O
have O
higher O
entropy B
, O
as O
illustrated O
in O
figure O
1.30. O
because O
0 O
( O
cid:1 O
) O
pi O
( O
cid:1 O
) O
1 O
, O
the O
entropy B
is O
nonnegative O
, O
and O
it O
will O
equal O
its O
minimum O
value O
of O
0 O
when O
one O
of O
the O
pi O
= O
1 O
and O
all O
other O
pj O
( O
cid:4 O
) O
=i O
= O
0. O
the O
maximum O
entropy O
conﬁguration O
can O
be O
found O
by O
maximizing O
h O
using O
a O
lagrange O
multiplier O
to O
enforce O
the O
normalization O
constraint O
on O
the O
probabilities O
. O
11.1.5 O
sampling-importance-resampling B
the O
rejection B
sampling I
method O
discussed O
in O
section O
11.1.2 O
depends O
in O
part O
for O
its O
success O
on O
the O
determination O
of O
a O
suitable O
value O
for O
the O
constant O
k. O
for O
many O
pairs O
of O
distributions O
p O
( O
z O
) O
and O
q O
( O
z O
) O
, O
it O
will O
be O
impractical O
to O
determine O
a O
suitable O
11.1. O
basic O
sampling O
algorithms O
535 O
value O
for O
k O
in O
that O
any O
value O
that O
is O
sufﬁciently O
large O
to O
guarantee O
a O
bound O
on O
the O
desired O
distribution O
will O
lead O
to O
impractically O
small O
acceptance O
rates O
. O
if O
we O
substitute O
the O
factor- O
ized O
expression O
( O
8.49 O
) O
for O
the O
joint O
distribution O
into O
( O
8.50 O
) O
, O
then O
we O
can O
rearrange O
the O
order O
of O
the O
summations O
and O
the O
multiplications O
to O
allow O
the O
required O
marginal B
to O
be O
evaluated O
much O
more O
efﬁciently O
. O
clearly O
, O
one O
possible O
solution O
is O
that O
the O
columns O
of O
u O
are O
eigenvectors O
of O
s O
, O
in O
which O
case O
h O
is O
a O
diagonal B
matrix O
containing O
the O
corresponding O
eigenvalues O
. O
note O
that O
the O
observed O
variables O
in O
an O
hmm O
may O
be O
discrete O
or O
continuous O
, O
and O
a O
variety O
of O
different O
conditional B
distributions O
can O
be O
used O
to O
model O
them O
. O
13.3.2 O
learning B
in O
lds O
. O
the O
location O
of O
the O
minimum O
of O
this O
quadratic O
approximation O
therefore O
depends O
on O
o O
( O
w O
2 O
) O
parameters O
, O
and O
we O
should O
not O
expect O
to O
be O
able O
to O
locate O
the O
minimum O
until O
we O
have O
gathered O
o O
( O
w O
2 O
) O
independent B
pieces O
of O
information O
. O
the O
convolution O
of O
a O
gaussian O
with O
a O
logistic B
sigmoid I
is O
intractable O
. O
2.34 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
to O
ﬁnd O
the O
maximum B
likelihood I
solution O
for O
the O
covariance B
matrix I
of O
a O
multivariate O
gaussian O
, O
we O
need O
to O
maximize O
the O
log O
likelihood O
function O
( O
2.118 O
) O
with O
respect O
to O
σ O
, O
noting O
that O
the O
covariance B
matrix I
must O
be O
symmetric O
and O
positive B
deﬁnite I
. O
in O
this O
way O
, O
the O
response O
of O
a O
unit O
in O
the O
subsampling B
layer O
will O
be O
relatively O
insensitive O
to O
small O
shifts O
of O
the O
image O
in O
the O
corresponding O
regions O
of O
the O
input O
space O
. O
( O
10.36 O
) O
however O
, O
if O
we O
maximize O
lm O
with O
respect O
to O
the O
q O
( O
z|m O
) O
, O
we O
ﬁnd O
that O
the O
solutions O
for O
different O
m O
are O
coupled O
, O
as O
we O
expect O
because O
they O
are O
conditioned O
on O
m. O
we O
proceed O
instead O
by O
ﬁrst O
optimizing O
each O
of O
the O
q O
( O
z|m O
) O
individually O
by O
optimization O
exercise O
10.10 O
exercise O
10.11 O
474 O
10. O
approximate O
inference B
of O
( O
10.35 O
) O
, O
and O
then O
subsequently O
determining O
the O
q O
( O
m O
) O
using O
( O
10.36 O
) O
. O
( O
11.38 O
) O
z O
( O
m O
) O
a O
distribution O
is O
said O
to O
be O
invariant O
, O
or O
stationary B
, O
with O
respect O
to O
a O
markov O
chain O
if O
each O
step O
in O
the O
chain O
leaves O
that O
distribution O
invariant O
. O
this O
is O
equivalent O
to O
the O
minimum O
misclassiﬁcation O
rate O
decision O
rule O
, O
which O
assigns O
each O
value O
of O
x O
to O
the O
class O
having O
the O
higher O
posterior B
probability I
p O
( O
ck|x O
) O
. O
( O
10.146 O
) O
( O
cid:6 O
) O
i O
( O
cid:2 O
) O
we O
now O
have O
the O
freedom O
to O
choose O
the O
variational B
parameter O
ξ O
, O
which O
we O
do O
by O
ﬁnding O
the O
value O
ξ O
( O
cid:1 O
) O
that O
maximizes O
the O
function O
f O
( O
ξ O
) O
. O
bayesian O
computation O
and O
stochastic B
systems O
. O
from O
( O
13.57 O
) O
, O
we O
see O
that O
the O
likelihood B
function I
can O
be O
found O
using O
n O
( O
cid:14 O
) O
p O
( O
x O
) O
= O
cn O
. O
thus O
the O
average O
additional O
information O
needed O
to O
specify O
y O
can O
be O
written O
as O
( O
cid:6 O
) O
( O
cid:6 O
) O
h O
[ O
y|x O
] O
= O
− O
p O
( O
y O
, O
x O
) O
ln O
p O
( O
y|x O
) O
dy O
dx O
( O
1.111 O
) O
1.6. O
information B
theory I
55 O
exercise O
1.37 O
which O
is O
called O
the O
conditional B
entropy I
of O
y O
given O
x. O
it O
is O
easily O
seen O
, O
using O
the O
product B
rule I
, O
that O
the O
conditional B
entropy I
satisﬁes O
the O
relation O
h O
[ O
x O
, O
y O
] O
= O
h O
[ O
y|x O
] O
+ O
h O
[ O
x O
] O
( O
1.112 O
) O
where O
h O
[ O
x O
, O
y O
] O
is O
the O
differential B
entropy I
of O
p O
( O
x O
, O
y O
) O
and O
h O
[ O
x O
] O
is O
the O
differential B
en- O
tropy O
of O
the O
marginal B
distribution O
p O
( O
x O
) O
. O
the O
boundary O
is O
chosen O
to O
represent O
a O
quantile O
of O
the O
density B
, O
that O
is O
, O
the O
probability B
that O
a O
data O
point O
drawn O
from O
the O
distribution O
will O
land O
inside O
that O
region O
is O
given O
by O
a O
ﬁxed O
number O
between O
0 O
and O
1 O
that O
is O
speciﬁed O
in O
advance O
. O
this O
is O
the O
decision O
step O
, O
and O
it O
is O
the O
subject O
of O
decision B
theory I
to O
tell O
us O
how O
to O
make O
optimal O
decisions O
given O
the O
appropriate O
probabilities O
. O
generative O
models O
can O
deal O
naturally O
with O
missing B
data I
and O
in O
the O
case O
of O
hidden O
markov O
models O
can O
handle O
sequences O
of O
varying O
length O
. O
the O
reader O
should O
take O
a O
moment O
to O
verify O
that O
this O
decomposition O
does O
indeed O
correspond O
to O
the O
probabilistic B
graphical I
model I
shown O
in O
figure O
10.5. O
note O
that O
only O
the O
variables O
x O
= O
{ O
x1 O
, O
. O
1.5.5 O
loss O
functions O
for B
regression I
so O
far O
, O
we O
have O
discussed O
decision B
theory I
in O
the O
context O
of O
classiﬁcation B
prob- O
lems O
. O
this O
is O
a O
more O
restricted O
problem O
than O
estimating O
the O
full O
density B
but O
may O
be O
sufﬁcient O
in O
speciﬁc O
applications O
. O
the O
price O
to O
be O
paid O
for O
this O
compactness O
, O
as O
with O
the O
relevance B
vector I
machine I
, O
is O
that O
the O
like- O
lihood O
function O
, O
which O
forms O
the O
basis O
for O
network O
training B
, O
is O
no O
longer O
a O
convex B
function I
of O
the O
model O
parameters O
. O
in O
the O
stationary B
case O
, O
the O
data O
evolves O
in O
time O
, O
but O
the O
distribution O
from O
which O
it O
is O
generated O
remains O
the O
same O
. O
note O
that O
the O
max-sum B
algorithm I
works O
with O
log O
probabilities O
and O
so O
there O
is O
no O
need O
to O
use O
re-scaled O
variables O
as O
was O
done O
with O
the O
forward-backward B
algorithm I
. O
however O
, O
there O
is O
a O
class O
of O
pattern O
recognition O
techniques O
, O
in O
which O
the O
training B
data O
points O
, O
or O
a O
subset O
of O
them O
, O
are O
kept O
and O
used O
also O
during O
the O
prediction O
phase O
. O
the O
stationary B
points O
of O
the O
marginal B
likelihood I
with O
respect O
to O
αi O
occur O
when O
the O
derivative B
dλ O
( O
αi O
) O
= O
α O
−1 O
i O
s2 O
i O
− O
( O
q2 O
2 O
( O
αi O
+ O
si O
) O
2 O
i O
− O
si O
) O
( O
7.100 O
) O
is O
equal O
to O
zero O
. O
2.56 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
express O
the O
beta B
distribution I
( O
2.13 O
) O
, O
the O
gamma B
distribution I
( O
2.146 O
) O
, O
and O
the O
von O
mises O
distribution O
( O
2.179 O
) O
as O
members O
of O
the O
exponential B
family I
( O
2.194 O
) O
and O
thereby O
identify O
their O
natural B
parameters I
. O
( O
5.185 O
) O
we O
can O
improve O
on O
this O
, O
however O
, O
by O
taking O
account O
of O
the O
variance B
of O
the O
posterior O
distribution O
. O
we O
shall O
see O
that O
the O
decision O
stage O
is O
generally O
very O
simple O
, O
even O
trivial O
, O
once O
we O
have O
solved O
the O
inference B
problem O
. O
by O
redefining O
the O
parameters O
of O
the O
model O
, O
show O
that O
this O
leads O
to O
an O
identical O
model O
for O
the O
marginal B
distribution O
p O
( O
x O
) O
over O
the O
observed O
variables O
for O
any O
valid O
choice O
of O
m O
and O
~ O
. O
( O
10.154 O
) O
( O
cid:27 O
) O
n O
( O
cid:2 O
) O
( O
cid:26 O
) O
n=1 O
substituting O
for O
the O
prior B
p O
( O
w O
) O
, O
the O
right-hand O
side O
of O
this O
inequality O
becomes O
, O
as O
a O
function O
of O
w O
n O
( O
cid:2 O
) O
n=1 O
+ O
( O
cid:26 O
) O
( O
w O
− O
m0 O
) O
ts O
0 O
( O
w O
− O
m0 O
) O
−1 O
−1 O
2 O
wtφn O
( O
tn O
− O
1/2 O
) O
− O
λ O
( O
ξn O
) O
wt O
( O
φnφt O
n O
) O
w O
( O
cid:27 O
) O
+ O
const O
. O
first O
suppose O
that O
v O
is O
orthogonal O
to O
the O
principal B
subspace I
, O
in O
other O
words O
it O
is O
given O
by O
some O
linear O
combination O
of O
the O
discarded O
eigenvectors O
. O
however O
, O
this O
does O
not O
lead O
to O
a O
simple O
learning B
algorithm O
because O
the O
error B
is O
a O
piecewise O
constant O
function O
of O
w O
, O
with O
discontinuities O
wherever O
a O
change O
in O
w O
causes O
the O
decision B
boundary I
to O
move O
across O
one O
of O
the O
data O
points O
. O
( O
d.3 O
) O
this O
can O
be O
seen O
as O
a O
natural O
extension O
of O
( O
d.2 O
) O
in O
which O
f O
[ O
y O
] O
now O
depends O
on O
a O
continuous O
set O
of O
variables O
, O
namely O
the O
values O
of O
y O
at O
all O
points O
x. O
requiring O
that O
the O
functional B
be O
stationary B
with O
respect O
to O
small O
variations O
in O
the O
function O
y O
( O
x O
) O
gives O
( O
cid:6 O
) O
δe O
δy O
( O
x O
) O
η O
( O
x O
) O
dx O
= O
0 O
. O
this O
allows O
us O
to O
use O
the O
inequality O
( O
10.152 O
) O
and O
place O
a O
lower B
bound I
on O
l O
( O
q O
) O
, O
which O
will O
therefore O
also O
be O
a O
lower B
bound I
on O
the O
log O
marginal O
likelihood O
dw O
dα O
. O
the O
effective O
dimensionality O
of O
the O
principal B
subspace I
is O
then O
determined O
by O
the O
number O
of O
finite O
o O
: O
i O
values O
, O
and O
the O
correspond O
( O
cid:173 O
) O
ing O
vectors O
wi O
can O
be O
thought O
of O
as O
'relevant O
' O
for O
modelling O
the O
data O
distribution O
. O
this O
tells O
us O
that O
in O
general O
the O
marginal B
density O
p O
( O
x O
) O
will O
not O
factorize O
with O
respect O
to O
the O
components O
of O
x. O
we O
encountered O
a O
simple O
application O
of O
the O
naive O
bayes O
model O
in O
the O
context O
of O
fusing O
data O
from O
different O
sources O
for O
medical O
diagnosis O
in O
section O
1.5. O
if O
we O
are O
given O
a O
labelled O
training B
set I
, O
comprising O
inputs O
{ O
x1 O
, O
. O
( O
3.36 O
) O
at O
this O
point O
, O
it O
is O
worth O
distinguishing O
between O
the O
squared O
loss B
function I
arising O
from O
decision B
theory I
and O
the O
sum-of-squares B
error I
function O
that O
arose O
in O
the O
maxi- O
mum O
likelihood O
estimation O
of O
model O
parameters O
. O
note O
that O
if O
the O
conditional B
distribution O
of O
the O
target O
data O
is O
multimodal O
, O
the O
conditional B
mean O
can O
give O
poor O
predictions O
. O
( O
2.225 O
) O
( O
2.226 O
) O
note O
that O
the O
covariance B
of O
u O
( O
x O
) O
can O
be O
expressed O
in O
terms O
of O
the O
second O
derivatives O
of O
g O
( O
η O
) O
, O
and O
similarly O
for O
higher O
order O
moments O
. O
the O
predictive B
distribution I
is O
obtained O
by O
marginalizing O
over O
the O
posterior O
dis- O
tribution O
, O
and O
takes O
the O
same O
form O
as O
for O
the O
laplace O
approximation O
discussed O
in O
section O
4.5.2. O
figure O
10.13 O
shows O
the O
variational B
predictive O
distributions O
for O
a O
syn- O
thetic O
data O
set O
. O
472 O
10. O
approximate O
inference B
( O
a O
) O
( O
c O
) O
τ O
2 O
1 O
0 O
−1 O
2 O
τ O
1 O
0 O
−1 O
( O
b O
) O
( O
d O
) O
τ O
2 O
1 O
0 O
−1 O
2 O
τ O
1 O
0 O
µ O
1 O
0 O
µ O
1 O
0 O
µ O
1 O
0 O
−1 O
0 O
µ O
1 O
figure O
10.4 O
illustration O
of O
variational B
inference I
for O
the O
mean B
µ O
and O
precision O
τ O
of O
a O
univariate O
gaussian O
distribu- O
tion O
. O
j=1 O
( O
10.49 O
) O
we O
see O
that O
the O
optimal O
solution O
for O
the O
factor O
q O
( O
z O
) O
takes O
the O
same O
functional B
form O
as O
the O
prior B
p O
( O
z|π O
) O
. O
furthermore O
, O
let O
one O
of O
the O
basis O
functions O
be O
constant O
, O
say O
φ0 O
( O
x O
) O
= O
1. O
by O
taking O
suitable O
linear O
combinations O
of O
these O
basis O
functions O
, O
we O
can O
construct O
a O
new O
basis O
set O
ψj O
( O
x O
) O
spanning O
the O
same O
space O
but O
that O
are O
orthonormal O
, O
so O
that O
n O
( O
cid:2 O
) O
n=1 O
ψj O
( O
xn O
) O
ψk O
( O
xn O
) O
= O
ijk O
( O
3.115 O
) O
where O
ijk O
is O
deﬁned O
to O
be O
1 O
if O
j O
= O
k O
and O
0 O
otherwise O
, O
and O
we O
take O
ψ0 O
( O
x O
) O
= O
1. O
show O
that O
for O
α O
= O
0 O
, O
the O
equivalent B
kernel I
can O
be O
written O
as O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
ψ O
( O
x O
) O
tψ O
( O
x O
( O
cid:4 O
) O
) O
where O
ψ O
= O
( O
ψ1 O
, O
. O
as O
we O
have O
noted O
earlier O
, O
the O
goal O
is O
to O
achieve O
good O
generalization B
by O
making O
accurate O
predictions O
for O
new O
data O
. O
this O
can O
be O
seen O
by O
considering O
a O
small O
region O
of O
phase B
space I
over O
which O
h O
is O
approximately O
constant O
. O
13.13 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
use O
the O
deﬁnition O
( O
8.64 O
) O
of O
the O
messages O
passed O
from O
a O
factor O
node O
to O
a O
variable O
node B
in O
a O
factor B
graph I
, O
together O
with O
the O
expression O
( O
13.6 O
) O
for O
the O
joint O
distribution O
in O
a O
hidden O
markov O
model O
, O
to O
show O
that O
the O
deﬁnition O
( O
13.50 O
) O
of O
the O
alpha O
message O
is O
the O
same O
as O
the O
deﬁnition O
( O
13.34 O
) O
. O
in O
the O
case O
where O
the O
subspace O
is O
constrained O
to O
be O
linear O
, O
the O
procedure O
converges O
to O
the O
first O
principal O
component O
and O
is O
equivalent O
to O
the O
power B
method I
for O
finding O
the O
largest O
eigenvector O
of O
the O
co O
( O
cid:173 O
) O
variance B
matrix O
. O
an O
example O
would O
be O
the O
entropy B
h O
[ O
p O
] O
, O
which O
takes O
a O
probability B
distribution O
p O
( O
x O
) O
as O
the O
input O
and O
returns O
the O
quantity O
( O
cid:6 O
) O
h O
[ O
p O
] O
= O
p O
( O
x O
) O
ln O
p O
( O
x O
) O
dx O
( O
10.1 O
) O
10.1. O
variational B
inference I
463 O
as O
the O
output O
. O
2 O
12.24 O
( O
*** O
) O
we O
saw O
in O
section O
2.3.7 O
that O
student O
's O
t-distribution O
can O
be O
viewed O
as O
an O
infinite O
mixture O
of O
gaussians O
in O
which O
we O
marginalize O
with O
respect O
to O
a O
continu O
( O
cid:173 O
) O
ous O
latent B
variable I
. O
, O
xn O
, O
and O
multiply O
by O
the O
transition B
probability I
p O
( O
zn|zn−1 O
) O
and O
the O
emission B
probability I
p O
( O
xn|zn O
) O
and O
then O
marginalize O
over O
zn−1 O
, O
we O
obtain O
a O
distribution O
over O
a O
quantity O
( O
cid:1 O
) O
α O
( O
zn−1 O
) O
, O
representing O
the O
posterior B
probability I
of O
zn O
given O
observations O
zn O
that O
is O
of O
the O
same O
functional B
form O
as O
that O
over O
( O
cid:1 O
) O
α O
( O
zn−1 O
) O
. O
thus O
, O
tive O
variance B
( O
3.59 O
) O
will O
go O
to O
zero O
, O
leaving O
only O
the O
noise O
contribution O
β O
the O
model O
becomes O
very O
conﬁdent O
in O
its O
predictions O
when O
extrapolating O
outside O
the O
region O
occupied O
by O
the O
basis O
functions O
, O
which O
is O
generally O
an O
undesirable O
behaviour O
. O
we O
shall O
assume O
that O
the O
kernel B
function I
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
is O
governed O
by O
a O
vector O
θ O
of O
parameters O
, O
and O
we O
shall O
later O
discuss O
how O
θ O
may O
be O
learned O
from O
the O
training B
data O
. O
because O
the O
directed B
graph O
does O
not O
have O
any O
nodes O
with O
more O
than O
one O
parent O
, O
this O
does O
not O
require O
the O
addition O
of O
any O
extra O
links O
, O
and O
the O
directed B
and O
undirected B
versions O
of O
this O
graph O
express O
exactly O
the O
same O
set O
of O
conditional B
inde- O
pendence O
statements O
. O
an O
example O
of O
gaussian O
process O
regression B
is O
shown O
in O
figure O
6.8. O
the O
only O
restriction O
on O
the O
kernel B
function I
is O
that O
the O
covariance B
matrix I
given O
by O
( O
6.62 O
) O
must O
be O
positive B
deﬁnite I
. O
as O
an O
elegant O
deterministic O
al- O
ternative O
, O
ghahramani O
and O
jordan O
( O
1997 O
) O
exploited O
variational B
inference I
techniques O
to O
obtain O
a O
tractable O
algorithm O
for O
approximate O
inference B
. O
3.3. O
bayesian O
linear B
regression I
in O
our O
discussion O
of O
maximum B
likelihood I
for O
setting O
the O
parameters O
of O
a O
linear O
re- O
gression O
model O
, O
we O
have O
seen O
that O
the O
effective O
model O
complexity O
, O
governed O
by O
the O
number O
of O
basis O
functions O
, O
needs O
to O
be O
controlled O
according O
to O
the O
size O
of O
the O
data O
set O
. O
the O
problem O
, O
however O
, O
is O
that O
the O
samples O
{ O
z O
( O
l O
) O
} O
might O
not O
be O
independent B
, O
and O
so O
the O
effective O
sample O
size O
might O
be O
much O
smaller O
than O
the O
apparent O
sample O
size O
. O
write O
down O
the O
conditional B
distribution O
p O
( O
y2iyl O
) O
and O
observe O
that O
this O
is O
dependent O
on O
yb O
showing O
that O
the O
two O
variables O
are O
not O
independent B
. O
similarly O
, O
for O
evaluating O
local B
marginal O
probabil- O
ities O
we O
can O
work O
with O
the O
unnormalized O
joint O
distribution O
and O
then O
normalize O
the O
marginals O
explicitly O
at O
the O
end O
. O
next O
we O
define O
a O
gaussian O
prior B
distribution O
p O
( O
z O
) O
over O
the O
latent B
variable I
, O
together O
with O
a O
gaussian O
conditional B
distribution O
p O
( O
xl O
z O
) O
for O
the O
observed B
variable I
x O
conditioned O
on O
the O
value O
of O
the O
latent B
variable I
. O
we O
can O
obtain O
a O
much O
more O
efﬁcient O
procedure O
by O
‘ O
overlaying O
’ O
these O
multiple O
message B
passing I
algorithms O
to O
obtain O
the O
general O
sum-product B
algorithm I
as O
follows O
. O
in O
such O
situations O
, O
we O
need O
to O
resort O
to O
approximation O
schemes O
, O
and O
these O
fall O
broadly O
into O
two O
classes O
, O
according O
to O
whether O
they O
rely O
on O
stochastic B
or O
determin- O
istic O
approximations O
. O
here O
we O
give O
an O
indication O
of O
the O
problem O
in O
the O
context O
of O
our O
solutions O
for O
the O
maximum B
likelihood I
param- O
eter O
settings O
for O
the O
univariate O
gaussian O
distribution O
. O
if O
we O
consider O
the O
special O
case O
in O
which O
the O
transformation O
of O
the O
inputs O
simply O
consists O
of O
the O
addition O
of O
random O
noise O
, O
so O
that O
x O
→ O
x O
+ O
ξ O
, O
then O
the O
regularizer O
takes O
the O
form O
( O
cid:6 O
) O
ω O
= O
1 O
2 O
( O
cid:5 O
) O
∇y O
( O
x O
) O
( O
cid:5 O
) O
2 O
p O
( O
x O
) O
dx O
( O
5.135 O
) O
which O
is O
known O
as O
tikhonov O
regularization B
( O
tikhonov O
and O
arsenin O
, O
1977 O
; O
bishop O
, O
1995b O
) O
. O
we O
note O
that O
the O
ﬁnal O
result O
is O
necessarily O
symmetric O
and O
positive B
deﬁnite I
( O
provided O
the O
sample O
covariance O
is O
nonsingular O
) O
. O
we O
see O
that O
any O
ﬁnite O
value O
of O
α O
reduces O
the O
probability B
of O
the O
observed O
data O
, O
and O
so O
for O
the O
most O
probable O
solution O
the O
basis O
vector O
is O
removed O
. O
µfs→x O
( O
x O
) O
≡ O
fs O
( O
x O
, O
xs O
) O
( O
8.63 O
) O
( O
8.64 O
) O
here O
we O
have O
introduced O
a O
set O
of O
functions O
µfs→x O
( O
x O
) O
, O
deﬁned O
by O
xs O
which O
can O
be O
viewed O
as O
messages O
from O
the O
factor O
nodes O
fs O
to O
the O
variable O
node B
x. O
we O
see O
that O
the O
required O
marginal B
p O
( O
x O
) O
is O
given O
by O
the O
product O
of O
all O
the O
incoming O
messages O
arriving O
at O
node B
x. O
in O
order O
to O
evaluate O
these O
messages O
, O
we O
again O
turn O
to O
figure O
8.46 O
and O
note O
that O
each O
factor O
fs O
( O
x O
, O
xs O
) O
is O
described O
by O
a O
factor O
( O
sub- O
) O
graph O
and O
so O
can O
itself O
be O
fac- O
torized O
. O
an O
important O
class O
of O
such O
models O
, O
known O
as O
independent B
component I
analysis I
, O
or O
lea O
, O
arises O
when O
we O
consider O
a O
distribution O
over O
the O
latent O
variables O
that O
factorizes O
, O
so O
that O
m O
p O
( O
z O
) O
= O
iip O
( O
zj O
) O
. O
this O
is O
know O
as O
the O
forward B
kinematics I
of O
the O
arm O
. O
the O
lower O
right O
panel O
of O
figure O
8.30 O
shows O
the O
result O
of O
applying O
a O
graph-cut B
algorithm I
to O
the O
de-noising O
problem O
. O
the O
evidence O
framework O
applied O
to O
classiﬁcation B
networks O
. O
this O
latter O
behaviour O
is O
known O
as O
over-ﬁtting B
. O
this O
can O
be O
viewed O
as O
a O
set O
of O
m O
simultaneous O
homogeneous B
linear O
equations O
, O
and O
the O
condition O
for O
a O
solution O
is O
that O
|a O
− O
λii| O
= O
0 O
( O
c.30 O
) O
which O
is O
known O
as O
the O
characteristic O
equation O
. O
10.2.2 O
variational B
lower O
bound O
. O
the O
data O
for O
this O
example O
is O
generated O
from O
the O
function O
sin O
( O
2πx O
) O
with O
random O
noise O
included O
in O
the O
target O
values O
, O
as O
described O
in O
detail O
in O
appendix O
a. O
now O
suppose O
that O
we O
are O
given O
a O
training B
set I
comprising O
n O
observations O
of O
x O
, O
written O
x O
≡ O
( O
x1 O
, O
. O
as O
a O
consequence O
of O
the O
triangulation O
step O
, O
the O
resulting O
tree B
satisﬁes O
the O
running B
intersection I
property I
, O
which O
means O
that O
if O
a O
variable O
is O
contained O
in O
two O
cliques O
, O
then O
it O
must O
also O
be O
con- O
tained O
in O
every O
clique B
on O
the O
path O
that O
connects O
them O
. O
the O
origins O
of O
factor B
analysis I
are O
as O
old O
as O
those O
of O
pca O
. O
however O
, O
the O
perceptron B
convergence O
theorem O
states O
that O
if O
there O
exists O
an O
ex- O
act O
solution O
( O
in O
other O
words O
, O
if O
the O
training B
data O
set O
is O
linearly B
separable I
) O
, O
then O
the O
perceptron B
learning O
algorithm O
is O
guaranteed O
to O
ﬁnd O
an O
exact O
solution O
in O
a O
ﬁnite O
num- O
ber O
of O
steps O
. O
models O
can O
then O
be O
compared O
di- O
rectly O
on O
the O
training B
data O
, O
without O
the O
need O
for O
a O
validation B
set I
. O
1 O
1 O
( O
7.125 O
) O
7.6 O
( O
( O
cid:12 O
) O
) O
consider O
the O
logistic B
regression I
model O
with O
a O
target O
variable O
t O
∈ O
{ O
−1 O
, O
1 O
} O
. O
the O
value O
of O
w O
found O
by O
minimizing O
e O
( O
w O
) O
will O
be O
denoted O
wml O
because O
it O
corresponds O
to O
the O
maximum B
likelihood I
solution O
. O
we O
begin O
by O
transforming O
the O
directed B
graph O
of O
figure O
13.5 O
into O
a O
factor B
graph I
, O
of O
which O
a O
representative O
fragment O
is O
shown O
in O
figure O
13.14. O
this O
form O
of O
the O
fac- O
tor O
graph O
shows O
all O
variables O
, O
both O
latent O
and O
observed O
, O
explicitly O
. O
a O
revolu- O
tion O
: O
belief B
propagation I
in O
graphs O
with O
cycles O
. O
the O
red O
line O
shows O
the O
mean B
of O
the O
gaussian O
process O
predictive O
distri- O
bution O
, O
and O
the O
shaded O
region O
cor- O
responds O
to O
plus O
and O
minus O
two O
standard O
deviations O
. O
we O
can O
hope O
to O
avoid O
the O
singularities B
by O
using O
suitable O
heuristics O
, O
for O
instance O
by O
detecting O
when O
a O
gaussian O
component O
is O
collapsing O
and O
resetting O
its O
mean B
to O
a O
randomly O
chosen O
value O
while O
also O
resetting O
its O
covariance B
to O
some O
large O
value O
, O
and O
then O
continuing O
with O
the O
optimization O
. O
we O
start O
by O
viewing O
the O
variable O
node B
x O
as O
the O
root O
of O
the O
factor B
graph I
and O
initiating O
messages O
at O
the O
leaves O
of O
the O
graph O
using O
( O
8.70 O
) O
and O
( O
8.71 O
) O
. O
if O
both O
µ O
and O
τ O
are O
unknown O
, O
their O
joint O
conjugate B
prior I
is O
the O
gaussian-gamma O
distribution O
. O
however O
, O
the O
loss B
function I
depends O
on O
the O
true O
class O
, O
which O
is O
unknown O
. O
because O
the O
integral O
is O
intractable O
, O
we O
employ O
the O
variational B
bound O
( O
10.144 O
) O
, O
which O
we O
write O
in O
the O
form O
σ O
( O
a O
) O
( O
cid:2 O
) O
f O
( O
a O
, O
ξ O
) O
where O
ξ O
is O
a O
variational B
parameter O
. O
instead O
, O
the O
mixing O
coefﬁcients O
are O
treated O
as O
parameters O
, O
whose O
values O
are O
to O
be O
found O
by O
maximizing O
the O
variational B
lower O
bound O
on O
the O
log O
marginal O
likelihood O
. O
in O
the O
annular O
conﬁguration O
the O
oil O
, O
water O
, O
and O
gas O
form O
concentric O
cylinders O
with O
the O
water O
around O
the O
outside O
and O
the O
gas O
in O
the O
centre O
, O
whereas O
in O
the O
homogeneous B
conﬁguration O
the O
oil O
, O
water O
and O
gas O
are O
assumed O
to O
be O
intimately O
mixed O
as O
might O
occur O
at O
high O
ﬂow O
velocities O
under O
turbulent O
conditions O
. O
the O
posterior B
probability I
for O
class O
p O
( O
x|c1 O
) O
p O
( O
c1 O
) O
p O
( O
c1|x O
) O
= O
p O
( O
x|c1 O
) O
p O
( O
c1 O
) O
+ O
p O
( O
x|c2 O
) O
p O
( O
c2 O
) O
1 O
+ O
exp O
( O
−a O
) O
a O
= O
ln O
p O
( O
x|c1 O
) O
p O
( O
c1 O
) O
p O
( O
x|c2 O
) O
p O
( O
c2 O
) O
and O
σ O
( O
a O
) O
is O
the O
logistic B
sigmoid I
function O
deﬁned O
by O
where O
we O
have O
deﬁned O
= O
σ O
( O
a O
) O
= O
1 O
σ O
( O
a O
) O
= O
1 O
1 O
+ O
exp O
( O
−a O
) O
( O
4.57 O
) O
( O
4.58 O
) O
( O
4.59 O
) O
which O
is O
plotted O
in O
figure O
4.9. O
the O
term O
‘ O
sigmoid B
’ O
means O
s-shaped O
. O
as O
we O
shall O
see O
in O
detail O
later O
, O
we O
can O
adopt O
a O
similar O
approach O
when O
making O
inferences O
about O
quantities O
such O
as O
the O
parameters O
w O
in O
the O
polynomial B
curve I
ﬁtting I
example O
. O
however O
, O
we O
then O
have O
to O
compensate O
for O
the O
effects O
of O
our O
modiﬁcations O
to O
the O
training B
data O
. O
a O
major O
challenge O
is O
that O
a O
game O
of O
backgammon B
can O
involve O
dozens O
of O
moves O
, O
and O
yet O
it O
is O
only O
at O
the O
end O
of O
the O
game O
that O
the O
reward O
, O
in O
the O
form O
of O
victory O
, O
is O
achieved O
. O
the O
goal O
of O
achieving O
the O
best O
generalization B
performance O
suggests O
that O
training B
should O
be O
stopped O
at O
the O
point O
shown O
by O
the O
vertical O
dashed O
lines O
, O
corresponding O
to O
the O
minimum O
of O
the O
validation B
set I
error O
. O
in O
general O
, O
we O
address O
this O
by O
treating O
the O
variational B
solutions O
as O
re-estimation O
equations O
and O
cycling O
through O
the O
variables O
in O
turn O
updating O
them O
until O
some O
convergence O
criterion O
is O
satisﬁed O
. O
however O
, O
for O
approximations O
q O
( O
θ O
) O
in O
the O
exponential B
family I
, O
if O
the O
iterations O
do O
converge O
, O
the O
resulting O
solution O
will O
be O
a O
stationary B
point O
of O
a O
particular O
energy B
function I
( O
minka O
, O
2001a O
) O
, O
although O
each O
iteration O
of O
ep O
does O
not O
necessarily O
decrease O
the O
value O
of O
this O
energy B
function I
. O
here O
we O
consider O
two O
simple O
examples O
of O
noninformative B
priors O
( O
berger O
, O
1985 O
) O
. O
( O
3.18 O
) O
setting O
the O
derivative B
with O
respect O
to O
w0 O
equal O
to O
zero O
, O
and O
solving O
for O
w0 O
, O
we O
obtain O
( O
3.19 O
) O
where O
we O
have O
deﬁned O
t O
= O
1 O
n O
n O
( O
cid:2 O
) O
n=1 O
tn O
, O
j=1 O
φj O
= O
1 O
n O
n O
( O
cid:2 O
) O
n=1 O
φj O
( O
xn O
) O
. O
in O
a O
factor B
graph I
, O
there O
is O
a O
node B
( O
depicted O
as O
usual O
by O
a O
circle O
) O
for O
every O
variable O
in O
the O
distribution O
, O
as O
was O
the O
case O
for O
directed O
and O
undirected B
graphs O
. O
2.14 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
this O
exercise O
demonstrates O
that O
the O
multivariate O
distribution O
with O
max- O
imum O
entropy B
, O
for O
a O
given O
covariance B
, O
is O
a O
gaussian O
. O
9.3.4 O
em O
for O
bayesian O
linear B
regression I
. O
13.11 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
starting O
from O
the O
expression O
( O
8.72 O
) O
for O
the O
marginal B
distribution O
over O
the O
vari- O
ables O
of O
a O
factor O
in O
a O
factor B
graph I
, O
together O
with O
the O
results O
for O
the O
messages O
in O
the O
sum-product B
algorithm I
obtained O
in O
section O
13.2.3 O
, O
derive O
the O
result O
( O
13.43 O
) O
for O
the O
joint O
posterior O
distribution O
over O
two O
successive O
latent O
variables O
in O
a O
hidden O
markov O
model O
. O
using O
the O
product B
rule I
, O
we O
can O
factor O
the O
joint O
distribution O
p O
( O
x1 O
, O
x2 O
) O
in O
the O
form O
p O
( O
x2|x1 O
) O
p O
( O
x1 O
) O
, O
which O
corresponds O
to O
a O
two-node O
graph O
with O
a O
link B
going O
from O
the O
x1 O
node B
to O
the O
x2 O
node B
as O
shown O
in O
figure O
8.9 O
( O
a O
) O
. O
we O
can O
similarly O
consider O
extensions O
to O
an O
m O
th O
order O
markov O
chain O
in O
which O
the O
conditional B
distri- O
bution O
for O
a O
particular O
variable O
depends O
on O
the O
previous O
m O
variables O
. O
these O
can O
broadly O
be O
divided O
into O
four O
categories O
: O
1. O
the O
training B
set I
is O
augmented O
using O
replicas O
of O
the O
training B
patterns O
, O
trans- O
formed O
according O
to O
the O
desired O
invariances O
. O
this O
form O
of O
kernel O
has O
, O
however O
, O
been O
used O
in O
practice O
( O
vapnik O
, O
1995 O
) O
, O
possibly O
because O
it O
gives O
kernel O
expansions O
such O
as O
the O
support B
vector I
machine I
a O
superﬁcial O
resemblance O
to O
neural B
network I
models O
. O
10 O
approximate O
inference B
a O
central O
task O
in O
the O
application O
of O
probabilistic O
models O
is O
the O
evaluation O
of O
the O
pos- O
terior O
distribution O
p O
( O
z|x O
) O
of O
the O
latent O
variables O
z O
given O
the O
observed O
( O
visible O
) O
data O
variables O
x O
, O
and O
the O
evaluation O
of O
expectations O
computed O
with O
respect O
to O
this O
dis- O
tribution O
. O
maximization O
with O
respect O
to O
µ0 O
and O
v0 O
is O
easily O
performed O
by O
making O
use O
of O
the O
maximum B
likelihood I
solution O
for O
a O
gaussian O
distribution O
discussed O
in O
section O
2.3.4 O
, O
giving O
exercise O
13.32 O
13.3. O
linear O
dynamical O
systems O
643 O
( O
13.110 O
) O
( O
13.111 O
) O
similarly O
, O
to O
optimize O
a O
and O
γ O
, O
we O
substitute O
for O
p O
( O
zn|zn−1 O
, O
a O
, O
γ O
) O
in O
( O
13.108 O
) O
1 O
] O
− O
e O
[ O
z1 O
] O
e O
[ O
zt O
1 O
] O
. O
show O
that O
the O
marginal B
distribution O
p O
( O
x O
) O
, O
obtained O
by O
summing O
p O
( O
z O
) O
p O
( O
x|z O
) O
over O
all O
possible O
values O
of O
z O
, O
is O
a O
gaussian O
mixture O
of O
the O
form O
( O
9.7 O
) O
. O
( O
5.52 O
) O
( O
5.53 O
) O
equation O
( O
5.53 O
) O
tells O
us O
that O
the O
required O
derivative B
is O
obtained O
simply O
by O
multiplying O
the O
value O
of O
δ O
for O
the O
unit O
at O
the O
output O
end O
of O
the O
weight O
by O
the O
value O
of O
z O
for O
the O
unit O
at O
the O
input O
end O
of O
the O
weight O
( O
where O
z O
= O
1 O
in O
the O
case O
of O
a O
bias B
) O
. O
this O
is O
sometimes O
called O
moment B
matching I
. O
9.17 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
as O
a O
consequence O
of O
the O
constraint O
0 O
( O
cid:1 O
) O
p O
( O
xn|µk O
) O
( O
cid:1 O
) O
1 O
for O
the O
discrete O
variable O
xn O
, O
the O
incomplete-data O
log O
likelihood O
function O
for O
a O
mixture O
of O
bernoulli O
distributions O
is O
bounded O
above O
, O
and O
hence O
that O
there O
are O
no O
singularities B
for O
which O
the O
likelihood O
goes O
to O
inﬁnity O
. O
similarly O
, O
for O
the O
multiclass B
case O
, O
the O
posterior B
probability I
of O
class O
ck O
is O
given O
by O
a O
softmax O
transformation O
of O
a O
linear O
function O
of O
x. O
for O
speciﬁc O
choices O
of O
the O
class-conditional O
densities O
p O
( O
x|ck O
) O
, O
we O
have O
used O
maximum B
likelihood I
to O
determine O
the O
parameters O
of O
the O
densities O
as O
well O
as O
the O
class O
priors O
p O
( O
ck O
) O
and O
then O
used O
bayes O
’ O
theorem O
to O
ﬁnd O
the O
posterior O
class O
probabilities O
. O
in O
practical O
applications O
, O
the O
variability O
of O
the O
input O
vectors O
will O
be O
such O
that O
the O
training B
data O
can O
comprise O
only O
a O
tiny O
fraction O
of O
all O
possible O
input O
vectors O
, O
and O
so O
generalization B
is O
a O
central O
goal O
in O
pattern O
recognition O
. O
we O
can O
use O
( O
6.13 O
) O
and O
( O
6.17 O
) O
to O
extend O
this O
class O
of O
kernels O
by O
considering O
sums O
over O
products O
of O
different O
probability B
distributions O
, O
with O
positive O
weighting O
coefﬁcients O
p O
( O
i O
) O
, O
of O
the O
form O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
p O
( O
x|i O
) O
p O
( O
x O
( O
cid:4 O
) O
|i O
) O
p O
( O
i O
) O
. O
13.33 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
verify O
the O
results O
( O
13.113 O
) O
and O
( O
13.114 O
) O
for O
the O
m-step O
equations O
for O
a O
and O
γ O
in O
the O
linear B
dynamical I
system I
. O
initially O
, O
rosenblatt O
simulated O
the O
perceptron B
on O
an O
ibm O
704 O
computer O
at O
cornell O
in O
1957 O
, O
but O
by O
the O
early O
1960s O
he O
had O
built O
special-purpose O
hardware B
that O
provided O
a O
direct O
, O
par- O
allel O
implementation O
of O
perceptron B
learning O
. O
it O
com- O
prises O
a O
training B
set I
of O
60 O
, O
000 O
examples O
and O
a O
test B
set I
of O
10 O
, O
000 O
examples O
. O
in O
pac O
learning B
we O
say O
that O
a O
function O
f O
( O
x O
; O
d O
) O
, O
drawn O
from O
a O
space O
f O
of O
such O
functions O
on O
the O
basis O
of O
the O
training B
set I
d O
, O
has O
good O
generalization B
if O
its O
expected O
error B
rate O
is O
below O
some O
pre-speciﬁed O
threshold O
 O
, O
so O
that O
( O
7.75 O
) O
where O
i O
( O
· O
) O
is O
the O
indicator O
function O
, O
and O
the O
expectation B
is O
with O
respect O
to O
the O
dis- O
tribution O
p O
( O
x O
, O
t O
) O
. O
( O
2.24 O
) O
the O
term O
on O
the O
left-hand O
side O
of O
( O
2.24 O
) O
is O
the O
prior B
variance O
of O
θ. O
on O
the O
right- O
hand O
side O
, O
the O
ﬁrst O
term O
is O
the O
average O
posterior O
variance O
of O
θ O
, O
and O
the O
second O
term O
measures O
the O
variance B
in O
the O
posterior O
mean O
of O
θ. O
because O
this O
variance B
is O
a O
positive O
quantity O
, O
this O
result O
shows O
that O
, O
on O
average O
, O
the O
posterior O
variance O
of O
θ O
is O
smaller O
than O
the O
prior B
variance O
. O
in O
order O
to O
sim- O
plify O
the O
discussion O
, O
we O
shall O
suppose O
that O
the O
noise O
precision B
parameter I
β O
is O
known O
, O
and O
is O
ﬁxed O
to O
its O
true O
value O
, O
although O
the O
framework O
is O
easily O
extended B
to O
include O
the O
distribution O
over O
β. O
for O
the O
linear B
regression I
model O
, O
the O
variational B
treatment O
will O
turn O
out O
to O
be O
equivalent O
to O
the O
evidence O
framework O
. O
note O
, O
however O
, O
that O
in O
contrast O
to O
the O
models O
used O
for B
regression I
, O
they O
are O
no O
longer O
linear O
in O
the O
parameters O
due O
to O
the O
presence O
of O
the O
nonlinear O
function O
f O
( O
· O
) O
. O
( O
c O
) O
in O
the O
subsequent O
m O
step O
, O
each O
cluster O
centre O
is O
re-computed O
to O
be O
the O
mean B
of O
the O
points O
assigned O
to O
the O
corresponding O
cluster O
. O
the O
primary O
role O
of O
the O
latent O
variables O
is O
to O
allow O
a O
complicated O
distribution O
over O
the O
observed O
variables O
to O
be O
represented O
in O
terms O
of O
a O
model O
constructed O
from O
simpler O
( O
typically O
exponential B
family I
) O
conditional B
distributions O
. O
the O
generative B
topographic I
mapping I
, O
or O
gtm O
( O
bishop O
et O
ai. O
, O
1996 O
; O
bishop O
et O
ai. O
, O
1997a O
; O
bishop O
et O
ai. O
, O
1998b O
) O
uses O
a O
latent O
distribution O
that O
is O
defined O
by O
a O
finite O
regular O
grid O
of O
delta O
functions O
over O
the O
( O
typically O
two-dimensional O
) O
latent O
space O
. O
the O
outputs O
of O
the O
convolutional B
units O
form O
the O
inputs O
to O
the O
subsampling B
layer O
of O
the O
network O
. O
improving O
the O
con- O
vergence O
of O
back-propagation O
learning B
with O
sec- O
ond O
order O
methods O
. O
the O
basis O
functions O
associated O
with O
these O
parameters O
therefore O
play O
no O
role O
7.2. O
relevance B
vector I
machines O
347 O
in O
the O
predictions O
made O
by O
the O
model O
and O
so O
are O
effectively O
pruned O
out O
, O
resulting O
in O
a O
sparse O
model O
. O
the O
training B
data O
is O
then O
discarded O
, O
and O
predictions O
for O
new O
inputs O
are O
based O
purely O
on O
the O
learned O
parameter O
vector O
w. O
this O
approach O
is O
also O
used O
in O
nonlinear O
parametric O
models O
such O
as O
neural O
networks O
. O
2.3.4 O
maximum B
likelihood I
for O
the O
gaussian O
. O
s O
t O
ϕ1 O
y O
ϕ2 O
and O
so O
we O
see O
that O
the O
inverse B
of O
the O
noise O
precision O
is O
given O
by O
the O
residual O
variance B
of O
the O
target O
values O
around O
the O
regression B
function I
. O
( O
b O
) O
the O
same O
data O
set O
but O
with O
three O
additional O
outlying O
data O
points O
showing O
how O
the O
gaussian O
( O
green O
curve O
) O
is O
strongly O
distorted O
by O
the O
outliers B
, O
whereas O
the O
t-distribution O
( O
red O
curve O
) O
is O
relatively O
unaffected O
. O
the O
joint O
distribution O
for O
a O
directed B
graph O
is O
deﬁned O
by O
( O
11.4 O
) O
. O
this O
is O
illustrated O
in O
figure O
9.7. O
these O
singularities B
provide O
another O
example O
of O
the O
severe O
over-ﬁtting B
that O
can O
occur O
in O
a O
maximum B
likelihood I
approach O
. O
we O
can O
2.3. O
the O
gaussian O
distribution O
79 O
n O
= O
1 O
3 O
2 O
1 O
0 O
0 O
n O
= O
2 O
3 O
2 O
1 O
0 O
0 O
n O
= O
10 O
3 O
2 O
1 O
0 O
0 O
0.5 O
1 O
0.5 O
1 O
0.5 O
1 O
figure O
2.6 O
histogram O
plots O
of O
the O
mean B
of O
n O
uniformly O
distributed O
numbers O
for O
various O
values O
of O
n. O
we O
observe O
that O
as O
n O
increases O
, O
the O
distribution O
tends O
towards O
a O
gaussian O
. O
this O
model O
can O
be O
492 O
10. O
approximate O
inference B
described O
by O
the O
directed B
graph O
shown O
in O
figure O
10.5. O
here O
we O
consider O
more O
gen- O
erally O
the O
use O
of O
variational B
methods O
for O
models O
described O
by O
directed B
graphs O
and O
derive O
a O
number O
of O
widely O
applicable O
results O
. O
however O
, O
we O
would O
like O
to O
address O
and O
quantify O
the O
uncertainty O
that O
surrounds O
the O
appropriate O
choice O
for O
the O
model O
param- O
eters O
w. O
we O
shall O
see O
that O
, O
from O
a O
bayesian O
perspective O
, O
we O
can O
use O
the O
machinery O
of O
probability B
theory O
to O
describe O
the O
uncertainty O
in O
model O
parameters O
such O
as O
w O
, O
or O
indeed O
in O
the O
choice O
of O
model O
itself O
. O
we O
therefore O
wish O
to O
make O
a O
free O
form O
( O
variational B
) O
optimization O
of O
l O
( O
q O
) O
with O
respect O
to O
all O
of O
the O
distributions O
qi O
( O
zi O
) O
, O
which O
we O
do O
by O
optimizing O
with O
respect O
to O
each O
of O
the O
factors O
in O
turn O
. O
the O
whole O
network O
can O
be O
trained O
by O
error B
minimization O
using O
backpropagation B
to O
evaluate O
the O
gradient O
of O
the O
error B
function I
. O
we O
now O
introduce O
a O
prior B
distribution O
over O
α. O
from O
our O
dis- O
cussion O
in O
section O
2.3.6 O
, O
we O
know O
that O
the O
conjugate B
prior I
for O
the O
precision O
of O
a O
gaussian O
is O
given O
by O
a O
gamma B
distribution I
, O
and O
so O
we O
choose O
( O
10.89 O
) O
where O
gam O
( O
·|· O
, O
· O
) O
is O
deﬁned O
by O
( O
b.26 O
) O
. O
in O
some O
cases O
this O
is O
straightforward O
, O
as O
in O
the O
simple O
example O
in O
figure O
8.32. O
here O
the O
joint O
distribution O
for O
the O
directed B
graph O
is O
given O
as O
a O
product O
of O
conditionals O
in O
the O
form O
p O
( O
x O
) O
= O
p O
( O
x1 O
) O
p O
( O
x2|x1 O
) O
p O
( O
x3|x2 O
) O
··· O
p O
( O
xn|xn−1 O
) O
. O
a O
much O
more O
ﬂexible O
model O
is O
obtained O
by O
using O
a O
multilevel O
gating B
function I
to O
give O
the O
hierarchical B
mixture I
of I
experts I
, O
or O
hme O
model O
( O
jordan O
and O
jacobs O
, O
1994 O
) O
. O
( O
13.92 O
) O
thus O
, O
given O
the O
values O
of O
µn−1 O
and O
vn−1 O
, O
together O
with O
the O
new O
observation O
xn O
, O
we O
can O
evaluate O
the O
gaussian O
marginal B
for O
zn O
having O
mean B
µn O
and O
covariance B
vn O
, O
as O
well O
as O
the O
normalization O
coefﬁcient O
cn O
. O
we O
have O
seen O
that O
a O
message O
can O
only O
be O
sent O
across O
a O
link B
from O
a O
node B
when O
all O
other O
messages O
have O
been O
received O
by O
that O
node B
across O
its O
other O
links O
. O
note O
that O
the O
left-hand O
mode O
of O
the O
class-conditional O
density B
p O
( O
x|c1 O
) O
, O
shown O
in O
blue O
on O
the O
left O
plot O
, O
has O
no O
effect O
on O
the O
posterior O
probabilities O
. O
although O
the O
resulting O
integral O
over O
w O
is O
no O
longer O
analytically O
tractable O
, O
it O
might O
be O
thought O
that O
approximating O
this O
integral O
, O
for O
example O
using O
the O
laplace O
approximation O
discussed O
( O
section O
4.4 O
) O
which O
is O
based O
on O
a O
local B
gaussian O
approxi- O
mation B
centred O
on O
the O
mode O
of O
the O
posterior O
distribution O
, O
might O
provide O
a O
practical O
alternative O
to O
the O
evidence O
framework O
( O
buntine O
and O
weigend O
, O
1991 O
) O
. O
from O
( O
9.10 O
) O
we O
can O
write O
down O
the O
conditional B
distribution O
of O
z O
, O
given O
the O
mixing O
coefﬁcients O
π O
, O
in O
the O
form O
p O
( O
z|π O
) O
= O
πznk O
k O
. O
by O
adopting O
a O
bayesian O
approach O
, O
the O
over-ﬁtting B
problem O
can O
be O
avoided O
. O
we O
can O
obtain O
some O
insight O
into O
the O
result O
( O
3.95 O
) O
for O
re-estimating O
β O
by O
com- O
paring O
it O
with O
the O
corresponding O
maximum B
likelihood I
result O
given O
by O
( O
3.21 O
) O
. O
2.25 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
sections O
2.3.1 O
and O
2.3.2 O
, O
we O
considered O
the O
conditional B
and O
marginal B
distri- O
butions O
for O
a O
multivariate O
gaussian O
. O
coefﬁcient O
1/z O
is O
then O
easily O
obtained O
by O
normalizing O
any O
one O
of O
these O
marginals O
, O
and O
this O
is O
computationally O
efﬁcient O
because O
the O
normalization O
is O
done O
over O
a O
single O
variable O
rather O
than O
over O
the O
entire O
set O
of O
variables O
as O
would O
be O
required O
to O
normalize O
at O
this O
point O
, O
it O
may O
be O
helpful O
to O
consider O
a O
simple O
example O
to O
illustrate O
the O
operation O
of O
the O
sum-product B
algorithm I
. O
a2 O
( O
2.130 O
) O
( O
2.131 O
) O
( O
2.132 O
) O
n O
=1 O
it O
can O
then O
be O
shown O
( O
robbins O
and O
monro O
, O
1951 O
; O
fukunaga O
, O
1990 O
) O
that O
the O
sequence O
of O
estimates O
given O
by O
( O
2.129 O
) O
does O
indeed O
converge O
to O
the O
root O
with O
probability B
one O
. O
( O
2.14 O
) O
( O
cid:6 O
) O
1 O
exercise O
2.6 O
the O
mean B
and O
variance B
of O
the O
beta B
distribution I
are O
given O
by O
0 O
e O
[ O
µ O
] O
= O
var O
[ O
µ O
] O
= O
a O
a O
+ O
b O
( O
a O
+ O
b O
) O
2 O
( O
a O
+ O
b O
+ O
1 O
) O
. O
as- O
sociated O
with O
this O
observation O
will O
be O
a O
corresponding O
latent B
variable I
( O
cid:1 O
) O
z O
, O
and O
the O
pre- O
p O
( O
( O
cid:1 O
) O
x| O
( O
cid:1 O
) O
z O
, O
µ O
, O
λ O
) O
p O
( O
( O
cid:1 O
) O
z|π O
) O
p O
( O
π O
, O
µ O
, O
λ|x O
) O
dπ O
dµ O
dλ O
( O
10.78 O
) O
dictive O
density B
is O
then O
given O
by O
p O
( O
( O
cid:1 O
) O
x|x O
) O
= O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:2 O
) O
bz O
10.2. O
illustration O
: O
variational B
mixture O
of O
gaussians O
483 O
where O
p O
( O
π O
, O
µ O
, O
λ|x O
) O
is O
the O
( O
unknown O
) O
true O
posterior O
distribution O
of O
the O
parameters O
. O
one O
particularly O
important O
problem O
concerned O
so-called O
in- O
verse O
probability B
. O
n O
rnk O
which O
we O
can O
easily O
solve O
for O
µk O
to O
give O
( O
9.3 O
) O
( O
9.4 O
) O
the O
denominator O
in O
this O
expression O
is O
equal O
to O
the O
number O
of O
points O
assigned O
to O
cluster O
k O
, O
and O
so O
this O
result O
has O
a O
simple O
interpretation O
, O
namely O
set O
µk O
equal O
to O
the O
mean B
of O
all O
of O
the O
data O
points O
xn O
assigned O
to O
cluster O
k. O
for O
this O
reason O
, O
the O
procedure O
is O
known O
as O
the O
k-means O
algorithm O
. O
it O
is O
often O
convenient O
to O
deﬁne O
an O
additional O
dummy O
‘ O
basis B
function I
’ O
φ0 O
( O
x O
) O
= O
1 O
so O
that O
m−1 O
( O
cid:2 O
) O
y O
( O
x O
, O
w O
) O
= O
wjφj O
( O
x O
) O
= O
wtφ O
( O
x O
) O
( O
3.3 O
) O
j=0 O
where O
w O
= O
( O
w0 O
, O
. O
this O
is O
especially O
problematic O
if O
we O
wish O
to O
ﬁnd O
the O
absolute O
value O
of O
the O
partition B
function I
for O
a O
complex O
distribution O
because O
it O
is O
only O
for O
relatively O
simple O
distributions O
that O
the O
partition B
function I
can O
be O
evaluated O
directly O
, O
and O
so O
attempting O
to O
estimate O
the O
ratio O
of O
partition O
functions O
directly O
is O
unlikely O
to O
be O
successful O
. O
the O
original O
sine O
function O
is O
shown O
by O
the O
green O
curve O
, O
the O
data O
points O
are O
shown O
in O
blue O
, O
and O
each O
is O
the O
centre O
of O
an O
isotropic B
gaussian O
kernel O
. O
( O
cid:17 O
) O
( O
cid:4 O
) O
x† O
y O
( O
x O
) O
= O
, O
wt O
( O
cid:4 O
) O
x O
= O
tt O
then O
obtain O
the O
discriminant B
function I
in O
the O
form O
ed O
( O
, O
w O
) O
= O
1 O
2 O
tr O
. O
2.5.1 O
kernel O
density O
estimators O
let O
us O
suppose O
that O
observations O
are O
being O
drawn O
from O
some O
unknown O
probabil- O
ity O
density B
p O
( O
x O
) O
in O
some O
d-dimensional O
space O
, O
which O
we O
shall O
take O
to O
be O
euclidean O
, O
and O
we O
wish O
to O
estimate O
the O
value O
of O
p O
( O
x O
) O
. O
speciﬁcally O
, O
its O
mean B
and O
covariance B
are O
given O
by O
( O
2.92 O
) O
and O
( O
2.93 O
) O
, O
respectively O
. O
, O
xmax O
1 O
as O
with O
the O
sum-product B
algorithm I
, O
the O
inclusion O
of O
evidence O
in O
the O
form O
of O
observed O
variables O
is O
straightforward O
. O
algorithms O
such O
as O
the O
snapshot O
method O
( O
sirovich O
, O
1987 O
) O
, O
which O
assume O
that O
the O
eigenvectors O
are O
linear O
combinations O
of O
the O
data O
vectors O
, O
avoid O
direct O
evaluation O
of O
the O
covariance B
matrix I
but O
are O
o O
( O
n3 O
) O
and O
hence O
unsuited O
to O
large O
data O
sets O
. O
( O
7.115 O
) O
deﬁning O
γi O
= O
1 O
− O
αiσii O
and O
rearranging O
then O
gives O
i O
= O
γi O
αnew O
( O
w O
( O
cid:1 O
) O
i O
) O
2 O
( O
7.116 O
) O
which O
is O
identical O
to O
the O
re-estimation O
formula O
( O
7.87 O
) O
obtained O
for O
the O
regression B
rvm O
. O
the O
sum-of-squares B
error I
function O
penalizes O
predictions O
that O
are O
‘ O
too O
correct O
’ O
in O
that O
they O
lie O
a O
long O
way O
on O
the O
correct O
side O
of O
the O
decision O
186 O
4. O
linear O
models O
for O
classification O
4 O
2 O
0 O
−2 O
−4 O
−6 O
−8 O
4 O
2 O
0 O
−2 O
−4 O
−6 O
−8 O
−4 O
−2 O
0 O
2 O
4 O
6 O
8 O
−4 O
−2 O
0 O
2 O
4 O
6 O
8 O
figure O
4.4 O
the O
left O
plot O
shows O
data O
from O
two O
classes O
, O
denoted O
by O
red O
crosses O
and O
blue O
circles O
, O
together O
with O
the O
decision B
boundary I
found O
by O
least O
squares O
( O
magenta O
curve O
) O
and O
also O
by O
the O
logistic B
regression I
model O
( O
green O
curve O
) O
, O
which O
is O
discussed O
later O
in O
section O
4.3.2. O
the O
right-hand O
plot O
shows O
the O
corresponding O
results O
obtained O
when O
extra O
data O
points O
are O
added O
at O
the O
bottom O
left O
of O
the O
diagram O
, O
showing O
that O
least O
squares O
is O
highly O
sensitive O
to O
outliers B
, O
unlike O
logistic B
regression I
. O
note O
that O
often O
the O
coefﬁcient O
w0 O
is O
omitted O
from O
the O
regularizer O
because O
its O
inclusion O
causes O
the O
results O
to O
depend O
on O
the O
choice O
of O
origin O
for O
the O
target O
variable O
( O
hastie O
et O
al. O
, O
2001 O
) O
, O
or O
it O
may O
be O
included O
but O
with O
its O
own O
regularization B
coefﬁcient O
( O
we O
shall O
discuss O
this O
topic O
in O
more O
detail O
in O
section O
5.5.1 O
) O
. O
in O
this O
case O
, O
the O
predictive B
distribution I
is O
a O
student O
’ O
s O
t-distribution O
. O
to O
do O
this O
, O
differentiate O
both O
sides O
of O
the O
normalization O
condition O
( O
2.264 O
) O
with O
respect O
to O
µ O
and O
then O
rearrange O
to O
obtain O
an O
expression O
for O
the O
mean B
of O
n. O
similarly O
, O
by O
differentiating O
( O
2.264 O
) O
twice O
with O
respect O
to O
µ O
and O
making O
use O
of O
the O
result O
( O
2.11 O
) O
for O
the O
mean B
of O
the O
binomial B
distribution I
prove O
the O
result O
( O
2.12 O
) O
for O
the O
variance B
of O
the O
binomial O
. O
finally O
, O
it O
is O
worth O
noting O
that O
the O
lower B
bound I
provides O
an O
alternative O
approach O
for O
deriving O
the O
variational B
re-estimation O
equations O
obtained O
in O
section O
10.2.1. O
to O
do O
this O
we O
use O
the O
fact O
that O
, O
since O
the O
model O
has O
conjugate B
priors O
, O
the O
functional B
form O
of O
the O
factors O
in O
the O
variational B
posterior O
distribution O
is O
known O
, O
namely O
discrete O
for O
z O
, O
dirichlet O
for O
π O
, O
and O
gaussian-wishart O
for O
( O
µk O
, O
λk O
) O
. O
this O
is O
achieved O
by O
using O
a O
network O
having O
the O
same O
number O
of O
outputs O
as O
inputs O
, O
and O
optimizing O
the O
weights O
so O
as O
to O
minimize O
some O
measure O
of O
the O
reconstruction O
error B
between O
inputs O
and O
outputs O
with O
respect O
to O
a O
set O
of O
training B
data O
. O
we O
can O
imagine O
that O
this O
is O
a O
damaged O
coin O
so O
that O
the O
probability B
of O
landing O
heads O
is O
not O
necessarily O
the O
same O
as O
that O
of O
landing O
tails O
. O
the O
graphs O
show O
the O
result O
of O
ﬁtting O
networks O
having O
m O
= O
1 O
, O
3 O
and O
10 O
hidden O
units O
, O
respectively O
, O
by O
minimizing O
a O
sum-of-squares B
error I
function O
using O
a O
scaled O
conjugate-gradient O
algorithm O
. O
( O
10.31 O
) O
then O
, O
using O
( O
10.26 O
) O
and O
( O
10.27 O
) O
, O
we O
obtain O
the O
ﬁrst O
and O
second B
order I
moments O
of O
10.1. O
variational B
inference I
473 O
qµ O
( O
µ O
) O
in O
the O
form O
e O
[ O
µ O
] O
= O
x O
, O
e O
[ O
µ2 O
] O
= O
x2 O
+ O
1 O
n O
e O
[ O
τ O
] O
. O
9.21 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
using O
the O
evidence O
framework O
of O
section O
3.5 O
, O
derive O
the O
m-step O
re-estimation O
equations O
for O
the O
parameter O
β O
in O
the O
bayesian O
linear B
regression I
model O
, O
analogous O
to O
the O
result O
( O
9.63 O
) O
for O
α O
. O
in O
physics O
terminology O
, O
the O
speciﬁc O
ar- O
rangements O
of O
objects O
in O
the O
bins O
is O
called O
a O
microstate B
, O
and O
the O
overall O
distribution O
of O
occupation O
numbers O
, O
expressed O
through O
the O
ratios O
ni/n O
, O
is O
called O
a O
macrostate B
. O
the O
initial O
message O
sent O
from O
the O
leaf O
node B
is O
simply O
the O
most O
probable O
value O
for O
xn O
is O
then O
given O
by O
µx1→f1,2 O
( O
x1 O
) O
= O
0 O
. O
, O
n O
, O
and O
deﬁne O
a O
matrix O
t O
whose O
nth O
row O
is O
the O
vector O
tt O
n O
, O
n. O
the O
sum-of-squares B
error I
function O
where O
, O
w O
is O
a O
matrix O
whose O
kth O
column O
comprises O
the O
d O
+ O
1-dimensional O
vector O
( O
cid:4 O
) O
wk O
= O
( O
wk0 O
, O
wt O
k O
) O
t O
and O
( O
cid:4 O
) O
x O
is O
the O
corresponding O
augmented O
input O
vector O
( O
1 O
, O
xt O
) O
t O
with O
new O
input O
x O
is O
then O
assigned O
to O
the O
class O
for O
which O
the O
output O
yk O
= O
( O
cid:4 O
) O
wt O
k O
( O
cid:4 O
) O
x O
is O
largest O
. O
in O
particular O
, O
sampling B
methods I
can O
be O
used O
to O
approximate O
the O
e O
step O
of O
the O
em O
algorithm O
for O
models O
in O
which O
the O
e O
step O
can O
not O
be O
performed O
analytically O
. O
the O
probability B
of O
x O
= O
1 O
will O
be O
denoted O
by O
the O
parameter O
µ O
so O
that O
p O
( O
x O
= O
1|µ O
) O
= O
µ O
( O
2.1 O
) O
2.1. O
binary O
variables O
69 O
where O
0 O
( O
cid:1 O
) O
µ O
( O
cid:1 O
) O
1 O
, O
from O
which O
it O
follows O
that O
p O
( O
x O
= O
0|µ O
) O
= O
1 O
− O
µ. O
the O
probability B
distribution O
over O
x O
can O
therefore O
be O
written O
in O
the O
form O
bern O
( O
x|µ O
) O
= O
µx O
( O
1 O
− O
µ O
) O
1−x O
( O
2.2 O
) O
exercise O
2.1 O
which O
is O
known O
as O
the O
bernoulli O
distribution O
. O
, O
wm O
are O
collectively O
denoted O
by O
the O
vector O
w. O
note O
that O
, O
although O
the O
polynomial O
function O
y O
( O
x O
, O
w O
) O
is O
a O
nonlinear O
function O
of O
x O
, O
it O
is O
a O
linear O
function O
of O
the O
coefﬁcients O
w. O
functions O
, O
such O
as O
the O
polynomial O
, O
which O
are O
linear O
in O
the O
unknown O
parameters O
have O
important O
properties O
and O
are O
called O
linear O
models O
and O
will O
be O
discussed O
extensively O
in O
chapters O
3 O
and O
4. O
the O
values O
of O
the O
coefﬁcients O
will O
be O
determined O
by O
ﬁtting O
the O
polynomial O
to O
the O
training B
data O
. O
this O
could O
be O
done O
by O
simply O
running O
the O
above O
algorithm O
afresh O
for O
each O
such O
node B
. O
furthermore O
, O
we O
are O
only O
interested O
in O
solutions O
for O
which O
all O
data O
points O
are O
cor- O
rectly O
classiﬁed O
, O
so O
that O
tny O
( O
xn O
) O
> O
0 O
for O
all O
n. O
thus O
the O
distance O
of O
a O
point O
xn O
to O
the O
decision B
surface I
is O
given O
by O
tny O
( O
xn O
) O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
= O
tn O
( O
wtφ O
( O
xn O
) O
+ O
b O
) O
( O
cid:12 O
) O
( O
cid:8 O
) O
( O
cid:10 O
) O
( O
cid:11 O
) O
( O
cid:9 O
) O
( O
cid:13 O
) O
. O
( O
8.8 O
) O
n=1 O
figure O
8.6 O
as O
in O
figure O
8.5 O
but O
with O
the O
nodes O
{ O
tn O
} O
shaded O
to O
indicate O
that O
the O
corresponding O
random O
vari- O
ables O
have O
been O
set O
to O
their O
observed O
( O
training B
set I
) O
values O
. O
we O
shall O
discuss O
how O
to O
evaluate O
the O
predictive B
distribution I
in O
later O
sections O
of O
this O
chap- O
ter O
. O
, O
xd O
, O
denoted O
collectively O
by O
the O
vector O
x O
, O
then O
we O
can O
deﬁne O
a O
joint O
probability B
density O
p O
( O
x O
) O
= O
p O
( O
x1 O
, O
. O
the O
objective O
function O
j O
is O
a O
quadratic O
function O
of O
µk O
, O
and O
it O
can O
be O
minimized O
by O
setting O
its O
derivative B
with O
respect O
to O
µk O
to O
zero O
giving O
n O
( O
cid:2 O
) O
n=1 O
2 O
µk O
= O
rnk O
( O
xn O
− O
µk O
) O
= O
0 O
( O
cid:5 O
) O
n O
rnkxn O
( O
cid:5 O
) O
. O
however O
, O
there O
is O
also O
interest O
in O
go- O
ing O
beyond O
the O
inference B
problem O
and O
learning B
the O
graph O
structure O
itself O
from O
data O
( O
friedman O
and O
koller O
, O
2003 O
) O
. O
we O
also O
showed O
that O
this O
error B
function I
could O
be O
motivated O
as O
the O
maximum B
likelihood I
solution O
under O
an O
assumed O
gaussian O
noise O
model O
. O
this O
conditional B
inde- O
pendence O
property O
again O
allows O
the O
formulation O
of O
a O
computationally O
efﬁcient O
learn- O
ing O
algorithm O
. O
we O
can O
gain O
further O
insight O
into O
the O
problems O
of O
high-dimensional O
spaces O
by O
returning O
to O
the O
example O
of O
polynomial B
curve I
ﬁtting I
and O
considering O
how O
we O
would O
section O
1.1 O
of O
figure O
1.21 O
illustration O
the O
curse B
of I
dimensionality I
, O
showing O
how O
the O
number O
of O
regions O
of O
a O
regular O
grid O
grows O
exponentially O
with O
the O
dimensionality O
d O
of O
the O
space O
. O
sometimes O
we O
will O
be O
considering O
expectations O
of O
functions O
of O
several O
variables O
, O
in O
which O
case O
we O
can O
use O
a O
subscript O
to O
indicate O
which O
variable O
is O
being O
averaged O
over O
, O
so O
that O
for O
instance O
( O
1.36 O
) O
denotes O
the O
average O
of O
the O
function O
f O
( O
x O
, O
y O
) O
with O
respect O
to O
the O
distribution O
of O
x. O
note O
that O
ex O
[ O
f O
( O
x O
, O
y O
) O
] O
will O
be O
a O
function O
of O
y. O
ex O
[ O
f O
( O
x O
, O
y O
) O
] O
we O
can O
also O
consider O
a O
conditional B
expectation I
with O
respect O
to O
a O
conditional B
distribution O
, O
so O
that O
ex O
[ O
f|y O
] O
= O
p O
( O
x|y O
) O
f O
( O
x O
) O
( O
cid:2 O
) O
( O
cid:8 O
) O
( O
f O
( O
x O
) O
− O
e O
[ O
f O
( O
x O
) O
] O
) O
2 O
( O
cid:9 O
) O
x O
with O
an O
analogous O
deﬁnition O
for O
continuous O
variables O
. O
we O
would O
like O
to O
choose O
a O
prior B
distribution O
that O
reﬂects O
this O
translation B
invariance I
property O
, O
and O
so O
we O
choose O
a O
prior B
that O
assigns O
equal O
probability B
mass O
to O
2.4. O
the O
exponential B
family I
119 O
an O
interval O
a O
( O
cid:1 O
) O
µ O
( O
cid:1 O
) O
b O
as O
to O
the O
shifted O
interval O
a O
− O
c O
( O
cid:1 O
) O
µ O
( O
cid:1 O
) O
b O
− O
c. O
this O
implies O
( O
cid:6 O
) O
b O
( O
cid:6 O
) O
b−c O
( O
cid:6 O
) O
b O
p O
( O
µ O
) O
dµ O
= O
a O
a−c O
p O
( O
µ O
) O
dµ O
= O
a O
p O
( O
µ O
− O
c O
) O
dµ O
( O
2.234 O
) O
and O
because O
this O
must O
hold O
for O
all O
choices O
of O
a O
and O
b O
, O
we O
have O
p O
( O
µ O
− O
c O
) O
= O
p O
( O
µ O
) O
( O
2.235 O
) O
which O
implies O
that O
p O
( O
µ O
) O
is O
constant O
. O
( O
c O
) O
find O
a O
regression B
function I
y O
( O
x O
) O
directly O
from O
the O
training B
data O
. O
4.1.5 O
relation O
to O
least O
squares O
the O
least-squares O
approach O
to O
the O
determination O
of O
a O
linear B
discriminant I
was O
based O
on O
the O
goal O
of O
making O
the O
model O
predictions O
as O
close O
as O
possible O
to O
a O
set O
of O
target O
values O
. O
( O
2.206 O
) O
( O
2.207 O
) O
( O
2.208 O
) O
note O
that O
the O
parameters O
ηk O
are O
not O
independent B
because O
the O
parameters O
µk O
are O
sub- O
ject O
to O
the O
constraint O
m O
( O
cid:2 O
) O
µk O
= O
1 O
k=1 O
( O
2.209 O
) O
so O
that O
, O
given O
any O
m O
− O
1 O
of O
the O
parameters O
µk O
, O
the O
value O
of O
the O
remaining O
parameter O
is O
ﬁxed O
. O
, O
n O
, O
together O
with O
corresponding O
target O
values O
{ O
tn O
} O
, O
the O
goal O
is O
to O
predict O
the O
value O
of O
t O
for O
a O
new O
value O
of O
x. O
in O
the O
simplest O
approach O
, O
this O
can O
be O
done O
by O
directly O
con- O
structing O
an O
appropriate O
function O
y O
( O
x O
) O
whose O
values O
for O
new O
inputs O
x O
constitute O
the O
predictions O
for O
the O
corresponding O
values O
of O
t. O
more O
generally O
, O
from O
a O
probabilistic O
perspective O
, O
we O
aim O
to O
model O
the O
predictive B
distribution I
p O
( O
t|x O
) O
because O
this O
expresses O
our O
uncertainty O
about O
the O
value O
of O
t O
for O
each O
value O
of O
x. O
from O
this O
conditional B
dis- O
tribution O
we O
can O
make O
predictions O
of O
t O
, O
for O
any O
new O
value O
of O
x O
, O
in O
such O
a O
way O
as O
to O
minimize O
the O
expected O
value O
of O
a O
suitably O
chosen O
loss B
function I
. O
there O
remains O
the O
problem O
of O
choosing O
the O
order O
m O
of O
the O
polynomial O
, O
and O
as O
we O
shall O
see O
this O
will O
turn O
out O
to O
be O
an O
example O
of O
an O
important O
concept O
called O
model B
comparison I
or O
model B
selection I
. O
( O
2001 O
) O
tries O
to O
ﬁnd O
a O
hyperplane O
that O
separates O
all O
but O
a O
ﬁxed O
fraction O
ν O
of O
the O
training B
data O
from O
the O
origin O
while O
at O
the O
same O
time O
maximizing O
the O
distance O
( O
margin B
) O
of O
the O
hyperplane O
from O
the O
origin O
, O
while O
tax O
and O
duin O
( O
1999 O
) O
look O
for O
the O
smallest O
sphere O
in O
feature B
space I
that O
contains O
all O
but O
a O
fraction O
ν O
of O
the O
data O
points O
. O
maximum B
likelihood I
from O
incomplete O
data O
via O
the O
em O
algorithm O
. O
we O
now O
modify O
this O
approach O
so O
that O
data O
points O
are O
allowed O
to O
be O
on O
the O
‘ O
wrong O
side O
’ O
of O
the O
margin B
boundary O
, O
but O
with O
a O
penalty O
that O
increases O
with O
the O
distance O
from O
that O
boundary O
. O
of O
course O
, O
successive O
samples O
from O
the O
markov O
chain O
will O
be O
highly O
correlated O
, O
and O
so O
to O
obtain O
samples O
that O
are O
nearly O
independent B
it O
will O
be O
necessary O
to O
subsample O
the O
sequence O
. O
( O
2.18 O
) O
we O
see O
that O
the O
effect O
of O
observing O
a O
data O
set O
of O
m O
observations O
of O
x O
= O
1 O
and O
l O
observations O
of O
x O
= O
0 O
has O
been O
to O
increase O
the O
value O
of O
a O
by O
m O
, O
and O
the O
value O
of O
b O
by O
l O
, O
in O
going O
from O
the O
prior B
distribution O
to O
the O
posterior O
distribution O
. O
thus O
, O
by O
changing O
the O
signs O
of O
a O
particular O
group O
of O
weights O
( O
and O
a O
bias B
) O
, O
the O
input–output O
mapping O
function O
represented O
by O
the O
network O
is O
unchanged O
, O
and O
so O
we O
have O
found O
two O
different O
weight O
vectors O
that O
give O
rise O
to O
the O
same O
mapping O
function O
. O
1.2.1 O
probability B
densities O
as O
well O
as O
considering O
probabilities O
deﬁned O
over O
discrete O
sets O
of O
events O
, O
we O
also O
wish O
to O
consider O
probabilities O
with O
respect O
to O
continuous O
variables O
. O
verify O
that O
the O
error B
function I
( O
5.21 O
) O
is O
obtained O
when O
 O
= O
0. O
note O
that O
this O
error B
function I
makes O
the O
model O
robust O
to O
incorrectly O
labelled O
data O
, O
in O
contrast O
to O
the O
usual O
error B
function I
. O
for O
instance O
in O
speech B
recognition I
, O
we O
might O
wish O
to O
ﬁnd O
the O
most O
probable O
phoneme O
sequence O
for O
a O
given O
series O
of O
acoustic O
observations O
. O
this O
does O
not O
hold O
for O
the O
transition B
probability I
constructed O
using O
( O
11.43 O
) O
, O
al- O
though O
by O
symmetrizing O
the O
order O
of O
application O
of O
the O
base O
transitions O
, O
in O
the O
form O
b1 O
, O
b2 O
, O
. O
in O
gen- O
eral O
, O
a O
gaussian O
process O
is O
deﬁned O
as O
a O
probability B
distribution O
over O
functions O
y O
( O
x O
) O
such O
that O
the O
set O
of O
values O
of O
y O
( O
x O
) O
evaluated O
at O
an O
arbitrary O
set O
of O
points O
x1 O
, O
. O
the O
last O
issue O
is O
particularly O
problematic O
in O
regression B
where O
we O
are O
typically O
aiming O
to O
model O
smooth O
functions O
, O
and O
yet O
the O
tree B
model O
produces O
piecewise-constant O
predictions O
with O
discontinuities O
at O
the O
split O
boundaries O
. O
however O
, O
the O
greater O
ﬂexibility O
of O
the O
variational B
approximation O
leads O
to O
improved O
accuracy O
compared O
to O
the O
laplace O
method O
. O
it O
was O
shown O
by O
tipping O
and O
bishop O
( O
1999b O
) O
that O
all O
of O
the O
stationary B
points O
of O
the O
log O
likelihood O
function O
can O
be O
written O
as O
( O
12.45 O
) O
where O
u O
m O
is O
a O
d O
x O
m O
matrix O
whose O
columns O
are O
given O
by O
any O
subset O
( O
of O
size O
m O
) O
of O
the O
eigenvectors O
of O
the O
data O
covariance O
matrix O
s O
, O
the O
m O
x O
m O
diagonal B
matrix O
l O
m O
has O
elements O
given O
by O
the O
corresponding O
eigenvalues O
..\ O
, O
and O
r O
is O
an O
arbitrary O
m O
x O
m O
orthogonal O
matrix O
. O
10.3.2 O
predictive B
distribution I
. O
2.49 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
using O
the O
deﬁnition O
( O
2.161 O
) O
of O
the O
multivariate O
student O
’ O
s O
t-distribution O
as O
a O
convolution O
of O
a O
gaussian O
with O
a O
gamma B
distribution I
, O
verify O
the O
properties O
( O
2.164 O
) O
, O
( O
2.165 O
) O
, O
and O
( O
2.166 O
) O
for O
the O
multivariate O
t-distribution O
deﬁned O
by O
( O
2.162 O
) O
. O
however O
, O
3.5. O
the O
evidence B
approximation I
165 O
a O
bayesian O
approach O
, O
like O
any O
approach O
to O
pattern O
recognition O
, O
needs O
to O
make O
as- O
sumptions O
about O
the O
form O
of O
the O
model O
, O
and O
if O
these O
are O
invalid O
then O
the O
results O
can O
be O
misleading O
. O
the O
speciﬁcation O
of O
the O
gaussian O
process O
is O
then O
completed O
by O
giving O
the O
covariance B
of O
y O
( O
x O
) O
evaluated O
at O
any O
two O
values O
of O
x O
, O
which O
is O
given O
by O
the O
kernel B
function I
e O
[ O
y O
( O
xn O
) O
y O
( O
xm O
) O
] O
= O
k O
( O
xn O
, O
xm O
) O
. O
the O
results O
above O
suggest O
a O
simple O
way O
of O
achieving O
this O
, O
namely O
by O
taking O
the O
available O
data O
and O
partitioning O
it O
into O
a O
training B
set I
, O
used O
to O
determine O
the O
coefﬁcients O
w O
, O
and O
a O
separate O
validation B
set I
, O
also O
called O
a O
hold-out B
set I
, O
used O
to O
optimize O
the O
model O
complexity O
( O
either O
m O
or O
λ O
) O
. O
furthermore O
, O
consider O
the O
case O
of O
independent B
, O
identically O
distributed O
data O
so O
that O
h O
is O
the O
sum O
of O
terms O
one O
for O
each O
data O
point O
. O
( O
1.133 O
) O
the O
coefﬁcients O
wi1i2···im O
comprise O
dm O
elements O
, O
but O
the O
number O
of O
independent B
parameters O
is O
signiﬁcantly O
fewer O
due O
to O
the O
many O
interchange O
symmetries B
of O
the O
factor O
xi1xi2 O
··· O
xim O
. O
10.28 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
rewrite O
the O
model O
for O
the O
bayesian O
mixture O
of O
gaussians O
, O
introduced O
in O
section O
10.2 O
, O
as O
a O
conjugate B
model O
from O
the O
exponential B
family I
, O
as O
discussed O
in O
section O
10.4. O
hence O
use O
the O
general O
results O
( O
10.115 O
) O
and O
( O
10.119 O
) O
to O
derive O
the O
speciﬁc O
results O
( O
10.48 O
) O
, O
( O
10.57 O
) O
, O
and O
( O
10.59 O
) O
. O
this O
is O
exploited O
in O
a O
general O
latent B
variable I
model O
called O
a O
density B
network I
( O
mackay O
, O
1995 O
; O
mackay O
and O
gibbs O
, O
1999 O
) O
in O
which O
the O
nonlinear O
function O
is O
governed O
by O
a O
multilayered O
neural B
network I
. O
7.1.2 O
relation O
to O
logistic O
regression B
. O
in O
the O
case O
of O
applications O
to O
probabilistic O
in- O
ference O
, O
the O
restriction O
may O
for O
example O
take O
the O
form O
of O
factorization B
assumptions O
( O
jordan O
et O
al. O
, O
1999 O
; O
jaakkola O
, O
2001 O
) O
. O
consider O
ﬁrst O
the O
case O
of O
conditional B
distributions O
. O
as O
in O
the O
case O
of O
rejection B
sampling I
, O
the O
sampling-importance-resampling B
( O
sir O
) O
approach O
also O
makes O
use O
of O
a O
sampling O
distribution O
q O
( O
z O
) O
but O
avoids O
having O
to O
de- O
termine O
the O
constant O
k. O
there O
are O
two O
stages O
to O
the O
scheme O
. O
values O
of O
x O
( O
cid:2 O
) O
bx O
are O
classiﬁed O
as O
class O
c2 O
and O
hence O
belong O
to O
decision B
region I
r2 O
, O
whereas O
points O
x O
< O
bx O
are O
classiﬁed O
as O
c1 O
and O
belong O
to O
r1 O
. O
an O
example O
is O
shown O
in O
figure O
13.18. O
this O
extends O
the O
hmm O
framework O
to O
the O
domain O
of O
supervised O
learn- O
ing O
for O
sequential B
data I
. O
once O
it O
is O
normalized O
to O
give O
a O
variational B
posterior O
distribution O
q O
( O
w O
) O
, O
however O
, O
it O
no O
longer O
represents O
a O
bound O
. O
the O
entropy B
in O
this O
case O
is O
given O
by O
− O
4 O
64 O
64 O
, O
1 O
64 O
, O
1 O
− O
1 O
1 O
2 O
4 O
1 O
4 O
− O
1 O
8 O
1 O
8 O
− O
1 O
16 O
log2 O
= O
2 O
bits B
. O
show O
that O
if O
the O
prior B
over O
parameters O
is O
gaussian O
of O
the O
form O
p O
( O
θ O
) O
= O
n O
( O
θ|m O
, O
v0 O
) O
, O
the O
log O
model O
evidence O
under O
the O
laplace O
approximation O
takes O
the O
form O
ln O
p O
( O
d O
) O
( O
cid:7 O
) O
ln O
p O
( O
d|θmap O
) O
− O
1 O
ln|h| O
+ O
const O
2 O
where O
h O
is O
the O
matrix O
of O
second O
derivatives O
of O
the O
log O
likelihood O
ln O
p O
( O
d|θ O
) O
evaluated O
at O
θmap O
. O
for O
the O
bayesian O
logistic B
regression I
model O
, O
the O
marginal B
likelihood I
takes O
the O
form O
( O
cid:6 O
) O
( O
cid:31 O
) O
n O
( O
cid:14 O
) O
p O
( O
tn|w O
) O
p O
( O
w O
) O
dw O
. O
an O
alternative O
‘ O
local B
’ O
approach O
involves O
ﬁnding O
bounds O
on O
functions O
over O
individual O
variables O
or O
groups O
of O
variables O
within O
a O
model O
. O
, O
w9 O
, O
and O
so O
can O
be O
tuned O
exactly O
to O
the O
10 O
data O
points O
in O
the O
training B
set I
. O
here O
we O
shall O
focus O
on O
the O
problem O
of O
evaluating O
local B
marginals O
over O
nodes O
or O
subsets O
of O
nodes O
, O
which O
will O
lead O
us O
to O
the O
sum-product B
algorithm I
. O
we O
also O
need O
to O
determine O
the O
parameters O
θ O
of O
the O
covariance B
function O
. O
in O
a O
fully O
bayesian O
approach O
, O
we O
should O
consistently O
apply O
the O
sum O
and O
product O
rules O
of O
probability B
, O
which O
requires O
, O
as O
we O
shall O
see O
shortly O
, O
that O
we O
integrate O
over O
all O
val- O
ues O
of O
w. O
such O
marginalizations O
lie O
at O
the O
heart O
of O
bayesian O
methods O
for O
pattern O
recognition O
. O
( O
10.55 O
) O
k O
( O
cid:14 O
) O
k=1 O
478 O
10. O
approximate O
inference B
identifying O
the O
terms O
on O
the O
right-hand O
side O
of O
( O
10.54 O
) O
that O
depend O
on O
π O
, O
we O
have O
ln O
q O
( O
cid:1 O
) O
( O
π O
) O
= O
( O
α0 O
− O
1 O
) O
ln O
πk O
+ O
rnk O
ln O
πk O
+ O
const O
( O
10.56 O
) O
k O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
k=1 O
k=1 O
n=1 O
where O
we O
have O
used O
( O
10.50 O
) O
. O
8.12 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
there O
are O
2m O
( O
m−1 O
) O
/2 O
distinct O
undirected B
graphs O
over O
a O
set O
of O
m O
distinct O
random O
variables O
. O
this O
is O
useful O
, O
for O
instance O
, O
when O
a O
is O
large O
and O
diagonal B
, O
and O
hence O
easy O
to O
invert O
, O
while O
b O
has O
many O
rows O
but O
few O
columns O
( O
and O
conversely O
for O
c O
) O
so O
that O
the O
right-hand O
side O
is O
much O
cheaper O
to O
evaluate O
than O
the O
left-hand O
side O
. O
suppose O
that O
a O
tree B
model O
a O
splits O
these O
into O
( O
300 O
, O
100 O
) O
at O
the O
ﬁrst O
leaf O
node B
and O
( O
100 O
, O
300 O
) O
at O
the O
second O
leaf O
node B
, O
where O
( O
n O
, O
m O
) O
denotes O
that O
n O
points O
are O
assigned O
to O
c1 O
and O
m O
points O
are O
assigned O
to O
c2 O
. O
5.15 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
section O
5.3.4 O
, O
we O
derived O
a O
procedure O
for O
evaluating O
the O
jacobian O
matrix O
of O
a O
neural B
network I
using O
a O
backpropagation B
procedure O
. O
conditional B
independence I
in O
statistical O
theory O
( O
with O
discussion O
) O
. O
1.28 O
( O
( O
cid:1 O
) O
) O
in O
section O
1.6 O
, O
we O
introduced O
the O
idea O
of O
entropy B
h O
( O
x O
) O
as O
the O
information O
gained O
on O
observing O
the O
value O
of O
a O
random O
variable O
x O
having O
distribution O
p O
( O
x O
) O
. O
then O
we O
define O
, O
for O
each O
data O
point O
x O
n O
, O
a O
transformed O
value O
given O
by O
where O
x O
is O
the O
sample B
mean I
defined O
by O
( O
12.1 O
) O
. O
furthermore O
, O
the O
posterior O
distribution O
can O
act O
as O
the O
prior B
if O
we O
subsequently O
observe O
additional O
data O
. O
an O
introduction O
to O
computational B
learning I
theory I
. O
consider O
the O
prob- O
lem O
of O
ﬁnding O
the O
mean B
µ O
of O
a O
single O
gaussian O
random O
variable O
x O
, O
in O
which O
we O
are O
given O
a O
set O
of O
independent B
observations O
{ O
x1 O
, O
. O
because O
each O
eigenvector O
of O
the O
covariance B
matrix I
is O
a O
vector O
exercise O
12.2 O
appendix O
a O
566 O
12. O
col\'tinuolis O
latf O
; O
i\'t O
\'ariaiiles O
figure O
12.3 O
the O
mean B
~ O
' O
'' O
x O
aklog O
with O
! O
he O
ii O
'' O
't O
lou O
' O
pca O
e O
; O
gerrvecl O
< O
) O
rll O
ul O
, O
. O
this O
has O
computational O
com- O
plexity O
o O
( O
n O
k O
2m O
) O
that O
is O
exponential O
in O
the O
number O
m O
of O
latent O
chains O
and O
so O
will O
be O
intractable O
for O
anything O
other O
than O
small O
values O
of O
m. O
one O
solution O
would O
be O
to O
use O
sampling B
methods I
( O
discussed O
in O
chapter O
11 O
) O
. O
( O
cid:2 O
) O
xn∈rτ O
1 O
nτ O
( O
cid:2 O
) O
xn∈rτ O
|t| O
( O
cid:2 O
) O
( O
14.30 O
) O
( O
14.31 O
) O
the O
pruning O
criterion O
is O
then O
given O
by O
c O
( O
t O
) O
= O
qτ O
( O
t O
) O
+ O
λ|t| O
τ O
=1 O
the O
regularization B
parameter O
λ O
determines O
the O
trade-off O
between O
the O
overall O
residual O
sum-of-squares B
error I
and O
the O
complexity O
of O
the O
model O
as O
measured O
by O
the O
number O
|t| O
of O
leaf O
nodes O
, O
and O
its O
value O
is O
chosen O
by O
cross-validation B
. O
5.19 O
( O
( O
cid:12 O
) O
) O
www O
derive O
the O
expression O
( O
5.85 O
) O
for O
the O
outer B
product I
approximation I
to O
the O
hessian O
matrix O
for O
a O
network O
having O
a O
single O
output O
with O
a O
logistic B
sigmoid I
output-unit O
activation B
function I
and O
a O
cross-entropy B
error I
function I
, O
corresponding O
to O
the O
result O
( O
5.84 O
) O
for O
the O
sum-of-squares B
error I
function O
. O
in O
contrast O
to O
maximum B
likelihood I
methods O
, O
no O
independent B
data O
set O
is O
required O
in O
order O
to O
optimize O
the O
model O
complexity O
. O
here O
φ O
is O
an O
n×m O
matrix O
, O
called O
the O
design B
matrix I
, O
whose O
elements O
are O
given O
by O
φnj O
= O
φj O
( O
xn O
) O
, O
so O
that O
φtφ O
⎛⎜⎜⎝ O
φ0 O
( O
x1 O
) O
φ0 O
( O
x2 O
) O
... O
φ1 O
( O
x1 O
) O
φ1 O
( O
x2 O
) O
... O
† O
≡ O
( O
cid:10 O
) O
φm−1 O
( O
x1 O
) O
φm−1 O
( O
x2 O
) O
··· O
··· O
... O
··· O
φm−1 O
( O
xn O
) O
... O
( O
cid:11 O
) O
−1 O
φt O
φ0 O
( O
xn O
) O
φ1 O
( O
xn O
) O
⎞⎟⎟⎠ O
. O
such O
a O
procedure O
can O
be O
motivated O
from O
a O
frequentist B
perspective O
by O
considering O
the O
trade-off O
between O
bias B
and O
variance B
, O
which O
decomposes O
the O
er- O
ror O
due O
to O
a O
model O
into O
the O
bias B
component O
that O
arises O
from O
differences O
between O
the O
model O
and O
the O
true O
function O
to O
be O
predicted O
, O
and O
the O
variance B
component O
that O
repre- O
sents O
the O
sensitivity O
of O
the O
model O
to O
the O
individual O
data O
points O
. O
2 O
) O
( O
z2 O
1 O
, O
2 O
) O
t O
√ O
( O
6.12 O
) O
2 O
) O
t O
and O
we O
see O
that O
the O
feature O
mapping O
takes O
the O
form O
φ O
( O
x O
) O
= O
( O
x2 O
1 O
, O
therefore O
comprises O
all O
possible O
second B
order I
terms O
, O
with O
a O
speciﬁc O
weighting O
be- O
tween O
them O
. O
note O
that O
the O
logistic O
and O
probit B
regression I
models O
behave O
differently O
in O
this O
respect O
because O
the O
tails O
of O
the O
logistic B
sigmoid I
decay O
asymptotically O
like O
exp O
( O
−x O
) O
for O
x O
→ O
∞ O
, O
whereas O
for O
the O
probit O
activation O
function O
they O
decay O
like O
exp O
( O
−x2 O
) O
, O
and O
so O
the O
probit O
model O
can O
be O
signiﬁcantly O
more O
sensitive O
to O
outliers B
. O
, O
tn O
} O
is O
expressed O
through O
the O
conditional B
probability I
p O
( O
d|w O
) O
, O
and O
we O
shall O
see O
later O
, O
in O
section O
1.2.5 O
, O
how O
this O
can O
be O
represented O
explicitly O
. O
in O
isometric O
feature O
mapping O
, O
or O
isomap B
( O
tenenbaum O
et O
ai. O
, O
2000 O
) O
, O
the O
goal O
is O
to O
project O
the O
data O
to O
a O
lower-dimensional O
space O
using O
mds O
, O
but O
where O
the O
dissim O
( O
cid:173 O
) O
ilarities O
are O
defined O
in O
terms O
of O
the O
geodesic O
distances O
measured O
along O
the O
mani- O
12.4. O
nonlinear O
latent B
variable I
models O
597 O
fold O
. O
gtm O
: O
the O
generative B
topographic I
mapping I
. O
for O
α O
= O
0 O
, O
the O
mode O
of O
the O
poste- O
rior O
is O
given O
by O
the O
maximum B
likelihood I
solution O
wml O
, O
whereas O
for O
nonzero O
α O
the O
mode O
is O
at O
wmap O
= O
mn O
. O
if O
we O
have O
k O
separate O
binary O
classiﬁcations O
to O
perform O
, O
then O
we O
can O
use O
a O
net- O
work O
having O
k O
outputs O
each O
of O
which O
has O
a O
logistic B
sigmoid I
activation O
function O
. O
for O
example O
, O
the O
switching B
state I
space I
model I
( O
ghahramani O
and O
hinton O
, O
1998 O
) O
can O
be O
viewed O
as O
a O
combination O
of O
the O
hidden O
markov O
model O
with O
a O
set O
of O
linear O
dynamical O
systems O
. O
this O
is O
offset O
, O
however O
, O
by O
the O
avoidance O
of O
cross-validation B
runs O
to O
set O
the O
model O
complexity O
parameters O
. O
10.4 O
exponential B
family I
distributions O
. O
the O
von O
mises O
dis- O
tribution O
is O
plotted O
in O
figure O
2.19 O
, O
and O
the O
function O
i0 O
( O
m O
) O
is O
plotted O
in O
figure O
2.20. O
now O
consider O
the O
maximum B
likelihood I
estimators O
for O
the O
parameters O
θ0 O
and O
m O
for O
the O
von O
mises O
distribution O
. O
to O
compute O
the O
new O
parameter O
values O
we O
maximize O
with O
respect O
to O
α O
and O
β O
to O
give O
450 O
9. O
mixture B
models O
and O
em O
αnew O
i O
= O
( O
βnew O
) O
−1 O
= O
1 O
i O
+ O
σii O
m2 O
( O
cid:5 O
) O
t O
− O
φmn O
( O
cid:5 O
) O
2 O
+ O
β O
−1 O
n O
( O
cid:5 O
) O
i O
γi O
( O
9.67 O
) O
( O
9.68 O
) O
exercise O
9.23 O
these O
re-estimation O
equations O
are O
formally O
equivalent O
to O
those O
obtained O
by O
direct O
maxmization O
. O
for O
the O
purposes O
of O
this O
simple O
illustration O
, O
we O
have O
ﬁxed O
the O
parameters O
to O
be O
β O
= O
1.0 O
, O
η O
= O
2.1 O
and O
h O
= O
0. O
note O
that O
leaving O
h O
= O
0 O
simply O
means O
that O
the O
prior B
probabilities O
of O
the O
two O
states O
of O
xi O
are O
equal O
. O
we O
have O
already O
shown O
in O
exercise O
2.5 O
that O
the O
beta B
distribution I
, O
which O
is O
a O
special O
case O
of O
the O
dirichlet O
for O
m O
= O
2 O
, O
is O
normalized O
. O
10.7.2 O
expectation B
propagation I
on O
graphs O
so O
far O
in O
our O
general O
discussion O
of O
ep O
, O
we O
have O
allowed O
the O
factors O
fi O
( O
θ O
) O
in O
the O
distribution O
p O
( O
θ O
) O
to O
be O
functions O
of O
all O
of O
the O
components O
of O
θ O
, O
and O
similarly O
for O
the O
approximating O
factors O
( O
cid:4 O
) O
f O
( O
θ O
) O
in O
the O
approximating O
distribution O
q O
( O
θ O
) O
. O
bayesian O
classiﬁcation B
with O
gaussian O
processes O
. O
it O
says O
that O
the O
log O
of O
the O
optimal O
so- O
lution O
for O
factor O
qj O
is O
obtained O
simply O
by O
considering O
the O
log O
of O
the O
joint O
distribution O
over O
all O
hidden O
and O
visible O
variables O
and O
then O
taking O
the O
expectation B
with O
respect O
to O
all O
of O
the O
other O
factors O
{ O
qi O
} O
for O
i O
( O
cid:9 O
) O
= O
j O
. O
here O
we O
have O
considered O
a O
batch O
learning O
context O
in O
which O
all O
of O
the O
training B
data O
is O
available O
at O
once O
. O
the O
ﬁrst O
layer O
of O
the O
network O
can O
be O
viewed O
as O
performing O
a O
nonlinear O
feature B
extraction I
, O
and O
the O
sharing O
of O
features O
between O
the O
different O
outputs O
can O
save O
on O
computation O
and O
can O
also O
lead O
to O
improved O
generalization B
. O
derive O
the O
variational B
update O
equations O
for O
the O
three O
factors O
in O
the O
variational B
distribution O
and O
also O
obtain O
an O
expression O
for O
the O
lower B
bound I
and O
for O
the O
predictive B
distribution I
. O
14.1. O
bayesian O
model B
averaging I
it O
is O
important O
to O
distinguish O
between O
model O
combination O
methods O
and O
bayesian O
model B
averaging I
, O
as O
the O
two O
are O
often O
confused O
. O
however O
, O
neural O
networks O
have O
also O
been O
applied O
to O
un O
( O
cid:173 O
) O
supervised B
learning I
where O
they O
have O
been O
used O
for O
dimensionality O
reduction O
. O
multinomial O
if O
we O
generalize O
the O
bernoulli O
distribution O
to O
an O
k-dimensional O
binary O
variable O
x O
with O
components O
xk O
∈ O
{ O
0 O
, O
1 O
} O
such O
that O
k O
xk O
= O
1 O
, O
then O
we O
obtain O
the O
following O
discrete O
distribution O
( O
cid:5 O
) O
k O
( O
cid:14 O
) O
p O
( O
x O
) O
= O
µxk O
k O
k=1 O
e O
[ O
xk O
] O
= O
µk O
var O
[ O
xk O
] O
= O
µk O
( O
1 O
− O
µk O
) O
cov O
[ O
xjxk O
] O
= O
ijkµk O
h O
[ O
x O
] O
= O
− O
m O
( O
cid:2 O
) O
µk O
ln O
µk O
k=1 O
( O
b.54 O
) O
( O
b.55 O
) O
( O
b.56 O
) O
( O
b.57 O
) O
( O
b.58 O
) O
b. O
probability B
distributions O
691 O
where O
ijk O
is O
the O
j O
, O
k O
element O
of O
the O
identity O
matrix O
. O
we O
then O
use O
this O
posterior O
distribution O
to O
ﬁnd O
the O
expectation B
of O
the O
complete-data O
log O
likelihood O
evaluated O
for O
some O
general O
parameter O
value O
θ. O
this O
expectation B
, O
denoted O
q O
( O
θ O
, O
θold O
) O
, O
is O
given O
by O
q O
( O
θ O
, O
θold O
) O
= O
p O
( O
z|x O
, O
θold O
) O
ln O
p O
( O
x O
, O
z|θ O
) O
. O
4.2.2 O
maximum B
likelihood I
solution O
once O
we O
have O
speciﬁed O
a O
parametric O
functional B
form O
for O
the O
class-conditional O
densities O
p O
( O
x|ck O
) O
, O
we O
can O
then O
determine O
the O
values O
of O
the O
parameters O
, O
together O
with O
the O
prior B
class O
probabilities O
p O
( O
ck O
) O
, O
using O
maximum B
likelihood I
. O
again O
, O
this O
will O
involve O
expressing O
the O
joint O
distribution O
p O
( O
x O
) O
as O
a O
product O
of O
functions O
deﬁned O
over O
sets O
of O
variables O
that O
are O
local B
to O
the O
graph O
. O
( O
10.182 O
) O
( O
10.183 O
) O
10.7. O
expectation B
propagation I
we O
conclude O
this O
chapter O
by O
discussing O
an O
alternative O
form O
of O
deterministic O
approx- O
imate O
inference B
, O
known O
as O
expectation B
propagation I
or O
ep O
( O
minka O
, O
2001a O
; O
minka O
, O
2001b O
) O
. O
using O
( O
12.76 O
) O
and O
( O
12.80 O
) O
, O
we O
have O
1 O
= O
v O
; O
vi O
= O
l O
l O
n O
n O
n=l O
m=l O
ainaim¢ O
( O
xn O
) O
t¢ O
( O
xm O
) O
= O
a O
; O
k~ O
= O
aina O
; O
ai' O
( O
12.81 O
) O
having O
solved O
the O
eigenvector O
problem O
, O
the O
resulting O
principal O
component O
pro O
( O
cid:173 O
) O
jections O
can O
then O
also O
be O
cast O
in O
terms O
of O
the O
kernel B
function I
so O
that O
, O
using O
( O
12.76 O
) O
, O
the O
projection O
of O
a O
point O
x O
onto O
eigenvector O
i O
is O
given O
by O
yi O
( O
x O
) O
= O
¢ O
( O
x O
) O
tvi O
= O
l O
ain¢ O
( O
x O
) O
t O
¢ O
( O
xn O
) O
= O
l O
aink O
( O
x O
, O
x O
n O
) O
n O
n O
n=l O
n=l O
( O
12.82 O
) O
and O
so O
again O
is O
expressed O
in O
terms O
of O
the O
kernel B
function I
. O
consider O
ﬁrst O
the O
problem O
of O
taking O
a O
model O
that O
is O
speciﬁed O
using O
a O
directed B
graph O
and O
trying O
to O
convert O
it O
to O
an O
undirected B
graph I
. O
another O
generalization B
is O
to O
group O
factors O
fi O
( O
θi O
) O
together O
into O
sets O
and O
to O
reﬁne O
all O
the O
factors O
in O
a O
set O
together O
at O
each O
iteration O
. O
distribution O
( O
a O
) O
1 O
z2 O
0.5 O
0 O
0 O
1 O
z2 O
0.5 O
0 O
0 O
z1 O
1 O
0.5 O
( O
a O
) O
z1 O
1 O
0.5 O
( O
b O
) O
is O
used O
in O
an O
alternative O
approximate O
inference B
framework O
called O
expectation B
prop- O
agation O
. O
this O
process O
can O
be O
repeated O
l O
times O
to O
generate O
l O
data O
sets O
each O
of O
size O
n O
and O
each O
obtained O
by O
sampling O
from O
the O
original O
data O
set O
x. O
the O
statistical O
accuracy O
of O
parameter O
estimates O
can O
then O
be O
evaluated O
by O
looking O
at O
the O
variability O
of O
predictions O
between O
the O
different O
bootstrap B
data O
sets O
. O
we O
see O
that O
there O
is O
an O
intimate O
relationship O
between O
data B
compression I
and O
den- O
sity O
estimation O
( O
i.e. O
, O
the O
problem O
of O
modelling O
an O
unknown O
probability B
distribution O
) O
because O
the O
most O
efﬁcient O
compression O
is O
achieved O
when O
we O
know O
the O
true O
distri- O
bution O
. O
finally O
, O
metric O
mds O
is O
applied O
to O
the O
geodesic B
distance I
matrix O
to O
find O
the O
low-dimensional O
projection O
. O
furthermore O
, O
the O
error B
function I
can O
be O
minimized O
by O
an O
efﬁcient O
iterative O
technique O
based O
on O
the O
newton-raphson O
iterative O
optimization O
scheme O
, O
which O
uses O
a O
local B
quadratic O
approximation O
to O
the O
log O
likelihood O
function O
. O
in O
practice O
, O
of O
course O
, O
we O
have O
only O
a O
single O
data O
set O
, O
and O
so O
we O
have O
to O
ﬁnd O
a O
way O
to O
introduce O
variability O
between O
the O
different O
models O
within O
the O
committee B
. O
assuming O
independent B
and O
identically O
distributed O
data O
, O
write O
down O
the O
error B
function I
corresponding O
to O
the O
negative O
log O
likelihood O
. O
similarly O
, O
to O
evaluate O
p O
( O
x2 O
) O
we O
need O
to O
propagate O
a O
messages O
µβ O
( O
· O
) O
from O
node B
xn O
back O
to O
node B
x3 O
. O
next O
we O
turn O
to O
the O
problem O
of O
ﬁnding O
the O
marginal B
for O
a O
node B
zn O
given O
all O
observations O
x1 O
to O
xn O
. O
using O
the O
matrix O
identities O
( O
c.7 O
) O
and O
( O
c.15 O
) O
, O
the O
determinant O
and O
inverse B
of O
c O
can O
then O
be O
written O
|c| O
= O
|c−i||1 O
+ O
α O
−1−i O
− O
c O
c−1 O
= O
c O
−1−i O
ϕi| O
−1 O
i O
ϕt O
i O
c O
−1−i O
ϕiϕt O
−1−i O
i O
c O
−1−i O
ϕi O
αi O
+ O
ϕt O
i O
c O
. O
this O
ensures O
that O
inference B
about O
variables O
will O
be O
consistent B
across O
the O
graph O
. O
this O
process O
is O
often O
called O
forward B
propagation I
because O
it O
can O
be O
regarded O
as O
a O
forward O
ﬂow O
of O
information O
through O
the O
network O
. O
the O
average O
prediction O
is O
estimated O
from O
y O
( O
x O
) O
= O
1 O
l O
l O
( O
cid:2 O
) O
l=1 O
y O
( O
l O
) O
( O
x O
) O
and O
the O
integrated O
squared O
bias B
and O
integrated O
variance B
are O
then O
given O
by O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
l O
( O
cid:2 O
) O
( O
cid:26 O
) O
1 O
l O
n=1 O
l=1 O
( O
bias B
) O
2 O
= O
variance B
= O
1 O
n O
1 O
n O
{ O
y O
( O
xn O
) O
− O
h O
( O
xn O
) O
} O
2 O
( O
cid:27 O
) O
2 O
y O
( O
l O
) O
( O
xn O
) O
− O
y O
( O
xn O
) O
( O
3.45 O
) O
( O
3.46 O
) O
( O
3.47 O
) O
where O
the O
integral O
over O
x O
weighted O
by O
the O
distribution O
p O
( O
x O
) O
is O
approximated O
by O
a O
ﬁnite O
sum O
over O
data O
points O
drawn O
from O
that O
distribution O
. O
this O
type O
of O
model O
is O
illustrated O
using O
a O
lattice B
diagram I
in O
figure O
13.10. O
many O
applications O
of O
hidden O
markov O
models O
, O
for O
example O
speech B
recognition I
, O
or O
on-line O
character O
recognition O
, O
make O
use O
of O
left-to-right B
architectures O
. O
as O
with O
gradient-based O
approaches O
for O
maximizing O
the O
log O
like- O
lihood O
, O
techniques O
must O
be O
employed O
to O
avoid O
singularities B
of O
the O
likelihood B
function I
in O
which O
a O
gaussian O
component O
collapses O
onto O
a O
particular O
data O
point O
. O
training B
can O
therefore O
be O
stopped O
at O
the O
point O
of O
smallest O
error B
with O
respect O
to O
the O
validation O
data O
set O
, O
as O
indicated O
in O
figure O
5.12 O
, O
in O
order O
to O
obtain O
a O
network O
having O
good O
generalization B
performance O
. O
( O
a O
) O
from O
the O
leaf O
nodes O
x1 O
and O
x4 O
towards O
the O
root B
node I
x3 O
. O
for O
a O
more O
extensive O
discus- O
sion B
of O
‘ O
kernel O
engineering O
’ O
, O
see O
shawe-taylor O
and O
cristianini O
( O
2004 O
) O
. O
similarly O
, O
for O
the O
second B
order I
moment O
e O
[ O
x2 O
] O
= O
−∞ O
x2 O
dx O
= O
µ2 O
+ O
σ2 O
. O
the O
structure O
of O
a O
convolutional B
network O
is O
illustrated O
in O
figure O
5.17. O
in O
the O
convolutional B
layer O
the O
units O
are O
organized O
into O
planes O
, O
each O
of O
which O
is O
called O
a O
feature B
map I
. O
this O
can O
be O
done O
using O
iterative B
reweighted I
least I
squares I
( O
irls O
) O
as O
discussed O
in O
section O
4.3.3. O
for O
this O
, O
we O
need O
the O
gradient O
vector O
and O
hessian O
matrix O
of O
the O
log O
posterior O
distribution O
, O
which O
from O
( O
7.109 O
) O
are O
given O
by O
∇∇ O
ln O
p O
( O
w|t O
, O
α O
) O
= O
− O
( O
cid:10 O
) O
( O
cid:11 O
) O
∇ O
ln O
p O
( O
w|t O
, O
α O
) O
= O
φt O
( O
t O
− O
y O
) O
− O
aw O
φtbφ O
+ O
a O
( O
7.110 O
) O
( O
7.111 O
) O
where O
b O
is O
an O
n O
× O
n O
diagonal B
matrix O
with O
elements O
bn O
= O
yn O
( O
1 O
− O
yn O
) O
, O
the O
vector O
y O
= O
( O
y1 O
, O
. O
the O
k O
classes O
themselves O
are O
represented O
as O
particular O
sets O
of O
responses O
from O
the O
two-class O
classiﬁers O
chosen O
, O
and O
together O
with O
a O
suitable O
decoding O
scheme O
, O
this O
gives O
robustness B
to O
errors O
and O
to O
ambiguity O
in O
the O
outputs O
of O
the O
individual O
classiﬁers O
. O
similarly O
, O
the O
marginal B
distribution O
of O
either O
set O
is O
also O
gaussian O
. O
thus O
the O
density B
for O
class O
ck O
is O
given O
by O
( O
cid:12 O
) O
( O
cid:13 O
) O
−1 O
( O
x O
− O
µk O
) O
p O
( O
x|ck O
) O
= O
1 O
( O
2π O
) O
d/2 O
1 O
|σ|1/2 O
exp O
−1 O
2 O
( O
x O
− O
µk O
) O
tς O
consider O
ﬁrst O
the O
case O
of O
two O
classes O
. O
show O
also O
that O
, O
if O
the O
kernel O
is O
chosen O
to O
be O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
xtx O
( O
cid:4 O
) O
, O
then O
the O
classiﬁcation B
rule O
reduces O
to O
simply O
assigning O
a O
new O
input O
vector O
to O
the O
class O
having O
the O
closest O
mean B
. O
wlf O
( O
z O
( O
l O
) O
) O
= O
l=1 O
l=1 O
= O
1 O
zq O
( O
cid:7 O
) O
1 O
l O
( O
11.21 O
) O
( O
11.22 O
) O
( O
11.23 O
) O
zp/zq O
with O
the O
result O
zp O
zq O
and O
hence O
where O
we O
have O
deﬁned O
wl O
= O
as O
with O
rejection B
sampling I
, O
the O
success O
of O
the O
importance B
sampling I
approach O
depends O
crucially O
on O
how O
well O
the O
sampling O
distribution O
q O
( O
z O
) O
matches O
the O
desired O
534 O
11. O
sampling B
methods I
distribution O
p O
( O
z O
) O
. O
its O
density B
is O
ﬁnite O
if O
a O
( O
cid:2 O
) O
1 O
and O
b O
( O
cid:2 O
) O
1 O
, O
otherwise O
there O
is O
a O
singularity O
at O
µ O
= O
0 O
and/or O
µ O
= O
1. O
for O
a O
= O
b O
= O
1 O
, O
it O
reduces O
to O
a O
uniform B
distribution I
. O
in O
a O
frequentist B
setting O
, O
w O
is O
considered O
to O
be O
a O
ﬁxed O
parameter O
, O
whose O
value O
is O
determined O
by O
some O
form O
of O
‘ O
estimator O
’ O
, O
and O
error B
bars O
1.2. O
probability B
theory O
23 O
on O
this O
estimate O
are O
obtained O
by O
considering O
the O
distribution O
of O
possible O
data O
sets O
d. O
by O
contrast O
, O
from O
the O
bayesian O
viewpoint O
there O
is O
only O
a O
single O
data O
set O
d O
( O
namely O
the O
one O
that O
is O
actually O
observed O
) O
, O
and O
the O
uncertainty O
in O
the O
parameters O
is O
expressed O
through O
a O
probability B
distribution O
over O
w. O
a O
widely O
used O
frequentist B
estimator O
is O
maximum B
likelihood I
, O
in O
which O
w O
is O
set O
to O
the O
value O
that O
maximizes O
the O
likelihood B
function I
p O
( O
d|w O
) O
. O
∂ O
∂θ O
1 O
n O
ln O
p O
( O
xn|θ O
) O
n O
( O
cid:2 O
) O
( O
cid:25 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:29 O
) O
( O
cid:30 O
) O
n O
( O
cid:2 O
) O
n=1 O
lim O
n→∞ O
1 O
n O
ln O
p O
( O
xn|θ O
) O
= O
ex O
∂ O
∂θ O
∂ O
∂θ O
ln O
p O
( O
x|θ O
) O
( O
2.134 O
) O
and O
so O
we O
see O
that O
ﬁnding O
the O
maximum B
likelihood I
solution O
corresponds O
to O
ﬁnd- O
ing O
the O
root O
of O
a O
regression B
function I
. O
the O
support B
vector I
machine I
approaches O
this O
problem O
through O
the O
concept O
of O
the O
margin B
, O
which O
is O
deﬁned O
to O
be O
the O
smallest O
distance O
between O
the O
decision B
boundary I
and O
any O
of O
the O
samples O
, O
as O
illustrated O
in O
figure O
7.1. O
in O
support B
vector I
machines O
the O
decision B
boundary I
is O
chosen O
to O
be O
the O
one O
for O
which O
the O
margin B
is O
maximized O
. O
too O
strong O
a O
focus O
on O
either O
exploration B
or O
exploitation B
will O
yield O
poor O
results O
. O
the O
training B
of O
nonlinear O
network O
models O
corresponds O
to O
an O
iterative O
reduction O
of O
the O
error B
function I
deﬁned O
with O
re- O
spect O
to O
a O
set O
of O
training B
data O
. O
as O
before O
, O
we O
consider O
the O
contribution O
to O
the O
error B
function I
from O
one O
pattern O
in O
the O
data O
set O
. O
using O
the O
result O
( O
4.135 O
) O
, O
we O
obtain O
the O
following O
approximation O
for O
the O
log O
of O
the O
likelihood B
function I
ln O
p O
( O
tn|θ O
) O
= O
ψ O
( O
a O
( O
cid:1 O
) O
n O
) O
− O
1 O
2 O
ln|wn O
+ O
c O
n O
| O
+ O
n O
−1 O
2 O
ln O
( O
2π O
) O
( O
6.90 O
) O
318 O
6. O
kernel O
methods O
n|θ O
) O
+ O
ln O
p O
( O
tn|a O
( O
cid:1 O
) O
n O
) O
= O
ln O
p O
( O
a O
( O
cid:1 O
) O
where O
ψ O
( O
a O
( O
cid:1 O
) O
n O
) O
. O
note O
that O
a O
positive B
semideﬁnite I
matrix I
is O
not O
the O
same O
thing O
as O
a O
matrix O
whose O
elements O
are O
nonnegative O
. O
however O
, O
these O
approaches O
still O
suffer O
from O
limitations O
due O
to O
the O
absence O
of O
an O
overall O
density B
model O
. O
due O
to O
numerical O
errors O
, O
the O
value O
of O
h O
may O
sometimes O
decrease O
, O
and O
we O
would O
like O
the O
metropolis O
criterion O
to O
remove O
any O
bias B
due O
to O
this O
effect O
and O
ensure O
that O
the O
resulting O
samples O
are O
indeed O
drawn O
from O
the O
required O
dis- O
tribution O
. O
( O
14.10 O
) O
( O
cid:8 O
) O
{ O
ym O
( O
x O
) O
− O
h O
( O
x O
) O
} O
2 O
( O
cid:9 O
) O
m O
( O
cid:2 O
) O
eav O
= O
ex O
m=1 O
( O
cid:8 O
) O
1 O
m O
⎡⎣ O
( O
cid:24 O
) O
⎡⎣ O
( O
cid:24 O
) O
m O
( O
cid:2 O
) O
m O
( O
cid:2 O
) O
m=1 O
m=1 O
1 O
m O
1 O
m O
ecom O
= O
ex O
= O
ex O
( O
cid:8 O
) O
( O
cid:9 O
) O
( O
cid:9 O
) O
( O
cid:25 O
) O
2 O
⎤⎦ O
ym O
( O
x O
) O
− O
h O
( O
x O
) O
( O
cid:25 O
) O
2 O
⎤⎦ O
m O
( O
x O
) O
similarly O
, O
the O
expected O
error B
from O
the O
committee B
( O
14.7 O
) O
is O
given O
by O
if O
we O
assume O
that O
the O
errors O
have O
zero O
mean B
and O
are O
uncorrelated O
, O
so O
that O
ex O
[ O
m O
( O
x O
) O
] O
= O
0 O
ex O
[ O
m O
( O
x O
) O
l O
( O
x O
) O
] O
= O
0 O
, O
m O
( O
cid:9 O
) O
= O
l O
( O
14.11 O
) O
( O
14.12 O
) O
( O
14.13 O
) O
exercise O
14.2 O
then O
we O
obtain O
14.3. O
boosting B
657 O
ecom O
= O
1 O
m O
eav O
. O
5.5.5 O
training B
with O
transformed O
data O
. O
exploring O
combinations O
of O
settings O
for O
such O
parameters O
could O
, O
in O
the O
worst O
case O
, O
require O
a O
number O
of O
training B
runs O
that O
is O
exponential O
in O
the O
number O
of O
parameters O
. O
8.4.7 O
loopy B
belief I
propagation I
for O
many O
problems O
of O
practical O
interest O
, O
it O
will O
not O
be O
feasible O
to O
use O
exact O
in- O
ference O
, O
and O
so O
we O
need O
to O
exploit O
effective O
approximation O
methods O
. O
here O
we O
consider O
a O
simple O
example O
corresponding O
to O
a O
mixture O
of O
linear O
regression B
models O
, O
which O
represents O
a O
straightforward O
extension O
of O
the O
gaus- O
sian O
mixture B
model I
discussed O
in O
section O
9.2 O
to O
the O
case O
of O
conditional B
gaussian O
distributions O
. O
learning B
machines O
. O
inference B
on O
a O
chain O
. O
the O
role O
of O
hidden O
units O
in O
a O
simple O
classiﬁcation B
problem O
is O
illustrated O
in O
figure O
5.4 O
using O
the O
synthetic O
classiﬁcation O
data O
set O
described O
in O
appendix O
a O
. O
fractional B
belief I
propagation I
. O
for O
the O
case O
of O
k O
> O
2 O
classes O
, O
we O
have O
p O
( O
ck|x O
) O
= O
= O
( O
cid:5 O
) O
p O
( O
x|ck O
) O
p O
( O
ck O
) O
j O
p O
( O
x|cj O
) O
p O
( O
cj O
) O
( O
cid:5 O
) O
exp O
( O
ak O
) O
j O
exp O
( O
aj O
) O
( O
4.62 O
) O
which O
is O
known O
as O
the O
normalized B
exponential I
and O
can O
be O
regarded O
as O
a O
multiclass B
generalization O
of O
the O
logistic B
sigmoid I
. O
we O
give O
an O
example O
of O
the O
use O
of O
this O
approach O
for O
data O
visualization B
in O
figure O
12.11. O
another O
elegant O
feature O
ofthe O
em O
approach O
is O
that O
we O
can O
take O
the O
limit O
a O
2 O
-- O
-- O
t O
0 O
, O
corresponding O
to O
standard O
pca O
, O
and O
still O
obtain O
a O
valid O
em-like O
algorithm O
( O
roweis O
, O
1998 O
) O
. O
suppose O
the O
proposal O
dis- O
tribution O
is O
q O
( O
z O
) O
and O
show O
that O
the O
probability B
of O
a O
sample O
value O
z O
being O
accepted O
is O
given O
by O
( O
cid:4 O
) O
p O
( O
z O
) O
/kq O
( O
z O
) O
where O
( O
cid:4 O
) O
p O
is O
any O
unnormalized O
distribution O
that O
is O
proportional O
to O
p O
( O
z O
) O
, O
and O
the O
constant O
k O
is O
set O
to O
the O
smallest O
value O
that O
ensures O
kq O
( O
z O
) O
( O
cid:2 O
) O
( O
cid:4 O
) O
p O
( O
z O
) O
for O
all O
values O
of O
z. O
note O
that O
the O
probability B
of O
drawing O
a O
value O
z O
is O
given O
by O
the O
probability B
of O
drawing O
that O
value O
from O
q O
( O
z O
) O
times O
the O
probability B
of O
accepting O
that O
value O
given O
that O
it O
has O
been O
drawn O
. O
if O
we O
introduce O
a O
gaussian O
prior B
over O
the O
weight B
vector I
w O
, O
then O
we O
obtain O
the O
model O
that O
has O
been O
considered O
already O
in O
chapter O
4. O
the O
difference O
here O
is O
that O
in O
the O
rvm O
, O
this O
model O
uses O
the O
ard O
prior B
( O
7.80 O
) O
in O
which O
there O
is O
a O
separate O
precision O
hyperparameter O
associated O
with O
each O
weight B
parameter I
. O
, O
k O
are O
zero O
except O
for O
a O
single O
value O
of O
1 O
indicating O
which O
component O
of O
the O
mixture B
was O
responsible O
for O
generating O
that O
data O
point O
. O
then O
in O
section O
13.2.3 O
, O
we O
shall O
see O
how O
the O
forward-backward B
algorithm I
can O
be O
obtained O
very O
simply O
as O
a O
speciﬁc O
example O
of O
the O
sum-product B
algorithm I
introduced O
in O
section O
8.4.4. O
it O
is O
worth O
emphasizing O
that O
evaluation O
of O
the O
posterior O
distributions O
of O
the O
latent O
variables O
is O
independent B
of O
the O
form O
of O
the O
emission O
density O
p O
( O
x|z O
) O
or O
indeed O
of O
whether O
the O
observed O
variables O
are O
continuous O
or O
discrete O
. O
8.4.5 O
the O
max-sum B
algorithm I
the O
sum-product B
algorithm I
allows O
us O
to O
take O
a O
joint O
distribution O
p O
( O
x O
) O
expressed O
as O
a O
factor B
graph I
and O
efﬁciently O
ﬁnd O
marginals O
over O
the O
component O
variables O
. O
note O
that O
there O
may O
be O
several O
different O
factor O
graphs O
that O
correspond O
to O
the O
same O
undirected B
graph I
. O
this O
can O
be O
useful O
for O
detecting O
new O
data O
points O
that O
have O
low O
probability B
under O
the O
model O
and O
for O
which O
the O
predictions O
may O
44 O
1. O
introduction O
s O
e O
i O
t O
i O
s O
n O
e O
d O
s O
s O
a O
c O
l O
5 O
4 O
3 O
2 O
1 O
0 O
0 O
p O
( O
x|c2 O
) O
p O
( O
x|c1 O
) O
0.2 O
0.4 O
0.6 O
0.8 O
1 O
x O
1.2 O
1 O
0.8 O
0.6 O
0.4 O
0.2 O
0 O
0 O
p O
( O
c1|x O
) O
p O
( O
c2|x O
) O
0.2 O
0.4 O
x O
0.6 O
0.8 O
1 O
figure O
1.27 O
example O
of O
the O
class-conditional O
densities O
for O
two O
classes O
having O
a O
single O
input O
variable O
x O
( O
left O
plot O
) O
together O
with O
the O
corresponding O
posterior O
probabilities O
( O
right O
plot O
) O
. O
a O
simple O
approximation O
to O
model B
averaging I
is O
to O
use O
the O
single O
most O
probable O
model O
alone O
to O
make O
predictions O
. O
interestingly O
, O
the O
rows O
of O
this O
matrix O
are O
also O
orthogonal O
, O
so O
that O
uut O
= O
i. O
to O
show O
this O
, O
note O
that O
( O
c.37 O
) O
implies O
utuu−1 O
= O
u−1 O
= O
ut O
and O
so O
uu−1 O
= O
uut O
= O
i. O
using O
( O
c.12 O
) O
, O
it O
also O
follows O
that O
|u| O
= O
1. O
the O
eigenvector O
equation O
( O
c.29 O
) O
can O
be O
expressed O
in O
terms O
of O
u O
in O
the O
form O
( O
c.38 O
) O
where O
λ O
is O
an O
m O
× O
m O
diagonal B
matrix O
whose O
diagonal B
elements O
are O
given O
by O
the O
eigenvalues O
λi O
. O
the O
quantity O
on O
the O
left-hand O
side O
is O
a O
random O
variable O
, O
because O
it O
depends O
on O
the O
training B
set I
d O
, O
and O
the O
pac O
framework O
requires O
that O
( O
7.75 O
) O
holds O
, O
with O
probability B
greater O
than O
1 O
− O
δ O
, O
for O
a O
data O
set O
d O
drawn O
randomly O
from O
p O
( O
x O
, O
t O
) O
. O
gradient-based O
learning B
applied O
to O
doc- O
ument O
recognition O
. O
show O
that O
this O
distribution O
is O
normalized O
so O
that O
and O
that O
it O
reduces O
to O
the O
gaussian O
when O
q O
= O
2. O
consider O
a O
regression B
model O
in O
which O
the O
target O
variable O
is O
given O
by O
t O
= O
y O
( O
x O
, O
w O
) O
+ O
 O
and O
 O
is O
a O
random O
noise O
exercises O
135 O
variable O
drawn O
from O
the O
distribution O
( O
2.293 O
) O
. O
suppose O
we O
are O
given O
a O
new O
input O
value O
( O
cid:1 O
) O
x O
and O
we O
wish O
to O
ﬁnd O
the O
corresponding O
probability B
dis- O
tribution O
for O
( O
cid:1 O
) O
t O
conditioned O
on O
the O
observed O
data O
. O
for O
any O
given O
data O
set O
d O
, O
we O
can O
run O
our O
learning B
algorithm O
and O
obtain O
a O
prediction O
function O
y O
( O
x O
; O
d O
) O
. O
this O
is O
easily O
veri- O
ﬁed O
by O
noting O
that O
every O
path O
from O
node B
zn−1 O
to O
node B
zn+1 O
passes O
through O
at O
least O
one O
observed O
node O
that O
is O
head-to-tail O
with O
respect O
to O
that O
path O
. O
these O
constraints O
can O
be O
relaxed O
, O
at O
the O
expense O
of O
interpretability O
, O
by O
allowing O
soft B
, O
probabilistic O
splits O
that O
can O
be O
functions O
of O
all O
of O
the O
input O
variables O
, O
not O
just O
one O
of O
them O
at O
a O
time O
. O
p O
( O
y O
) O
8.4.1 O
inference B
on O
a O
chain O
now O
consider O
a O
more O
complex O
problem O
involving O
the O
chain O
of O
nodes O
of O
the O
form O
shown O
in O
figure O
8.32. O
this O
example O
will O
lay O
the O
foundation O
for O
a O
discussion O
of O
exact O
inference O
in O
more O
general O
graphs O
later O
in O
this O
section O
. O
bn O
= O
b0 O
+ O
similarly O
, O
we O
can O
ﬁnd O
the O
variational B
re-estimation O
equation O
for O
the O
posterior O
distribution O
over O
w. O
again O
, O
using O
the O
general O
result O
( O
10.9 O
) O
, O
and O
keeping O
only O
those O
terms O
that O
have O
a O
functional B
dependence O
on O
w O
, O
we O
have O
ln O
q O
( O
cid:1 O
) O
( O
w O
) O
= O
ln O
p O
( O
t|w O
) O
+ O
eα O
[ O
ln O
p O
( O
w|α O
) O
] O
+ O
const O
( O
10.96 O
) O
( O
10.93 O
) O
( O
10.94 O
) O
( O
10.95 O
) O
( O
10.97 O
) O
( O
10.98 O
) O
( O
10.99 O
) O
n O
( O
cid:2 O
) O
{ O
wtφn O
− O
tn O
} O
2 O
− O
1 O
( O
cid:10 O
) O
( O
cid:11 O
) O
n=1 O
= O
− O
β O
2 O
= O
−1 O
2 O
2 O
e O
[ O
α O
] O
wtw O
+ O
const O
wt O
e O
[ O
α O
] O
i O
+ O
βφtφ O
w O
+ O
βwtφtt O
+ O
const O
. O
this O
is O
an O
example O
of O
a O
problem O
called O
blind B
source I
separation I
in O
which O
'blind O
' O
refers O
to O
the O
fact O
that O
we O
are O
given O
only O
the O
mixed O
data O
, O
and O
neither O
the O
original O
sources O
nor O
the O
mixing O
coefficients O
are O
observed O
( O
cardoso O
, O
1998 O
) O
. O
this O
shows O
that O
the O
maximum O
of O
a O
probability B
density O
( O
in O
contrast O
to O
a O
simple O
function O
) O
is O
dependent O
on O
the O
choice O
of O
variable O
. O
( O
a O
) O
contours O
of O
constant O
probability B
density O
for O
the O
emission O
distributions O
corresponding O
to O
each O
of O
the O
three O
states O
of O
the O
latent B
variable I
. O
intuitively O
, O
we O
can O
think O
of O
the O
distribution O
p O
( O
x O
) O
as O
being O
defined O
by O
taking O
an O
isotropic B
gaussian O
'spray O
can O
' O
and O
moving O
it O
across O
the O
principal B
subspace I
spraying O
2 O
and O
weighted O
by O
the O
prior B
distribution O
. O
11.1.3 O
adaptive B
rejection I
sampling I
. O
because O
the O
jacobian O
matrix O
provides O
a O
measure O
of O
the O
local B
sensitivity O
of O
the O
outputs O
to O
changes O
in O
each O
of O
the O
input O
variables O
, O
it O
also O
allows O
any O
known O
errors O
∆xi O
248 O
5. O
neural O
networks O
( O
cid:2 O
) O
associated O
with O
the O
inputs O
to O
be O
propagated O
through O
the O
trained O
network O
in O
order O
to O
estimate O
their O
contribution O
∆yk O
to O
the O
errors O
at O
the O
outputs O
, O
through O
the O
relation O
∆yk O
( O
cid:7 O
) O
∂yk O
∂xi O
∆xi O
( O
5.72 O
) O
which O
is O
valid O
provided O
the O
|∆xi| O
are O
small O
. O
points O
for O
which O
0 O
< O
ξn O
( O
cid:1 O
) O
1 O
lie O
inside O
the O
margin B
, O
but O
on O
the O
cor- O
rect O
side O
of O
the O
decision B
boundary I
, O
and O
those O
data O
points O
for O
which O
ξn O
> O
1 O
lie O
on O
the O
wrong O
side O
of O
the O
decision B
boundary I
and O
are O
misclassiﬁed O
, O
as O
illustrated O
in O
fig- O
ure O
7.3. O
this O
is O
sometimes O
described O
as O
relaxing O
the O
hard O
margin B
constraint O
to O
give O
a O
soft B
margin I
and O
allows O
some O
of O
the O
training B
set I
data O
points O
to O
be O
misclassiﬁed O
. O
curvature-driven O
smoothing O
: O
a O
learning B
algorithm O
for O
feedforward O
networks O
. O
however O
, O
as O
we O
shall O
see O
, O
for O
a O
ﬁnite O
training O
set O
we O
only O
need O
to O
consider O
the O
values O
of O
the O
function O
at O
the O
discrete O
set O
of O
input O
values O
xn O
corresponding O
to O
the O
training B
set I
and O
test B
set I
data O
points O
, O
and O
so O
in O
practice O
we O
can O
work O
in O
a O
ﬁnite O
space O
. O
this O
says O
that O
the O
variables O
a O
and O
b O
are O
statistically O
independent B
, O
given O
c. O
note O
that O
our O
deﬁnition O
of O
conditional B
independence I
will O
require O
that O
( O
8.20 O
) O
, O
8.2. O
conditional B
independence I
373 O
figure O
8.15 O
the O
ﬁrst O
of O
three O
examples O
of O
graphs O
over O
three O
variables O
a O
, O
b O
, O
and O
c O
used O
to O
discuss O
conditional B
independence I
properties O
of O
directed B
graphical O
models O
. O
we O
see O
that O
the O
computations O
that O
must O
be O
performed O
in O
order O
to O
update O
the O
variational B
posterior O
distribution O
over O
the O
model O
parameters O
involve O
evaluation O
of O
the O
same O
sums O
over O
the O
data O
set O
, O
as O
arose O
in O
the O
maximum B
likelihood I
treatment O
. O
how'e O
'' O
e.. O
the O
som O
is O
tioi O
optimizing O
any O
well.ddine O
< O
! O
cost B
function I
( O
erwin O
.. O
al.. O
1992 O
) O
making O
'' O
difficult O
to O
s. O
'' O
the O
parameters O
of O
the O
model O
and O
10 O
assess O
con'-ergence O
. O
hence O
explain O
whether O
the O
variational B
gaussian O
mixture B
will O
tend O
to O
under-estimate O
or O
over-estimate O
the O
optimal O
number O
of O
components O
. O
now O
let O
us O
consider O
how O
a O
general O
maximum B
likelihood I
problem O
can O
be O
solved O
sequentially O
using O
the O
robbins-monro O
algorithm O
. O
this O
example O
provides O
interesting O
insights O
into O
the O
concept O
of O
‘ O
large B
margin I
’ O
, O
which O
was O
discussed O
in O
section O
7.1 O
and O
which O
has O
qualitatively O
similar O
be- O
haviour O
to O
the O
bayesian O
solution O
. O
this O
is O
called O
the O
parametric O
approach O
to O
density B
modelling O
. O
this O
is O
in O
contrast O
to O
variational B
bayes O
, O
which O
iteratively O
maximizes O
a O
lower B
bound I
on O
the O
log O
marginal O
likelihood O
, O
in O
which O
each O
iteration O
is O
guaranteed O
not O
to O
decrease O
the O
bound O
. O
furthermore O
, O
marginals O
such O
as O
( O
cid:1 O
) O
α O
( O
zn O
) O
are O
also O
gaussian O
, O
so O
that O
the O
functional B
form O
of O
the O
mes- O
each O
of O
which O
has O
a O
mean B
that O
is O
linear O
in O
zn O
. O
data O
points O
that O
are O
drawn O
independently O
from O
the O
same O
distribution O
are O
said O
to O
be O
independent B
and O
identically O
distributed O
, O
which O
is O
often O
abbreviated O
to O
i.i.d O
. O
2.42 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
evaluate O
the O
mean B
, O
variance B
, O
and O
mode O
of O
the O
gamma B
distribution I
( O
2.146 O
) O
. O
finally O
, O
if O
u0 O
> O
( O
cid:4 O
) O
p O
( O
z0 O
) O
then O
the O
sample O
is O
rejected O
, O
otherwise O
ure O
11.4. O
the O
remaining O
pairs O
then O
have O
uniform B
distribution I
under O
the O
curve O
of O
( O
cid:4 O
) O
p O
( O
z O
) O
, O
ples O
are O
then O
accepted O
with O
probability B
( O
cid:4 O
) O
p O
( O
z O
) O
/kq O
( O
z O
) O
, O
and O
so O
the O
probability B
that O
a O
and O
hence O
the O
corresponding O
z O
values O
are O
distributed O
according O
to O
p O
( O
z O
) O
, O
as O
desired O
. O
a O
sufﬁcient O
condition O
for O
ergodicity O
is O
that O
none O
of O
the O
conditional B
distributions O
be O
anywhere O
zero O
. O
using O
( O
2.115 O
) O
, O
we O
see O
that O
the O
marginal B
distribution O
of O
t O
is O
given O
by O
( O
cid:6 O
) O
p O
( O
t O
) O
= O
p O
( O
t|y O
) O
p O
( O
y O
) O
dy O
= O
n O
( O
t|0 O
, O
c O
) O
( O
6.61 O
) O
6.4. O
gaussian O
processes O
307 O
where O
the O
covariance B
matrix I
c O
has O
elements O
c O
( O
xn O
, O
xm O
) O
= O
k O
( O
xn O
, O
xm O
) O
+ O
β O
−1δnm O
. O
not O
surprisingly O
, O
this O
turns O
out O
to O
be O
equivalent O
to O
the O
forward-backward B
algorithm I
considered O
in O
the O
previous O
section O
, O
and O
so O
the O
sum-product B
algorithm I
therefore O
provides O
us O
with O
a O
simple O
way O
to O
derive O
the O
alpha-beta O
recursion O
formulae O
. O
the O
data O
points O
will O
not O
be O
confined O
precisely O
to O
a O
smooth O
low O
( O
cid:173 O
) O
dimensional O
manifold B
, O
and O
we O
can O
interpret O
the O
departures O
of O
data O
points O
from O
the O
manifold B
as O
·noise O
' O
. O
now O
let O
us O
consider O
the O
factor O
q O
( O
π O
, O
µ O
, O
λ O
) O
in O
the O
variational B
posterior O
distribu- O
tion O
. O
the O
ap- O
proximate O
conditional B
mode O
, O
shown O
by O
the O
red O
points O
, O
of O
the O
conditional B
density O
. O
a O
theory B
of O
the O
learnable O
. O
) O
, O
independent B
component I
analysis I
: O
principles O
and O
practice O
. O
it O
turns O
out O
that O
these O
factors O
account O
for O
all O
of O
the O
symmetries B
in O
weight O
space O
( O
except O
for O
possible O
accidental O
symmetries B
due O
to O
speciﬁc O
choices O
for O
the O
weight O
val- O
ues O
) O
. O
for O
a O
density B
deﬁned O
over O
multiple O
continuous O
variables O
, O
denoted O
collectively O
by O
the O
vector O
x O
, O
the O
differential B
entropy I
is O
given O
by O
( O
cid:6 O
) O
h O
[ O
x O
] O
= O
− O
p O
( O
x O
) O
ln O
p O
( O
x O
) O
dx O
. O
for O
a O
symmetric O
proposal B
distribution I
the O
metropolis-hastings O
criterion O
( O
11.44 O
) O
reduces O
to O
the O
stan- O
dard O
metropolis O
criterion O
given O
by O
( O
11.33 O
) O
. O
8. O
if O
solving O
a O
regression B
problem O
, O
update O
β O
. O
( O
7.124 O
) O
358 O
7. O
sparse O
kernel O
machines O
7.8 O
( O
( O
cid:12 O
) O
) O
www O
for O
the O
regression B
support O
vector O
machine O
considered O
in O
section O
7.1.4 O
, O
show O
that O
all O
training B
data O
points O
for O
which O
ξn O
> O
0 O
will O
have O
an O
= O
c O
, O
and O
similarly O
all O
points O
for O
which O
( O
cid:1 O
) O
ξn O
> O
0 O
will O
have O
( O
cid:1 O
) O
an O
= O
c. O
7.9 O
( O
( O
cid:12 O
) O
) O
verify O
the O
results O
( O
7.82 O
) O
and O
( O
7.83 O
) O
for O
the O
mean B
and O
covariance B
of O
the O
posterior O
distribution O
over O
weights O
in O
the O
regression B
rvm O
. O
here O
we O
consider O
the O
evaluation O
of O
the O
jacobian O
matrix O
, O
whose O
elements O
are O
given O
by O
the O
derivatives O
of O
the O
network O
outputs O
with O
respect O
to O
the O
inputs O
jki O
≡ O
∂yk O
∂xi O
( O
5.70 O
) O
where O
each O
such O
derivative B
is O
evaluated O
with O
all O
other O
inputs O
held O
ﬁxed O
. O
the O
singularity O
can O
be O
avoided O
by O
inclusion O
of O
a O
prior B
and O
ﬁnding O
a O
map O
solution O
for O
w O
, O
or O
equivalently O
by O
adding O
a O
regularization B
term O
to O
the O
error B
function I
. O
11 O
sampling B
methods I
for O
most O
probabilistic O
models O
of O
practical O
interest O
, O
exact O
inference O
is O
intractable O
, O
and O
so O
we O
have O
to O
resort O
to O
some O
form O
of O
approximation O
. O
similarly O
samples O
from O
the O
marginal B
distribution O
p O
( O
x O
) O
are O
obtained O
by O
taking O
the O
samples O
from O
the O
joint O
distribution O
and O
ignoring O
the O
values O
of O
z. O
these O
are O
illustrated O
in O
figure O
9.5 O
( O
b O
) O
by O
plotting O
the O
x O
values O
without O
any O
coloured O
labels O
. O
the O
covari- O
the O
inferred O
positions O
ances O
of O
are O
indicated O
by O
the O
red O
ellipses O
, O
which O
correspond O
to O
contours O
having O
one O
standard B
deviation I
. O
reducing O
the O
dependence O
on O
the O
prior B
is O
one O
motivation O
for O
so-called O
noninformative B
priors O
. O
12.18 O
( O
* O
) O
derive O
an O
expression O
for O
the O
number O
of O
independent B
parameters O
in O
the O
factor B
analysis I
model O
described O
in O
section O
12.2.4 O
. O
this O
can O
be O
represented O
by O
the O
directed B
graph O
shown O
in O
figure O
8.23 O
in O
which O
the O
joint O
distribution O
is O
deﬁned O
by O
a O
prior B
p O
( O
µ O
) O
to- O
gether O
with O
a O
set O
of O
conditional B
distributions O
p O
( O
xn|µ O
) O
for O
n O
= O
1 O
, O
. O
we O
again O
make O
use O
of O
the O
restricted O
form O
( O
4.84 O
) O
of O
exponential B
family I
distribu- O
tions O
. O
( O
13.94 O
) O
( O
13.95 O
) O
( O
13.96 O
) O
( O
13.97 O
) O
similarly O
, O
the O
likelihood B
function I
for O
the O
linear B
dynamical I
system I
is O
given O
by O
( O
13.63 O
) O
in O
which O
the O
factors O
cn O
are O
found O
using O
the O
kalman O
ﬁltering O
equations O
. O
in O
each O
case O
, O
we O
see O
that O
the O
variational B
posterior O
distribution O
has O
the O
same O
functional B
form O
as O
the O
corresponding O
factor O
in O
the O
joint O
distribution O
( O
10.41 O
) O
. O
however O
, O
limiting O
the O
number O
of O
basis O
functions O
in O
order O
to O
avoid O
over-ﬁtting B
has O
the O
side O
effect O
of O
limiting O
the O
ﬂexibility O
of O
the O
model O
to O
capture O
interesting O
and O
important O
trends O
in O
the O
data O
. O
the O
distribution O
of O
initial O
states O
must O
also O
be O
speciﬁed O
in O
order O
to O
complete O
the O
algorithm O
, O
although O
samples O
drawn O
after O
many O
iterations O
will O
effectively O
become O
independent B
of O
this O
distribution O
. O
neural B
network I
models O
, O
which O
use O
adaptive O
basis O
functions O
having O
sigmoidal O
nonlinearities O
, O
can O
adapt O
the O
parameters O
so O
that O
the O
regions O
of O
input O
space O
over O
which O
the O
basis O
functions O
vary O
corresponds O
to O
the O
data O
manifold O
. O
in O
many O
applications O
, O
their O
parameters O
are O
determined O
by O
maximum B
likelihood I
, O
typically O
using O
the O
em O
algorithm O
. O
( O
cid:5 O
) O
m O
( O
cid:23 O
) O
αm−1 O
m−1 O
( O
cid:14 O
) O
µαk−1 O
k O
( O
cid:22 O
) O
1 O
− O
m−1 O
( O
cid:2 O
) O
130 O
2. O
probability B
distributions O
2.11 O
( O
( O
cid:12 O
) O
) O
www O
by O
expressing O
the O
expectation B
of O
ln O
µj O
under O
the O
dirichlet O
distribution O
( O
2.38 O
) O
as O
a O
derivative B
with O
respect O
to O
αj O
, O
show O
that O
e O
[ O
ln O
µj O
] O
= O
ψ O
( O
αj O
) O
− O
ψ O
( O
α0 O
) O
where O
α0 O
is O
given O
by O
( O
2.39 O
) O
and O
ψ O
( O
a O
) O
≡ O
d O
da O
ln O
γ O
( O
a O
) O
is O
the O
digamma B
function I
. O
indeed O
, O
the O
technique O
of O
logistic B
regression I
, O
described O
later O
in O
this O
chapter O
, O
gives O
a O
satisfac- O
tory O
solution O
as O
seen O
in O
the O
right-hand O
plot O
. O
( O
c O
) O
the O
same O
samples O
in O
which O
the O
colours O
represent O
the O
value O
of O
the O
responsibilities O
γ O
( O
znk O
) O
associated O
with O
data O
point O
xn O
, O
obtained O
by O
plotting O
the O
corresponding O
point O
using O
proportions O
of O
red O
, O
blue O
, O
and O
green O
ink O
given O
by O
γ O
( O
znk O
) O
for O
k O
= O
1 O
, O
2 O
, O
3 O
, O
respectively O
matrix O
x O
in O
which O
the O
nth O
row O
is O
given O
by O
xt O
n. O
similarly O
, O
the O
corresponding O
latent O
variables O
will O
be O
denoted O
by O
an O
n O
× O
k O
matrix O
z O
with O
rows O
zt O
n. O
if O
we O
assume O
that O
the O
data O
points O
are O
drawn O
independently O
from O
the O
distribution O
, O
then O
we O
can O
express O
the O
gaussian O
mixture B
model I
for O
this O
i.i.d O
. O
similarly O
, O
we O
may O
wish O
to O
use O
an O
importance B
sampling I
where O
( O
cid:4 O
) O
rl O
= O
( O
cid:4 O
) O
p O
( O
z O
( O
l O
) O
) O
/ O
( O
cid:4 O
) O
q O
( O
z O
( O
l O
) O
) O
. O
given O
a O
greedy O
strategy O
for O
growing O
the O
tree B
, O
there O
remains O
the O
issue O
of O
when O
to O
stop O
adding O
nodes O
. O
note O
that O
in O
this O
case O
, O
the O
partition B
function I
z O
= O
1. O
let O
us O
consider O
how O
to O
generalize O
this O
construction O
, O
so O
that O
we O
can O
convert O
any O
distribution O
speciﬁed O
by O
a O
factorization B
over O
a O
directed B
graph O
into O
one O
speciﬁed O
by O
a O
factorization B
over O
an O
undirected B
graph I
. O
the O
result O
( O
1.89 O
) O
shows O
that O
the O
function O
y O
( O
x O
, O
w O
) O
that O
minimizes O
this O
error B
is O
given O
by O
the O
conditional B
expectation I
of O
t O
given O
x. O
use O
this O
result O
to O
show O
that O
the O
second O
derivative O
of O
e O
with O
respect O
to O
two O
elements O
wr O
and O
ws O
of O
the O
vector O
w O
, O
is O
given O
by O
( O
cid:6 O
) O
∂2e O
∂wr∂ws O
= O
∂y O
∂wr O
∂y O
∂ws O
p O
( O
x O
) O
dx O
. O
one O
important O
variant O
of O
the O
committee B
method O
, O
known O
as O
boosting B
, O
involves O
training B
multiple O
models O
in O
sequence O
in O
which O
the O
error B
function I
used O
to O
train O
a O
par- O
ticular O
model O
depends O
on O
the O
performance O
of O
the O
previous O
models O
. O
this O
is O
similar O
in O
spirit O
to O
the O
update O
of O
factors O
in O
the O
variational B
bayes O
framework O
expectation B
propagation I
makes O
a O
much O
better O
approximation O
by O
optimizing O
each O
factor O
in O
turn O
in O
the O
context O
of O
all O
of O
the O
remaining O
factors O
. O
x1 O
x1 O
( O
a O
) O
( O
b O
) O
x2 O
x2 O
xn−1 O
xn O
xn O
xn−1 O
will O
have O
converged O
to O
a O
local B
maximum O
of O
the O
probability B
. O
this O
is O
known O
as O
the O
nadaraya-watson O
model O
and O
will O
be O
derived O
again O
from O
a O
different O
perspective O
in O
section O
6.3.1. O
if O
the O
noise O
distribution O
ν O
( O
ξ O
) O
is O
isotropic B
, O
so O
that O
it O
is O
a O
function O
only O
of O
( O
cid:5 O
) O
ξ O
( O
cid:5 O
) O
, O
then O
the O
basis O
functions O
will O
be O
radial O
. O
suppose O
we O
have O
an O
input O
vector O
x O
together O
with O
a O
corresponding O
vector O
t O
of O
target O
variables O
, O
and O
our O
goal O
is O
to O
predict O
t O
given O
a O
new O
value O
for O
x. O
for B
regression I
problems O
, O
t O
will O
comprise O
continuous O
variables O
, O
whereas O
for O
classiﬁcation O
problems O
t O
will O
represent O
class O
labels O
. O
maximization O
of O
this O
approxi- O
mate O
marginal B
likelihood I
then O
leads O
to O
a O
re-estimated O
value O
for O
α O
, O
and O
the O
process O
is O
repeated O
until O
convergence O
. O
( O
b O
) O
a O
factor B
graph I
with O
factor O
f O
( O
x1 O
, O
x2 O
, O
x3 O
) O
= O
ψ O
( O
x1 O
, O
x2 O
, O
x3 O
) O
representing O
the O
same O
distribution O
as O
the O
undirected B
graph I
. O
this O
arises O
because O
any O
ﬁnite O
value O
for O
α O
will O
always O
assign O
a O
lower O
probability O
to O
the O
data O
, O
thereby O
decreasing O
the O
value O
of O
the O
density B
at O
t O
, O
pro- O
vided O
that O
β O
is O
set O
to O
its O
optimal O
value O
. O
note O
that O
a O
corresponds O
to O
the O
matrix O
of O
second O
derivatives O
of O
the O
error B
function I
a O
= O
∇∇e O
( O
w O
) O
( O
3.83 O
) O
and O
is O
known O
as O
the O
hessian O
matrix O
. O
here O
we O
give O
an O
introduction O
to O
the O
key O
ideas O
of O
decision B
theory I
as O
required O
for O
1.5. O
decision B
theory I
39 O
the O
rest O
of O
the O
book O
. O
in O
particular O
, O
we O
consider O
continuous O
latent O
variables O
in O
which O
the O
summations O
of O
the O
sum-product B
algorithm I
become O
integrals O
. O
it O
follows O
that O
the O
sequence O
of O
individually O
most O
probable O
latent B
variable I
values O
is O
the O
same O
as O
the O
most O
probable O
latent O
sequence O
. O
there O
is O
thus O
no O
need O
to O
consider O
the O
analogue O
of O
the O
viterbi O
algorithm O
for O
the O
linear B
dynamical I
system I
. O
8.1.3 O
discrete O
variables O
we O
have O
discussed O
the O
importance O
of O
probability B
distributions O
that O
are O
members O
of O
the O
exponential B
family I
, O
and O
we O
have O
seen O
that O
this O
family O
includes O
many O
well- O
known O
distributions O
as O
particular O
cases O
. O
more O
generally O
, O
it O
is O
straightforward O
to O
obtain O
the O
required O
ex- O
pectations O
for O
any O
member O
of O
the O
exponential B
family I
, O
provided O
it O
can O
be O
normalized O
, O
because O
the O
expected O
statistics O
can O
be O
related O
to O
the O
derivatives O
of O
the O
normalization O
coefﬁcient O
, O
as O
given O
by O
( O
2.226 O
) O
. O
probability B
theory O
. O
however O
, O
if O
we O
examine O
a O
single O
time O
slice O
of O
the O
model O
, O
we O
see O
that O
it O
corresponds O
to O
a O
mixture B
distribution I
, O
with O
component O
densities O
given O
by O
p O
( O
x|z O
) O
. O
hybrid O
monte O
carlo O
( O
duane O
et O
al. O
, O
1987 O
; O
neal O
, O
1996 O
) O
combines O
hamiltonian O
dynamics O
with O
the O
metropolis O
algorithm O
and O
thereby O
removes O
any O
bias B
associated O
with O
the O
discretization O
. O
figure O
8.45 O
shows O
an O
example O
of O
a O
fully B
connected I
undirected O
graph O
along O
with O
two O
different O
factor O
graphs O
. O
if O
we O
choose O
the O
origin O
at O
0◦ O
, O
then O
the O
sample B
mean I
of O
this O
data O
set O
will O
be O
180◦ O
with O
standard B
deviation I
179◦ O
, O
whereas O
if O
we O
choose O
the O
origin O
at O
180◦ O
, O
then O
the O
mean B
will O
be O
0◦ O
. O
from O
( O
c.38 O
) O
, O
it O
follows O
that O
utau O
= O
λ O
( O
c.42 O
) O
and O
because O
λ O
is O
a O
diagonal B
matrix O
, O
we O
say O
that O
the O
matrix O
a O
is O
diagonalized O
by O
the O
matrix O
u. O
if O
we O
left O
multiply O
by O
u O
and O
right O
multiply O
by O
ut O
, O
we O
obtain O
( O
c.43 O
) O
taking O
the O
inverse B
of O
this O
equation O
, O
and O
using O
( O
c.3 O
) O
together O
with O
u−1 O
= O
ut O
, O
we O
have O
a O
= O
uλut O
a−1 O
= O
uλ O
−1ut O
. O
the O
m O
step O
for O
the O
factorial B
hmm O
model O
is O
straightforward O
. O
from O
the O
product B
rule I
, O
together O
with O
the O
symmetry O
property O
p O
( O
x O
, O
y O
) O
= O
p O
( O
y O
, O
x O
) O
, O
we O
immediately O
obtain O
the O
following O
relationship O
between O
conditional B
probabilities O
p O
( O
y O
|x O
) O
= O
p O
( O
x|y O
) O
p O
( O
y O
) O
p O
( O
x O
) O
( O
1.12 O
) O
which O
is O
called O
bayes O
’ O
theorem O
and O
which O
plays O
a O
central O
role O
in O
pattern O
recognition O
and O
machine O
learning O
. O
this O
can O
be O
achieved O
by O
combining O
the O
global O
and O
local B
variational O
approximations O
into O
a O
single O
framework O
, O
so O
as O
to O
maintain O
a O
lower B
bound I
on O
the O
marginal B
likelihood I
at O
each O
stage O
. O
this O
might O
seem O
like O
a O
strange O
criterion O
because O
, O
from O
our O
fore- O
going O
discussion O
of O
probability B
theory O
, O
it O
would O
seem O
more O
natural O
to O
maximize O
the O
probability B
of O
the O
parameters O
given O
the O
data O
, O
not O
the O
probability B
of O
the O
data O
given O
the O
parameters O
. O
in O
the O
former O
case O
, O
the O
function O
g O
( O
x O
) O
plays O
no O
role O
and O
so O
the O
stationary B
condition O
is O
simply O
∇f O
( O
x O
) O
= O
0. O
this O
again O
corresponds O
to O
a O
stationary B
point O
of O
the O
lagrange O
function O
( O
e.4 O
) O
but O
this O
time O
with O
λ O
= O
0. O
the O
latter O
case O
, O
where O
the O
solution O
lies O
on O
the O
boundary O
, O
is O
analogous O
to O
the O
equality O
con- O
straint O
discussed O
previously O
and O
corresponds O
to O
a O
stationary B
point O
of O
the O
lagrange O
function O
( O
e.4 O
) O
with O
λ O
( O
cid:6 O
) O
= O
0. O
now O
, O
however O
, O
the O
sign O
of O
the O
lagrange O
multiplier O
is O
crucial O
, O
because O
the O
function O
f O
( O
x O
) O
will O
only O
be O
at O
a O
maximum O
if O
its O
gradient O
is O
ori- O
ented O
away O
from O
the O
region O
g O
( O
x O
) O
> O
0 O
, O
as O
illustrated O
in O
figure O
e.3 O
. O
10.2. O
illustration O
: O
variational B
mixture O
of O
gaussians O
481 O
indeed O
, O
these O
singularities B
are O
removed O
if O
we O
simply O
introduce O
a O
prior B
and O
then O
use O
a O
map O
estimate O
instead O
of O
maximum B
likelihood I
. O
the O
centre O
plot O
shows O
the O
result O
of O
ﬁtting O
a O
single O
logistic B
regression I
model O
using O
maximum B
likelihood I
, O
in O
which O
the O
background O
colour O
denotes O
the O
corresponding O
probability B
of O
the O
class O
label O
. O
note O
that O
in O
the O
neural O
networks O
literature O
, O
it O
is O
usual O
to O
con- O
sider O
the O
minimization O
of O
an O
error B
function I
rather O
than O
the O
maximization O
of O
the O
( O
log O
) O
likelihood O
, O
and O
so O
here O
we O
shall O
follow O
this O
convention O
. O
to O
obtain O
a O
gaussian O
approximation O
to O
the O
posterior O
dis- O
tribution O
, O
we O
ﬁrst O
maximize O
the O
posterior O
distribution O
to O
give O
the O
map O
( O
maximum B
posterior I
) O
solution O
wmap O
, O
which O
deﬁnes O
the O
mean B
of O
the O
gaussian O
. O
1 O
3 O
5 O
7 O
9 O
10.4. O
exponential B
family I
distributions O
in O
chapter O
2 O
, O
we O
discussed O
the O
important O
role O
played O
by O
the O
exponential B
family I
of O
distributions O
and O
their O
conjugate B
priors O
. O
10.27 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
making O
use O
of O
the O
formulae O
given O
in O
appendix O
b O
show O
that O
the O
variational B
lower O
bound O
for O
the O
linear O
basis O
function O
regression O
model O
, O
deﬁned O
by O
( O
10.107 O
) O
, O
can O
be O
written O
in O
the O
form O
( O
10.107 O
) O
with O
the O
various O
terms O
deﬁned O
by O
( O
10.108 O
) O
– O
( O
10.112 O
) O
. O
note O
that O
the O
ep O
distribution O
is O
broader O
than O
that O
variational B
inference I
, O
as O
a O
consequence O
of O
the O
different O
form O
of O
kl O
divergence O
. O
( O
5.81 O
) O
note O
that O
the O
number O
of O
computational O
steps O
required O
to O
evaluate O
this O
approximation O
is O
o O
( O
w O
) O
, O
where O
w O
is O
the O
total O
number O
of O
weight O
and O
bias B
parameters O
in O
the O
network O
, O
compared O
with O
o O
( O
w O
2 O
) O
for O
the O
full O
hessian O
. O
from O
( O
10.133 O
) O
the O
corresponding O
conjugate B
function O
then O
takes O
the O
form O
g O
( O
λ O
) O
= O
min O
x O
{ O
λx O
− O
f O
( O
x O
) O
} O
= O
−λ O
ln O
λ O
− O
( O
1 O
− O
λ O
) O
ln O
( O
1 O
− O
λ O
) O
( O
10.135 O
) O
which O
we O
recognize O
as O
the O
binary B
entropy I
function O
for O
a O
variable O
whose O
probability B
of O
having O
the O
value O
1 O
is O
λ. O
using O
( O
10.132 O
) O
, O
we O
then O
obtain O
an O
upper O
bound O
on O
the O
log O
sigmoid O
ln O
σ O
( O
x O
) O
( O
cid:1 O
) O
λx O
− O
g O
( O
λ O
) O
( O
10.136 O
) O
exercise O
10.30 O
appendix O
b O
496 O
10. O
approximate O
inference B
1 O
0.5 O
0 O
−6 O
λ O
= O
0.2 O
λ O
= O
0.7 O
0 O
1 O
0.5 O
ξ O
= O
2.5 O
6 O
0 O
−6 O
−ξ O
0 O
ξ O
6 O
figure O
10.12 O
the O
left-hand O
plot O
shows O
the O
logistic B
sigmoid I
function O
σ O
( O
x O
) O
deﬁned O
by O
( O
10.134 O
) O
in O
red O
, O
together O
with O
two O
examples O
of O
the O
exponential O
upper O
bound O
( O
10.137 O
) O
shown O
in O
blue O
. O
we O
can O
obtain O
an O
analogous O
result O
by O
considering O
a O
point O
m=1 O
b. O
as O
with O
the O
classiﬁcation B
case O
, O
there O
is O
an O
alternative O
formulation O
of O
the O
svm O
for B
regression I
in O
which O
the O
parameter O
governing O
complexity O
has O
a O
more O
intuitive O
interpretation O
( O
sch¨olkopf O
et O
al. O
, O
2000 O
) O
. O
note O
that O
, O
as O
the O
weight B
vector I
evolves O
during O
training B
, O
the O
set O
of O
patterns O
that O
are O
misclassiﬁed O
will O
change O
. O
practical O
details O
of O
irls O
for O
the O
multiclass B
case O
can O
be O
found O
in O
bishop O
and O
nabney O
( O
2008 O
) O
. O
in O
either O
case O
, O
the O
technique O
of O
ancestral B
sampling I
applied O
to O
a O
generative B
model I
mimics O
the O
creation O
of O
the O
observed O
data O
and O
would O
therefore O
give O
rise O
to O
‘ O
fantasy O
’ O
data O
whose O
probability B
distribution O
( O
if O
the O
model O
were O
a O
perfect O
representation O
of O
reality O
) O
would O
be O
the O
same O
as O
that O
of O
the O
observed O
data O
. O
if O
we O
follow O
the O
evolution O
of O
the O
hamiltonian O
equations O
for O
a O
ﬁnite O
time O
, O
then O
the O
volume O
of O
this O
region O
will O
remain O
unchanged O
as O
will O
the O
value O
of O
h O
in O
this O
region O
, O
and O
hence O
the O
probability B
density O
, O
which O
is O
a O
function O
only O
of O
h O
, O
will O
also O
be O
unchanged O
. O
thus O
, O
y O
= O
h O
transform O
the O
uniformly O
distributed O
random O
numbers O
using O
a O
function O
which O
is O
the O
inverse B
of O
the O
indeﬁnite O
integral O
of O
the O
desired O
distribution O
. O
monte O
carlo O
sampling B
methods I
using O
markov O
chains O
and O
their O
applica- O
tions O
. O
, O
tn O
} O
, O
the O
likelihood B
function I
is O
given O
by O
p O
( O
w|α O
) O
= O
n O
( O
w|0 O
, O
α O
−1i O
) O
. O
in O
the O
expectation B
step I
, O
or O
e O
step O
, O
we O
use O
the O
current O
values O
for O
the O
parameters O
to O
evaluate O
the O
posterior O
probabilities O
, O
or O
responsibilities O
, O
given O
by O
( O
9.13 O
) O
. O
similarly O
, O
show O
that O
the O
use O
of O
the O
result O
( O
8.72 O
) O
after O
running O
the O
sum-product B
algorithm I
on O
this O
graph O
gives O
the O
correct O
joint O
distribution O
for O
x1 O
, O
x2 O
. O
( O
c O
) O
find O
a O
function O
f O
( O
x O
) O
, O
called O
a O
discriminant B
function I
, O
which O
maps O
each O
input O
x O
directly O
onto O
a O
class O
label O
. O
12.10 O
( O
** O
) O
by O
evaluating O
the O
second O
derivatives O
of O
the O
log O
likelihood O
function O
( O
12.43 O
) O
for O
the O
probabilistic O
pca O
model O
with O
respect O
to O
the O
parameter O
jl O
, O
show O
that O
the O
stationary B
point O
jlml O
= O
x O
represents O
the O
unique O
maximum O
. O
10.33 O
( O
( O
cid:12 O
) O
) O
by O
differentiating O
the O
quantity O
q O
( O
ξ O
, O
ξold O
) O
deﬁned O
by O
( O
10.161 O
) O
with O
respect O
to O
the O
variational B
parameter O
ξn O
show O
that O
the O
update O
equation O
for O
ξn O
for O
the O
bayesian O
logistic B
regression I
model O
is O
given O
by O
( O
10.163 O
) O
. O
however O
, O
we O
will O
not O
always O
be O
able O
to O
de- O
rive O
a O
sequential O
algorithm O
by O
this O
route O
, O
and O
so O
we O
seek O
a O
more O
general O
formulation O
of O
sequential B
learning I
, O
which O
leads O
us O
to O
the O
robbins-monro O
algorithm O
. O
in O
general O
, O
for O
a O
given O
probability B
distribution O
p O
( O
x|η O
) O
, O
we O
can O
seek O
a O
prior B
p O
( O
η O
) O
that O
is O
conjugate B
to O
the O
likelihood B
function I
, O
so O
that O
the O
posterior O
distribution O
has O
the O
same O
functional B
form O
as O
the O
prior B
. O
the O
predictive B
distribution I
in O
gaussian O
process O
regression B
does O
not O
7.2. O
relevance B
vector I
machines O
349 O
figure O
7.9 O
illustration O
of O
rvm O
regression B
us- O
ing O
the O
same O
data O
set O
, O
and O
the O
same O
gaussian O
kernel O
functions O
, O
as O
used O
in O
figure O
7.8 O
for O
the O
ν-svm O
regression B
model O
. O
any O
given O
164 O
3. O
linear O
models O
for B
regression I
p O
( O
d O
) O
m1 O
figure O
3.13 O
schematic O
illustration O
of O
the O
distribution O
of O
data O
sets O
for O
three O
models O
of O
different O
com- O
in O
which O
m1 O
is O
the O
plexity O
, O
simplest O
and O
m3 O
is O
the O
most O
complex O
. O
6.4.1 O
linear B
regression I
revisited O
in O
order O
to O
motivate O
the O
gaussian O
process O
viewpoint O
, O
let O
us O
return O
to O
the O
linear B
regression I
example O
and O
re-derive O
the O
predictive B
distribution I
by O
working O
in O
terms O
of O
distributions O
over O
functions O
y O
( O
x O
, O
w O
) O
. O
we O
see O
that O
the O
marginal B
distribution O
for O
the O
observed O
12.2.l'ru O
'' O
: O
ohilislk O
i'ca O
585 O
flliure12.15 O
gillbs O
. O
using O
the O
factorization B
property O
( O
8.5 O
) O
, O
we O
can O
express O
this O
conditional B
distribution O
in O
the O
form O
p O
( O
xi|x O
{ O
j O
( O
cid:9 O
) O
=i O
} O
) O
= O
p O
( O
x1 O
, O
. O
this O
can O
be O
viewed O
as O
a O
generalization B
of O
the O
voting O
scheme O
of O
the O
one-versus-one O
approach O
in O
which O
more O
general O
partitions O
of O
the O
classes O
are O
used O
to O
train O
the O
individual O
classiﬁers O
. O
it O
also O
allows O
multiple O
complexity O
parame- O
ters O
to O
be O
determined O
simultaneously O
as O
part O
of O
the O
training B
process O
. O
to O
do O
this O
, O
we O
substitute O
the O
inequality O
( O
10.152 O
) O
back O
into O
the O
marginal B
likeli- O
( O
cid:6 O
) O
hood O
to O
give O
ln O
p O
( O
t O
) O
= O
ln O
( O
cid:6 O
) O
p O
( O
t|w O
) O
p O
( O
w O
) O
dw O
( O
cid:2 O
) O
ln O
h O
( O
w O
, O
ξ O
) O
p O
( O
w O
) O
dw O
= O
l O
( O
ξ O
) O
. O
consider O
our O
medical O
x-ray O
problem O
again O
, O
and O
suppose O
that O
we O
have O
collected O
a O
large O
number O
of O
x-ray O
images O
from O
the O
gen- O
eral O
population O
for O
use O
as O
training B
data O
in O
order O
to O
build O
an O
automated O
screening O
system O
. O
referring O
back O
to O
figure O
1.4 O
, O
we O
see O
that O
the O
m O
= O
0 O
polynomial O
has O
very O
poor O
ﬁt O
to O
the O
data O
and O
consequently O
gives O
a O
relatively O
low O
value O
168 O
3. O
linear O
models O
for B
regression I
figure O
3.14 O
plot O
of O
the O
model B
evidence I
versus O
the O
order O
m O
, O
for O
the O
polynomial O
re- O
gression O
model O
, O
showing O
that O
the O
evidence O
favours O
the O
model O
with O
m O
= O
3 O
. O
( O
b O
) O
from O
the O
root B
node I
towards O
the O
leaf O
nodes O
. O
1.6.1 O
relative B
entropy I
and O
mutual B
information I
. O
because O
the O
expectation B
of O
a O
binary O
random O
variable O
is O
just O
the O
probability B
that O
it O
takes O
the O
value O
1 O
, O
we O
have O
γ O
( O
znk O
) O
= O
e O
[ O
znk O
] O
= O
γ O
( O
z O
) O
znk O
( O
13.15 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
ξ O
( O
zn−1 O
, O
j O
, O
znk O
) O
= O
e O
[ O
zn−1 O
, O
jznk O
] O
= O
z O
γ O
( O
z O
) O
zn−1 O
, O
jznk O
. O
here O
we O
consider O
approxi- O
mate O
inference B
methods O
based O
on O
numerical O
sampling O
, O
also O
known O
as O
monte O
carlo O
techniques O
. O
( O
3.20 O
) O
thus O
the O
bias B
w0 O
compensates O
for O
the O
difference O
between O
the O
averages O
( O
over O
the O
training B
set I
) O
of O
the O
target O
values O
and O
the O
weighted O
sum O
of O
the O
averages O
of O
the O
basis B
function I
values O
. O
is O
held O
ﬁxed O
and O
the O
lower B
bound I
l O
( O
q O
, O
θ O
) O
is O
maximized O
with O
respect O
to O
the O
parameter O
vector O
θ O
to O
give O
a O
revised O
value O
θnew O
. O
( O
2.270 O
) O
( O
2.271 O
) O
here O
ex O
[ O
x|y O
] O
denotes O
the O
expectation B
of O
x O
under O
the O
conditional B
distribution O
p O
( O
x|y O
) O
, O
with O
a O
similar O
notation O
for O
the O
conditional B
variance O
. O
the O
resulting O
model O
can O
be O
represented O
as O
a O
directed B
graph O
as O
shown O
in O
fig- O
ure O
10.5. O
note O
that O
there O
is O
a O
link B
from O
λ O
to O
µ O
since O
the O
variance B
of O
the O
distribution O
over O
µ O
in O
( O
10.40 O
) O
is O
a O
function O
of O
λ. O
this O
example O
provides O
a O
nice O
illustration O
of O
the O
distinction O
between O
latent O
vari- O
ables O
and O
parameters O
. O
the O
projection O
formula O
( O
4.20 O
) O
transforms O
the O
set O
of O
labelled O
data O
points O
in O
x O
into O
a O
labelled O
set O
in O
the O
one-dimensional O
space O
y. O
the O
within-class B
variance O
of O
the O
transformed O
data O
from O
class O
ck O
is O
therefore O
given O
by O
( O
yn O
− O
mk O
) O
2 O
( O
cid:2 O
) O
s2 O
k O
= O
n∈ck O
( O
4.24 O
) O
where O
yn O
= O
wtxn O
. O
612 O
13. O
sequential B
data I
figure O
13.7 O
if O
we O
unfold O
the O
state O
transition O
dia- O
gram O
of O
figure O
13.6 O
over O
time O
, O
we O
obtain O
a O
lattice O
, O
or O
trellis O
, O
representation O
of O
the O
latent O
states O
. O
specifi O
( O
cid:173 O
) O
cally O
, O
the O
prior B
distribution O
over O
z O
is O
given O
by O
a O
zero-mean O
unit-covariance O
gaussian O
p O
( O
z O
) O
= O
n O
( O
zio O
, O
i O
) O
. O
( O
5.122 O
) O
note O
that O
priors O
of O
this O
form O
are O
improper B
( O
they O
can O
not O
be O
normalized O
) O
because O
the O
bias B
parameters O
are O
unconstrained O
. O
a O
different O
approach O
to O
multiclass B
classiﬁcation O
, O
based O
on O
error-correcting O
out- O
put O
codes O
, O
was O
developed O
by O
dietterich O
and O
bakiri O
( O
1995 O
) O
and O
applied O
to O
support B
vector I
machines O
by O
allwein O
et O
al O
. O
this O
can O
be O
done O
by O
showing O
that O
the O
corresponding O
conditional B
distribution O
p O
( O
t|x O
) O
can O
not O
be O
correctly O
normalized O
. O
the O
model O
might O
also O
contain O
some O
deterministic O
parameters O
, O
which O
we O
will O
leave O
implicit O
for O
the O
moment O
, O
or O
it O
may O
be O
a O
fully O
bayesian O
model O
in O
which O
any O
unknown O
parameters O
are O
given O
prior B
distributions O
and O
are O
absorbed O
into O
the O
set O
of O
latent O
variables O
denoted O
by O
the O
vector O
z. O
for O
instance O
, O
in O
the O
em O
algorithm O
we O
need O
to O
evaluate O
the O
expectation B
of O
the O
complete-data O
log O
likelihood O
with O
respect O
to O
the O
posterior O
distribution O
of O
the O
latent O
variables O
. O
( O
1.74 O
) O
exercise O
1.16 O
i=1 O
i=1 O
j=1 O
i=1 O
j=1 O
k=1 O
as O
d O
increases O
, O
so O
the O
number O
of O
independent B
coefﬁcients O
( O
not O
all O
of O
the O
coefﬁcients O
are O
independent B
due O
to O
interchange O
symmetries B
amongst O
the O
x O
variables O
) O
grows O
pro- O
portionally O
to O
d3 O
. O
write O
down O
expressions O
for O
the O
conditional B
density O
p O
( O
t|x O
) O
and O
for O
the O
conditional B
mean O
e O
[ O
t|x O
] O
and O
variance B
var O
[ O
t|x O
] O
, O
in O
terms O
of O
the O
kernel B
function I
k O
( O
x O
, O
xn O
) O
. O
10.5. O
local B
variational O
methods O
493 O
10.5. O
local B
variational O
methods O
the O
variational B
framework O
discussed O
in O
sections O
10.1 O
and O
10.2 O
can O
be O
considered O
a O
‘ O
global O
’ O
method O
in O
the O
sense O
that O
it O
directly O
seeks O
an O
approximation O
to O
the O
full O
poste- O
rior O
distribution O
over O
all O
random O
variables O
. O
( O
14.37 O
) O
−1 O
) O
exercise O
14.14 O
k O
( O
cid:2 O
) O
the O
responsibilities O
are O
then O
used O
to O
determine O
the O
expectation B
, O
with O
respect O
to O
the O
posterior O
distribution O
p O
( O
z|t O
, O
θold O
) O
, O
of O
the O
complete-data O
log O
likelihood O
, O
which O
takes O
n O
( O
cid:2 O
) O
( O
cid:27 O
) O
the O
form O
q O
( O
θ O
, O
θold O
) O
= O
ez O
[ O
ln O
p O
( O
t O
, O
z|θ O
) O
] O
= O
( O
cid:5 O
) O
in O
the O
m O
step O
, O
we O
maximize O
the O
function O
q O
( O
θ O
, O
θold O
) O
with O
respect O
to O
θ O
, O
keeping O
the O
γnk O
ﬁxed O
. O
( O
8.101 O
) O
( O
cid:8 O
) O
( O
cid:9 O
) O
to O
understand O
better O
what O
is O
happening O
, O
it O
is O
helpful O
to O
represent O
the O
chain O
of O
vari- O
ables O
in O
terms O
of O
a O
lattice O
or O
trellis B
diagram I
as O
shown O
in O
figure O
8.53. O
note O
that O
this O
is O
not O
a O
probabilistic B
graphical I
model I
because O
the O
nodes O
represent O
individual O
states O
of O
variables O
, O
while O
each O
variable O
corresponds O
to O
a O
column O
of O
such O
states O
in O
the O
di- O
agram O
. O
, O
n. O
zn O
xn O
π O
µ O
σ O
n O
434 O
9. O
mixture B
models O
and O
em O
figure O
9.7 O
illustration O
of O
how O
singularities B
in O
the O
likelihood B
function I
arise O
with O
mixtures O
of O
gaussians O
. O
the O
directed B
graphs O
that O
we O
are O
considering O
are O
subject O
to O
an O
important O
restric- O
tion O
namely O
that O
there O
must O
be O
no O
directed B
cycles O
, O
in O
other O
words O
there O
are O
no O
closed O
paths O
within O
the O
graph O
such O
that O
we O
can O
move O
from O
node B
to O
node B
along O
links O
follow- O
ing O
the O
direction O
of O
the O
arrows O
and O
end O
up O
back O
at O
the O
starting O
node B
. O
first O
of O
all O
, O
we O
note O
that O
a O
coefﬁcient O
an O
can O
only O
be O
nonzero O
if O
 O
+ O
ξn O
+ O
yn O
− O
tn O
= O
0 O
, O
which O
implies O
that O
the O
data O
point O
either O
lies O
on O
the O
upper O
boundary O
of O
the O
-tube B
( O
ξn O
= O
0 O
) O
or O
lies O
above O
the O
upper O
boundary O
( O
ξn O
> O
0 O
) O
. O
by O
run- O
p O
( O
hi|v O
= O
( O
cid:1 O
) O
v O
) O
up O
to O
a O
normalization O
coefﬁcient O
whose O
value O
can O
be O
found O
efﬁciently O
ning O
the O
sum-product B
algorithm I
, O
we O
can O
efﬁciently O
calculate O
the O
posterior O
marginals O
using O
a O
local B
computation O
. O
the O
price O
we O
have O
paid O
is O
that O
we O
have O
introduced O
a O
variational B
parameter O
λ O
, O
and O
to O
obtain O
the O
tightest O
bound O
we O
must O
optimize O
with O
respect O
to O
λ. O
we O
can O
formulate O
this O
approach O
more O
generally O
using O
the O
framework O
of O
convex B
duality I
( O
rockafellar O
, O
1972 O
; O
jordan O
et O
al. O
, O
1999 O
) O
. O
( O
12.92 O
) O
for O
a O
given O
continuous O
density B
, O
there O
can O
be O
many O
principal O
curves O
. O
we O
see O
that O
this O
expression O
involves O
expectations O
with O
respect O
to O
the O
variational B
distributions O
of O
the O
parameters O
, O
and O
these O
are O
easily O
evaluated O
to O
give O
( O
cid:8 O
) O
( O
cid:9 O
) O
( O
xn O
− O
µk O
) O
tλk O
( O
xn O
− O
µk O
) O
eµk O
, O
λk O
= O
dβ O
ln O
( O
cid:4 O
) O
λk O
≡ O
e O
[ O
ln|λk| O
] O
= O
ln O
( O
cid:4 O
) O
πk O
≡ O
e O
[ O
ln O
πk O
] O
= O
ψ O
( O
αk O
) O
− O
ψ O
( O
( O
cid:1 O
) O
α O
) O
i=1 O
ψ O
2 O
( O
cid:15 O
) O
d O
( O
cid:2 O
) O
k O
+ O
νk O
( O
xn O
− O
mk O
) O
twk O
( O
xn O
− O
mk O
) O
−1 O
( O
cid:16 O
) O
νk O
+ O
1 O
− O
i O
( O
10.64 O
) O
+ O
d O
ln O
2 O
+ O
ln|wk| O
( O
10.65 O
) O
( O
10.66 O
) O
appendix O
b O
( O
cid:5 O
) O
10.2. O
illustration O
: O
variational B
mixture O
of O
gaussians O
where O
we O
have O
introduced O
deﬁnitions O
of O
( O
cid:4 O
) O
λk O
and O
( O
cid:4 O
) O
πk O
, O
and O
ψ O
( O
· O
) O
is O
the O
digamma B
function I
deﬁned O
by O
( O
b.25 O
) O
, O
with O
( O
cid:1 O
) O
α O
= O
( O
cid:12 O
) O
if O
we O
substitute O
( O
10.64 O
) O
, O
( O
10.65 O
) O
, O
and O
( O
10.66 O
) O
into O
( O
10.46 O
) O
and O
make O
use O
of O
the O
standard O
properties O
of O
the O
wishart O
and O
dirichlet O
distributions O
. O
we O
therefore O
need O
to O
ﬁnd O
density B
models O
that O
are O
very O
ﬂexible O
and O
yet O
for O
which O
the O
complexity O
of O
the O
models O
can O
be O
controlled O
independently O
of O
the O
size O
of O
the O
training B
set I
, O
and O
we O
shall O
see O
in O
subsequent O
chapters O
how O
to O
achieve O
this O
. O
we O
can O
make O
deterministic O
approximations O
such O
as O
assumed O
den- O
sity O
ﬁltering O
or O
expectation B
propagation I
, O
or O
we O
can O
make O
use O
of O
sampling B
methods I
, O
as O
discussed O
in O
section O
13.3.4. O
one O
widely O
used O
approach O
is O
to O
make O
a O
gaussian O
approximation O
by O
linearizing O
around O
the O
mean B
of O
the O
predicted O
distribution O
, O
which O
gives O
rise O
to O
the O
extended B
kalman O
ﬁlter O
( O
zarchan O
and O
musoff O
, O
2005 O
) O
. O
we O
can O
define O
additional O
principal O
components O
in O
an O
incremental O
fashion O
by O
choosing O
each O
new O
direction O
to O
be O
that O
which O
maximizes O
the O
projected O
variance B
( O
12.6 O
) O
exercise O
12.1 O
section O
12.2.2 O
appendix O
c O
12.1. O
principal B
component I
analysis I
563 O
amongst O
all O
possible O
directions O
orthogonal O
to O
those O
already O
considered O
. O
for O
each O
choice O
of O
m O
, O
we O
can O
then O
evaluate O
the O
residual O
value O
of O
e O
( O
w O
( O
cid:1 O
) O
) O
given O
by O
( O
1.2 O
) O
for O
the O
training B
data O
, O
and O
we O
can O
also O
evaluate O
e O
( O
w O
( O
cid:1 O
) O
) O
for O
the O
test O
data O
set O
. O
1.2 O
( O
( O
cid:1 O
) O
) O
write O
down O
the O
set O
of O
coupled O
linear O
equations O
, O
analogous O
to O
( O
1.122 O
) O
, O
satisﬁed O
by O
the O
coefﬁcients O
wi O
which O
minimize O
the O
regularized O
sum-of-squares O
error B
function I
given O
by O
( O
1.4 O
) O
. O
64 O
1. O
introduction O
1.24 O
( O
( O
cid:1 O
) O
( O
cid:1 O
) O
) O
www O
consider O
a O
classiﬁcation B
problem O
in O
which O
the O
loss O
incurred O
when O
an O
input O
vector O
from O
class O
ck O
is O
classiﬁed O
as O
belonging O
to O
class O
cj O
is O
given O
by O
the O
loss B
matrix I
lkj O
, O
and O
for O
which O
the O
loss O
incurred O
in O
selecting O
the O
reject B
option I
is O
λ. O
find O
the O
decision O
criterion O
that O
will O
give O
the O
minimum O
expected O
loss O
. O
a O
stochastic B
approximation O
method O
. O
note O
that O
we O
shall O
shortly O
introduce O
a O
dual B
representation I
expressed O
in O
terms O
of O
kernel O
functions O
, O
which O
avoids O
having O
to O
work O
explicitly O
in O
feature B
space I
. O
an O
example O
involving O
speech O
data O
is O
shown O
in O
figure O
13.1. O
sequential B
data I
can O
also O
arise O
in O
contexts O
other O
than O
time O
series O
, O
for O
example O
the O
sequence O
of O
nucleotide O
base O
pairs O
along O
a O
strand O
of O
dna O
or O
the O
sequence O
of O
characters O
in O
an O
english O
sentence O
. O
for O
instance O
, O
one O
common O
criticism O
of O
the O
bayesian O
approach O
is O
that O
the O
prior B
distribution O
is O
of- O
ten O
selected O
on O
the O
basis O
of O
mathematical O
convenience O
rather O
than O
as O
a O
reﬂection O
of O
any O
prior B
beliefs O
. O
11.12 O
( O
( O
cid:12 O
) O
) O
consider O
the O
distribution O
shown O
in O
figure O
11.15. O
discuss O
whether O
the O
standard O
gibbs O
sampling O
procedure O
for O
this O
distribution O
is O
ergodic O
, O
and O
therefore O
whether O
it O
would O
sample O
correctly O
from O
this O
distribution O
11.13 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
simple O
3-node O
graph O
shown O
in O
figure O
11.16 O
in O
which O
the O
observed O
node O
x O
is O
given O
by O
a O
gaussian O
distribution O
n O
( O
x|µ O
, O
τ O
−1 O
) O
with O
mean B
µ O
and O
precision O
τ O
. O
they O
arose O
naturally O
from O
the O
structure O
of O
the O
likelihood B
function I
and O
the O
corresponding O
conjugate B
priors O
. O
by O
contrast O
, O
bayesian O
inference B
automatically O
makes O
the O
trade-off O
between O
model O
complexity O
and O
ﬁtting O
the O
data O
. O
thus O
the O
fraction O
of O
examples O
that O
get O
rejected O
is O
controlled O
by O
the O
value O
of O
θ. O
we O
can O
easily O
extend O
the O
reject O
criterion O
to O
minimize O
the O
expected O
loss O
, O
when O
a O
loss B
matrix I
is O
given O
, O
taking O
account O
of O
the O
loss O
incurred O
when O
a O
reject O
decision O
is O
made O
. O
12.2 O
( O
** O
) O
show O
that O
the O
minimum O
value O
of O
the O
pca O
distortion B
measure I
j O
given O
by O
( O
12.15 O
) O
with O
respect O
to O
the O
ui O
, O
subject O
to O
the O
orthonormality O
constraints O
( O
12.7 O
) O
, O
is O
obtained O
when O
the O
ui O
are O
eigenvectors O
of O
the O
data O
covariance O
matrix O
s. O
to O
do O
this O
, O
introduce O
a O
matrix O
h O
of O
lagrange O
multipliers O
, O
one O
for O
each O
constraint O
, O
so O
that O
the O
modified O
distortion B
measure I
, O
in O
matrix O
notation O
reads O
] O
= O
tr O
{ O
utsu O
} O
+ O
tr O
{ O
h O
( O
i O
- O
utu O
) O
} O
( O
12.93 O
) O
where O
uis O
a O
m~trix O
of O
dimensio~d O
x O
( O
d O
- O
m O
) O
whose O
columns O
are O
gi O
: O
: O
: O
..en O
b~ O
ui O
. O
we O
therefore O
need O
to O
solve O
the O
inverse B
problem I
, O
which O
has O
two O
solutions O
as O
seen O
in O
figure O
5.18. O
forward O
problems O
often O
corresponds O
to O
causality B
in O
a O
physical O
system O
and O
gen- O
erally O
have O
a O
unique O
solution O
. O
if O
we O
assume O
that O
its O
value O
is O
uncorrelated O
with O
the O
value O
of O
the O
second O
derivative O
term O
on O
the O
right-hand O
side O
of O
( O
5.83 O
) O
, O
then O
the O
whole O
term O
will O
average O
to O
zero O
in O
the O
summation O
over O
n. O
by O
neglecting O
the O
second O
term O
in O
( O
5.83 O
) O
, O
we O
arrive O
at O
the O
levenberg–marquardt O
approximation O
or O
outer B
product I
approximation I
( O
because O
the O
hessian O
matrix O
is O
built O
up O
from O
a O
sum O
of O
outer O
products O
of O
vectors O
) O
, O
given O
by O
bnbt O
n O
n=1 O
( O
5.84 O
) O
where O
bn O
= O
∇yn O
= O
∇an O
because O
the O
activation B
function I
for O
the O
output O
units O
is O
simply O
the O
identity O
. O
2.3.5 O
2.3.6 O
bayesian O
inference B
for O
the O
gaussian O
. O
in O
the O
case O
of O
a O
directed B
graph O
with O
no O
observed O
variables O
, O
it O
is O
11. O
sampling B
methods I
525 O
straightforward O
to O
sample O
from O
the O
joint O
distribution O
( O
assuming O
that O
it O
is O
possible O
to O
sample O
from O
the O
conditional B
distributions O
at O
each O
node B
) O
using O
the O
following O
ances- O
tral O
sampling O
approach O
, O
discussed O
brieﬂy O
in O
section O
8.1.2. O
the O
joint O
distribution O
is O
speciﬁed O
by O
m O
( O
cid:14 O
) O
p O
( O
z O
) O
= O
p O
( O
zi|pai O
) O
( O
11.4 O
) O
i=1 O
where O
zi O
are O
the O
set O
of O
variables O
associated O
with O
node B
i O
, O
and O
pai O
denotes O
the O
set O
of O
variables O
associated O
with O
the O
parents O
of O
node B
i. O
to O
obtain O
a O
sample O
from O
the O
joint O
distribution O
, O
we O
make O
one O
pass O
through O
the O
set O
of O
variables O
in O
the O
order O
z1 O
, O
. O
multiplication O
of O
this O
ratio O
by O
the O
ratio O
of O
prior B
probabilities O
gives O
the O
ratio O
of O
posterior O
probabilities O
, O
which O
can O
then O
be O
used O
for O
model O
selection O
or O
model B
averaging I
. O
however O
, O
if O
there O
is O
at O
least O
one O
such O
path O
that O
is O
not O
blocked O
, O
then O
the O
property O
does O
not O
necessarily O
hold O
, O
or O
more O
precisely O
there O
will O
exist O
at O
least O
some O
distributions O
corresponding O
to O
the O
graph O
that O
do O
not O
satisfy O
this O
conditional B
independence I
relation O
. O
these O
results O
can O
be O
used O
as O
an O
easy-to-compute O
approximation O
to O
the O
full O
evidence O
re-estimation O
172 O
3. O
linear O
models O
for B
regression I
−5 O
0 O
ln O
α O
5 O
−5 O
0 O
ln O
α O
5 O
figure O
3.16 O
the O
left O
plot O
shows O
γ O
( O
red O
curve O
) O
and O
2αew O
( O
mn O
) O
( O
blue O
curve O
) O
versus O
ln O
α O
for O
the O
sinusoidal O
synthetic O
data O
set O
. O
for O
simple O
unconditional O
mixtures O
, O
this O
hierarchical O
mixture O
is O
trivially O
equivalent O
to O
a O
single O
ﬂat O
mixture B
distribution I
. O
super- O
vised O
learning B
from O
incomplete O
data O
via O
an O
em O
appproach O
. O
exercise O
4.15 O
the O
newton-raphson O
update O
formula O
for O
the O
logistic B
regression I
model O
then O
be- O
comes O
w O
( O
new O
) O
= O
w O
( O
old O
) O
− O
( O
φtrφ O
) O
−1φt O
( O
y O
− O
t O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
φtrφw O
( O
old O
) O
− O
φt O
( O
y O
− O
t O
) O
= O
( O
φtrφ O
) O
−1 O
= O
( O
φtrφ O
) O
−1φtrz O
where O
z O
is O
an O
n-dimensional O
vector O
with O
elements O
z O
= O
φw O
( O
old O
) O
− O
r−1 O
( O
y O
− O
t O
) O
. O
( O
cid:20 O
) O
( O
cid:20 O
) O
substituting O
into O
the O
mean B
error O
function O
( O
5.130 O
) O
and O
expanding O
, O
we O
then O
have O
( O
cid:4 O
) O
e O
= O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:29 O
) O
1 O
2 O
{ O
y O
( O
x O
) O
− O
t O
} O
2p O
( O
t|x O
) O
p O
( O
x O
) O
dx O
dt O
+ O
e O
[ O
ξ O
] O
{ O
y O
( O
x O
) O
− O
t O
} O
τ O
t∇y O
( O
x O
) O
p O
( O
t|x O
) O
p O
( O
x O
) O
dx O
dt O
+ O
e O
[ O
ξ2 O
] O
( O
cid:10 O
) O
{ O
y O
( O
x O
) O
− O
t O
} O
1 O
2 O
. O
in O
practice O
, O
however O
, O
we O
are O
not O
given O
the O
complete B
data I
set I
{ O
x O
, O
z O
} O
, O
but O
only O
the O
incomplete O
data O
x. O
our O
state O
of O
knowledge O
of O
the O
values O
of O
the O
latent O
variables O
in O
z O
is O
given O
only O
by O
the O
posterior O
distribution O
p O
( O
z|x O
, O
θ O
) O
. O
( O
3.76 O
) O
proceed O
by O
evaluating O
the O
marginal B
likelihood I
for O
the O
linear O
basis O
function O
model O
and O
then O
ﬁnding O
its O
maxima O
. O
we O
can O
determine O
the O
mean B
and O
covariance B
of O
the O
joint O
distribution O
recursively O
as O
follows O
. O
reinforcement B
learning I
continues O
to O
be O
an O
active O
area O
of O
machine O
learning O
research O
. O
we O
close O
this O
chapter O
by O
introducing O
some O
additional O
concepts O
from O
the O
ﬁeld O
of O
information B
theory I
, O
which O
will O
also O
prove O
useful O
in O
our O
development O
of O
pattern O
recognition O
and O
machine O
learning O
techniques O
. O
0 O
, O
the O
posterior O
mean O
for O
the O
probabilistic O
pca O
model O
becomes O
an O
orthogonal O
projection O
onto O
the O
principal B
subspace I
, O
as O
in O
conventional O
pca O
. O
j O
( O
10.243 O
) O
( O
cid:14 O
) O
p O
( O
d O
) O
( O
cid:7 O
) O
10.37 O
( O
( O
cid:12 O
) O
) O
www O
consider O
the O
expectation B
propagation I
algorithm O
from O
section O
10.7 O
, O
and O
suppose O
that O
one O
of O
the O
factors O
f0 O
( O
θ O
) O
in O
the O
deﬁnition O
( O
10.188 O
) O
has O
the O
same O
expo- O
nential O
family O
functional B
form O
as O
the O
approximating O
distribution O
q O
( O
θ O
) O
. O
expansions O
in O
radial O
basis O
functions O
also O
arise O
from O
regularization B
theory O
( O
pog- O
gio O
and O
girosi O
, O
1990 O
; O
bishop O
, O
1995a O
) O
. O
we O
shall O
see O
that O
an O
important O
role O
is O
played O
by O
conjugate B
priors O
, O
that O
lead O
to O
posterior O
distributions O
having O
the O
same O
functional B
form O
as O
the O
prior B
, O
and O
that O
there- O
fore O
lead O
to O
a O
greatly O
simpliﬁed O
bayesian O
analysis O
. O
6.10 O
( O
( O
cid:12 O
) O
) O
show O
that O
an O
excellent O
choice O
of O
kernel O
for O
learning B
a O
function O
f O
( O
x O
) O
is O
given O
by O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
f O
( O
x O
) O
f O
( O
x O
( O
cid:4 O
) O
) O
by O
showing O
that O
a O
linear O
learning O
machine O
based O
on O
this O
kernel O
will O
always O
ﬁnd O
a O
solution O
proportional O
to O
f O
( O
x O
) O
. O
the O
parameter O
αb O
2 O
, O
and O
αw O
1 O
, O
αw O
1 O
, O
αb O
a O
minimum O
of O
the O
training B
error O
has O
been O
reached O
then O
represents O
a O
way O
of O
limiting O
the O
effective O
network O
complexity O
. O
4.20 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
hessian O
matrix O
for O
the O
multiclass B
logistic O
regression B
problem O
, O
deﬁned O
by O
( O
4.110 O
) O
, O
is O
positive O
semideﬁnite O
. O
7.15 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
using O
the O
results O
( O
7.94 O
) O
and O
( O
7.95 O
) O
, O
show O
that O
the O
marginal B
likelihood I
( O
7.85 O
) O
can O
be O
written O
in O
the O
form O
( O
7.96 O
) O
, O
where O
λ O
( O
αn O
) O
is O
deﬁned O
by O
( O
7.97 O
) O
and O
the O
sparsity B
and O
quality O
factors O
are O
deﬁned O
by O
( O
7.98 O
) O
and O
( O
7.99 O
) O
, O
respectively O
. O
in O
particular O
, O
the O
basis O
functions O
are O
given O
by O
kernels O
, O
with O
one O
kernel O
associated O
with O
each O
of O
the O
data O
points O
from O
the O
training B
set I
. O
for O
continuous O
observed O
variables O
x O
, O
one O
possible O
choice O
of O
emission O
model O
is O
a O
linear-gaussian O
density B
in O
which O
the O
mean B
of O
the O
gaussian O
is O
a O
linear O
combi- O
nation O
of O
the O
states O
of O
the O
corresponding O
latent O
variables O
. O
13.1. O
markov O
models O
the O
easiest O
way O
to O
treat O
sequential B
data I
would O
be O
simply O
to O
ignore O
the O
sequential O
aspects O
and O
treat O
the O
observations O
as O
i.i.d. B
, O
corresponding O
to O
the O
graph O
in O
figure O
13.2. O
such O
an O
approach O
, O
however O
, O
would O
fail O
to O
exploit O
the O
sequential O
patterns O
in O
the O
data O
, O
such O
as O
correlations O
between O
observations O
that O
are O
close O
in O
the O
sequence O
. O
to O
begin O
with O
, O
consider O
a O
two-class O
problem O
in O
which O
there O
are O
n1 O
points O
of O
class O
c1 O
and O
n2 O
points O
of O
class O
c2 O
, O
so O
that O
the O
mean B
vectors O
of O
the O
two O
classes O
are O
given O
by O
xn O
, O
m2 O
= O
xn O
. O
( O
c O
) O
( O
d O
) O
a O
related O
technique O
, O
called O
tangent B
distance I
, O
can O
be O
used O
to O
build O
invariance B
properties O
into O
distance-based O
methods O
such O
as O
nearest-neighbour O
classiﬁers O
( O
simard O
et O
al. O
, O
1993 O
) O
. O
, O
xd O
) O
t O
, O
which O
we O
denote O
by O
x. O
we O
shall O
suppose O
that O
the O
observations O
are O
drawn O
independently O
from O
a O
gaussian O
distribution O
whose O
mean B
µ O
and O
variance B
σ2 O
are O
unknown O
, O
and O
we O
would O
like O
to O
determine O
these O
parameters O
from O
the O
data O
set O
. O
in O
order O
to O
minimize O
the O
error B
function I
, O
we O
need O
to O
calculate O
the O
derivatives O
of O
the O
error B
e O
( O
w O
) O
with O
respect O
to O
the O
components O
of O
w. O
these O
can O
be O
evaluated O
by O
using O
the O
standard O
backpropagation O
procedure O
, O
provided O
we O
obtain O
suitable O
expres- O
sions O
for O
the O
derivatives O
of O
the O
error B
with O
respect O
to O
the O
output-unit O
activations O
. O
thus O
the O
maximization O
of O
the O
log O
likelihood O
function O
is O
not O
a O
well O
posed O
problem O
because O
such O
singularities B
will O
always O
be O
present O
and O
will O
occur O
whenever O
one O
of O
the O
gaussian O
components O
‘ O
collapses O
’ O
onto O
a O
speciﬁc O
data O
point O
. O
a O
c O
b O
figure O
8.18 O
as O
in O
figure O
8.17 O
but O
now O
conditioning O
on O
node B
c. O
a O
c O
8.2. O
conditional B
independence I
which O
in O
general O
does O
not O
factorize O
into O
p O
( O
a O
) O
p O
( O
b O
) O
, O
and O
so O
a O
( O
cid:9 O
) O
⊥⊥ O
b O
| O
∅ O
375 O
b O
( O
8.27 O
) O
as O
before O
. O
however O
, O
the O
latent O
variables O
{ O
zn O
} O
are O
no O
longer O
treated O
as O
independent B
but O
now O
form O
a O
markov O
chain O
. O
here O
the O
input-dependent O
variance B
is O
given O
by O
σ2 O
( O
x O
) O
= O
1 O
β O
+ O
φ O
( O
x O
) O
tsn O
φ O
( O
x O
) O
. O
in O
principle O
, O
a O
histogram O
density O
model O
is O
also O
dependent O
on O
the O
choice O
of O
edge B
location O
for O
the O
bins O
, O
though O
this O
is O
typically O
much O
less O
signiﬁcant O
than O
the O
value O
of O
∆ O
. O
later O
we O
shall O
discuss O
a O
more O
effective O
algorithm O
for O
ﬁnding O
high O
probability B
so- O
lutions O
called O
the O
max-product O
algorithm O
, O
which O
typically O
leads O
to O
better O
solutions O
, O
although O
this O
is O
still O
not O
guaranteed O
to O
ﬁnd O
the O
global O
maximum O
of O
the O
posterior O
dis- O
tribution O
. O
instead O
, O
a O
greedy O
opti- O
mization O
is O
generally O
done O
by O
starting O
with O
a O
single O
root B
node I
, O
corresponding O
to O
the O
whole O
input O
space O
, O
and O
then O
growing O
the O
tree B
by O
adding O
nodes O
one O
at O
a O
time O
. O
as O
we O
shall O
see O
, O
this O
is O
more O
complex O
than O
the O
bayesian O
treatment O
of O
linear B
regression I
models O
, O
discussed O
in O
sections O
3.3 O
and O
3.5. O
in O
particular O
, O
we O
can O
not O
integrate O
exactly O
214 O
4. O
linear O
models O
for O
classification O
chapter O
10 O
chapter O
11 O
over O
the O
parameter O
vector O
w O
since O
the O
posterior O
distribution O
is O
no O
longer O
gaussian O
. O
5.6. O
mixture O
density O
networks O
exercise O
5.33 O
the O
goal O
of O
supervised B
learning I
is O
to O
model O
a O
conditional B
distribution O
p O
( O
t|x O
) O
, O
which O
for O
many O
simple O
regression B
problems O
is O
chosen O
to O
be O
gaussian O
. O
evaluation O
of O
the O
predictive B
distribution I
is O
similarly O
intractable O
. O
equivalently O
, O
we O
can O
use O
the O
‘ O
tanh O
’ O
function O
because O
this O
is O
related O
to O
the O
logistic B
sigmoid I
by O
tanh O
( O
a O
) O
= O
2σ O
( O
a O
) O
− O
1 O
, O
and O
so O
a O
general O
linear O
combination O
of O
logistic B
sigmoid I
functions O
is O
equivalent O
to O
a O
general O
linear O
combination O
of O
‘ O
tanh O
’ O
functions O
. O
5.1.1 O
weight-space O
symmetries O
one O
property O
of O
feed-forward O
networks O
, O
which O
will O
play O
a O
role O
when O
we O
consider O
bayesian O
model B
comparison I
, O
is O
that O
multiple O
distinct O
choices O
for O
the O
weight B
vector I
w O
can O
all O
give O
rise O
to O
the O
same O
mapping O
function O
from O
inputs O
to O
outputs O
( O
chen O
et O
al. O
, O
1993 O
) O
. O
speciﬁcally O
, O
we O
shall O
consider O
a O
two-layer O
network O
of O
the O
form O
illustrated O
in O
figure O
5.1 O
, O
together O
with O
a O
sum-of-squares B
error I
, O
in O
which O
the O
output O
units O
have O
linear O
activation O
functions O
, O
so O
that O
yk O
= O
ak O
, O
while O
the O
hidden O
units O
have O
logistic B
sigmoid I
activation O
functions O
given O
by O
where O
( O
5.58 O
) O
( O
5.59 O
) O
( O
5.62 O
) O
( O
5.63 O
) O
( O
5.64 O
) O
a O
useful O
feature O
of O
this O
function O
is O
that O
its O
derivative B
can O
be O
expressed O
in O
a O
par- O
ticularly O
simple O
form O
: O
( O
5.60 O
) O
we O
also O
consider O
a O
standard O
sum-of-squares O
error B
function I
, O
so O
that O
for O
pattern O
n O
the O
error B
is O
given O
by O
h O
( O
cid:4 O
) O
( O
a O
) O
= O
1 O
− O
h O
( O
a O
) O
2 O
. O
as O
we O
have O
seen O
already O
, O
for O
the O
output O
units O
, O
we O
have O
δk O
= O
yk O
− O
tk O
( O
5.54 O
) O
244 O
5. O
neural O
networks O
figure O
5.7 O
illustration O
of O
the O
calculation O
of O
δj O
for O
hidden O
unit O
j O
by O
backpropagation B
of O
the O
δ O
’ O
s O
from O
those O
units O
k O
to O
which O
unit O
j O
sends O
connections O
. O
in O
order O
to O
arrive O
at O
an O
ergodic O
sampling O
scheme O
, O
we O
can O
introduce O
additional O
moves O
in O
phase B
space I
that O
change O
the O
value O
of O
h O
while O
also O
leaving O
the O
distribution O
p O
( O
z O
, O
r O
) O
invariant O
. O
3.20 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
starting O
from O
( O
3.86 O
) O
verify O
all O
of O
the O
steps O
needed O
to O
show O
that O
maxi- O
mization O
of O
the O
log O
marginal O
likelihood B
function I
( O
3.86 O
) O
with O
respect O
to O
α O
leads O
to O
the O
re-estimation O
equation O
( O
3.92 O
) O
. O
a O
classical B
maximum O
likelihood O
estimate O
of O
the O
probability B
of O
landing O
heads O
would O
give O
1 O
, O
implying O
that O
all O
future O
tosses O
will O
land O
heads O
! O
by O
contrast O
, O
a O
bayesian O
approach O
with O
any O
reasonable O
prior B
will O
lead O
to O
a O
much O
less O
extreme O
conclusion O
. O
show O
that O
, O
for O
q O
= O
1 O
, O
this O
solution O
represents O
the O
conditional B
median O
, O
i.e. O
, O
the O
function O
y O
( O
x O
) O
such O
that O
the O
probability B
mass O
for O
t O
< O
y O
( O
x O
) O
is O
the O
same O
as O
for O
t O
( O
cid:2 O
) O
y O
( O
x O
) O
. O
the O
centres O
and O
variances O
of O
the O
gaussian O
components O
, O
as O
well O
as O
the O
mixing O
coefﬁcients O
, O
will O
be O
considered O
as O
adjustable O
parameters O
to O
be O
determined O
as O
part O
of O
the O
learning B
process O
. O
2.15 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
entropy B
of O
the O
multivariate O
gaussian O
n O
( O
x|µ O
, O
σ O
) O
is O
given O
by O
h O
[ O
x O
] O
= O
1 O
2 O
ln|σ| O
+ O
d O
2 O
( O
1 O
+ O
ln O
( O
2π O
) O
) O
( O
2.283 O
) O
where O
d O
is O
the O
dimensionality O
of O
x. O
exercises O
131 O
2.16 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
two O
random O
variables O
x1 O
and O
x2 O
having O
gaussian O
distri- O
butions O
with O
means O
µ1 O
, O
µ2 O
and O
precisions O
τ1 O
, O
τ2 O
respectively O
. O
given O
a O
data O
set O
x O
= O
{ O
xn O
} O
of O
observed O
data O
points O
, O
the O
probabilistic O
pea O
model O
can O
be O
expressed O
as O
a O
directed B
graph O
, O
as O
shown O
in O
figure O
12.10. O
the O
corresponding O
log O
likelihood O
function O
is O
given O
, O
from O
( O
12.35 O
) O
, O
by O
inp O
( O
xijl O
, O
w O
, O
o O
' O
2 O
n O
) O
= O
l O
ln O
p O
( O
xn O
iw O
, O
jl O
, O
o'2 O
1 O
'' O
'' O
n=l O
n O
n O
) O
nd O
-- O
2- O
ln O
( O
2n O
) O
- O
2 O
ln O
ie O
[ O
- O
2 O
l O
, O
.. O
( O
xn O
- O
jl O
) O
c- O
( O
xn O
- O
jl O
) O
. O
components O
whose O
expected O
mixing B
coefﬁcient I
are O
numerically O
in- O
distinguishable O
from O
zero O
are O
not O
plotted O
. O
in O
the O
case O
of O
models O
of O
the O
form O
( O
7.78 O
) O
, O
the O
inputs O
xn O
corresponding O
to O
the O
remaining O
nonzero O
weights O
are O
called O
relevance O
vectors O
, O
because O
they O
are O
iden- O
tiﬁed O
through O
the O
mechanism O
of O
automatic B
relevance I
determination I
, O
and O
are O
analo- O
gous O
to O
the O
support O
vectors O
of O
an O
svm O
. O
our O
goal O
in O
regression B
, O
however O
, O
is O
to O
make O
predictions O
of O
the O
target O
variables O
for O
new O
inputs O
, O
given O
a O
set O
of O
training B
data O
. O
however O
, O
because O
a O
large O
number O
of O
sample O
points O
may O
be O
required O
in O
order O
to O
give O
an O
accurate O
representation O
of O
the O
marginal B
, O
this O
procedure O
can O
be O
computationally O
costly O
. O
although O
the O
introduction O
of O
regularization B
terms O
can O
control O
over-ﬁtting B
for O
models O
with O
many O
parameters O
, O
this O
raises O
the O
question O
of O
how O
to O
determine O
a O
suitable O
value O
for O
the O
regularization B
coefﬁcient O
λ. O
seeking O
the O
solution O
that O
minimizes O
the O
regularized O
error O
function O
with O
respect O
to O
both O
the O
weight B
vector I
w O
and O
the O
regularization B
coefﬁcient O
λ O
is O
clearly O
not O
the O
right O
approach O
since O
this O
leads O
to O
the O
unregularized O
solution O
with O
λ O
= O
0. O
as O
we O
have O
seen O
in O
earlier O
chapters O
, O
the O
phenomenon O
of O
over-ﬁtting B
is O
really O
an O
unfortunate O
property O
of O
maximum B
likelihood I
and O
does O
not O
arise O
when O
we O
marginalize O
over O
parameters O
in O
a O
bayesian O
setting O
. O
adaptive O
mixtures O
of O
local B
ex- O
perts O
. O
it O
might O
be O
thought O
that O
the O
limitations O
of O
a O
linear O
dimensionality O
reduction O
could O
be O
overcome O
by O
using O
nonlinear O
( O
sigmoidal O
) O
activation O
functions O
for O
the O
hidden O
units O
in O
the O
network O
in O
figure O
12.18. O
however O
, O
even O
with O
nonlinear O
hidden O
units O
, O
the O
min O
( O
cid:173 O
) O
imum O
error B
solution O
is O
again O
given O
by O
the O
projection O
onto O
the O
principal O
component O
subspace O
( O
bourlard O
and O
kamp O
, O
1988 O
) O
. O
ieee O
transactions O
on O
infor- O
mation B
theory O
it-11 O
, O
21–27 O
. O
this O
is O
known O
as O
a O
one-versus-the-rest B
classiﬁer I
. O
( O
3.68 O
) O
chapter O
11 O
from O
a O
sampling O
perspective O
, O
the O
marginal B
likelihood I
can O
be O
viewed O
as O
the O
proba- O
bility O
of O
generating O
the O
data O
set O
d O
from O
a O
model O
whose O
parameters O
are O
sampled O
at O
random O
from O
the O
prior B
. O
note O
that O
no O
particular O
modiﬁcation O
to O
the O
em O
results O
are O
required O
for O
the O
case O
of O
left-to-right B
models O
beyond O
choosing O
initial O
values O
for O
the O
elements O
ajk O
in O
which O
the O
appropriate O
elements O
are O
set O
to O
zero O
, O
because O
these O
will O
remain O
zero O
throughout O
. O
if O
, O
however O
, O
we O
are O
to O
choose O
a O
prior B
distribution O
that O
is O
constant O
, O
we O
must O
take O
care O
to O
use O
an O
appropriate O
representation O
for O
the O
parameters O
. O
the O
links O
are O
undirected B
, O
that O
is O
they O
do O
not O
carry O
arrows O
. O
( O
10.92 O
) O
we O
recognize O
this O
as O
the O
log O
of O
a O
gamma B
distribution I
, O
and O
so O
identifying O
the O
coefﬁ- O
cients O
of O
α O
and O
ln O
α O
we O
obtain O
q O
( O
cid:1 O
) O
( O
α O
) O
= O
gam O
( O
α|an O
, O
bn O
) O
where O
an O
= O
a0 O
+ O
m O
2 O
1 O
2 O
e O
[ O
wtw O
] O
. O
5.5. O
regularization B
in O
neural O
networks O
265 O
( O
a O
) O
( O
b O
) O
figure O
5.16 O
illustration O
showing O
( O
a O
) O
the O
original O
image O
x O
of O
a O
hand- O
written O
digit O
, O
( O
b O
) O
the O
tangent O
vector O
τ O
corresponding O
to O
an O
inﬁnitesimal O
clockwise O
rotation O
, O
( O
c O
) O
the O
result O
of O
adding O
a O
small O
contribution O
from O
the O
tangent O
vector O
to O
the O
original O
image O
giving O
x O
+ O
τ O
with O
 O
= O
15 O
degrees O
, O
and O
( O
d O
) O
the O
true O
image O
rotated O
for O
comparison O
. O
if O
we O
multiply O
by O
the O
likelihood B
function I
( O
2.145 O
) O
, O
then O
we O
obtain O
a O
posterior O
distribution O
( O
cid:25 O
) O
p O
( O
λ|x O
) O
∝ O
λa0−1λn/2 O
exp O
−b0λ O
− O
λ O
2 O
( O
xn O
− O
µ O
) O
2 O
( O
2.149 O
) O
which O
we O
recognize O
as O
a O
gamma B
distribution I
of O
the O
form O
gam O
( O
λ|an O
, O
bn O
) O
where O
an O
= O
a0 O
+ O
n O
2 O
1 O
2 O
bn O
= O
b0 O
+ O
n O
( O
cid:2 O
) O
n=1 O
( O
xn O
− O
µ O
) O
2 O
= O
b0 O
+ O
n O
2 O
σ2 O
ml O
( O
2.150 O
) O
( O
2.151 O
) O
where O
σ2 O
ml O
is O
the O
maximum B
likelihood I
estimator O
of O
the O
variance B
. O
in O
the O
univariate O
form O
, O
student O
’ O
s O
t-distribution O
is O
obtained O
by O
placing O
a O
conjugate B
gamma O
prior B
over O
the O
precision O
of O
a O
univariate O
gaussian O
distribution O
and O
then O
inte- O
grating O
out O
the O
precision O
variable O
. O
p O
( O
c1|φ O
) O
= O
y O
( O
φ O
) O
= O
σ O
wtφ O
for O
an O
m-dimensional O
feature B
space I
φ O
, O
this O
model O
has O
m O
adjustable O
parameters O
. O
( O
4.118 O
) O
using O
the O
same O
line O
of O
argument O
as O
led O
to O
the O
derivation O
of O
the O
result O
( O
2.226 O
) O
, O
we O
see O
that O
the O
conditional B
mean O
of O
t O
, O
which O
we O
denote O
by O
y O
, O
is O
given O
by O
y O
≡ O
e O
[ O
t|η O
] O
= O
−s O
d O
dη O
ln O
g O
( O
η O
) O
. O
in O
the O
e O
step O
, O
the O
lower B
bound I
l O
( O
q O
, O
θold O
) O
is O
maximized O
with O
respect O
to O
q O
( O
z O
) O
while O
holding O
θold O
ﬁxed O
. O
in O
practice O
, O
the O
results O
found O
using O
probit B
regression I
tend O
to O
be O
similar O
to O
those O
of O
logistic B
regression I
. O
the O
prior B
probabilities O
p O
( O
ck O
) O
enter O
only O
through O
the O
bias B
parameter I
w0 O
so O
that O
changes O
in O
the O
priors O
have O
the O
effect O
of O
making O
parallel O
shifts O
of O
the O
decision B
boundary I
and O
more O
generally O
of O
the O
parallel O
contours O
of O
constant O
posterior B
probability I
. O
similarly O
, O
evaluate O
the O
cross-entropy O
( O
14.32 O
) O
and O
gini O
index O
( O
14.33 O
) O
for O
the O
two O
trees O
and O
show O
that O
they O
are O
both O
lower O
for O
tree B
b O
than O
for O
tree O
a O
. O
the O
identity O
of O
the O
test O
point O
is O
predicted O
as O
being O
the O
same O
as O
the O
class O
having O
the O
largest O
number O
of O
training B
points O
in O
the O
same O
cell O
as O
the O
test O
point O
( O
with O
ties O
being O
broken O
at O
random O
) O
. O
4.14 O
( O
( O
cid:12 O
) O
) O
show O
that O
for O
a O
linearly B
separable I
data O
set O
, O
the O
maximum B
likelihood I
solution O
for O
the O
logistic B
regression I
model O
is O
obtained O
by O
ﬁnding O
a O
vector O
w O
whose O
decision B
boundary I
wtφ O
( O
x O
) O
= O
0 O
separates O
the O
classes O
and O
then O
taking O
the O
magnitude O
of O
w O
to O
inﬁnity O
. O
a O
comparison O
of O
gcv O
and O
gml O
for O
choosing O
the O
smoothing B
parameter I
in O
the O
gen- O
eralized O
spline O
smoothing O
problem O
. O
given O
a O
training B
data O
set O
, O
we O
then O
evaluated O
the O
posterior O
distribution O
over O
w O
and O
thereby O
obtained O
the O
corresponding O
posterior O
distribution O
over O
regression B
functions O
, O
which O
in O
turn O
( O
with O
the O
addition O
of O
noise O
) O
implies O
a O
predictive B
distribution I
p O
( O
t|x O
) O
for O
new O
input O
vectors O
x. O
in O
the O
gaussian O
process O
viewpoint O
, O
we O
dispense O
with O
the O
parametric O
model O
and O
instead O
deﬁne O
a O
prior B
probability O
distribution O
over O
functions O
directly O
. O
this O
allows O
relatively O
complex O
marginal B
distributions O
over O
observed O
variables O
to O
be O
ex- O
pressed O
in O
terms O
of O
more O
tractable O
joint O
distributions O
over O
the O
expanded O
space O
of O
observed O
and O
latent O
variables O
. O
8.3.1 O
conditional B
independence I
properties O
in O
the O
case O
of O
directed B
graphs O
, O
we O
saw O
that O
it O
was O
possible O
to O
test O
whether O
a O
par- O
ticular O
conditional B
independence I
property O
holds O
by O
applying O
a O
graphical O
test O
called O
d-separation B
. O
it O
should O
be O
emphasized O
that O
the O
conditional B
independence I
properties O
obtained O
from O
d-separation B
apply O
to O
any O
probabilistic O
model O
described O
by O
that O
particular O
di- O
rected O
graph O
. O
directed B
graphs O
are O
useful O
for O
expressing O
causal O
relationships O
between O
random O
variables O
, O
whereas O
undirected B
graphs O
are O
better O
suited O
to O
expressing O
soft B
con- O
straints O
between O
random O
variables O
. O
7.10 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
derive O
the O
result O
( O
7.85 O
) O
for O
the O
marginal B
likelihood I
function O
in O
the O
regression B
rvm O
, O
by O
performing O
the O
gaussian O
integral O
over O
w O
in O
( O
7.84 O
) O
using O
the O
technique O
of O
completing B
the I
square I
in O
the O
exponential O
. O
because O
the O
model O
has O
latent O
variables O
, O
this O
can O
be O
ad- O
dressed O
using O
the O
em O
algorithm O
, O
which O
was O
discussed O
in O
general O
terms O
in O
chapter O
9. O
we O
can O
derive O
the O
em O
algorithm O
for O
the O
linear B
dynamical I
system I
as O
follows O
. O
0 O
60 O
15 O
120 O
the O
prior B
tightly O
constrains O
the O
mixing O
coefﬁcients O
so O
that O
α0 O
→ O
∞ O
, O
then O
e O
[ O
πk O
] O
→ O
1/k O
. O
the O
role O
of O
the O
sum-product B
algorithm I
is O
to O
provide O
a O
more O
efﬁcient O
way O
to O
perform O
such O
computations O
. O
this O
may O
be O
used O
directly O
for O
sequential O
optimization O
, O
or O
the O
results O
can O
be O
accumulated O
over O
the O
training B
set I
in O
the O
case O
of O
batch O
methods O
. O
such O
a O
network O
is O
trained O
to O
map O
input O
vectors O
onto O
themselves O
by O
minimiza O
( O
cid:173 O
) O
tion O
ot O
a O
sum-ot-squares O
error B
. O
( O
2.262 O
) O
128 O
2. O
probability B
distributions O
use O
this O
result O
to O
prove O
by O
induction O
the O
following O
result O
( O
cid:15 O
) O
n O
( O
cid:2 O
) O
( O
cid:16 O
) O
n O
m O
m=0 O
( O
1 O
+ O
x O
) O
n O
= O
xm O
( O
2.263 O
) O
which O
is O
known O
as O
the O
binomial O
theorem O
, O
and O
which O
is O
valid O
for O
all O
real O
values O
of O
x. O
finally O
, O
show O
that O
the O
binomial B
distribution I
is O
normalized O
, O
so O
that O
µm O
( O
1 O
− O
µ O
) O
n−m O
= O
1 O
( O
2.264 O
) O
( O
cid:15 O
) O
n O
( O
cid:2 O
) O
( O
cid:16 O
) O
n O
m O
m=0 O
( O
cid:6 O
) O
1 O
( O
cid:6 O
) O
∞ O
0 O
which O
can O
be O
done O
by O
ﬁrst O
pulling O
out O
a O
factor O
( O
1 O
− O
µ O
) O
n O
out O
of O
the O
summation O
and O
then O
making O
use O
of O
the O
binomial O
theorem O
. O
we O
know O
from O
the O
form O
of O
the O
normalized O
gaussian O
given O
by O
( O
2.43 O
) O
, O
that O
this O
coefﬁcient O
is O
independent B
of O
the O
mean B
and O
depends O
only O
on O
the O
determinant O
of O
the O
covariance B
matrix I
. O
the O
graphical B
model I
for O
the O
complete O
data O
is O
shown O
in O
figure O
9.9 O
. O
as O
with O
the O
variational B
bayes O
methods O
discussed O
so O
far O
, O
this O
too O
is O
based O
on O
the O
minimization O
of O
a O
kullback-leibler O
divergence O
but O
now O
of O
the O
reverse O
form O
, O
which O
gives O
the O
approximation O
rather O
different O
properties O
. O
the O
gradient O
and O
hessian O
of O
this O
error B
function I
are O
given O
by O
w O
( O
new O
) O
= O
w O
( O
old O
) O
− O
h−1∇e O
( O
w O
) O
. O
30 O
years O
of O
adap- O
tive O
neural O
networks O
: O
perceptron B
, O
madeline O
, O
and O
backpropagation B
. O
13.2.2 O
the O
forward-backward B
algorithm I
. O
a O
markov O
random O
ﬁeld O
, O
also O
known O
as O
a O
markov O
network O
or O
an O
undirected B
graphical O
model O
( O
kindermann O
and O
snell O
, O
1980 O
) O
, O
has O
a O
set O
of O
nodes O
each O
of O
which O
corresponds O
to O
a O
variable O
or O
group O
of O
variables O
, O
as O
well O
as O
a O
set O
of O
links O
each O
of O
which O
connects O
a O
pair O
of O
nodes O
. O
suppose O
that O
the O
dependence O
of O
the O
error B
function I
on O
w O
takes O
the O
form O
j O
( O
w O
) O
= O
f O
( O
wtφ O
( O
x1 O
) O
, O
. O
the O
initial O
latent B
variable I
also O
has O
a O
gaussian O
distribution O
which O
we O
write O
as O
p O
( O
z1 O
) O
= O
n O
( O
z1|µ0 O
, O
v0 O
) O
. O
2.61 O
( O
( O
cid:12 O
) O
) O
show O
that O
the O
k-nearest-neighbour O
density B
model O
deﬁnes O
an O
improper B
distribu- O
tion O
whose O
integral O
over O
all O
space O
is O
divergent O
. O
using O
the O
1-of-k O
encoding O
scheme O
, O
we O
can O
represent O
these O
classes O
by O
a O
k- O
dimensional O
binary O
vector O
z. O
we O
can O
then O
deﬁne O
a O
generative B
model I
by O
introducing O
a O
multinomial O
prior O
p O
( O
z|µ O
) O
over O
the O
class O
labels O
, O
where O
the O
kth O
component O
µk O
of O
µ O
is O
the O
prior B
probability O
of O
class O
ck O
, O
together O
with O
a O
conditional B
distribution O
p O
( O
x|z O
) O
for O
the O
observed O
vector O
x. O
the O
key O
assumption O
of O
the O
naive O
bayes O
model O
is O
that O
, O
conditioned O
on O
the O
class O
z O
, O
the O
distributions O
of O
the O
input O
variables O
x1 O
, O
. O
similarly O
, O
consider O
the O
result O
( O
2.142 O
) O
for O
the O
variance B
of O
the O
posterior O
distribution O
. O
we O
have O
seen O
in O
section O
5.1.1 O
that O
any O
given O
mode O
in O
a O
two-layer O
network O
is O
a O
member O
of O
a O
set O
of O
m O
! O
2m O
equivalent O
modes O
that O
differ O
by O
interchange O
and O
sign-change O
symmetries B
, O
where O
m O
is O
the O
num- O
ber O
of O
hidden O
units O
. O
appendix O
a O
6.4. O
gaussian O
processes O
319 O
2 O
0 O
−2 O
−2 O
0 O
2 O
figure O
6.12 O
illustration O
of O
the O
use O
of O
a O
gaussian O
process O
for O
classiﬁcation O
, O
showing O
the O
data O
on O
the O
left O
together O
with O
the O
optimal O
decision B
boundary I
from O
the O
true O
distribution O
in O
green O
, O
and O
the O
decision B
boundary I
from O
the O
gaussian O
process O
classiﬁer O
in O
black O
. O
because O
our O
goal O
is O
to O
integrate O
out O
xb O
, O
this O
is O
most O
easily O
achieved O
by O
ﬁrst O
considering O
the O
terms O
involving O
xb O
and O
then O
completing B
the I
square I
in O
order O
to O
facilitate O
integration O
. O
2.6 O
( O
( O
cid:12 O
) O
) O
make O
use O
of O
the O
result O
( O
2.265 O
) O
to O
show O
that O
the O
mean B
, O
variance B
, O
and O
mode O
of O
the O
beta B
distribution I
( O
2.13 O
) O
are O
given O
respectively O
by O
e O
[ O
µ O
] O
= O
var O
[ O
µ O
] O
= O
mode O
[ O
µ O
] O
= O
a O
a O
+ O
b O
ab O
( O
a O
+ O
b O
) O
2 O
( O
a O
+ O
b O
+ O
1 O
) O
a O
− O
1 O
a O
+ O
b O
− O
2 O
. O
this O
can O
be O
viewed O
as O
a O
mixture B
representation O
of O
the O
form O
( O
13.119 O
) O
. O
arbitrarily O
pick O
any O
( O
variable O
or O
factor O
) O
node B
and O
designate O
it O
as O
the O
root O
. O
we O
might O
use O
more O
sophisticated O
techniques O
than O
least O
squares O
, O
for O
example O
regularization B
or O
a O
fully O
bayesian O
ap- O
proach O
, O
to O
determine O
the O
conditional B
distribution O
p O
( O
t|x O
) O
. O
we O
begin O
by O
taking O
some O
examples O
of O
the O
distributions O
introduced O
earlier O
in O
the O
chapter O
and O
showing O
that O
they O
are O
indeed O
members O
of O
the O
exponential B
family I
. O
extend O
this O
approach O
by O
inclusion O
of O
hyperpriors O
given O
by O
gamma O
distributions O
of O
the O
form O
( O
b.26 O
) O
and O
obtain O
the O
corresponding O
re-estimation O
formulae O
for O
α O
and O
β O
by O
maximizing O
the O
corresponding O
posterior B
probability I
p O
( O
t O
, O
α O
, O
β|x O
) O
with O
respect O
to O
α O
and O
β O
. O
the O
effect O
of O
mislabelling O
is O
easily O
incorporated O
into O
a O
probabilistic O
model O
by O
introducing O
a O
probability B
 O
that O
the O
target O
value O
t O
has O
been O
ﬂipped O
to O
the O
wrong O
value O
( O
opper O
and O
winther O
, O
2000a O
) O
, O
leading O
to O
a O
target O
value O
distribution O
for O
data O
point O
x O
of O
the O
form O
p O
( O
t|x O
) O
= O
( O
1 O
− O
 O
) O
σ O
( O
x O
) O
+ O
 O
( O
1 O
− O
σ O
( O
x O
) O
) O
= O
 O
+ O
( O
1 O
− O
2 O
) O
σ O
( O
x O
) O
( O
4.117 O
) O
where O
σ O
( O
x O
) O
is O
the O
activation B
function I
with O
input O
vector O
x. O
here O
 O
may O
be O
set O
in O
advance O
, O
or O
it O
may O
be O
treated O
as O
a O
hyperparameter B
whose O
value O
is O
inferred O
from O
the O
data O
. O
least O
squares O
corresponds O
to O
maximum B
likelihood I
under O
a O
gaussian O
assumption O
. O
here O
the O
black O
points O
de- O
note O
a O
data O
set O
of O
values O
{ O
xn O
} O
, O
and O
the O
likelihood B
function I
given O
by O
( O
1.53 O
) O
corresponds O
to O
the O
product O
of O
the O
blue O
values O
. O
if O
, O
as O
is O
often O
the O
case O
, O
p O
( O
z O
) O
f O
( O
z O
) O
is O
strongly O
varying O
and O
has O
a O
sig- O
niﬁcant O
proportion O
of O
its O
mass O
concentrated O
over O
relatively O
small O
regions O
of O
z O
space O
, O
then O
the O
set O
of O
importance B
weights I
{ O
rl O
} O
may O
be O
dominated O
by O
a O
few O
weights O
hav- O
ing O
large O
values O
, O
with O
the O
remaining O
weights O
being O
relatively O
insigniﬁcant O
. O
such O
solutions O
are O
discussed O
in O
more O
detail O
in O
the O
context O
of O
gaussian O
mixture B
models O
in O
section O
9.2.1. O
for O
the O
derivatives O
with O
respect O
to O
the O
mixing O
coefﬁcients O
πj O
, O
we O
need O
to O
take O
account O
of O
the O
constraints O
( O
cid:2 O
) O
πj O
= O
1 O
, O
0 O
( O
cid:1 O
) O
πi O
( O
cid:1 O
) O
1 O
( O
5.145 O
) O
j O
which O
follow O
from O
the O
interpretation O
of O
the O
πj O
as O
prior B
probabilities O
. O
bayesian O
neural O
networks O
and O
density B
networks O
. O
in O
this O
chapter O
, O
we O
consider O
linear O
models O
for O
classiﬁcation O
, O
by O
which O
we O
mean B
that O
the O
decision O
surfaces O
are O
linear O
functions O
of O
the O
input O
vector O
x O
and O
hence O
are O
deﬁned O
by O
( O
d O
− O
1 O
) O
-dimensional O
hyperplanes O
within O
the O
d-dimensional O
input O
space O
. O
a O
bayesian O
treatment O
of O
the O
factor B
analysis I
model O
can O
be O
obtained O
by O
a O
straightforward O
application O
of O
the O
techniques O
discussed O
in O
this O
book O
. O
in O
practice O
, O
a O
direct O
solution O
of O
the O
normal B
equations I
can O
lead O
to O
numerical O
difﬁ- O
culties O
when O
φtφ O
is O
close O
to O
singular O
. O
however O
, O
fhe O
projec1ioo O
of O
poinl O
> O
in O
feature O
< O
j'3c O
'' O
`` O
'' O
to O
the O
linear O
rca O
, O
ub O
, O
p O
'' O
'' O
'' O
in O
that O
'pace O
will O
typically O
'' O
' O
'' O
lie O
on O
fhe O
nonlinear O
d O
( O
cid:173 O
) O
dimensional O
manifold B
and O
! O
io O
will O
nul O
ha. O
, O
. O
( O
b.41 O
) O
−1 O
is O
the O
precision B
matrix I
, O
which O
is O
also O
the O
inverse B
of O
the O
covariance B
matrix I
λ O
= O
σ O
symmetric O
and O
positive B
deﬁnite I
. O
4.2.4 O
exponential B
family I
. O
the O
model O
deﬁnes O
a O
conditional B
distribution O
for O
a O
real-valued O
target O
variable O
t O
, O
given O
an O
input O
vector O
x O
, O
which O
takes O
the O
form O
p O
( O
t|x O
, O
w O
, O
β O
) O
= O
n O
( O
t|y O
( O
x O
) O
, O
β O
−1 O
) O
( O
7.76 O
) O
346 O
7. O
sparse O
kernel O
machines O
m O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
where O
β O
= O
σ O
by O
a O
linear O
model O
of O
the O
form O
−2 O
is O
the O
noise O
precision O
( O
inverse B
noise O
variance B
) O
, O
and O
the O
mean B
is O
given O
y O
( O
x O
) O
= O
wiφi O
( O
x O
) O
= O
wtφ O
( O
x O
) O
( O
7.77 O
) O
i=1 O
with O
ﬁxed O
nonlinear O
basis O
functions O
φi O
( O
x O
) O
, O
which O
will O
typically O
include O
a O
constant O
term O
so O
that O
the O
corresponding O
weight B
parameter I
represents O
a O
‘ O
bias B
’ O
. O
gaussian O
mixture B
models O
are O
widely O
used O
in O
data O
mining O
, O
pattern O
recognition O
, O
machine O
learning O
, O
and O
statistical O
analysis O
. O
as O
an O
illustration O
, O
consider O
the O
chain O
of O
nodes O
shown O
in O
figure O
8.10. O
the O
marginal B
distribution O
p O
( O
x1 O
) O
requires O
k O
− O
1 O
parameters O
, O
whereas O
each O
of O
the O
m O
− O
1 O
condi- O
tional O
distributions O
p O
( O
xi|xi−1 O
) O
, O
for O
i O
= O
2 O
, O
. O
1lic O
marginal B
likelihood I
v.lues O
. O
we O
now O
apply O
the O
stochastic B
gradient I
descent I
algorithm O
to O
this O
error B
function I
. O
3.17 O
( O
( O
cid:12 O
) O
) O
show O
that O
the O
evidence B
function I
for O
the O
bayesian O
linear B
regression I
model O
can O
be O
written O
in O
the O
form O
( O
3.78 O
) O
in O
which O
e O
( O
w O
) O
is O
deﬁned O
by O
( O
3.79 O
) O
. O
make O
use O
of O
the O
matrix O
identity O
( O
appendix O
c O
) O
( O
cid:10 O
) O
m O
+ O
vvt O
( O
cid:11 O
) O
−1 O
= O
m−1 O
− O
( O
m−1v O
) O
vtm−1 O
1 O
+ O
vtm−1v O
( O
cid:10 O
) O
( O
cid:11 O
) O
to O
show O
that O
the O
uncertainty O
σ2 O
given O
by O
( O
3.59 O
) O
satisﬁes O
n O
( O
x O
) O
associated O
with O
the O
linear B
regression I
function O
n O
+1 O
( O
x O
) O
( O
cid:1 O
) O
σ2 O
σ2 O
n O
( O
x O
) O
. O
using O
the O
general O
result O
( O
4.135 O
) O
for O
an O
integral O
evaluated O
using O
the O
laplace O
approxi- O
7.2. O
relevance B
vector I
machines O
355 O
mation B
, O
we O
have O
( O
cid:6 O
) O
( O
cid:7 O
) O
p O
( O
t|w O
( O
cid:1 O
) O
) O
p O
( O
w O
( O
cid:1 O
) O
|α O
) O
( O
2π O
) O
m/2|σ|1/2 O
. O
this O
corresponds O
to O
acting O
on O
the O
original O
forward-propagation O
and O
back- O
propagation O
equations O
with O
a O
differential B
operator O
vt∇ O
. O
suppose O
we O
consider O
a O
particular O
joint O
probability B
distribution O
p O
( O
x O
) O
over O
the O
variables O
x O
corresponding O
to O
the O
( O
nonobserved O
) O
nodes O
of O
the O
graph O
. O
finite O
mixture B
models O
. O
for O
0- O
is O
shifted O
towards O
the O
origin O
, O
relative B
to O
the O
orthogonal O
projection O
. O
the O
severe O
difﬁculty O
that O
can O
arise O
in O
spaces O
of O
many O
dimensions O
is O
sometimes O
called O
the O
curse B
of I
dimensionality I
( O
bellman O
, O
1961 O
) O
. O
again O
, O
we O
saw O
in O
figure O
4.2 O
that O
this O
can O
lead O
to O
ambiguities O
in O
the O
resulting O
classiﬁcation B
. O
the O
total O
error B
function I
is O
therefore O
piecewise O
linear O
. O
by O
contrast O
, O
conventional O
pca O
will O
assign O
a O
low O
reconstruction O
cost O
to O
data O
points O
that O
are O
close O
to O
the O
principal B
subspace I
even O
if O
they O
lie O
arbitrarily O
far O
from O
the O
training B
data O
. O
the O
change O
in O
the O
weight B
vector I
w O
is O
then O
given O
by O
w O
( O
τ O
+1 O
) O
= O
w O
( O
τ O
) O
− O
η∇ep O
( O
w O
) O
= O
w O
( O
τ O
) O
+ O
ηφntn O
( O
4.55 O
) O
where O
η O
is O
the O
learning B
rate I
parameter I
and O
τ O
is O
an O
integer O
that O
indexes O
the O
steps O
of O
the O
algorithm O
. O
for O
the O
purposes O
of O
solving O
inference B
problems O
, O
it O
is O
often O
convenient O
to O
convert O
both O
directed B
and O
undirected B
graphs O
into O
a O
different O
representation O
called O
a O
factor B
graph I
. O
contour O
and O
surface O
plots O
for O
a O
gaussian O
mixture B
having O
3 O
components O
are O
shown O
in O
figure O
2.23. O
in O
this O
section O
we O
shall O
consider O
gaussian O
components O
to O
illustrate O
the O
frame- O
work O
of O
mixture B
models O
. O
one O
of O
the O
sim- O
plest O
forms O
of O
regularizer O
is O
given O
by O
the O
sum-of-squares O
of O
the O
weight B
vector I
ele- O
ments O
if O
we O
also O
consider O
the O
sum-of-squares B
error I
function O
given O
by O
ew O
( O
w O
) O
= O
wtw O
. O
for O
tutorial O
purposes O
, O
however O
, O
we O
shall O
consider O
a O
simple O
toy O
problem O
for O
which O
we O
can O
easily O
visualize O
the O
multimodality B
. O
however O
, O
these O
results O
do O
suggest O
a O
simple O
iterative O
scheme O
for O
ﬁnding O
a O
solution O
to O
the O
maximum B
likelihood I
problem O
, O
which O
as O
we O
shall O
see O
turns O
out O
to O
be O
an O
instance O
of O
the O
em O
algorithm O
for O
the O
particular O
case O
of O
the O
gaussian O
mixture B
model I
. O
a O
( O
13.7 O
) O
k O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
k=1 O
j=1 O
k O
( O
cid:14 O
) O
k=1 O
the O
initial O
latent O
node O
z1 O
is O
special O
in O
that O
it O
does O
not O
have O
a O
parent B
node I
, O
and O
so O
it O
has O
a O
marginal B
distribution O
p O
( O
z1 O
) O
represented O
by O
a O
vector O
of O
probabilities O
π O
with O
elements O
πk O
≡ O
p O
( O
z1k O
= O
1 O
) O
, O
so O
that O
p O
( O
z1|π O
) O
= O
πz1k O
k O
( O
13.8 O
) O
( O
cid:5 O
) O
where O
k O
πk O
= O
1. O
the O
transition O
matrix O
is O
sometimes O
illustrated O
diagrammatically O
by O
drawing O
the O
states O
as O
nodes O
in O
a O
state O
transition O
diagram O
as O
shown O
in O
figure O
13.6 O
for O
the O
case O
of O
k O
= O
3. O
note O
that O
this O
does O
not O
represent O
a O
probabilistic B
graphical I
model I
, O
because O
the O
nodes O
are O
not O
separate O
variables O
but O
rather O
states O
of O
a O
single O
variable O
, O
and O
so O
we O
have O
shown O
the O
states O
as O
boxes O
rather O
than O
circles O
. O
because O
the O
conditional B
mode O
for O
the O
mixture B
density I
network I
does O
not O
have O
a O
simple O
analytical O
solution O
, O
this O
would O
require O
numerical O
iteration O
. O
by O
separating O
inference B
and O
decision O
, O
we O
gain O
numerous O
beneﬁts O
, O
as O
discussed O
in O
section O
1.5.4. O
there O
are O
two O
different O
approaches O
to O
determining O
the O
conditional B
probabilities O
p O
( O
ck|x O
) O
. O
the O
general O
inference B
problem O
then O
involves O
determining O
the O
joint O
distribution O
p O
( O
x O
, O
ck O
) O
, O
or O
equivalently O
p O
( O
x O
, O
t O
) O
, O
which O
gives O
us O
the O
most O
complete O
probabilistic O
description O
of O
the O
situation O
. O
in O
particular O
, O
we O
can O
apply O
the O
sampling- O
importance-resampling O
formalism O
of O
section O
11.1.5 O
to O
obtain O
a O
sequential O
monte O
carlo O
algorithm O
known O
as O
the O
particle B
ﬁlter I
. O
13.4 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
hidden O
markov O
model O
in O
which O
the O
emission O
densities O
are O
represented O
by O
a O
parametric O
model O
p O
( O
x|z O
, O
w O
) O
, O
such O
as O
a O
linear B
regression I
model O
or O
a O
neural B
network I
, O
in O
which O
w O
is O
a O
vector O
of O
adaptive O
parameters O
. O
although O
the O
data O
set O
is O
not O
linearly B
separable I
in O
the O
two-dimensional O
data O
space O
x O
, O
it O
is O
linearly B
separable I
in O
the O
nonlinear O
feature B
space I
deﬁned O
implicitly O
by O
the O
nonlinear O
kernel B
function I
. O
the O
idea O
proposed O
by O
fisher O
is O
to O
maximize O
a O
function O
that O
will O
give O
a O
large O
separation O
between O
the O
projected O
class O
means O
while O
also O
giving O
a O
small O
variance B
within O
each O
class O
, O
thereby O
minimizing O
the O
class O
overlap O
. O
in O
the O
e O
step O
, O
we O
need O
to O
solve O
the O
inference B
problem O
of O
determining O
the O
local B
posterior O
marginals O
for O
the O
latent O
variables O
, O
which O
can O
be O
solved O
efﬁciently O
using O
the O
sum-product B
algorithm I
, O
as O
we O
discuss O
in O
the O
next O
section O
. O
consider O
a O
joint O
distribution O
p O
( O
x O
, O
y O
) O
deﬁned O
by O
the O
marginal B
and O
conditional B
distributions O
given O
by O
( O
2.99 O
) O
and O
( O
2.100 O
) O
. O
from O
fig- O
ure O
8.48 O
, O
we O
see O
that O
term O
gm O
( O
xm O
, O
xsm O
) O
associated O
with O
node B
xm O
is O
given O
by O
a O
product O
of O
terms O
fl O
( O
xm O
, O
xml O
) O
each O
associated O
with O
one O
of O
the O
factor O
nodes O
fl O
that O
is O
linked O
to O
node B
xm O
( O
excluding O
node B
fs O
) O
, O
so O
that O
gm O
( O
xm O
, O
xsm O
) O
= O
fl O
( O
xm O
, O
xml O
) O
( O
8.68 O
) O
( O
cid:14 O
) O
l∈ne O
( O
xm O
) O
\fs O
where O
the O
product O
is O
taken O
over O
all O
neighbours O
of O
node B
xm O
except O
for O
node O
fs O
. O
( O
5.112 O
) O
this O
regularizer O
is O
also O
known O
as O
weight B
decay I
and O
has O
been O
discussed O
at O
length O
in O
chapter O
3. O
the O
effective O
model O
complexity O
is O
then O
determined O
by O
the O
choice O
of O
the O
regularization B
coefﬁcient O
λ. O
as O
we O
have O
seen O
previously O
, O
this O
regularizer O
can O
be O
interpreted O
as O
the O
negative O
logarithm O
of O
a O
zero-mean O
gaussian O
prior B
distribution O
over O
the O
weight B
vector I
w. O
5.5.1 O
consistent B
gaussian O
priors O
one O
of O
the O
limitations O
of O
simple O
weight B
decay I
in O
the O
form O
( O
5.112 O
) O
is O
that O
is O
inconsistent O
with O
certain O
scaling O
properties O
of O
network O
mappings O
. O
because O
this O
has O
just O
two O
adap- O
tive O
parameters O
, O
we O
can O
plot O
the O
prior B
and O
posterior O
distributions O
directly O
in O
parameter O
space O
. O
5.3.2 O
a O
simple O
example O
the O
above O
derivation O
of O
the O
backpropagation B
procedure O
allowed O
for O
general O
forms O
for O
the O
error B
function I
, O
the O
activation O
functions O
, O
and O
the O
network O
topology O
. O
because O
we O
can O
not O
use O
the O
complete-data O
log O
likelihood O
, O
we O
consider O
instead O
its O
expected O
value O
under O
the O
posterior O
distribution O
of O
the O
latent B
variable I
, O
which O
corresponds O
( O
as O
we O
shall O
see O
) O
to O
the O
e O
step O
of O
the O
em O
algorithm O
. O
from O
a O
graphical O
point O
of O
view O
, O
each O
node B
then O
acquires O
an O
additional O
parent O
representing O
the O
dirichlet O
distribution O
over O
the O
pa- O
rameters O
associated O
with O
the O
corresponding O
discrete O
node B
. O
, O
n O
, O
with O
one O
slack B
variable I
for O
each O
training B
data O
point O
( O
bennett O
, O
1992 O
; O
cortes O
and O
vapnik O
, O
1995 O
) O
. O
the O
two O
forms O
of O
kullback-leibler O
divergence O
are O
members O
of O
the O
alpha O
family O
section O
10.7 O
470 O
10. O
approximate O
inference B
( O
cid:16 O
) O
( O
cid:25 O
) O
( O
cid:11 O
) O
( O
cid:11 O
) O
( O
cid:15 O
) O
( O
cid:6 O
) O
of O
divergences O
( O
ali O
and O
silvey O
, O
1966 O
; O
amari O
, O
1985 O
; O
minka O
, O
2005 O
) O
deﬁned O
by O
4 O
1 O
− O
dα O
( O
p O
( O
cid:5 O
) O
q O
) O
= O
p O
( O
x O
) O
( O
1+α O
) O
/2q O
( O
x O
) O
( O
1−α O
) O
/2 O
dx O
1 O
− O
α2 O
( O
10.19 O
) O
where O
−∞ O
< O
α O
< O
∞ O
is O
a O
continuous O
parameter O
. O
the O
other O
major O
class O
of O
graphical O
models O
are O
markov O
random O
ﬁelds O
, O
also O
known O
as O
undirected B
graphical O
models O
, O
in O
which O
the O
links O
do O
not O
carry O
arrows O
and O
have O
no O
directional O
signiﬁcance O
. O
if O
the O
model O
design O
is O
iterated O
many O
times O
using O
a O
lim- O
ited O
size O
data O
set O
, O
then O
some O
over-ﬁtting B
to O
the O
validation O
data O
can O
occur O
and O
so O
it O
may O
be O
necessary O
to O
keep O
aside O
a O
third O
test B
set I
on O
which O
the O
performance O
of O
the O
selected O
model O
is O
ﬁnally O
evaluated O
. O
unsurprisingly O
, O
the O
least O
squares O
approach O
to O
regression B
does O
not O
exhibit O
robustness B
, O
because O
it O
cor- O
responds O
to O
maximum B
likelihood I
under O
a O
( O
conditional B
) O
gaussian O
distribution O
. O
because O
this O
distri- O
bution O
is O
completely O
characterized O
by O
its O
mean B
and O
its O
covariance B
, O
our O
goal O
will O
be O
to O
identify O
expressions O
for O
the O
mean B
and O
covariance B
of O
p O
( O
xa|xb O
) O
by O
inspection O
of O
( O
2.70 O
) O
. O
the O
expected O
value O
of O
the O
indicator O
variable O
znk O
under O
this O
posterior O
distribution O
is O
then O
given O
by O
( O
cid:2 O
) O
( O
cid:8 O
) O
znk O
[ O
πkn O
( O
xn|µk O
, O
σk O
) O
] O
znk O
πjn O
( O
xn|µj O
, O
σj O
) O
k O
( O
cid:2 O
) O
πkn O
( O
xn|µk O
, O
σk O
) O
πjn O
( O
xn|µj O
, O
σj O
) O
( O
cid:9 O
) O
znj O
e O
[ O
znk O
] O
= O
= O
γ O
( O
znk O
) O
( O
9.39 O
) O
znk O
znj O
= O
which O
is O
just O
the O
responsibility B
of O
component O
k O
for O
data O
point O
xn O
. O
the O
remaining O
ﬁgures O
show O
histogram O
estimates O
of O
the O
marginal B
distributions O
p O
( O
x O
) O
and O
p O
( O
y O
) O
, O
as O
well O
as O
the O
conditional B
distribution O
p O
( O
x|y O
= O
1 O
) O
corresponding O
to O
the O
bottom O
row O
in O
the O
top O
left O
ﬁgure O
. O
stochastic B
simulation O
algorithms O
for O
dynamic O
probabilistic O
networks O
. O
even O
with O
isotropic B
components O
, O
the O
conditional B
distribution O
p O
( O
t|x O
) O
does O
not O
assume O
factorization B
with O
respect O
to O
the O
components O
of O
t O
( O
in O
contrast O
to O
the O
standard O
sum-of-squares O
regression B
model O
) O
as O
a O
consequence O
of O
the O
mixture B
distribution I
. O
show O
that O
this O
result O
reduces O
to O
( O
1.89 O
) O
for O
the O
case O
of O
a O
single O
target O
variable O
t. O
1.26 O
( O
( O
cid:1 O
) O
) O
by O
expansion O
of O
the O
square O
in O
( O
1.151 O
) O
, O
derive O
a O
result O
analogous O
to O
( O
1.90 O
) O
and O
hence O
show O
that O
the O
function O
y O
( O
x O
) O
that O
minimizes O
the O
expected O
squared O
loss O
for O
the O
case O
of O
a O
vector O
t O
of O
target O
variables O
is O
again O
given O
by O
the O
conditional B
expectation I
of O
t. O
1.27 O
( O
( O
cid:1 O
) O
( O
cid:1 O
) O
) O
www O
consider O
the O
expected O
loss O
for O
regression B
problems O
under O
the O
lq O
loss B
function I
given O
by O
( O
1.91 O
) O
. O
this O
is O
known O
as O
the O
bayesian O
information B
criterion I
( O
bic O
) O
or O
the O
schwarz O
criterion O
( O
schwarz O
, O
1978 O
) O
. O
as O
a O
ﬁnal O
example O
, O
we O
consider O
a O
closely O
related O
model O
, O
namely O
the O
relevance B
vector I
machine I
for O
regression B
discussed O
in O
section O
7.2.1. O
there O
we O
used O
direct O
max- O
imization O
of O
the O
marginal B
likelihood I
to O
derive O
re-estimation O
equations O
for O
the O
hyper- O
parameters O
α O
and O
β. O
here O
we O
consider O
an O
alternative O
approach O
in O
which O
we O
view O
the O
weight B
vector I
w O
as O
a O
latent B
variable I
and O
apply O
the O
em O
algorithm O
. O
if O
the O
joint O
distribution O
p O
( O
z O
, O
x|θ O
) O
comprises O
a O
member O
of O
the O
exponential B
family I
, O
or O
a O
product O
of O
such O
members O
, O
then O
we O
see O
that O
the O
logarithm O
will O
cancel O
the O
exponential O
and O
lead O
to O
an O
m O
step O
that O
will O
be O
typically O
much O
simpler O
than O
the O
maximization O
of O
the O
corresponding O
incomplete-data O
log O
likelihood O
function O
p O
( O
x|θ O
) O
. O
however O
, O
it O
allows O
the O
model O
to O
be O
reformulated O
using O
kernels O
, O
and O
so O
the O
maximum B
margin I
classiﬁer O
can O
be O
applied O
efﬁciently O
to O
feature O
spaces O
whose O
dimensionality O
exceeds O
the O
number O
of O
data O
points O
, O
including O
inﬁnite O
feature O
spaces O
. O
the O
right-hand O
plot O
shows O
the O
posterior B
probability I
given O
by O
the O
rvm O
output O
in O
which O
the O
proportion O
of O
red O
( O
blue O
) O
ink O
indicates O
the O
probability B
of O
that O
point O
belonging O
to O
the O
red O
( O
blue O
) O
class O
. O
information B
theory I
. O
note O
that O
the O
model O
produced O
by O
k O
nearest O
neighbours O
is O
not O
a O
true O
density B
model O
because O
the O
integral O
over O
all O
space O
diverges O
. O
( O
10.9 O
) O
it O
is O
worth O
taking O
a O
few O
moments O
to O
study O
the O
form O
of O
this O
solution O
as O
it O
provides O
the O
basis O
for O
applications O
of O
variational B
methods O
. O
discuss O
how O
the O
standard O
backpropagation O
algorithm O
must O
be O
modiﬁed O
in O
order O
to O
ensure O
that O
such O
constraints O
are O
satisﬁed O
when O
evaluating O
the O
derivatives O
of O
an O
error B
function I
with O
respect O
to O
the O
adjustable O
parameters O
in O
the O
network O
. O
10.3.3 O
lower B
bound I
. O
the O
reader O
should O
take O
a O
few O
moments O
to O
verify O
each O
of O
these O
properties O
in O
turn O
, O
as O
an O
exercise O
in O
the O
application O
of O
d-separation B
. O
iterative B
reweighted I
least I
squares I
probit O
regression B
. O
s O
( O
3.53 O
) O
( O
3.54 O
) O
the O
log O
of O
the O
posterior O
distribution O
is O
given O
by O
the O
sum O
of O
the O
log O
likelihood O
and O
the O
log O
of O
the O
prior B
and O
, O
as O
a O
function O
of O
w O
, O
takes O
the O
form O
ln O
p O
( O
w|t O
) O
= O
− O
β O
2 O
{ O
tn O
− O
wtφ O
( O
xn O
) O
} O
2 O
− O
α O
2 O
wtw O
+ O
const O
. O
+ O
xn O
, O
and O
for O
each O
observation O
the O
mean B
and O
variance B
are O
2.1. O
binary O
variables O
71 O
given O
by O
( O
2.3 O
) O
and O
( O
2.4 O
) O
, O
respectively O
, O
we O
have O
e O
[ O
m O
] O
≡ O
n O
( O
cid:2 O
) O
var O
[ O
m O
] O
≡ O
n O
( O
cid:2 O
) O
mbin O
( O
m|n O
, O
µ O
) O
= O
n O
µ O
m=0 O
( O
m O
− O
e O
[ O
m O
] O
) O
2 O
bin O
( O
m|n O
, O
µ O
) O
= O
n O
µ O
( O
1 O
− O
µ O
) O
. O
( O
10.188 O
) O
( O
cid:14 O
) O
i O
this O
would O
arise O
, O
for O
example O
, O
in O
a O
model O
for O
independent B
, O
identically O
distributed O
data O
in O
which O
there O
is O
one O
factor O
fn O
( O
θ O
) O
= O
p O
( O
xn|θ O
) O
for O
each O
data O
point O
xn O
, O
along O
with O
a O
factor O
f0 O
( O
θ O
) O
= O
p O
( O
θ O
) O
corresponding O
to O
the O
prior B
. O
the O
sum-of-squares B
error I
( O
3.12 O
) O
is O
then O
equal O
( O
up O
to O
a O
factor O
of O
1/2 O
) O
to O
the O
squared O
euclidean O
distance O
between O
y O
and O
t. O
thus O
the O
least-squares O
solution O
for O
w O
corresponds O
to O
that O
choice O
of O
y O
that O
lies O
in O
subspace O
s O
and O
that O
is O
closest O
to O
t. O
intuitively O
, O
from O
figure O
3.2 O
, O
we O
anticipate O
that O
this O
solution O
corresponds O
to O
the O
orthogonal O
projection O
of O
t O
onto O
the O
subspace O
s. O
this O
is O
indeed O
the O
case O
, O
as O
can O
easily O
be O
veriﬁed O
by O
noting O
that O
the O
solution O
for O
y O
is O
given O
by O
φwml O
, O
and O
then O
conﬁrming O
that O
this O
takes O
the O
form O
of O
an O
orthogonal O
projection O
. O
for O
instance O
in O
the O
case O
of O
gaussians O
, O
the O
parameters O
µk O
might O
be O
ini- O
tialized O
by O
applying O
the O
k-means O
algorithm O
to O
the O
data O
, O
and O
σk O
might O
be O
initialized O
to O
the O
covariance B
matrix I
of O
the O
corresponding O
k O
means O
cluster O
. O
( O
1.27 O
) O
exercise O
1.4 O
one O
consequence O
of O
this O
property O
is O
that O
the O
concept O
of O
the O
maximum O
of O
a O
probability B
density O
is O
dependent O
on O
the O
choice O
of O
variable O
. O
statistical O
modelling O
by O
wavelets B
. O
in O
fact O
, O
‘ O
multilayer B
perceptron I
’ O
is O
really O
a O
misnomer O
, O
because O
the O
model O
comprises O
multi- O
ple O
layers O
of O
logistic B
regression I
models O
( O
with O
continuous O
nonlinearities O
) O
rather O
than O
multiple O
perceptrons O
( O
with O
discontinuous O
nonlinearities O
) O
. O
, O
xn O
} O
of O
observed O
values O
of O
x. O
we O
can O
construct O
the O
likelihood B
function I
, O
which O
is O
a O
function O
of O
µ O
, O
on O
the O
assumption O
that O
the O
observations O
are O
drawn O
independently O
from O
p O
( O
x|µ O
) O
, O
so O
that O
n O
( O
cid:14 O
) O
n O
( O
cid:14 O
) O
p O
( O
d|µ O
) O
= O
p O
( O
xn|µ O
) O
= O
µxn O
( O
1 O
− O
µ O
) O
1−xn O
. O
the O
number O
of O
such O
degrees B
of I
freedom I
will O
be O
small O
compared O
to O
the O
dimensionality O
of O
ihe O
data O
set O
. O
in O
this O
case O
we O
maintain O
a O
gaussian O
posterior O
distribution O
over O
w O
, O
which O
is O
initialized O
using O
the O
prior B
p O
( O
w O
) O
. O
( O
2.288 O
) O
by O
making O
use O
of O
the O
results O
of O
section O
2.3 O
, O
ﬁnd O
an O
expression O
for O
the O
conditional B
distribution O
p O
( O
xa|xb O
) O
in O
which O
xc O
has O
been O
marginalized O
out O
. O
this O
corresponds O
to O
joint O
distributions O
which O
factorize O
into O
the O
product O
of O
the O
marginal B
distributions O
over O
the O
variables O
comprising O
the O
nodes O
of O
the O
graph O
. O
linear O
dynamical O
systems O
have O
the O
identical O
factorization B
, O
given O
by O
( O
13.6 O
) O
, O
to O
hidden O
markov O
models O
, O
and O
are O
again O
described O
by O
the O
factor O
graphs O
in O
figures O
13.14 O
and O
13.15. O
inference B
algorithms O
therefore O
take O
precisely O
the O
same O
form O
except O
that O
summations O
over O
latent O
variables O
are O
replaced O
by O
integrations O
. O
row O
shows O
the O
direction O
of O
message B
passing I
in O
the O
max-product O
algorithm O
. O
the O
equivalent B
kernel I
is O
illustrated O
for O
the O
case O
of O
gaussian O
basis O
functions O
in O
( O
cid:4 O
) O
) O
have O
been O
plotted O
as O
a O
function O
of O
figure O
3.10 O
in O
which O
the O
kernel O
functions O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
for O
three O
different O
values O
of O
x. O
we O
see O
that O
they O
are O
localized O
around O
x O
, O
and O
so O
the O
x O
mean B
of O
the O
predictive B
distribution I
at O
x O
, O
given O
by O
y O
( O
x O
, O
mn O
) O
, O
is O
obtained O
by O
forming O
a O
weighted O
combination O
of O
the O
target O
values O
in O
which O
data O
points O
close O
to O
x O
are O
given O
higher O
weight O
than O
points O
further O
removed O
from O
x. O
intuitively O
, O
it O
seems O
reasonable O
that O
we O
should O
weight O
local O
evidence O
more O
strongly O
than O
distant O
evidence O
. O
note O
that O
there O
is O
no O
hyperparameter B
β O
, O
because O
the O
data O
points O
are O
assumed O
to O
be O
correctly O
labelled O
. O
note O
that O
this O
path O
is O
also O
blocked O
by O
node B
e O
because O
e O
is O
a O
head-to-head O
node O
and O
neither O
it O
nor O
its O
descendant O
are O
in O
the O
conditioning O
set O
. O
a O
b O
c O
d O
8.4. O
inference B
in O
graphical O
models O
we O
turn O
now O
to O
the O
problem O
of O
inference B
in O
graphical O
models O
, O
in O
which O
some O
of O
the O
nodes O
in O
a O
graph O
are O
clamped O
to O
observed O
values O
, O
and O
we O
wish O
to O
compute O
the O
posterior O
distributions O
of O
one O
or O
more O
subsets O
of O
other O
nodes O
. O
use O
proof O
by O
induction O
to O
show O
that O
the O
messages O
can O
be O
passed O
in O
such O
an O
order O
that O
at O
every O
step O
, O
each O
node B
that O
must O
send O
a O
message O
has O
received O
all O
of O
the O
incoming O
messages O
necessary O
to O
construct O
its O
outgoing O
messages O
. O
consider O
a O
gaussian O
distribution O
p O
( O
z O
) O
= O
n O
( O
z|µ O
, O
λ O
−1 O
) O
over O
two O
correlated O
variables O
z O
= O
( O
z1 O
, O
z2 O
) O
in O
which O
the O
mean B
and O
precision O
have O
elements O
( O
cid:15 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
( O
cid:16 O
) O
( O
10.10 O
) O
µ O
= O
, O
λ O
= O
µ1 O
µ2 O
λ11 O
λ12 O
λ21 O
λ22 O
and O
λ21 O
= O
λ12 O
due O
to O
the O
symmetry O
of O
the O
precision B
matrix I
. O
( O
8.1 O
) O
a O
second O
application O
of O
the O
product B
rule I
, O
this O
time O
to O
the O
second O
term O
on O
the O
right- O
hand O
side O
of O
( O
8.1 O
) O
, O
gives O
p O
( O
a O
, O
b O
, O
c O
) O
= O
p O
( O
c|a O
, O
b O
) O
p O
( O
b|a O
) O
p O
( O
a O
) O
. O
the O
loga- O
rithm O
now O
acts O
directly O
on O
the O
gaussian O
distribution O
, O
which O
itself O
is O
a O
member O
of O
the O
exponential B
family I
. O
we O
now O
represent O
the O
right-hand O
side O
of O
( O
8.2 O
) O
in O
terms O
of O
a O
simple O
graphical B
model I
as O
follows O
. O
the O
use O
of O
the O
evidence O
procedure O
to O
determine O
α O
is O
illustrated O
in O
figure O
5.22 O
for O
the O
synthetic O
two-dimensional O
data O
discussed O
in O
appendix O
a. O
finally O
, O
we O
need O
the O
predictive B
distribution I
, O
which O
is O
deﬁned O
by O
( O
5.168 O
) O
. O
5. O
if O
q2 O
i O
> O
si O
, O
and O
αi O
< O
∞ O
, O
so O
that O
the O
basis O
vector O
ϕi O
is O
already O
included O
in O
i O
> O
si O
, O
and O
αi O
= O
∞ O
, O
then O
add O
ϕi O
to O
the O
model O
, O
and O
evaluate O
hyperpa- O
i O
( O
cid:1 O
) O
si O
, O
and O
αi O
< O
∞ O
then O
remove O
basis B
function I
ϕi O
from O
the O
model O
, O
the O
model O
, O
then O
update O
αi O
using O
( O
7.101 O
) O
. O
this O
can O
be O
obtained O
, O
for O
instance O
, O
by O
using O
a O
clustering B
technique O
such O
as O
k O
-means O
based O
on O
euclidean O
distance O
to O
partition O
the O
data O
set O
into O
local B
groups O
with O
standard O
pca O
ap O
( O
cid:173 O
) O
plied O
to O
each O
group O
. O
and O
discussions O
of O
factor B
analysis I
can O
be O
found O
in O
the O
books O
by O
everitt O
( O
1984 O
) O
. O
6.4.4 O
automatic B
relevance I
determination I
. O
( O
5.42 O
) O
on-line O
gradient O
descent O
, O
also O
known O
as O
sequential B
gradient I
descent I
or O
stochastic B
gradient I
descent I
, O
makes O
an O
update O
to O
the O
weight B
vector I
based O
on O
one O
data O
point O
at O
a O
time O
, O
so O
that O
w O
( O
τ O
+1 O
) O
= O
w O
( O
τ O
) O
− O
η∇en O
( O
w O
( O
τ O
) O
) O
. O
the O
likelihood B
function I
is O
then O
given O
by O
n O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
n O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
p O
( O
t|w1 O
, O
. O
one O
of O
the O
most O
popular O
approaches O
to O
training B
support O
vector O
machines O
is O
called O
sequential B
minimal I
optimization I
, O
or O
smo O
( O
platt O
, O
1999 O
) O
. O
8.4. O
inference B
in O
graphical O
models O
395 O
the O
joint O
distribution O
for O
this O
graph O
takes O
the O
form O
p O
( O
x O
) O
= O
1 O
z O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
ψ2,3 O
( O
x2 O
, O
x3 O
) O
··· O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
. O
different O
algorithms O
involve O
different O
choices O
for O
the O
weight B
vector I
update O
∆w O
( O
τ O
) O
. O
for O
a O
distribution O
over O
m O
independent B
discrete O
variables O
, O
each O
having O
k O
states O
, O
the O
total O
number O
of O
parameters O
would O
be O
m O
( O
k O
− O
1 O
) O
, O
which O
therefore O
grows O
linearly O
with O
the O
number O
of O
variables O
. O
if O
the O
value O
of O
α0 O
is O
small O
, O
then O
the O
posterior O
distribution O
will O
be O
inﬂuenced O
primarily O
by O
the O
data O
rather O
than O
by O
the O
prior B
. O
analogue O
hardware B
implementations O
of O
the O
perceptron B
were O
built O
by O
rosenblatt O
, O
based O
on O
motor-driven O
variable O
resistors O
to O
implement O
the O
adaptive O
parameters O
wj O
. O
next O
we O
consider O
the O
evaluation O
of O
the O
quantities O
ξ O
( O
zn−1 O
, O
zn O
) O
, O
which O
correspond O
to O
the O
values O
of O
the O
conditional B
probabilities O
p O
( O
zn−1 O
, O
zn|x O
) O
for O
each O
of O
the O
k O
× O
k O
settings O
for O
( O
zn−1 O
, O
zn O
) O
. O
in O
most O
applications O
of O
such O
models O
, O
the O
conditional B
distributions O
p O
( O
xn|xn−1 O
) O
that O
deﬁne O
the O
model O
will O
be O
constrained O
to O
be O
equal O
, O
corresponding O
to O
the O
assump- O
tion O
of O
a O
stationary B
time O
series O
. O
this O
makes O
possible O
the O
construction O
of O
general O
purpose O
software O
for O
variational O
inference B
in O
which O
the O
form O
of O
the O
model O
does O
not O
need O
to O
be O
speciﬁed O
in O
advance O
( O
bishop O
et O
al. O
, O
2003 O
) O
. O
propagation O
of O
uncertainty O
by O
logic B
sampling I
in O
bayes O
’ O
networks O
. O
in O
particular O
at O
step O
τ O
of O
the O
algorithm O
, O
in O
which O
the O
cur- O
rent O
state O
is O
z O
( O
τ O
) O
, O
we O
draw O
a O
sample O
z O
( O
cid:1 O
) O
from O
the O
distribution O
qk O
( O
z|z O
( O
τ O
) O
) O
and O
then O
accept O
it O
with O
probability B
ak O
( O
z O
( O
cid:1 O
) O
, O
zτ O
) O
where O
( O
cid:15 O
) O
( O
cid:16 O
) O
( O
cid:4 O
) O
p O
( O
z O
( O
cid:1 O
) O
) O
qk O
( O
z O
( O
τ O
) O
|z O
( O
cid:1 O
) O
) O
( O
cid:4 O
) O
p O
( O
z O
( O
τ O
) O
) O
qk O
( O
z O
( O
cid:1 O
) O
|z O
( O
τ O
) O
) O
ak O
( O
z O
( O
cid:1 O
) O
, O
z O
( O
τ O
) O
) O
= O
min O
1 O
, O
. O
the O
distributions O
introduced O
in O
this O
chapter O
will O
also O
serve O
another O
important O
purpose O
, O
namely O
to O
provide O
us O
with O
the O
opportunity O
to O
discuss O
some O
key O
statistical O
concepts O
, O
such O
as O
bayesian O
inference B
, O
in O
the O
context O
of O
simple O
models O
before O
we O
encounter O
them O
in O
more O
complex O
situations O
in O
later O
chapters O
. O
furthermore O
, O
the O
division O
of O
weights O
into O
groups O
, O
the O
mean B
weight O
value O
for O
each O
group O
, O
and O
the O
spread O
of O
values O
within O
the O
groups O
are O
all O
determined O
as O
part O
of O
the O
learning B
process O
. O
x1 O
xm O
( O
cid:14 O
) O
( O
cid:14 O
) O
m∈ne O
( O
fs O
) O
\x O
m∈ne O
( O
fs O
) O
\x O
8.4. O
inference B
in O
graphical O
models O
405 O
figure O
8.47 O
illustration O
of O
the O
factorization B
of O
the O
subgraph O
as- O
sociated O
with O
factor O
node O
fs O
. O
this O
technique O
is O
sometimes O
called O
a O
tapped B
delay I
line I
because O
it O
corresponds O
to O
storing O
( O
delaying O
) O
the O
previous O
m O
values O
of O
the O
observed B
variable I
in O
order O
to O
predict O
the O
next O
value O
. O
such O
priors O
are O
called O
improper B
. O
in O
this O
case O
, O
the O
entropy B
is O
measured O
in O
units O
of O
‘ O
nats B
’ O
instead O
of O
bits B
, O
which O
differ O
simply O
by O
a O
factor O
of O
ln O
2. O
we O
have O
introduced O
the O
concept O
of O
entropy B
in O
terms O
of O
the O
average O
amount O
of O
information O
needed O
to O
specify O
the O
state O
of O
a O
random O
variable O
. O
as O
a O
simple O
check O
, O
let O
us O
verify O
that O
the O
marginal B
p O
( O
x2 O
) O
is O
given O
by O
the O
correct O
expression O
. O
now O
let O
us O
consider O
a O
general O
factor B
graph I
corresponding O
to O
the O
distribution O
( O
cid:14 O
) O
p O
( O
θ O
) O
= O
fi O
( O
θi O
) O
( O
10.236 O
) O
where O
θi O
represents O
the O
subset O
of O
variables O
associated O
with O
factor O
fi O
. O
in O
the O
case O
of O
a O
gaussian O
conditional B
distribution O
of O
the O
form O
( O
3.8 O
) O
, O
the O
conditional B
mean O
will O
be O
simply O
e O
[ O
t|x O
] O
= O
tp O
( O
t|x O
) O
dt O
= O
y O
( O
x O
, O
w O
) O
. O
in O
section O
4.5.2 O
, O
we O
derived O
the O
approximate O
formula O
( O
4.153 O
) O
for O
the O
convolution O
of O
a O
logistic B
sigmoid I
with O
a O
gaussian O
distribution O
. O
the O
likelihood B
function I
is O
given O
by O
p O
( O
d|µ O
, O
τ O
) O
= O
( O
xn O
− O
µ O
) O
2 O
. O
because O
y O
( O
xa O
) O
= O
y O
( O
xb O
) O
= O
0 O
, O
we O
have O
wt O
( O
xa O
− O
xb O
) O
= O
0 O
and O
hence O
the O
vector O
w O
is O
orthogonal O
to O
every O
vector O
lying O
within O
the O
decision B
surface I
, O
and O
so O
w O
determines O
the O
orientation O
of O
the O
decision B
surface I
. O
variational B
learning O
for O
switching O
state-space O
models O
. O
( O
9.38 O
) O
n=1 O
k=1 O
and O
hence O
factorizes O
over O
n O
so O
that O
under O
the O
posterior O
distribution O
the O
{ O
zn O
} O
are O
independent B
. O
we O
shall O
show O
an O
example O
of O
the O
application O
of O
this O
technique O
in O
the O
context O
of O
principal B
component I
analysis I
in O
figure O
12.11. O
this O
will O
be O
a O
valid O
procedure O
if O
the O
data O
values O
are O
missing B
at I
random I
, O
meaning O
that O
the O
mechanism O
causing O
values O
to O
be O
missing O
does O
not O
depend O
on O
the O
unobserved O
values O
. O
however O
, O
once O
we O
have O
observed O
that O
the O
piece O
of O
selected O
fruit O
is O
an O
orange O
, O
we O
ﬁnd O
that O
the O
posterior B
probability I
of O
the O
red O
box O
is O
now O
2/3 O
, O
so O
that O
it O
is O
now O
more O
likely O
that O
the O
box O
we O
selected O
was O
in O
fact O
the O
red O
one O
. O
note O
, O
however O
, O
that O
in O
this O
context O
it O
is O
not O
interpreted O
as O
a O
probability B
density O
, O
and O
hence O
the O
normalization O
coefﬁcient O
is O
6.2. O
constructing O
kernels O
297 O
omitted O
. O
similarly O
, O
we O
can O
use O
( O
8.14 O
) O
and O
( O
8.15 O
) O
to O
obtain O
the O
i O
, O
j O
element O
of O
the O
covariance B
matrix I
for O
p O
( O
x O
) O
in O
the O
form O
of O
a O
recursion O
relation O
cov O
[ O
xi O
, O
xj O
] O
= O
e O
[ O
( O
xi O
− O
e O
[ O
xi O
] O
) O
( O
xj O
− O
e O
[ O
xj O
] O
) O
] O
⎡⎣ O
( O
xi O
− O
e O
[ O
xi O
] O
) O
( O
cid:2 O
) O
⎧⎨⎩ O
( O
cid:2 O
) O
k∈paj O
= O
e O
= O
wjkcov O
[ O
xi O
, O
xk O
] O
+ O
iijvj O
k∈paj O
wjk O
( O
xk O
− O
e O
[ O
xk O
] O
) O
+ O
√ O
vjj O
⎤⎦ O
⎫⎬⎭ O
( O
8.16 O
) O
and O
so O
the O
covariance B
can O
similarly O
be O
evaluated O
recursively O
starting O
from O
the O
lowest O
numbered O
node B
. O
in O
the O
case O
of O
a O
quadratic O
error O
function O
, O
we O
can O
verify O
this O
insight O
, O
and O
show O
that O
early B
stopping I
should O
exhibit O
similar O
behaviour O
to O
regularization B
using O
a O
sim- O
ple O
weight-decay O
term O
. O
exercises O
323 O
6.25 O
( O
( O
cid:12 O
) O
) O
www O
using O
the O
newton-raphson O
formula O
( O
4.92 O
) O
, O
derive O
the O
iterative O
update O
n O
of O
the O
posterior O
distribution O
in O
the O
gaussian O
formula O
( O
6.83 O
) O
for O
ﬁnding O
the O
mode O
a O
( O
cid:1 O
) O
process O
classiﬁcation B
model O
. O
if O
we O
have O
multiple O
target O
variables O
, O
and O
we O
assume O
that O
they O
are O
inde- O
pendent O
conditional B
on O
x O
and O
w O
with O
shared O
noise O
precision O
β O
, O
then O
the O
conditional B
distribution O
of O
the O
target O
values O
is O
given O
by O
( O
cid:11 O
) O
t|y O
( O
x O
, O
w O
) O
, O
β O
−1i O
. O
in O
order O
to O
apply O
this O
approach O
, O
we O
need O
to O
deﬁne O
a O
message B
passing I
schedule O
. O
10.5 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
model O
in O
which O
the O
set O
of O
all O
hidden O
stochastic O
variables O
, O
de- O
noted O
collectively O
by O
z O
, O
comprises O
some O
latent O
variables O
z O
together O
with O
some O
model O
parameters O
θ. O
suppose O
we O
use O
a O
variational B
distribution O
that O
factorizes O
between O
la- O
tent O
variables O
and O
parameters O
so O
that O
q O
( O
z O
, O
θ O
) O
= O
qz O
( O
z O
) O
qθ O
( O
θ O
) O
, O
in O
which O
the O
distribution O
qθ O
( O
θ O
) O
is O
approximated O
by O
a O
point O
estimate O
of O
the O
form O
qθ O
( O
θ O
) O
= O
δ O
( O
θ O
− O
θ0 O
) O
where O
θ0 O
is O
a O
vector O
of O
free O
parameters O
. O
z O
( O
2 O
) O
n−1 O
z O
( O
2 O
) O
n O
z O
( O
2 O
) O
n+1 O
z O
( O
1 O
) O
n−1 O
z O
( O
1 O
) O
n O
z O
( O
1 O
) O
n+1 O
xn−1 O
xn O
xn+1 O
markov O
chains O
of O
latent O
variables O
, O
and O
the O
distribution O
of O
the O
observed B
variable I
at O
a O
given O
time O
step O
is O
conditional B
on O
the O
states O
of O
all O
of O
the O
corresponding O
latent O
vari- O
ables O
at O
that O
same O
time O
step O
. O
solving O
multiclass B
learning O
problems O
via O
error-correcting B
output I
codes I
. O
which O
are O
combined O
using O
a O
softmax B
function I
to O
give O
outputs O
exp O
( O
ak O
) O
( O
cid:2 O
) O
exp O
( O
aj O
) O
yk O
( O
x O
) O
= O
. O
thus O
our O
variational B
approximation O
to O
the O
predictive B
distribution I
becomes O
σ O
( O
a O
) O
n O
( O
a|µa O
, O
σ2 O
σ O
( O
a O
) O
p O
( O
a O
) O
da O
= O
p O
( O
c1|t O
) O
= O
( O
4.151 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
a O
) O
da O
. O
w O
( O
τ O
+1 O
) O
= O
w O
( O
τ O
) O
− O
η∇en O
3.1.4 O
regularized B
least I
squares I
in O
section O
1.1 O
, O
we O
introduced O
the O
idea O
of O
adding O
a O
regularization B
term O
to O
an O
error B
function I
in O
order O
to O
control O
over-ﬁtting B
, O
so O
that O
the O
total O
error B
function I
to O
be O
minimized O
takes O
the O
form O
ed O
( O
w O
) O
+ O
λew O
( O
w O
) O
( O
3.24 O
) O
where O
λ O
is O
the O
regularization B
coefﬁcient O
that O
controls O
the O
relative B
importance O
of O
the O
data-dependent O
error B
ed O
( O
w O
) O
and O
the O
regularization B
term O
ew O
( O
w O
) O
. O
by O
using O
prob O
( O
cid:173 O
) O
abilistic O
pca O
it O
is O
straightforward O
to O
define O
a O
fully O
probabilistic O
model O
simply O
by O
considering O
a O
mixture B
distribution I
in O
which O
the O
components O
are O
probabilistic O
pca O
models O
( O
tipping O
and O
bishop O
, O
1999a O
) O
. O
consider O
, O
for O
example O
, O
a O
distribution O
that O
is O
expressed O
in O
terms O
of O
the O
factorization B
p O
( O
x O
) O
= O
fa O
( O
x1 O
, O
x2 O
) O
fb O
( O
x1 O
, O
x2 O
) O
fc O
( O
x2 O
, O
x3 O
) O
fd O
( O
x3 O
) O
. O
bayesian O
learning B
for O
neural O
networks O
. O
the O
ﬁnal O
layer O
of O
the O
network O
would O
typically O
be O
a O
fully B
connected I
, O
fully O
adaptive O
layer O
, O
with O
a O
softmax O
output O
nonlinearity O
in O
the O
case O
of O
multiclass B
classiﬁcation O
. O
in O
order O
to O
make O
this O
clear O
, O
it O
is O
helpful O
to O
think O
of O
a O
directed B
graph O
as O
a O
ﬁlter O
. O
( O
cid:24 O
) O
( O
cid:25 O
) O
( O
2.145 O
) O
− O
λ O
2 O
n=1 O
n O
( O
cid:14 O
) O
n=1 O
100 O
2. O
probability B
distributions O
2 O
1 O
0 O
0 O
a O
= O
0.1 O
b O
= O
0.1 O
2 O
1 O
a O
= O
1 O
b O
= O
1 O
2 O
1 O
a O
= O
4 O
b O
= O
6 O
λ O
1 O
0 O
0 O
2 O
λ O
1 O
0 O
0 O
2 O
λ O
1 O
2 O
figure O
2.13 O
plot O
of O
the O
gamma B
distribution I
gam O
( O
λ|a O
, O
b O
) O
deﬁned O
by O
( O
2.146 O
) O
for O
various O
values O
of O
the O
parameters O
a O
and O
b. O
the O
corresponding O
conjugate B
prior I
should O
therefore O
be O
proportional O
to O
the O
product O
of O
a O
power O
of O
λ O
and O
the O
exponential O
of O
a O
linear O
function O
of O
λ. O
this O
corresponds O
to O
the O
gamma B
distribution I
which O
is O
deﬁned O
by O
gam O
( O
λ|a O
, O
b O
) O
= O
1 O
γ O
( O
a O
) O
baλa−1 O
exp O
( O
−bλ O
) O
. O
thus O
the O
maximum B
posterior I
weight O
vector O
is O
simply O
given O
by O
wmap O
= O
mn O
. O
the O
logistic B
sigmoid I
arises O
frequently O
in O
probabilistic O
models O
over O
binary O
vari- O
ables O
because O
it O
is O
the O
function O
that O
transforms O
a O
log B
odds I
ratio O
into O
a O
posterior O
prob- O
ability O
. O
( O
4.64 O
) O
( O
4.65 O
) O
( O
4.66 O
) O
( O
4.67 O
) O
we O
see O
that O
the O
quadratic O
terms O
in O
x O
from O
the O
exponents O
of O
the O
gaussian O
densities O
have O
cancelled O
( O
due O
to O
the O
assumption O
of O
common O
covariance B
matrices O
) O
leading O
to O
a O
linear O
function O
of O
x O
in O
the O
argument O
of O
the O
logistic B
sigmoid I
. O
( O
4.10 O
) O
this O
has O
the O
same O
form O
as O
the O
decision B
boundary I
for O
the O
two-class O
case O
discussed O
in O
section O
4.1.1 O
, O
and O
so O
analogous O
geometrical O
properties O
apply O
. O
here O
we O
shall O
suppose O
that O
we O
are O
given O
a O
gaussian O
marginal B
distribution O
p O
( O
x O
) O
and O
a O
gaussian O
conditional B
distribution O
p O
( O
y|x O
) O
in O
which O
p O
( O
y|x O
) O
has O
a O
mean B
that O
is O
a O
linear O
function O
of O
x O
, O
and O
a O
covariance B
which O
is O
independent B
of O
x. O
this O
is O
an O
example O
of O
2.3. O
the O
gaussian O
distribution O
91 O
a O
linear O
gaussian O
model O
( O
roweis O
and O
ghahramani O
, O
1999 O
) O
, O
which O
we O
shall O
study O
in O
greater O
generality O
in O
section O
8.1.4. O
we O
wish O
to O
ﬁnd O
the O
marginal B
distribution O
p O
( O
y O
) O
and O
the O
conditional B
distribution O
p O
( O
x|y O
) O
. O
because O
our O
data O
set O
x O
is O
i.i.d. B
, O
we O
can O
therefore O
write O
the O
probability B
of O
the O
data O
set O
, O
given O
µ O
and O
σ2 O
, O
in O
the O
form O
n O
( O
cid:14 O
) O
n O
( O
cid:10 O
) O
( O
cid:11 O
) O
p O
( O
x|µ O
, O
σ2 O
) O
= O
xn|µ O
, O
σ2 O
. O
13.3.1 O
inference B
in O
lds O
. O
this O
can O
all O
be O
achieved O
through O
the O
elegant O
, O
and O
very O
general O
, O
bayesian O
interpretation O
of O
probability B
. O
−1 O
the O
result O
( O
4.30 O
) O
is O
known O
as O
fisher O
’ O
s O
linear B
discriminant I
, O
although O
strictly O
it O
is O
not O
a O
discriminant O
but O
rather O
a O
speciﬁc O
choice O
of O
direction O
for O
projection O
of O
the O
data O
down O
to O
one O
dimension O
. O
we O
see O
that O
the O
solution O
for O
the O
n O
u O
( O
xn O
) O
, O
which O
maximum B
likelihood I
estimator O
depends O
on O
the O
data O
only O
through O
is O
therefore O
called O
the O
sufﬁcient O
statistic O
of O
the O
distribution O
( O
2.194 O
) O
. O
here O
we O
return O
to O
the O
curve B
ﬁtting I
example O
and O
view O
it O
from O
a O
probabilistic O
perspective O
, O
thereby O
gaining O
some O
insights O
into O
error B
functions O
and O
regularization B
, O
as O
well O
as O
taking O
us O
towards O
a O
full O
bayesian O
treatment O
. O
bayesian O
model B
comparison I
by O
monte O
carlo O
chaining B
. O
yi O
= O
ut O
y O
= O
u O
( O
x O
− O
µ O
) O
( O
2.52 O
) O
2.3. O
the O
gaussian O
distribution O
81 O
x2 O
figure O
2.7 O
the O
red O
curve O
shows O
the O
ellip- O
tical O
surface O
of O
constant O
proba- O
bility O
density B
for O
a O
gaussian O
in O
a O
two-dimensional O
space O
x O
= O
( O
x1 O
, O
x2 O
) O
on O
which O
the O
density B
is O
exp O
( O
−1/2 O
) O
of O
its O
value O
at O
x O
= O
µ. O
the O
major O
axes O
of O
the O
ellipse O
are O
deﬁned O
by O
the O
eigenvectors O
ui O
of O
the O
covari- O
ance O
matrix O
, O
with O
correspond- O
ing O
eigenvalues O
λi O
. O
this O
follows O
from O
the O
fact O
that O
when O
we O
sample O
from O
p O
( O
zi| O
{ O
z\i O
) O
, O
the O
marginal B
distribution O
p O
( O
z\i O
) O
is O
clearly O
invariant O
because O
the O
value O
of O
z\i O
is O
unchanged O
. O
because O
the O
discussion O
of O
support B
vector I
machines O
makes O
extensive O
use O
of O
lagrange O
multipliers O
, O
the O
reader O
is O
325 O
326 O
7. O
sparse O
kernel O
machines O
encouraged O
to O
review O
the O
key O
concepts O
covered O
in O
appendix O
e. O
additional O
infor- O
mation B
on O
support B
vector I
machines O
can O
be O
found O
in O
vapnik O
( O
1995 O
) O
, O
burges O
( O
1998 O
) O
, O
cristianini O
and O
shawe-taylor O
( O
2000 O
) O
, O
m¨uller O
et O
al O
. O
ln O
p O
( O
z O
) O
z1 O
z2 O
z3 O
z O
11.1. O
basic O
sampling O
algorithms O
531 O
figure O
11.7 O
illustrative O
example O
of O
rejection B
sampling I
involving O
sampling O
from O
a O
gaussian O
distribution O
p O
( O
z O
) O
shown O
by O
the O
green O
curve O
, O
by O
using O
rejection B
sampling I
from O
a O
proposal O
distri- O
bution O
q O
( O
z O
) O
that O
is O
also O
gaussian O
and O
whose O
scaled O
version O
kq O
( O
z O
) O
is O
shown O
by O
the O
red O
curve O
. O
one O
way O
to O
construct O
the O
maximum B
likelihood I
density O
model O
would O
simply O
be O
to O
find O
the O
eigenvectors O
and O
eigenvalues O
of O
the O
data O
covariance O
matrix O
and O
then O
to O
evaluate O
wand O
( O
j'2 O
using O
the O
results O
given O
above O
. O
this O
will O
allow O
us O
to O
determine O
values O
for O
these O
hyperpa- O
rameters O
from O
the O
training B
data O
alone O
, O
without O
recourse O
to O
cross-validation B
. O
7.2.2 O
analysis O
of O
sparsity B
we O
have O
noted O
earlier O
that O
the O
mechanism O
of O
automatic B
relevance I
determination I
causes O
a O
subset O
of O
parameters O
to O
be O
driven O
to O
zero O
. O
natural O
variations O
in O
writing O
style O
will O
cause O
the O
relative B
sizes O
of O
the O
two O
sections O
to O
vary O
, O
and O
hence O
the O
location O
of O
the O
cusp O
or O
loop O
within O
the O
temporal O
sequence O
will O
vary O
. O
once O
again O
, O
the O
derivative B
of O
the O
error B
function I
with O
respect O
to O
the O
activation O
for O
in O
summary O
, O
there O
is O
a O
natural O
choice O
of O
both O
output O
unit O
activation B
function I
and O
matching O
error B
function I
, O
according O
to O
the O
type O
of O
problem O
being O
solved O
. O
6.4.7 O
connection O
to O
neural O
networks O
we O
have O
seen O
that O
the O
range O
of O
functions O
which O
can O
be O
represented O
by O
a O
neural B
network I
is O
governed O
by O
the O
number O
m O
of O
hidden O
units O
, O
and O
that O
, O
for O
sufﬁciently O
large O
m O
, O
a O
two-layer O
network O
can O
approximate O
any O
given O
function O
with O
arbitrary O
accuracy O
. O
we O
therefore O
obtain O
an O
expression O
for O
the O
distortion B
measure I
j O
as O
a O
function O
purely O
of O
the O
{ O
ud O
in O
the O
form O
1 O
~ O
~ O
( O
t O
j O
= O
n O
l O
l O
_t O
) O
2 O
= O
l O
u O
i O
sui O
. O
these O
are O
known O
as O
emission O
probabilities O
, O
and O
might O
for O
example O
be O
given O
by O
gaussians O
of O
the O
form O
( O
9.11 O
) O
if O
the O
elements O
of O
x O
are O
continuous O
variables O
, O
or O
by O
conditional B
probability I
tables O
if O
x O
is O
discrete O
. O
6.4.3 O
learning B
the O
hyperparameters O
the O
predictions O
of O
a O
gaussian O
process O
model O
will O
depend O
, O
in O
part O
, O
on O
the O
choice O
of O
covariance B
function O
. O
let O
us O
assume O
that O
one O
message O
is O
passed O
at O
a O
time O
on O
any O
given O
link B
and O
in O
any O
given O
direction O
. O
4.5.2 O
predictive B
distribution I
. O
5.7.2 O
hyperparameter B
optimization O
. O
in O
fact O
, O
we O
shall O
shortly O
examine O
the O
relation O
between O
the O
inverse B
of O
a O
partitioned B
matrix O
and O
the O
inverses O
of O
its O
partitions O
. O
the O
data O
points O
will O
therefore O
live O
on O
a O
subspace O
of O
the O
data O
space O
whose O
intrinsic B
dimensionality I
is O
three O
. O
( O
10.203 O
) O
( O
10.204 O
) O
( O
10.205 O
) O
510 O
10. O
approximate O
inference B
( O
c O
) O
evaluate O
the O
new O
posterior O
by O
setting O
the O
sufﬁcient B
statistics I
( O
moments O
) O
\j O
( O
θ O
) O
fj O
( O
θ O
) O
, O
including O
evaluation O
of O
the O
of O
qnew O
( O
θ O
) O
equal O
to O
those O
of O
q O
normalization O
constant O
( O
cid:6 O
) O
zj O
= O
\j O
( O
θ O
) O
fj O
( O
θ O
) O
dθ O
. O
the O
three O
possibilities O
of O
general O
, O
diagonal B
, O
and O
isotropic B
covari- O
ance O
matrices O
are O
illustrated O
in O
figure O
2.8. O
unfortunately O
, O
whereas O
such O
approaches O
limit O
the O
number O
of O
degrees B
of I
freedom I
in O
the O
distribution O
and O
make O
inversion O
of O
the O
covariance B
matrix I
a O
much O
faster O
operation O
, O
they O
also O
greatly O
restrict O
the O
form O
of O
the O
probability B
density O
and O
limit O
its O
ability O
to O
capture O
interesting O
correlations O
in O
the O
data O
. O
in O
the O
framework O
of O
maximum B
likelihood I
, O
the O
number O
of O
hidden O
units O
needs O
to O
be O
limited O
( O
to O
a O
level O
dependent O
on O
the O
size O
of O
the O
training B
set I
) O
in O
order O
to O
avoid O
over-ﬁtting B
. O
when O
w'e O
applied O
the O
`` O
.nlcans O
algorill O
'' O
'' O
10 O
thi O
< O
data O
set O
, O
`` O
.-e O
first O
made O
a O
separ.te O
linear O
re-sealing O
of O
the O
individual O
`` O
anable O
' O
socb O
thm O
each O
`` O
ariable O
had O
zero O
mean B
and O
unit O
`` O
ariance O
. O
as O
a O
simple O
example O
of O
an O
inverse B
problem I
, O
consider O
the O
kinematics O
of O
a O
robot B
arm I
, O
as O
illustrated O
in O
figure O
5.18. O
the O
forward B
problem I
involves O
ﬁnding O
the O
end O
ef- O
fector O
position O
given O
the O
joint O
angles O
and O
has O
a O
unique O
solution O
. O
we O
can O
also O
deﬁne O
the O
kernel B
function I
directly O
, O
rather O
than O
indirectly O
through O
a O
choice O
of O
basis B
function I
. O
an O
upper O
bound O
on O
the O
bayesian O
error B
bars O
for O
generalized O
linear B
regression I
. O
( O
11.60 O
) O
a O
second O
important O
property O
of O
hamiltonian O
dynamical O
systems O
, O
known O
as O
li- O
ouville O
’ O
s O
theorem O
, O
is O
that O
they O
preserve O
volume O
in O
phase B
space I
. O
show O
that O
the O
e O
step O
remains O
the O
same O
as O
in O
the O
maximum B
likelihood I
case O
, O
whereas O
in O
the O
m O
step O
the O
quantity O
to O
be O
maximized O
is O
given O
by O
q O
( O
θ O
, O
θold O
) O
+ O
ln O
p O
( O
θ O
) O
where O
q O
( O
θ O
, O
θold O
) O
is O
deﬁned O
by O
( O
9.30 O
) O
. O
the O
em O
algorithm O
described O
here O
also O
does O
not O
construct O
the O
covariance B
matrix I
explicitly O
. O
the O
gamma B
distribution I
has O
a O
ﬁnite O
integral O
if O
a O
> O
0 O
, O
and O
the O
distribution O
itself O
is O
ﬁnite O
if O
a O
( O
cid:2 O
) O
1. O
it O
is O
plotted O
, O
for O
various O
values O
of O
a O
and O
b O
, O
in O
figure O
2.13. O
the O
mean B
and O
variance B
of O
the O
gamma B
distribution I
are O
given O
by O
( O
2.147 O
) O
e O
[ O
λ O
] O
= O
a O
b O
var O
[ O
λ O
] O
= O
a O
b2 O
. O
if O
we O
think O
of O
the O
units O
as O
feature O
detectors O
, O
then O
all O
of O
the O
units O
in O
a O
feature B
map I
detect O
the O
same O
pattern O
but O
at O
different O
locations O
in O
the O
input O
image O
. O
however O
, O
for O
evaluation O
of O
local B
conditional O
distributions O
, O
the O
partition B
function I
is O
not O
needed O
because O
a O
conditional B
is O
the O
ratio O
of O
two O
marginals O
, O
and O
the O
partition B
function I
cancels O
between O
numerator O
and O
denom- O
inator O
when O
evaluating O
this O
ratio O
. O
to O
optimize O
the O
hyperparameter B
α O
, O
we O
again O
maximize O
the O
marginal B
likelihood I
, O
which O
is O
easily O
shown O
to O
take O
the O
form O
ln O
p O
( O
d|α O
) O
( O
cid:7 O
) O
−e O
( O
wmap O
) O
− O
1 O
2 O
where O
the O
regularized O
error O
function O
is O
deﬁned O
by O
e O
( O
wmap O
) O
= O
− O
n O
( O
cid:2 O
) O
n=1 O
ln|a| O
+ O
w O
2 O
ln O
α O
+ O
const O
( O
5.183 O
) O
{ O
tn O
ln O
yn O
+ O
( O
1 O
− O
tn O
) O
ln O
( O
1 O
− O
yn O
) O
} O
+ O
α O
2 O
wt O
mapwmap O
( O
5.184 O
) O
in O
which O
yn O
≡ O
y O
( O
xn O
, O
wmap O
) O
. O
14.17 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
mixture B
model I
for O
a O
conditional B
distribution O
p O
( O
t|x O
) O
of O
the O
form O
k O
( O
cid:2 O
) O
p O
( O
t|x O
) O
= O
πkψk O
( O
t|x O
) O
k=1 O
( O
14.58 O
) O
in O
which O
each O
mixture B
component I
ψk O
( O
t|x O
) O
is O
itself O
a O
mixture B
model I
. O
as O
we O
have O
seen O
in O
chapter O
1 O
, O
the O
use O
of O
maximum B
likelihood I
, O
or O
equivalently O
least O
squares O
, O
can O
lead O
to O
severe O
over-ﬁtting B
if O
complex O
models O
are O
trained O
using O
data O
sets O
of O
limited O
size O
. O
fmn' O
the O
sum O
aod O
p O
, O
oduct O
rules O
`` O
fprobability O
, O
in O
the O
form O
( O
11,34 O
) O
e O
, O
e O
, O
,-ise O
12,7 O
ll O
'' O
'' O
aus O
( O
: O
this O
corresponds O
to O
a O
linear·gau O
'' O
i O
, O
n O
lt1 O
( O
llicl O
thi O
< O
marginal B
di O
, O
tribulion O
is O
again O
gaussian O
. O
( O
8.46 O
) O
we O
see O
that O
the O
factor O
p O
( O
x4|x1 O
, O
x2 O
, O
x3 O
) O
involves O
the O
four O
variables O
x1 O
, O
x2 O
, O
x3 O
, O
and O
x4 O
, O
and O
so O
these O
must O
all O
belong O
to O
a O
single O
clique B
if O
this O
conditional B
distribution O
is O
to O
be O
absorbed O
into O
a O
clique B
potential O
. O
2.60 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
histogram-like O
density B
model O
in O
which O
the O
space O
x O
is O
di- O
vided O
into O
ﬁxed O
regions O
for O
which O
the O
density B
p O
( O
x O
) O
takes O
the O
constant O
value O
hi O
over O
the O
ith O
region O
, O
and O
that O
the O
volume O
of O
region O
i O
is O
denoted O
∆i O
. O
also O
, O
referring O
back O
to O
figure O
11.1 O
, O
we O
note O
that O
if O
f O
( O
z O
) O
is O
small O
in O
regions O
where O
p O
( O
z O
) O
is O
large O
, O
and O
vice O
versa O
, O
then O
the O
expectation B
may O
be O
dominated O
by O
regions O
of O
small O
probability B
, O
implying O
that O
relatively O
large O
sample O
sizes O
will O
be O
required O
to O
achieve O
sufﬁcient O
accuracy O
. O
simpler O
approach O
introducoo O
by O
b.ased O
on O
the O
rddmu O
`` O
p- O
582 O
12. O
continuous O
latent O
variables O
figure O
12.13 O
probabilistic B
graphical I
model I
for O
bayesian O
pea O
in O
which O
the O
distribution O
over O
the O
parameter O
matrix O
w O
is O
governed O
by O
a O
vector O
a O
of O
hyperparameters O
. O
if O
the O
goal O
is O
to O
learn O
a O
model O
that O
can O
take O
an O
input O
image O
and O
output O
the O
orientation O
of O
the O
object O
irrespective O
of O
its O
position O
, O
then O
there O
is O
only O
one O
degree O
of O
freedom O
of O
variability O
within O
the O
manifold B
that O
is O
signiﬁcant O
. O
in O
this O
case O
the O
e O
step O
remains O
the O
same O
as O
in O
the O
maximum B
likelihood I
case O
, O
whereas O
in O
the O
m O
step O
the O
quantity O
to O
be O
maximized O
is O
given O
by O
q O
( O
θ O
, O
θold O
) O
+ O
ln O
p O
( O
θ O
) O
. O
to O
address O
this O
issue O
, O
platt O
( O
2000 O
) O
has O
proposed O
ﬁtting O
a O
logistic B
sigmoid I
to O
the O
outputs O
of O
a O
previously O
trained O
support B
vector I
machine I
. O
to O
obtain O
a O
sample O
from O
some O
marginal B
distribu- O
tion O
corresponding O
to O
a O
subset O
of O
the O
variables O
, O
we O
simply O
take O
the O
sampled O
values O
for O
the O
required O
nodes O
and O
ignore O
the O
sampled O
values O
for O
the O
remaining O
nodes O
. O
symmetric O
matrices O
have O
the O
property O
that O
aij O
= O
aji O
, O
or O
equivalently O
at O
= O
a. O
the O
inverse B
of O
a O
symmetric O
matrix O
is O
also O
sym- O
metric O
, O
as O
can O
be O
seen O
by O
taking O
the O
transpose O
of O
a−1a O
= O
i O
and O
using O
aa−1 O
= O
i O
together O
with O
the O
symmetry O
of O
i. O
in O
general O
, O
the O
eigenvalues O
of O
a O
matrix O
are O
complex O
numbers O
, O
but O
for O
symmetric O
matrices O
the O
eigenvalues O
λi O
are O
real O
. O
if O
we O
divide O
each O
variable O
in O
a O
d-dimensional O
space O
into O
m O
bins O
, O
then O
the O
total O
number O
of O
bins O
will O
be O
m O
d. O
this O
exponential O
scaling O
with O
d O
is O
an O
example O
of O
the O
curse B
of I
dimensionality I
. O
8.2 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
property O
of O
there O
being O
no O
directed B
cycles O
in O
a O
directed B
graph O
follows O
from O
the O
statement O
that O
there O
exists O
an O
ordered O
numbering O
of O
the O
nodes O
such O
that O
for O
each O
node B
there O
are O
no O
links O
going O
to O
a O
lower-numbered O
node B
. O
we O
therefore O
turn O
in O
this O
section O
to O
a O
very O
general O
and O
powerful O
framework O
called O
markov O
chain O
monte O
carlo O
( O
mcmc O
) O
, O
which O
allows O
sampling O
from O
a O
large O
class O
of O
distributions O
, O
538 O
11. O
sampling B
methods I
and O
which O
scales O
well O
with O
the O
dimensionality O
of O
the O
sample O
space O
. O
here O
there O
are O
far O
fewer O
line O
measurements O
than O
in O
a O
typical O
tomography B
application O
. O
−1 O
2 O
exp O
( O
w O
− O
mn O
) O
ta O
( O
w O
− O
mn O
) O
( O
cid:13 O
) O
dw O
( O
3.85 O
) O
using O
( O
3.78 O
) O
we O
can O
then O
write O
the O
log O
of O
the O
marginal B
likelihood I
in O
the O
form O
ln O
p O
( O
t|α O
, O
β O
) O
= O
m O
2 O
ln O
α O
+ O
n O
2 O
ln O
β O
− O
e O
( O
mn O
) O
− O
1 O
2 O
ln|a| O
− O
n O
2 O
ln O
( O
2π O
) O
( O
3.86 O
) O
which O
is O
the O
required O
expression O
for O
the O
evidence B
function I
. O
290 O
5. O
neural O
networks O
5.39 O
( O
( O
cid:12 O
) O
) O
www O
make O
use O
of O
the O
laplace O
approximation O
result O
( O
4.135 O
) O
to O
show O
that O
the O
evidence B
function I
for O
the O
hyperparameters O
α O
and O
β O
in O
the O
bayesian O
neural B
network I
model O
can O
be O
approximated O
by O
( O
5.175 O
) O
. O
( O
10.84 O
) O
using O
the O
general O
result O
( O
10.9 O
) O
, O
together O
with O
the O
product B
rule I
for O
probabilities O
, O
we O
see O
that O
the O
optimal O
solution O
for O
q O
( O
a O
, O
b O
) O
is O
given O
by O
ln O
q O
( O
cid:1 O
) O
( O
a O
, O
b O
) O
= O
ec O
[ O
ln O
p O
( O
x O
, O
a O
, O
b O
, O
c O
) O
] O
+ O
const O
= O
ec O
[ O
ln O
p O
( O
a O
, O
b|x O
, O
c O
) O
] O
+ O
const O
. O
not O
only O
does O
this O
limit O
the O
type O
of O
data O
variables O
that O
can O
be O
considered O
( O
it O
would O
be O
inappropriate O
for O
cases O
where O
some O
or O
all O
of O
the O
variables O
represent O
categorical O
labels O
for O
instance O
) O
, O
428 O
9. O
mixture B
models O
and O
em O
section O
2.3.7 O
but O
it O
can O
also O
make O
the O
determination O
of O
the O
cluster O
means O
nonrobust O
to O
outliers B
. O
instead O
, O
computationally O
more O
expensive O
techniques O
must O
be O
employed O
, O
such O
as O
gibbs O
sampling O
, O
which O
is O
discussed O
in O
section O
11.3. O
as O
well O
as O
sampling O
from O
conditional B
distributions O
, O
we O
may O
also O
require O
samples O
from O
a O
marginal B
distribution O
. O
we O
therefore O
choose O
a O
dirichlet O
distribution O
over O
the O
mixing O
coefﬁcients O
π O
p O
( O
π O
) O
= O
dir O
( O
π|α0 O
) O
= O
c O
( O
α0 O
) O
πα0−1 O
k O
( O
10.39 O
) O
where O
by O
symmetry O
we O
have O
chosen O
the O
same O
parameter O
α0 O
for O
each O
of O
the O
compo- O
nents O
, O
and O
c O
( O
α0 O
) O
is O
the O
normalization O
constant O
for O
the O
dirichlet O
distribution O
deﬁned O
k=1 O
( O
cid:11 O
) O
znk O
k O
( O
cid:14 O
) O
section O
10.4.1 O
10.2. O
illustration O
: O
variational B
mixture O
of O
gaussians O
475 O
figure O
10.5 O
directed B
acyclic I
graph I
representing O
the O
bayesian O
mix- O
ture O
of O
gaussians O
model O
, O
in O
which O
the O
box O
( O
plate B
) O
de- O
notes O
a O
set O
of O
n O
i.i.d O
. O
the O
decision O
surfaces O
correspond O
to O
y O
( O
x O
) O
= O
constant O
, O
so O
that O
wtx O
+ O
w0 O
= O
constant O
and O
hence O
the O
deci- O
sion B
surfaces O
are O
linear O
functions O
of O
x O
, O
even O
if O
the O
function O
f O
( O
· O
) O
is O
nonlinear O
. O
one O
attempt O
to O
improve O
the O
tightness O
of O
the O
pac O
bounds O
is O
the O
pac-bayesian O
framework O
( O
mcallester O
, O
2003 O
) O
, O
which O
considers O
a O
distribution O
over O
the O
space O
f O
of O
functions O
, O
somewhat O
analogous O
to O
the O
prior B
in O
a O
bayesian O
treatment O
. O
the O
max-sum B
algorithm I
is O
more O
complex O
because O
the O
messages O
are O
functions O
of O
node B
variables O
x O
and O
hence O
comprise O
a O
set O
of O
k O
values O
for O
each O
pos- O
sible O
state O
of O
x. O
unlike O
max-sum O
, O
however O
, O
icm O
is O
not O
guaranteed O
to O
ﬁnd O
a O
global O
maximum O
even O
for O
tree-structured O
graphs O
. O
error B
backpropagation I
1. O
apply O
an O
input O
vector O
xn O
to O
the O
network O
and O
forward O
propagate O
through O
the O
network O
using O
( O
5.48 O
) O
and O
( O
5.49 O
) O
to O
ﬁnd O
the O
activations O
of O
all O
the O
hidden O
and O
output O
units O
. O
at O
this O
stage O
, O
we O
can O
also O
evaluate O
the O
likelihood B
function I
. O
as O
a O
consequence O
of O
the O
lack O
of O
any O
assumptions O
about O
the O
form O
of O
the O
distribution O
, O
the O
pac O
bounds O
are O
very O
conservative O
, O
in O
other O
words O
they O
strongly O
over-estimate O
the O
size O
of O
data O
sets O
required O
to O
achieve O
a O
given O
generalization B
performance O
. O
in O
this O
case O
, O
the O
columns O
of O
w O
define O
the O
principal B
subspace I
of O
stan O
( O
cid:173 O
) O
dard O
pca O
. O
−∞ O
consider O
for O
example O
the O
exponential B
distribution I
p O
( O
y O
) O
= O
λ O
exp O
( O
−λy O
) O
( O
11.7 O
) O
where O
0 O
( O
cid:1 O
) O
y O
< O
∞ O
. O
the O
images O
of O
the O
mark O
1 O
perceptron B
and O
of O
frank O
rosenblatt O
are O
repro- O
duced O
with O
the O
permission O
of O
arvin O
calspan O
advanced O
technology O
center O
. O
we O
end O
this O
chapter O
by O
considering O
three O
nonparametric B
methods I
based O
respectively O
on O
histograms O
, O
nearest-neighbours O
, O
and O
kernels O
. O
any O
other O
minima O
corresponding O
to O
higher O
values O
of O
the O
error B
function I
are O
said O
to O
be O
local B
minima O
. O
10.18 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
this O
exercise O
, O
we O
shall O
derive O
the O
variational B
re-estimation O
equations O
for O
the O
gaussian O
mixture B
model I
by O
direct O
differentiation O
of O
the O
lower B
bound I
. O
a O
simple O
approach O
would O
be O
to O
stop O
when O
the O
reduction O
in O
residual O
error B
falls O
below O
some O
threshold O
. O
figure O
8.12 O
as O
in O
figure O
8.11 O
but O
with O
a O
sin- O
gle O
set O
of O
parameters O
µ O
shared O
amongst O
all O
of O
the O
conditional B
distributions O
p O
( O
xi|xi−1 O
) O
. O
the O
candidate O
sample O
is O
then O
accepted O
with O
probability B
a O
( O
z O
( O
cid:1 O
) O
, O
z O
( O
τ O
) O
) O
= O
min O
1 O
, O
. O
x1 O
x3 O
x1 O
x3 O
x2 O
x2 O
8.3. O
markov O
random O
fields O
391 O
x4 O
( O
a O
) O
x4 O
( O
b O
) O
this O
is O
easily O
done O
by O
identifying O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
= O
p O
( O
x1 O
) O
p O
( O
x2|x1 O
) O
ψ2,3 O
( O
x2 O
, O
x3 O
) O
= O
p O
( O
x3|x2 O
) O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
= O
p O
( O
xn|xn−1 O
) O
... O
where O
we O
have O
absorbed O
the O
marginal B
p O
( O
x1 O
) O
for O
the O
ﬁrst O
node O
into O
the O
ﬁrst O
potential O
function O
. O
this O
involves O
introducing O
latent O
variables O
znk O
that O
correspond O
to O
a O
1-of-k O
coded O
binary O
indicator O
variable O
for O
each O
data O
point O
n. O
the O
complete-data O
likelihood B
function I
is O
then O
given O
by O
p O
( O
t O
, O
z|θ O
) O
= O
πkytn O
nk O
[ O
1 O
− O
ynk O
] O
1−tn O
( O
14.47 O
) O
n=1 O
k=1 O
where O
z O
is O
the O
matrix O
of O
latent O
variables O
with O
elements O
znk O
. O
( O
2.243 O
) O
using O
( O
2.11 O
) O
, O
we O
see O
that O
the O
mean B
fraction O
of O
points O
falling O
inside O
the O
region O
is O
e O
[ O
k/n O
] O
= O
p O
, O
and O
similarly O
using O
( O
2.12 O
) O
we O
see O
that O
the O
variance B
around O
this O
mean B
is O
var O
[ O
k/n O
] O
= O
p O
( O
1 O
− O
p O
) O
/n O
. O
in O
each O
case O
, O
we O
see O
that O
messages O
passed O
along O
a O
link B
are O
always O
a O
function O
of O
the O
variable O
associated O
with O
the O
variable O
node B
that O
link B
connects O
to O
. O
( O
5.152 O
) O
the O
adaptive O
parameters O
of O
the O
mixture B
density I
network I
comprise O
the O
vector O
w O
of O
weights O
and O
biases O
in O
the O
neural B
network I
, O
that O
can O
be O
set O
by O
maximum B
likelihood I
, O
or O
equivalently O
by O
minimizing O
an O
error B
function I
deﬁned O
to O
be O
the O
negative O
logarithm O
of O
the O
likelihood O
. O
3.13 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
predictive B
distribution I
p O
( O
t|x O
, O
t O
) O
for O
the O
model O
discussed O
in O
ex- O
ercise O
3.12 O
is O
given O
by O
a O
student O
’ O
s O
t-distribution O
of O
the O
form O
p O
( O
t|x O
, O
t O
) O
= O
st O
( O
t|µ O
, O
λ O
, O
ν O
) O
( O
3.114 O
) O
and O
obtain O
expressions O
for O
µ O
, O
λ O
and O
ν O
. O
10.11 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
by O
using O
a O
lagrange O
multiplier O
to O
enforce O
the O
normalization O
constraint O
on O
the O
distribution O
q O
( O
m O
) O
, O
show O
that O
the O
maximum O
of O
the O
lower B
bound I
( O
10.35 O
) O
is O
given O
by O
( O
10.36 O
) O
. O
the O
network O
will O
therefore O
have O
an O
overall O
weight-space B
symmetry I
factor O
of O
m O
! O
2m O
. O
10.21 O
( O
( O
cid:12 O
) O
) O
show O
that O
the O
number O
of O
equivalent O
parameter O
settings O
due O
to O
interchange O
sym- O
metries O
in O
a O
mixture B
model I
with O
k O
components O
is O
k O
! O
. O
this O
follows O
from O
the O
fact O
that O
, O
except O
for O
a O
network O
with O
very O
sparse O
connections O
, O
the O
number O
of O
weights O
is O
typically O
much O
greater O
than O
the O
number O
of O
units O
, O
and O
so O
the O
bulk O
of O
the O
computational O
effort O
in O
forward B
propagation I
is O
concerned O
with O
evaluat- O
ing O
the O
sums O
in O
( O
5.48 O
) O
, O
with O
the O
evaluation O
of O
the O
activation O
functions O
representing O
a O
small O
overhead O
. O
however O
, O
knowledge O
of O
the O
value O
of O
ze O
can O
be O
useful O
for O
bayesian O
model B
comparison I
since O
it O
represents O
the O
model B
evidence I
( O
i.e. O
, O
the O
probability B
of O
the O
observed O
data O
given O
the O
model O
) O
, O
and O
so O
it O
is O
of O
interest O
to O
consider O
how O
its O
value O
might O
be O
obtained O
. O
similarly O
, O
the O
optimal O
solution O
for O
the O
factor O
qτ O
( O
τ O
) O
is O
given O
by O
τ O
( O
τ O
) O
= O
eµ O
[ O
ln O
p O
( O
d|µ O
, O
τ O
) O
+ O
ln O
p O
( O
µ|τ O
) O
] O
+ O
ln O
p O
( O
τ O
) O
+ O
const O
ln O
q O
( O
cid:1 O
) O
= O
( O
a0 O
− O
1 O
) O
ln O
τ O
− O
b0τ O
+ O
n O
2 O
ln O
τ O
( O
cid:31 O
) O
n O
( O
cid:2 O
) O
n=1 O
− O
τ O
2 O
eµ O
( O
xn O
− O
µ O
) O
2 O
+ O
λ0 O
( O
µ O
− O
µ0 O
) O
2 O
+ O
const O
( O
10.28 O
) O
and O
hence O
qτ O
( O
τ O
) O
is O
a O
gamma B
distribution I
gam O
( O
τ|an O
, O
bn O
) O
with O
parameters O
an O
= O
a0 O
+ O
n O
2 O
1 O
2 O
eµ O
bn O
= O
b0 O
+ O
( O
cid:31 O
) O
n O
( O
cid:2 O
) O
n=1 O
( O
xn O
− O
µ O
) O
2 O
+ O
λ0 O
( O
µ O
− O
µ0 O
) O
2 O
. O
we O
end O
our O
discussion O
of O
conditional B
independence I
properties O
by O
exploring O
the O
concept O
of O
a O
markov O
blanket O
or O
markov O
boundary O
. O
similarly O
, O
show O
that O
the O
inverse B
matrix O
σ O
representation O
of O
the O
form O
( O
2.49 O
) O
. O
3.6 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
linear O
basis O
function O
regression O
model O
for O
a O
multivariate O
target O
variable O
t O
having O
a O
gaussian O
distribution O
of O
the O
form O
p O
( O
t|w O
, O
σ O
) O
= O
n O
( O
t|y O
( O
x O
, O
w O
) O
, O
σ O
) O
where O
y O
( O
x O
, O
w O
) O
= O
wtφ O
( O
x O
) O
( O
3.107 O
) O
( O
3.108 O
) O
exercises O
175 O
together O
with O
a O
training B
data O
set O
comprising O
input O
basis O
vectors O
φ O
( O
xn O
) O
and O
corre- O
sponding O
target O
vectors O
tn O
, O
with O
n O
= O
1 O
, O
. O
the O
left O
plot O
shows O
data O
points O
drawn O
from O
two O
classes O
denoted O
red O
and O
blue O
, O
in O
which O
the O
background O
colour O
( O
which O
varies O
from O
pure O
red O
to O
pure O
blue O
) O
denotes O
the O
true O
probability B
of O
the O
class O
label O
. O
to O
understand O
the O
difference O
, O
con- O
sider O
the O
example O
of O
density B
estimation I
using O
a O
mixture O
of O
gaussians O
in O
which O
several O
gaussian O
components O
are O
combined O
probabilistically O
. O
( O
b O
) O
by O
dropping O
the O
link B
between O
the O
nodes O
, O
the O
number O
of O
parameters O
is O
reduced O
to O
2 O
( O
k O
− O
1 O
) O
. O
both O
models O
are O
described O
by O
di- O
rected O
graphs O
having O
a O
tree B
structure O
( O
no O
loops O
) O
for O
which O
inference B
can O
be O
performed O
efﬁciently O
using O
the O
sum-product B
algorithm I
. O
so O
far O
our O
discussion O
of O
polynomial B
curve I
ﬁtting I
has O
appealed O
largely O
to O
in- O
tuition O
. O
( O
14.44 O
) O
in O
figure O
14.8 O
, O
we O
illustrate O
this O
em O
algorithm O
using O
the O
simple O
example O
of O
ﬁtting O
a O
mixture O
of O
two O
straight O
lines O
to O
a O
data O
set O
having O
one O
input O
variable O
x O
and O
one O
target O
variable O
t. O
the O
predictive O
density O
( O
14.34 O
) O
is O
plotted O
in O
figure O
14.9 O
using O
the O
converged O
parameter O
values O
obtained O
from O
the O
em O
algorithm O
, O
corresponding O
to O
the O
right-hand O
plot O
in O
figure O
14.8. O
also O
shown O
in O
this O
ﬁgure O
is O
the O
result O
of O
ﬁtting O
a O
single O
linear B
regression I
model O
, O
which O
gives O
a O
unimodal O
predictive O
density O
. O
3.5.3 O
effective B
number I
of I
parameters I
. O
we O
can O
extend O
the O
deﬁnition O
of O
entropy B
to O
include O
distributions O
p O
( O
x O
) O
over O
con- O
tinuous O
variables O
x O
as O
follows O
. O
for O
the O
case O
of O
the O
sum-of-squares B
error I
function O
( O
3.12 O
) O
, O
this O
gives O
w O
( O
τ O
+1 O
) O
= O
w O
( O
τ O
) O
+ O
η O
( O
tn O
− O
w O
( O
τ O
) O
tφn O
) O
φn O
( O
3.23 O
) O
where O
φn O
= O
φ O
( O
xn O
) O
. O
p O
( O
z O
) O
q O
( O
z O
) O
f O
( O
z O
) O
z O
furthermore O
, O
the O
exponential O
decrease O
of O
acceptance O
rate O
with O
dimensionality O
is O
a O
generic O
feature O
of O
rejection B
sampling I
. O
a O
generalization B
of O
principal O
component O
analy- O
sis O
to O
the O
exponential B
family I
. O
show O
that O
maximization O
of O
the O
function O
q O
( O
θ O
, O
θold O
) O
with O
respect O
to O
the O
mean B
and O
covariance B
parameters O
of O
the O
gaussians O
gives O
rise O
to O
the O
m-step O
equations O
( O
13.20 O
) O
and O
( O
13.21 O
) O
. O
( O
5.168 O
) O
however O
, O
even O
with O
the O
gaussian O
approximation O
to O
the O
posterior O
, O
this O
integration O
is O
still O
analytically O
intractable O
due O
to O
the O
nonlinearity O
of O
the O
network O
function O
y O
( O
x O
, O
w O
) O
as O
a O
function O
of O
w. O
to O
make O
progress O
, O
we O
now O
assume O
that O
the O
posterior O
distribution O
has O
small O
variance B
compared O
with O
the O
characteristic O
scales O
of O
w O
over O
which O
y O
( O
x O
, O
w O
) O
is O
varying O
. O
in O
fact O
, O
the O
evidence O
is O
sufﬁciently O
strong O
that O
it O
outweighs O
the O
prior B
and O
makes O
it O
more O
likely O
that O
the O
red O
box O
was O
chosen O
rather O
than O
the O
blue O
one O
. O
show O
that O
the O
distribution O
( O
2.261 O
) O
is O
normalized O
, O
and O
evaluate O
its O
mean B
, O
variance B
, O
and O
entropy B
. O
as O
before O
, O
the O
prior B
is O
taken O
to O
be O
an O
isotropic B
gaussian O
of O
the O
form O
( O
5.162 O
) O
. O
note O
that O
the O
m O
step O
does O
not O
have O
a O
closed-form O
solution O
and O
must O
be O
solved O
iteratively O
using O
, O
for O
instance O
, O
the O
iterative B
reweighted I
least I
squares I
( O
irls O
) O
algorithm O
. O
5.2.1 O
parameter O
optimization O
we O
turn O
next O
to O
the O
task O
of O
ﬁnding O
a O
weight B
vector I
w O
which O
minimizes O
the O
chosen O
function O
e O
( O
w O
) O
. O
thus O
we O
have O
p O
( O
t|x O
, O
w O
, O
β O
) O
= O
n O
( O
cid:10 O
) O
( O
cid:11 O
) O
t|y O
( O
x O
, O
w O
) O
, O
β O
−1 O
( O
1.60 O
) O
where O
, O
for O
consistency O
with O
the O
notation O
in O
later O
chapters O
, O
we O
have O
deﬁned O
a O
preci- O
sion B
parameter O
β O
corresponding O
to O
the O
inverse B
variance O
of O
the O
distribution O
. O
from O
( O
4.91 O
) O
we O
see O
that O
the O
gradient O
and O
hessian O
of O
this O
error B
function I
are O
given O
by O
n O
( O
cid:2 O
) O
∇e O
( O
w O
) O
= O
( O
yn O
− O
tn O
) O
φn O
= O
φt O
( O
y O
− O
t O
) O
n=1 O
h O
= O
∇∇e O
( O
w O
) O
= O
yn O
( O
1 O
− O
yn O
) O
φnφt O
n O
= O
φtrφ O
n O
( O
cid:2 O
) O
n=1 O
( O
4.95 O
) O
( O
4.96 O
) O
( O
4.97 O
) O
208 O
4. O
linear O
models O
for O
classification O
rnn O
= O
yn O
( O
1 O
− O
yn O
) O
. O
for O
any O
given O
setting O
of O
the O
parameters O
in O
a O
gaussian O
mixture B
model I
( O
except O
for O
speciﬁc O
degenerate O
settings O
) O
, O
there O
will O
exist O
other O
parameter O
settings O
for O
which O
the O
density B
over O
the O
observed O
vari- O
ables O
will O
be O
identical O
. O
in O
the O
latter O
case O
, O
the O
variational B
inference I
algorithms O
involves O
running O
independent O
forward O
and O
backward O
recursions O
along O
each O
chain O
, O
which O
is O
computationally O
efﬁcient O
and O
yet O
is O
also O
able O
to O
capture O
correlations O
between O
variables O
within O
the O
same O
chain O
. O
using O
the O
results O
( O
8.93 O
) O
and O
( O
8.94 O
) O
, O
we O
see O
that O
the O
messages O
passed O
in O
the O
max-sum B
algorithm I
are O
given O
by O
( O
cid:27 O
) O
( O
13.66 O
) O
. O
( O
4.36 O
) O
exercise O
4.6 O
after O
some O
straightforward O
algebra O
, O
and O
again O
making O
use O
of O
the O
choice O
of O
tn O
, O
the O
second O
equation O
( O
4.33 O
) O
becomes O
( O
cid:15 O
) O
( O
cid:16 O
) O
sw O
+ O
n1n2 O
n O
sb O
w O
= O
n O
( O
m1 O
− O
m2 O
) O
( O
4.37 O
) O
where O
sw O
is O
deﬁned O
by O
( O
4.28 O
) O
, O
sb O
is O
deﬁned O
by O
( O
4.27 O
) O
, O
and O
we O
have O
substituted O
for O
the O
bias B
using O
( O
4.34 O
) O
. O
it O
is O
a O
general O
result O
that O
a O
factorized O
variational O
ap- O
proximation B
tends O
to O
give O
approximations O
to O
the O
posterior O
distribution O
that O
are O
too O
compact O
. O
the O
simplest O
such O
penalty O
term O
takes O
the O
form O
of O
a O
sum O
of O
squares O
of O
all O
of O
the O
coefﬁcients O
, O
leading O
to O
a O
modiﬁed O
error B
function I
of O
the O
form O
n O
( O
cid:2 O
) O
( O
cid:4 O
) O
e O
( O
w O
) O
= O
1 O
2 O
{ O
y O
( O
xn O
, O
w O
) O
− O
tn O
} O
2 O
+ O
λ O
2 O
( O
cid:6 O
) O
w O
( O
cid:6 O
) O
2 O
( O
1.4 O
) O
n=1 O
0 O
+ O
w2 O
1 O
+ O
. O
( O
2.146 O
) O
exercise O
2.41 O
exercise O
2.42 O
here O
γ O
( O
a O
) O
is O
the O
gamma B
function I
that O
is O
deﬁned O
by O
( O
1.141 O
) O
and O
that O
ensures O
that O
( O
2.146 O
) O
is O
correctly O
normalized O
. O
these O
quantities O
, O
along O
with O
their O
sum O
, O
are O
plotted O
as O
a O
function O
of O
ln O
λ O
in O
figure O
3.6. O
we O
see O
that O
small O
values O
of O
λ O
allow O
the O
model O
to O
become O
ﬁnely O
tuned O
to O
the O
noise O
on O
each O
individual O
152 O
3. O
linear O
models O
for B
regression I
data O
set O
leading O
to O
large O
variance O
. O
because O
the O
samples O
will O
be O
generated O
by O
a O
computer O
algorithm O
they O
will O
in O
fact O
be O
pseudo-random B
numbers I
, O
that O
is O
, O
they O
will O
be O
deter- O
ministically O
calculated O
, O
but O
must O
nevertheless O
pass O
appropriate O
tests O
for O
randomness O
. O
of O
all O
possible O
trees O
that O
link B
up O
the O
cliques O
, O
the O
one O
that O
is O
chosen O
is O
one O
for O
which O
the O
weight O
of O
the O
tree B
is O
largest O
, O
where O
the O
weight O
for O
a O
link B
is O
the O
number O
of O
nodes O
shared O
by O
the O
two O
cliques O
it O
connects O
, O
and O
the O
weight O
for O
the O
tree B
is O
the O
sum O
of O
the O
weights O
for O
the O
links O
. O
next O
we O
discuss O
various O
approaches O
to O
regulariza- O
tion O
of O
neural B
network I
training O
and O
the O
relationships O
between O
them O
. O
the O
origins O
of O
this O
sparsity B
were O
discussed O
earlier O
in O
the O
context O
of O
relevance B
vector I
machines O
. O
( O
8.54 O
) O
1 O
z O
we O
shall O
interpret O
µα O
( O
xn O
) O
as O
a O
message O
passed O
forwards O
along O
the O
chain O
from O
node B
xn−1 O
to O
node B
xn O
. O
p O
( O
θ|θ0 O
, O
m O
) O
= O
1 O
2πi0 O
( O
m O
) O
exp O
{ O
m O
cos O
( O
θ O
− O
θ0 O
) O
} O
( O
2.179 O
) O
which O
is O
called O
the O
von O
mises O
distribution O
, O
or O
the O
circular B
normal I
. O
( O
4.158 O
) O
exercises O
221 O
to O
do O
so O
, O
assume O
that O
one O
of O
the O
basis O
functions O
φ0 O
( O
x O
) O
= O
1 O
so O
that O
the O
corresponding O
parameter O
w0 O
plays O
the O
role O
of O
a O
bias B
. O
there O
are O
, O
however O
, O
other O
ways O
to O
control O
the O
complexity O
of O
a O
neural B
network I
model O
in O
order O
to O
avoid O
over-ﬁtting B
. O
even O
the O
subjective O
nature O
of O
the O
conclusions O
through O
their O
de- O
pendence O
on O
the O
choice O
of O
prior B
is O
seen O
by O
some O
as O
a O
source O
of O
difﬁculty O
. O
the O
motivation O
for O
the O
logistic B
sigmoid I
representation O
was O
discussed O
in O
section O
4.2. O
figure O
8.13 O
a O
graph O
comprising O
m O
parents O
x1 O
, O
. O
for O
graphs O
that O
have O
a O
tree B
structure O
, O
any O
schedule B
that O
sends O
only O
pending O
messages O
will O
eventually O
terminate O
once O
a O
message O
has O
passed O
in O
each O
direction O
across O
every O
link B
. O
figure O
7.2 O
shows O
an O
example O
of O
the O
classiﬁcation B
resulting O
from O
training B
a O
sup- O
port O
vector O
machine O
on O
a O
simple O
synthetic O
data O
set O
using O
a O
gaussian O
kernel O
of O
the O
7.1. O
maximum B
margin I
classiﬁers O
331 O
figure O
7.2 O
example O
of O
synthetic O
data O
from O
two O
classes O
in O
two O
dimensions O
showing O
contours O
of O
constant O
y O
( O
x O
) O
obtained O
from O
a O
support B
vector I
machine I
having O
a O
gaus- O
sian O
kernel B
function I
. O
we O
now O
ﬁnd O
an O
expression O
for O
the O
regression B
function I
y O
( O
x O
) O
, O
corresponding O
to O
the O
conditional B
average O
of O
the O
target O
variable O
conditioned O
on O
302 O
6. O
kernel O
methods O
the O
input O
variable O
, O
which O
is O
given O
by O
y O
( O
x O
) O
= O
e O
[ O
t|x O
] O
= O
( O
cid:6 O
) O
∞ O
−∞ O
tp O
( O
t|x O
) O
dt O
tp O
( O
x O
, O
t O
) O
dt O
p O
( O
x O
, O
t O
) O
dt O
= O
= O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:6 O
) O
∞ O
m O
n O
−∞ O
g O
( O
x O
− O
xn O
) O
tn O
g O
( O
x O
− O
xm O
) O
k O
( O
x O
, O
xn O
) O
tn O
y O
( O
x O
) O
= O
= O
k O
( O
x O
, O
xn O
) O
= O
g O
( O
x O
− O
xn O
) O
g O
( O
x O
− O
xm O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
m O
n O
n O
( O
cid:2 O
) O
( O
cid:6 O
) O
∞ O
m O
( O
6.44 O
) O
( O
6.45 O
) O
( O
6.46 O
) O
( O
6.47 O
) O
tf O
( O
x O
− O
xn O
, O
t O
− O
tn O
) O
dt O
f O
( O
x O
− O
xm O
, O
t O
− O
tm O
) O
dt O
. O
because O
we O
seek O
a O
gaussian O
representation O
for O
the O
posterior O
distribution O
, O
it O
is O
natural O
to O
begin O
with O
a O
gaussian O
prior B
, O
which O
we O
write O
in O
the O
general O
form O
p O
( O
w O
) O
= O
n O
( O
w|m0 O
, O
s0 O
) O
( O
4.140 O
) O
218 O
4. O
linear O
models O
for O
classification O
where O
m0 O
and O
s0 O
are O
ﬁxed O
hyperparameters O
. O
because O
the O
value O
of O
this O
hyperparameter B
may O
itself O
be O
unknown O
, O
we O
can O
again O
treat O
it O
from O
a O
bayesian O
perspective O
by O
introducing O
a O
prior B
over O
the O
hyperparameter B
, O
sometimes O
called O
a O
hyperprior B
, O
which O
is O
again O
given O
by O
a O
gaussian O
distribution O
. O
this O
can O
be O
verified O
directly O
by O
noting O
that O
the O
marginal B
density O
( O
12.35 O
) O
, O
and O
hence O
the O
likelihood B
function I
, O
is O
unchanged O
if O
we O
make O
the O
transformation O
w O
- O
) O
wr O
where O
r O
is O
an O
orthogonal O
matrix O
satisfying O
rrt O
= O
i O
, O
because O
the O
matrix O
c O
given O
by O
( O
12.36 O
) O
is O
itself O
invariant O
. O
summing O
this O
arithmetic O
series O
, O
we O
see O
that O
r O
has O
a O
total O
of O
m O
( O
m O
-1 O
) O
/2 O
independent B
parameters O
. O
one O
technique O
is O
based O
on O
variational B
inference I
( O
gibbs O
and O
mackay O
, O
2000 O
) O
and O
makes O
use O
of O
the O
local B
variational O
bound O
( O
10.144 O
) O
on O
the O
logistic B
sigmoid I
. O
the O
marginal B
likelihood I
for O
this O
model O
now O
takes O
the O
form O
( O
cid:6 O
) O
( O
cid:6 O
) O
p O
( O
t O
) O
= O
p O
( O
w O
, O
α O
, O
t O
) O
dw O
dα O
( O
10.167 O
) O
where O
the O
joint O
distribution O
is O
given O
by O
p O
( O
w O
, O
α O
, O
t O
) O
= O
p O
( O
t|w O
) O
p O
( O
w|α O
) O
p O
( O
α O
) O
. O
the O
correspond- O
ing O
values O
of O
{ O
tn O
} O
, O
shown O
in O
green O
, O
are O
obtained O
by O
adding O
independent B
gaussian O
noise O
to O
each O
of O
the O
{ O
yn O
} O
. O
( O
4.103 O
) O
= O
φt O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
4.3. O
probabilistic O
discriminative O
models O
209 O
section O
4.2 O
4.3.4 O
multiclass B
logistic O
regression B
in O
our O
discussion O
of O
generative O
models O
for O
multiclass O
classiﬁcation B
, O
we O
have O
seen O
that O
for O
a O
large O
class O
of O
distributions O
, O
the O
posterior O
probabilities O
are O
given O
by O
a O
softmax O
transformation O
of O
linear O
functions O
of O
the O
feature O
variables O
, O
so O
that O
p O
( O
ck|φ O
) O
= O
yk O
( O
φ O
) O
= O
( O
cid:5 O
) O
exp O
( O
ak O
) O
j O
exp O
( O
aj O
) O
( O
4.104 O
) O
( O
4.105 O
) O
where O
the O
‘ O
activations O
’ O
ak O
are O
given O
by O
ak O
= O
wt O
k O
φ. O
there O
we O
used O
maximum B
likelihood I
to O
determine O
separately O
the O
class-conditional O
densities O
and O
the O
class O
priors O
and O
then O
found O
the O
corresponding O
posterior O
probabilities O
using O
bayes O
’ O
theorem O
, O
thereby O
implicitly O
determining O
the O
parameters O
{ O
wk O
} O
. O
an O
improvement O
on O
this O
approach O
is O
called O
likelihood B
weighted I
sampling I
( O
fung O
and O
chang O
, O
1990 O
; O
shachter O
and O
peot O
, O
1990 O
) O
and O
is O
based O
on O
ancestral B
sampling I
of O
the O
variables O
. O
our O
uncertainty O
is O
expressed O
through O
a O
prior B
probability O
distribution O
p O
( O
mi O
) O
. O
1.1. O
example O
: O
polynomial O
curve O
fitting O
we O
begin O
by O
introducing O
a O
simple O
regression B
problem O
, O
which O
we O
shall O
use O
as O
a O
run- O
ning O
example O
throughout O
this O
chapter O
to O
motivate O
a O
number O
of O
key O
concepts O
. O
it O
is O
worth O
emphasizing O
that O
the O
results O
( O
9.17 O
) O
, O
( O
9.19 O
) O
, O
and O
( O
9.22 O
) O
do O
not O
con- O
stitute O
a O
closed-form O
solution O
for O
the O
parameters O
of O
the O
mixture B
model I
because O
the O
responsibilities O
γ O
( O
znk O
) O
depend O
on O
those O
parameters O
in O
a O
complex O
way O
through O
( O
9.13 O
) O
. O
) O
, O
advances O
in O
neural O
infor- O
mation B
processing O
systems O
, O
volume O
15 O
, O
pp O
. O
substituting O
( O
7.57 O
) O
into O
( O
7.1 O
) O
, O
we O
see O
that O
predictions O
for O
new O
inputs O
can O
be O
made O
using O
y O
( O
x O
) O
= O
( O
an O
− O
( O
cid:1 O
) O
an O
) O
k O
( O
x O
, O
xn O
) O
+ O
b O
which O
is O
again O
expressed O
in O
terms O
of O
the O
kernel B
function I
. O
next O
we O
consider O
the O
messages O
that O
are O
propagated O
from O
the O
root B
node I
back O
to O
the O
leaf O
node B
. O
( O
5.35 O
) O
( O
cid:2 O
) O
i O
this O
can O
be O
regarded O
as O
a O
transformation O
of O
the O
coordinate O
system O
in O
which O
the O
origin O
is O
translated O
to O
the O
point O
w O
( O
cid:1 O
) O
, O
and O
the O
axes O
are O
rotated O
to O
align O
with O
the O
eigenvectors O
( O
through O
the O
orthogonal O
matrix O
whose O
columns O
are O
the O
ui O
) O
, O
and O
is O
discussed O
in O
more O
detail O
in O
appendix O
c. O
substituting O
( O
5.35 O
) O
into O
( O
5.32 O
) O
, O
and O
using O
( O
5.33 O
) O
and O
( O
5.34 O
) O
, O
allows O
the O
error B
function I
to O
be O
written O
in O
the O
form O
e O
( O
w O
) O
= O
e O
( O
w O
( O
cid:1 O
) O
) O
+ O
1 O
2 O
λiα2 O
i O
. O
then O
the O
total O
number O
of O
parameters O
wij O
is O
obtained O
by O
taking O
the O
number O
d2 O
of O
elements O
in O
a O
d× O
d O
matrix O
, O
subtracting O
d O
to O
account O
for O
the O
absence O
of O
elements O
on O
the O
lead- O
ing O
diagonal O
, O
and O
then O
dividing O
by O
2 O
because O
the O
matrix O
has O
elements O
only O
below O
the O
diagonal B
, O
giving O
a O
total O
of O
d O
( O
d−1 O
) O
/2 O
. O
finally O
, O
use O
( O
5.73 O
) O
to O
do O
the O
backpropagation B
to O
the O
inputs O
. O
for O
example O
, O
the O
ﬂooding B
schedule I
simultaneously O
passes O
a O
message O
across O
every O
link B
in O
both O
directions O
at O
each O
time O
step O
, O
whereas O
schedules O
that O
pass O
one O
message O
at O
a O
time O
are O
called O
serial O
schedules O
. O
the O
goal O
is O
to O
infer O
the O
mean B
of O
the O
green O
gaussian O
from O
the O
observed O
data O
. O
we O
conclude O
with O
a O
brief O
introduction O
to O
an O
alternative O
variational B
framework O
known O
as O
expectation B
propagation I
. O
fine O
: O
feedforward O
neural B
network I
methodology O
. O
if O
the O
input O
image O
is O
shifted O
, O
the O
activations O
of O
the O
feature B
map I
will O
be O
shifted O
by O
the O
same O
amount O
but O
will O
otherwise O
be O
unchanged O
. O
finally O
, O
we O
note O
that O
the O
equivalent B
kernel I
( O
3.62 O
) O
satisﬁes O
an O
important O
property O
shared O
by O
kernel O
functions O
in O
general O
, O
namely O
that O
it O
can O
be O
expressed O
in O
the O
form O
an O
inner O
product O
with O
respect O
to O
a O
vector O
ψ O
( O
x O
) O
of O
nonlinear O
functions O
, O
so O
that O
k O
( O
x O
, O
z O
) O
= O
ψ O
( O
x O
) O
tψ O
( O
z O
) O
( O
3.65 O
) O
where O
ψ O
( O
x O
) O
= O
β1/2s1/2 O
n O
φ O
( O
x O
) O
. O
, O
h O
with O
prior B
probabilities O
p O
( O
h O
) O
. O
11.6 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
in O
this O
exercise O
, O
we O
show O
more O
carefully O
that O
rejection B
sampling I
does O
indeed O
draw O
samples O
from O
the O
desired O
distribution O
p O
( O
z O
) O
. O
however O
, O
in O
special O
cases O
, O
for O
instance O
where O
the O
undirected B
graph I
is O
constructed O
by O
starting O
with O
a O
directed B
graph O
, O
the O
potential O
functions O
may O
indeed O
have O
such O
an O
interpretation O
, O
as O
we O
shall O
see O
shortly O
. O
having O
determined O
the O
parameters O
w O
and O
β O
, O
we O
can O
now O
make O
predictions O
for O
new O
values O
of O
x. O
because O
we O
now O
have O
a O
probabilistic O
model O
, O
these O
are O
expressed O
in O
terms O
of O
the O
predictive B
distribution I
that O
gives O
the O
probability B
distribution O
over O
t O
, O
rather O
than O
simply O
a O
point O
estimate O
, O
and O
is O
obtained O
by O
substituting O
the O
maximum B
likelihood I
parameters O
into O
( O
1.60 O
) O
to O
give O
( O
cid:11 O
) O
t|y O
( O
x O
, O
wml O
) O
, O
β O
−1 O
ml O
. O
the O
general O
idea O
behind O
sampling B
methods I
is O
to O
obtain O
a O
set O
of O
samples O
z O
( O
l O
) O
( O
where O
l O
= O
1 O
, O
. O
~ O
lobo O
halnro O
mean B
, O
fu O
ji O
) O
! O
hal O
l. O
4 O
> O
( O
`` O
'.j O
.. O
o. O
we O
dwl O
rctlll'1l O
10 O
itl O
, O
~ O
pol O
' O
'' O
.ihonly O
. O
4.16 O
( O
( O
cid:12 O
) O
) O
consider O
a O
binary O
classiﬁcation O
problem O
in O
which O
each O
observation O
xn O
is O
known O
to O
belong O
to O
one O
of O
two O
classes O
, O
corresponding O
to O
t O
= O
0 O
and O
t O
= O
1 O
, O
and O
suppose O
that O
the O
procedure O
for O
collecting O
training B
data O
is O
imperfect O
, O
so O
that O
training B
points O
are O
sometimes O
mislabelled O
. O
in O
this O
section O
, O
we O
shall O
focus O
primarily O
on O
techniques O
for O
exact O
inference B
, O
and O
in O
chapter O
10 O
we O
shall O
consider O
a O
number O
of O
approximate O
inference B
algorithms O
. O
because O
this O
is O
a O
quadratic O
form O
, O
the O
distribution O
q O
( O
cid:1 O
) O
( O
w O
) O
is O
gaussian O
, O
and O
so O
we O
can O
complete O
the O
square O
in O
the O
usual O
way O
to O
identify O
the O
mean B
and O
covariance B
, O
giving O
q O
( O
cid:1 O
) O
( O
w O
) O
= O
n O
( O
w|mn O
, O
sn O
) O
488 O
10. O
approximate O
inference B
where O
( O
cid:10 O
) O
mn O
= O
βsn O
φtt O
sn O
= O
e O
[ O
α O
] O
i O
+ O
βφtφ O
( O
cid:11 O
) O
−1 O
. O
the O
quadratic O
form O
for O
the O
joint O
distribution O
can O
be O
expressed O
, O
using O
the O
par- O
titioned O
precision B
matrix I
, O
in O
the O
form O
( O
2.70 O
) O
. O
4.1.7 O
the O
perceptron B
algorithm O
another O
example O
of O
a O
linear B
discriminant I
model O
is O
the O
perceptron B
of O
rosenblatt O
( O
1962 O
) O
, O
which O
occupies O
an O
important O
place O
in O
the O
history O
of O
pattern O
recognition O
al- O
gorithms O
. O
we O
have O
seen O
in O
figure O
10.2 O
that O
if O
the O
true O
posterior O
distribution O
is O
multimodal O
, O
variational B
inference I
based O
on O
the O
minimization O
of O
kl O
( O
q O
( O
cid:5 O
) O
p O
) O
will O
tend O
to O
approximate O
the O
distribution O
in O
the O
neighbourhood O
of O
one O
of O
the O
modes O
and O
ignore O
the O
others O
. O
πk O
= O
nk O
n O
( O
4.159 O
) O
4.10 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
classiﬁcation B
model O
of O
exercise O
4.9 O
and O
now O
suppose O
that O
the O
class-conditional O
densities O
are O
given O
by O
gaussian O
distributions O
with O
a O
shared O
covari- O
ance O
matrix O
, O
so O
that O
p O
( O
φ|ck O
) O
= O
n O
( O
φ|µk O
, O
σ O
) O
. O
from O
( O
5.165 O
) O
, O
this O
is O
given O
by O
a O
= O
−∇∇ O
ln O
p O
( O
w|d O
, O
α O
, O
β O
) O
= O
αi O
+ O
βh O
( O
5.166 O
) O
where O
h O
is O
the O
hessian O
matrix O
comprising O
the O
second O
derivatives O
of O
the O
sum-of- O
squares O
error B
function I
with O
respect O
to O
the O
components O
of O
w. O
algorithms O
for O
comput- O
ing O
and O
approximating O
the O
hessian O
were O
discussed O
in O
section O
5.4. O
the O
corresponding O
gaussian O
approximation O
to O
the O
posterior O
is O
then O
given O
from O
( O
4.134 O
) O
by O
q O
( O
w|d O
) O
= O
n O
( O
w|wmap O
, O
a−1 O
) O
. O
10.3.3 O
lower B
bound I
another O
quantity O
of O
importance O
is O
the O
lower B
bound I
l O
deﬁned O
by O
l O
( O
q O
) O
= O
e O
[ O
ln O
p O
( O
w O
, O
α O
, O
t O
) O
] O
− O
e O
[ O
ln O
q O
( O
w O
, O
α O
) O
] O
−eα O
[ O
ln O
q O
( O
w O
) O
] O
w O
− O
e O
[ O
ln O
q O
( O
α O
) O
] O
. O
11.14 O
( O
( O
cid:12 O
) O
) O
verify O
that O
the O
over-relaxation B
update O
( O
11.50 O
) O
, O
in O
which O
zi O
has O
mean B
µi O
and O
( O
cid:4 O
) O
i O
with O
variance B
σi O
, O
and O
where O
ν O
has O
zero O
mean B
and O
unit O
variance B
, O
gives O
a O
value O
z O
mean B
µi O
and O
variance B
σ2 O
i O
. O
2 O
( O
cid:1 O
) O
1. O
this O
leads O
to O
a O
uniform O
next O
we O
discard O
each O
pair O
unless O
it O
satisﬁes O
z2 O
distribution O
of O
points O
inside O
the O
unit O
circle O
with O
p O
( O
z1 O
, O
z2 O
) O
= O
1/π O
, O
as O
illustrated O
in O
figure O
11.3. O
then O
, O
for O
each O
pair O
z1 O
, O
z2 O
we O
evaluate O
the O
quantities O
1 O
+ O
z2 O
figure O
11.3 O
the O
box-muller O
method O
for O
generating O
gaussian O
dis- O
tributed O
random O
numbers O
starts O
by O
generating O
samples O
from O
a O
uniform B
distribution I
inside O
the O
unit O
circle O
. O
one O
message O
has O
now O
passed O
in O
each O
direction O
across O
each O
link B
, O
and O
we O
can O
now O
evaluate O
the O
marginals O
. O
the O
graph O
for O
the O
hidden O
markov O
model O
, O
shown O
in O
figure O
13.5 O
, O
is O
a O
tree B
, O
and O
so O
we O
know O
that O
the O
posterior O
distribution O
of O
the O
latent O
variables O
can O
be O
obtained O
efﬁciently O
using O
a O
two- O
stage O
message B
passing I
algorithm O
. O
unfortunately O
, O
the O
algorithm O
must O
work O
with O
the O
joint O
distributions O
within O
each O
node B
( O
each O
of O
which O
corresponds O
to O
a O
clique B
of O
the O
triangulated B
graph I
) O
and O
so O
the O
compu- O
tational O
cost O
of O
the O
algorithm O
is O
determined O
by O
the O
number O
of O
variables O
in O
the O
largest O
8.4. O
inference B
in O
graphical O
models O
417 O
clique B
and O
will O
grow O
exponentially O
with O
this O
number O
in O
the O
case O
of O
discrete O
variables O
. O
if O
we O
present O
to O
the O
ﬁlter O
the O
set O
of O
all O
possible O
distributions O
p O
( O
x O
) O
over O
the O
set O
of O
variables O
x O
, O
then O
the O
subset O
of O
distributions O
that O
are O
passed O
by O
the O
ﬁlter O
will O
be O
denoted O
df O
, O
for O
directed O
factorization B
. O
, O
e O
[ O
xd O
] O
) O
t O
by O
starting O
at O
the O
lowest O
numbered O
node B
and O
working O
recursively O
through O
the O
graph O
( O
here O
we O
again O
assume O
that O
the O
nodes O
are O
numbered O
such O
that O
each O
node B
has O
a O
higher O
number O
than O
its O
parents O
) O
. O
it O
should O
be O
emphasized O
that O
we O
did O
not O
assume O
these O
speciﬁc O
functional B
forms O
for O
the O
optimal O
distributions O
qµ O
( O
µ O
) O
and O
qτ O
( O
τ O
) O
. O
to O
generate O
vector- O
valued O
variables O
having O
a O
multivariate O
gaussian O
distribution O
with O
mean B
µ O
and O
co- O
variance B
σ O
, O
we O
can O
make O
use O
of O
the O
cholesky O
decomposition O
, O
which O
takes O
the O
form O
σ O
= O
llt O
( O
press O
et O
al. O
, O
1992 O
) O
. O
for O
instance O
, O
11001110 O
decodes O
uniquely O
into O
the O
state O
sequence O
c O
, O
a O
, O
d. O
this O
relation O
between O
entropy B
and O
shortest O
coding O
length O
is O
a O
general O
one O
. O
2.40 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
d-dimensional O
gaussian O
random O
variable O
x O
with O
distribu- O
tion O
n O
( O
x|µ O
, O
σ O
) O
in O
which O
the O
covariance B
σ O
is O
known O
and O
for O
which O
we O
wish O
to O
infer O
the O
mean B
µ O
from O
a O
set O
of O
observations O
x O
= O
{ O
x1 O
, O
. O
( O
3.12 O
) O
having O
written O
down O
the O
likelihood B
function I
, O
we O
can O
use O
maximum B
likelihood I
to O
determine O
w O
and O
β. O
consider O
ﬁrst O
the O
maximization O
with O
respect O
to O
w. O
as O
observed O
already O
in O
section O
1.2.5 O
, O
we O
see O
that O
maximization O
of O
the O
likelihood B
function I
under O
a O
conditional B
gaussian O
noise O
distribution O
for O
a O
linear O
model O
is O
equivalent O
to O
minimizing O
a O
sum-of-squares B
error I
function O
given O
by O
ed O
( O
w O
) O
. O
430 O
9. O
mixture B
models O
and O
em O
the O
image O
segmentation O
problem O
discussed O
above O
also O
provides O
an O
illustration O
of O
the O
use O
of O
clustering B
for O
data B
compression I
. O
in O
the O
limit O
of O
an O
inﬁnite O
number O
of O
data O
points O
, O
the O
3.3. O
bayesian O
linear B
regression I
155 O
figure O
3.7 O
illustration O
of O
sequential O
bayesian O
learning B
for O
a O
simple O
linear O
model O
of O
the O
form O
y O
( O
x O
, O
w O
) O
= O
w0 O
+ O
w1x O
. O
show O
that O
the O
conditional B
mean O
for O
the O
mixture O
of O
linear O
regression B
models O
discussed O
in O
section O
14.5.1 O
is O
given O
by O
a O
linear O
combination O
of O
the O
means O
of O
each O
component O
dis- O
tribution O
. O
from O
the O
recursion O
relations O
( O
8.15 O
) O
and O
( O
8.16 O
) O
, O
we O
see O
that O
the O
mean B
of O
p O
( O
x O
) O
is O
given O
by O
( O
b1 O
, O
. O
we O
therefore O
choose O
a O
prior B
, O
called O
the O
beta B
distribution I
, O
given O
by O
exercise O
2.5 O
beta O
( O
µ|a O
, O
b O
) O
= O
γ O
( O
a O
+ O
b O
) O
γ O
( O
a O
) O
γ O
( O
b O
) O
µa−1 O
( O
1 O
− O
µ O
) O
b−1 O
( O
2.13 O
) O
where O
γ O
( O
x O
) O
is O
the O
gamma B
function I
deﬁned O
by O
( O
1.141 O
) O
, O
and O
the O
coefﬁcient O
in O
( O
2.13 O
) O
ensures O
that O
the O
beta B
distribution I
is O
normalized O
, O
so O
that O
beta O
( O
µ|a O
, O
b O
) O
dµ O
= O
1 O
. O
these O
include O
variational B
message O
passing O
, O
loopy B
belief I
propagation I
, O
and O
expectation B
propagation I
, O
as O
well O
as O
a O
range O
of O
other O
algorithms O
, O
which O
we O
do O
not O
have O
space O
to O
discuss O
here O
, O
such O
as O
tree-reweighted O
message O
pass- O
ing O
( O
wainwright O
et O
al. O
, O
2005 O
) O
, O
fractional B
belief I
propagation I
( O
wiegerinck O
and O
heskes O
, O
2003 O
) O
, O
and O
power O
ep O
( O
minka O
, O
2004 O
) O
. O
in O
the O
case O
of O
a O
target O
variable O
t O
∈ O
{ O
−1 O
, O
1 O
} O
, O
we O
have O
seen O
that O
the O
error B
function I
is O
given O
by O
ln O
( O
1 O
+ O
exp O
( O
−yt O
) O
) O
. O
, O
n O
, O
can O
n O
( O
θ|· O
, O
· O
) O
does O
not O
imply O
that O
the O
right-hand O
side O
is O
a O
well-deﬁned O
gaussian O
density B
( O
in O
fact O
, O
as O
we O
shall O
see O
, O
the O
variance B
parameter O
vn O
can O
be O
negative O
) O
but O
is O
simply O
a O
be O
initialized O
to O
unity O
, O
corresponding O
to O
sn O
= O
( O
2πvn O
) O
d/2 O
, O
vn O
→ O
∞ O
and O
mn O
= O
0 O
, O
where O
d O
is O
the O
dimensionality O
of O
x O
and O
hence O
of O
θ. O
the O
initial O
q O
( O
θ O
) O
, O
deﬁned O
by O
( O
10.191 O
) O
, O
is O
therefore O
equal O
to O
the O
prior B
. O
( O
3.104 O
) O
find O
an O
expression O
for O
the O
solution O
w O
( O
cid:1 O
) O
that O
minimizes O
this O
error B
function I
. O
the O
downside O
of O
having O
such O
a O
flexible O
model O
is O
that O
the O
marginalization O
over O
the O
latent O
variables O
, O
required O
in O
order O
to O
obtain O
the O
likelihood B
function I
, O
is O
no O
longer O
analytically O
tractable O
. O
in O
this O
chapter O
we O
shall O
look O
at O
kernel-based O
algorithms O
that O
have O
sparse O
solutions O
, O
so O
that O
predictions O
for O
new O
inputs O
depend O
only O
on O
the O
kernel B
function I
evaluated O
at O
a O
subset O
of O
the O
training B
data O
points O
. O
scoring O
rules O
and O
the O
in- O
evitability O
of O
probability B
. O
the O
following O
general O
procedure O
for O
solving O
such O
problems O
was O
given O
by O
96 O
2. O
probability B
distributions O
( O
cid:8 O
) O
( O
cid:9 O
) O
robbins O
and O
monro O
( O
1951 O
) O
. O
finally O
, O
consider O
the O
maximum B
likelihood I
solution O
for O
the O
shared O
covariance O
matrix O
σ. O
picking O
out O
the O
terms O
in O
the O
log O
likelihood O
function O
that O
depend O
on O
σ O
, O
we O
have O
n O
( O
cid:2 O
) O
1 O
n1 O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
n=1 O
−1 O
( O
xn O
− O
µ1 O
) O
−1 O
2 O
tn O
ln|σ| O
− O
1 O
2 O
−1 O
2 O
= O
− O
n O
2 O
n=1 O
( O
1 O
− O
tn O
) O
ln|σ| O
− O
1 O
2 O
−1s O
ln|σ| O
− O
n O
( O
cid:26 O
) O
σ O
2 O
tr O
tn O
( O
xn O
− O
µ1 O
) O
tς O
n O
( O
cid:2 O
) O
( O
cid:27 O
) O
n=1 O
( O
1 O
− O
tn O
) O
( O
xn O
− O
µ2 O
) O
tς O
−1 O
( O
xn O
− O
µ2 O
) O
( O
4.77 O
) O
202 O
4. O
linear O
models O
for O
classification O
where O
we O
have O
deﬁned O
s1 O
= O
s O
= O
n1 O
n O
1 O
n1 O
1 O
n2 O
s2 O
= O
( O
cid:2 O
) O
( O
cid:2 O
) O
n∈c1 O
n∈c2 O
s2 O
s1 O
+ O
n2 O
n O
( O
xn O
− O
µ1 O
) O
( O
xn O
− O
µ1 O
) O
t O
( O
xn O
− O
µ2 O
) O
( O
xn O
− O
µ2 O
) O
t. O
( O
4.78 O
) O
( O
4.79 O
) O
( O
4.80 O
) O
using O
the O
standard O
result O
for O
the O
maximum B
likelihood I
solution O
for O
a O
gaussian O
distri- O
bution O
, O
we O
see O
that O
σ O
= O
s O
, O
which O
represents O
a O
weighted O
average O
of O
the O
covariance B
matrices O
associated O
with O
each O
of O
the O
two O
classes O
separately O
. O
unfortunately O
, O
the O
lower B
bound I
derived O
here O
for O
the O
logistic B
sigmoid I
does O
not O
directly O
extend O
to O
the O
softmax O
. O
, O
xn O
, O
then O
, O
because O
we O
have O
represented O
the O
marginal B
distribution O
in O
the O
form O
p O
( O
x O
) O
= O
z O
p O
( O
x O
, O
z O
) O
, O
it O
follows O
that O
for O
every O
observed O
data O
point O
xn O
there O
is O
a O
corresponding O
latent B
variable I
zn O
. O
nevertheless O
, O
even O
if O
this O
assumption O
is O
not O
precisely O
satisﬁed O
, O
the O
model O
may O
still O
give O
good O
classiﬁcation B
performance O
in O
practice O
because O
the O
decision O
boundaries O
can O
be O
insensitive O
to O
some O
of O
the O
details O
in O
the O
class-conditional O
densities O
, O
as O
illustrated O
in O
figure O
1.27. O
we O
have O
seen O
that O
a O
particular O
directed B
graph O
represents O
a O
speciﬁc O
decomposition O
of O
a O
joint O
probability B
distribution O
into O
a O
product O
of O
conditional B
probabilities O
. O
data O
sets O
whose O
classes O
can O
be O
separated O
exactly O
by O
linear O
decision O
surfaces O
are O
said O
to O
be O
linearly B
separable I
. O
( O
7.50 O
) O
to O
obtain O
sparse O
solutions O
, O
the O
quadratic O
error O
function O
is O
replaced O
by O
an O
-insensitive B
error I
function I
( O
vapnik O
, O
1995 O
) O
, O
which O
gives O
zero O
error B
if O
the O
absolute O
difference O
be- O
tween O
the O
prediction O
y O
( O
x O
) O
and O
the O
target O
t O
is O
less O
than O
 O
where O
 O
> O
0. O
a O
simple O
example O
of O
an O
-insensitive B
error I
function I
, O
having O
a O
linear O
cost O
associated O
with O
errors O
outside O
the O
insensitive O
region O
, O
is O
given O
by O
e O
( O
y O
( O
x O
) O
− O
t O
) O
= O
0 O
, O
|y O
( O
x O
) O
− O
t| O
− O
 O
, O
otherwise O
if O
|y O
( O
x O
) O
− O
t| O
< O
 O
; O
n O
( O
cid:2 O
) O
n=1 O
1 O
2 O
( O
cid:12 O
) O
n O
( O
cid:2 O
) O
c O
n=1 O
and O
is O
illustrated O
in O
figure O
7.6. O
we O
therefore O
minimize O
a O
regularized O
error O
function O
given O
by O
e O
( O
y O
( O
xn O
) O
− O
tn O
) O
+ O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
2 O
1 O
2 O
where O
y O
( O
x O
) O
is O
given O
by O
( O
7.1 O
) O
. O
the O
distribution O
( O
2.26 O
) O
can O
be O
and O
regarded O
as O
a O
generalization B
of O
the O
bernoulli O
distribution O
to O
more O
than O
two O
outcomes O
. O
section O
13.2 O
416 O
8. O
graphical O
models O
8.4.6 O
exact O
inference O
in O
general O
graphs O
the O
sum-product O
and O
max-sum O
algorithms O
provide O
efﬁcient O
and O
exact O
solutions O
to O
inference B
problems O
in O
tree-structured O
graphs O
. O
as O
we O
have O
emphasized O
already O
, O
in O
the O
application O
of O
variational B
methods O
it O
is O
useful O
to O
be O
able O
to O
evaluate O
the O
lower B
bound I
l O
( O
ξ O
) O
given O
by O
( O
10.159 O
) O
. O
at O
this O
point O
, O
it O
is O
useful O
to O
have O
a O
geometrical O
picture O
of O
the O
error B
function I
, O
which O
we O
can O
view O
as O
a O
surface O
sitting O
over O
weight O
space O
as O
shown O
in O
figure O
5.5. O
first O
note O
that O
if O
we O
make O
a O
small O
step O
in O
weight O
space O
from O
w O
to O
w+ O
δw O
then O
the O
change O
in O
the O
error B
function I
is O
δe O
( O
cid:7 O
) O
δwt∇e O
( O
w O
) O
, O
where O
the O
vector O
∇e O
( O
w O
) O
points O
in O
the O
direction O
of O
greatest O
rate O
of O
increase O
of O
the O
error B
function I
. O
a O
gen- O
eral O
technique O
for O
ﬁnding O
maximum B
likelihood I
estimators O
in O
latent B
variable I
models O
is O
the O
expectation-maximization O
( O
em O
) O
algorithm O
. O
( O
2.265 O
) O
( O
cid:6 O
) O
∞ O
from O
the O
deﬁnition O
( O
1.141 O
) O
of O
the O
gamma B
function I
, O
we O
have O
γ O
( O
a O
) O
γ O
( O
b O
) O
= O
exp O
( O
−x O
) O
xa−1 O
dx O
exp O
( O
−y O
) O
yb−1 O
dy O
. O
each O
leaf O
of O
the O
tree B
is O
then O
associated O
with O
a O
speciﬁc O
diagnosis O
. O
if O
this O
mixture B
has O
k O
components O
, O
then O
the O
forward O
recursion O
equations O
( O
13.85 O
) O
will O
lead O
to O
a O
mixture O
of O
k O
gaussians O
over O
each O
hidden B
variable I
zn O
, O
and O
so O
the O
model O
is O
again O
tractable O
. O
this O
concept O
will O
play O
an O
important O
role O
when O
we O
consider O
support B
vector I
machines O
in O
the O
next O
chapter O
. O
the O
simplest O
form O
of O
linear B
regression I
models O
are O
also O
linear O
functions O
of O
the O
input O
variables O
. O
zi O
wji O
δj O
zj O
wkj O
δk O
δ1 O
provided O
we O
are O
using O
the O
canonical O
link O
as O
the O
output-unit O
activation B
function I
. O
we O
can O
obtain O
some O
quantitative O
insight O
into O
the O
dependence O
of O
the O
generalization B
performance O
on O
m O
by O
considering O
a O
separate O
test B
set I
comprising O
100 O
data O
points O
generated O
using O
exactly O
the O
same O
procedure O
used O
to O
generate O
the O
training B
set I
points O
but O
with O
new O
choices O
for O
the O
random O
noise O
values O
included O
in O
the O
target O
values O
. O
the O
decision B
surface I
, O
shown O
in O
red O
, O
is O
perpen- O
dicular O
to O
w O
, O
and O
its O
displacement O
from O
the O
origin O
is O
controlled O
by O
the O
bias B
parameter I
w0 O
. O
we O
see O
that O
the O
discrete O
and O
continuous O
forms O
of O
the O
entropy B
differ O
by O
a O
quantity O
ln O
∆ O
, O
which O
diverges O
in O
the O
limit O
∆ O
→ O
0. O
this O
reﬂects O
the O
fact O
that O
to O
specify O
a O
continuous O
variable O
very O
precisely O
requires O
a O
large O
number O
of O
bits B
. O
this O
approach O
may O
be O
impractical O
, O
however O
, O
if O
the O
number O
of O
training B
examples O
is O
limited O
, O
or O
if O
there O
are O
several O
invariants O
( O
because O
the O
number O
of O
combinations O
of O
transformations O
grows O
exponentially O
with O
the O
number O
of O
such O
transformations O
) O
. O
w2 O
u2 O
wml O
wmap O
u1 O
w1 O
3.5.3 O
effective B
number I
of I
parameters I
the O
result O
( O
3.92 O
) O
has O
an O
elegant O
interpretation O
( O
mackay O
, O
1992a O
) O
, O
which O
provides O
insight O
into O
the O
bayesian O
solution O
for O
α. O
to O
see O
this O
, O
consider O
the O
contours O
of O
the O
like- O
lihood O
function O
and O
the O
prior B
as O
illustrated O
in O
figure O
3.15. O
here O
we O
have O
implicitly O
transformed O
to O
a O
rotated O
set O
of O
axes O
in O
parameter O
space O
aligned O
with O
the O
eigenvec- O
tors O
ui O
deﬁned O
in O
( O
3.87 O
) O
. O
we O
now O
consider O
second B
order I
moments O
of O
the O
gaussian O
. O
here O
we O
consider O
a O
linear B
regression I
model O
whose O
parameters O
are O
determined O
by O
minimizing O
a O
regularized O
sum-of-squares O
error B
function I
given O
by O
j O
( O
w O
) O
= O
1 O
2 O
wtφ O
( O
xn O
) O
− O
tn O
wtw O
( O
6.2 O
) O
where O
λ O
( O
cid:2 O
) O
0. O
if O
we O
set O
the O
gradient O
of O
j O
( O
w O
) O
with O
respect O
to O
w O
equal O
to O
zero O
, O
we O
see O
that O
the O
solution O
for O
w O
takes O
the O
form O
of O
a O
linear O
combination O
of O
the O
vectors O
φ O
( O
xn O
) O
, O
with O
coefﬁcients O
that O
are O
functions O
of O
w O
, O
of O
the O
form O
w O
= O
− O
1 O
λ O
wtφ O
( O
xn O
) O
− O
tn O
n=1 O
n=1 O
φ O
( O
xn O
) O
= O
anφ O
( O
xn O
) O
= O
φta O
( O
6.3 O
) O
where O
φ O
is O
the O
design B
matrix I
, O
whose O
nth O
row O
is O
given O
by O
φ O
( O
xn O
) O
t. O
here O
the O
vector O
a O
= O
( O
a1 O
, O
. O
) O
, O
advances O
in O
large B
margin I
classiﬁers O
, O
pp O
. O
our O
goal O
is O
to O
extend O
this O
model O
by O
making O
the O
basis O
functions O
φj O
( O
x O
) O
depend O
on O
parameters O
and O
then O
to O
allow O
these O
parameters O
to O
be O
adjusted O
, O
along O
with O
the O
coefﬁcients O
{ O
wj O
} O
, O
during O
training B
. O
12.2.4 O
factor B
analysis I
. O
this O
is O
intrinsically O
a O
difﬁcult O
problem O
as O
we O
have O
to O
generalize O
from O
a O
ﬁnite O
data O
our O
goal O
is O
to O
exploit O
this O
training B
set I
in O
order O
to O
make O
predictions O
of O
the O
value O
in O
section O
1.2 O
, O
provides O
a O
framework O
for O
expressing O
such O
uncertainty O
in O
a O
precise O
and O
quantitative O
manner O
, O
and O
decision B
theory I
, O
discussed O
in O
section O
1.5 O
, O
allows O
us O
to O
exploit O
this O
probabilistic O
representation O
in O
order O
to O
make O
predictions O
that O
are O
optimal O
according O
to O
appropriate O
criteria O
. O
= O
e O
[ O
z1 O
] O
= O
e O
[ O
z1zt O
µnew O
vnew O
0 O
0 O
using O
( O
13.75 O
) O
giving O
( O
cid:31 O
) O
q O
( O
θ O
, O
θold O
) O
= O
− O
n O
− O
1 O
−ez|θold O
n O
( O
cid:2 O
) O
2 O
1 O
2 O
n=2 O
ln|γ| O
( O
zn O
− O
azn−1 O
) O
tγ O
−1 O
( O
zn O
− O
azn−1 O
) O
+ O
const O
( O
13.112 O
) O
in O
which O
the O
constant O
comprises O
terms O
that O
are O
independent B
of O
a O
and O
γ. O
maximizing O
with O
respect O
to O
these O
parameters O
then O
gives O
anew O
= O
e O
znzt O
n−1 O
zn−1zt O
n−1 O
( O
cid:22 O
) O
n O
( O
cid:2 O
) O
γnew O
= O
n=2 O
( O
cid:8 O
) O
( O
cid:9 O
) O
1 O
n O
− O
1 O
znzt O
n−1 O
−e O
( O
cid:8 O
) O
n O
( O
cid:2 O
) O
( O
cid:26 O
) O
e O
( O
cid:9 O
) O
( O
cid:23 O
) O
( O
cid:22 O
) O
n O
( O
cid:2 O
) O
( O
cid:8 O
) O
( O
cid:9 O
) O
− O
anew O
( O
cid:8 O
) O
n=2 O
e O
znzt O
n O
( O
cid:8 O
) O
n=2 O
anew O
+ O
anew O
( O
cid:9 O
) O
( O
cid:23 O
) O
−1 O
( O
cid:9 O
) O
( O
cid:20 O
) O
( O
cid:8 O
) O
( O
cid:9 O
) O
e O
zn−1zt O
n O
e O
zn−1zt O
n−1 O
( O
anew O
) O
t O
( O
13.113 O
) O
. O
then O
we O
start O
a O
set O
of O
messages O
propagating O
inwards O
from O
the O
leaves O
of O
the O
tree B
towards O
the O
root O
, O
with O
each O
node B
sending O
its O
message O
towards O
the O
root O
once O
it O
has O
received O
all O
incoming O
messages O
from O
its O
other O
neighbours O
. O
for O
nodes O
on O
the O
directed B
graph O
having O
just O
one O
parent O
, O
this O
is O
achieved O
simply O
by O
replacing O
the O
directed B
link O
with O
an O
undirected B
link O
. O
let O
us O
consider O
the O
inference B
problem O
of O
ﬁnding O
the O
marginal B
distribution O
p O
( O
xn O
) O
for O
a O
speciﬁc O
node B
xn O
that O
is O
part O
way O
along O
the O
chain O
. O
for O
the O
variational B
mixture O
of O
gaussians O
, O
the O
lower B
bound I
( O
10.3 O
) O
is O
given O
by O
l O
= O
p O
( O
x O
, O
z O
, O
π O
, O
µ O
, O
λ O
) O
q O
( O
z O
, O
π O
, O
µ O
, O
λ O
) O
ln O
q O
( O
z O
, O
π O
, O
µ O
, O
λ O
) O
= O
e O
[ O
ln O
p O
( O
x O
, O
z O
, O
π O
, O
µ O
, O
λ O
) O
] O
− O
e O
[ O
ln O
q O
( O
z O
, O
π O
, O
µ O
, O
λ O
) O
] O
= O
e O
[ O
ln O
p O
( O
x|z O
, O
µ O
, O
λ O
) O
] O
+ O
e O
[ O
ln O
p O
( O
z|π O
) O
] O
+ O
e O
[ O
ln O
p O
( O
π O
) O
] O
+ O
e O
[ O
ln O
p O
( O
µ O
, O
λ O
) O
] O
dπ O
dµ O
dλ O
z O
−e O
[ O
ln O
q O
( O
z O
) O
] O
− O
e O
[ O
ln O
q O
( O
π O
) O
] O
− O
e O
[ O
ln O
q O
( O
µ O
, O
λ O
) O
] O
( O
10.70 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:2 O
) O
( O
cid:12 O
) O
( O
cid:13 O
) O
where O
, O
to O
keep O
the O
notation O
uncluttered O
, O
we O
have O
omitted O
the O
( O
cid:12 O
) O
superscript O
on O
the O
q O
distributions O
, O
along O
with O
the O
subscripts O
on O
the O
expectation B
operators O
because O
each O
expectation B
is O
taken O
with O
respect O
to O
all O
of O
the O
random O
variables O
in O
its O
argument O
. O
8.4.4 O
the O
sum-product B
algorithm I
we O
shall O
now O
make O
use O
of O
the O
factor B
graph I
framework O
to O
derive O
a O
powerful O
class O
of O
efﬁcient O
, O
exact O
inference O
algorithms O
that O
are O
applicable O
to O
tree-structured O
graphs O
. O
it O
can O
be O
applied O
to O
any O
differentiable O
error B
function I
that O
can O
be O
expressed O
as O
a O
function O
of O
the O
network O
outputs O
and O
to O
networks O
having O
arbitrary O
differentiable O
activation O
func- O
tions O
. O
1.35 O
( O
( O
cid:1 O
) O
) O
www O
use O
the O
results O
( O
1.106 O
) O
and O
( O
1.107 O
) O
to O
show O
that O
the O
entropy B
of O
the O
univariate O
gaussian O
( O
1.109 O
) O
is O
given O
by O
( O
1.110 O
) O
. O
, O
n O
, O
together O
with O
a O
corresponding O
set O
of O
5.2. O
network O
training B
233 O
n O
( O
cid:2 O
) O
n=1 O
target O
vectors O
{ O
tn O
} O
, O
we O
minimize O
the O
error B
function I
e O
( O
w O
) O
= O
1 O
2 O
( O
cid:5 O
) O
y O
( O
xn O
, O
w O
) O
− O
tn O
( O
cid:5 O
) O
2 O
. O
( O
14.3 O
) O
( O
cid:2 O
) O
z O
section O
9.2 O
14.2. O
committees O
655 O
in O
the O
case O
of O
our O
gaussian O
mixture B
example O
, O
this O
leads O
to O
a O
distribution O
of O
the O
form O
p O
( O
x O
) O
= O
πkn O
( O
x|µk O
, O
σk O
) O
( O
14.4 O
) O
k O
( O
cid:2 O
) O
k=1 O
h O
( O
cid:2 O
) O
with O
the O
usual O
interpretation O
of O
the O
symbols O
. O
in O
the O
robotics O
example O
, O
the O
kinematics O
is O
deﬁned O
by O
geometrical O
equations O
, O
and O
the O
multimodality B
is O
readily O
apparent O
. O
( O
b.32 O
) O
( O
b.33 O
) O
( O
b.34 O
) O
( O
b.35 O
) O
( O
b.36 O
) O
the O
inverse B
of O
the O
variance B
τ O
= O
1/σ2 O
is O
called O
the O
precision O
, O
and O
the O
square O
root O
of O
the O
variance B
σ O
is O
called O
the O
standard B
deviation I
. O
for O
instance O
, O
the O
parzen O
probability B
density O
model O
comprised O
a O
linear O
combination O
of O
‘ O
kernel O
’ O
functions O
each O
one O
centred O
on O
one O
of O
the O
training B
data O
points O
. O
this O
allows O
us O
to O
make O
a O
single O
training B
run O
in O
which O
we O
start O
with O
a O
relatively O
large O
initial O
value O
of O
k O
, O
and O
allow O
surplus O
components O
to O
be O
pruned O
out O
of O
the O
model O
. O
the O
square O
root O
of O
the O
variance B
, O
given O
by O
σ O
, O
is O
called O
the O
standard B
deviation I
, O
and O
the O
reciprocal O
of O
the O
variance B
, O
written O
as O
β O
= O
1/σ2 O
, O
is O
called O
the O
precision O
. O
13.9 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
use O
the O
d-separation B
criterion O
to O
verify O
that O
the O
conditional B
indepen- O
dence O
properties O
( O
13.24 O
) O
– O
( O
13.31 O
) O
are O
satisﬁed O
by O
the O
joint O
distribution O
for O
the O
hidden O
markov O
model O
deﬁned O
by O
( O
13.6 O
) O
. O
note O
that O
for O
both O
viewpoints O
, O
the O
matrix O
inversion O
must O
be O
performed O
once O
for O
the O
given O
training B
set I
. O
note O
, O
however O
, O
that O
this O
result O
only O
holds O
on O
average O
, O
and O
that O
for O
a O
particular O
observed O
data O
set O
it O
is O
possible O
for O
the O
posterior O
variance O
to O
be O
larger O
than O
the O
prior B
variance O
. O
this O
is O
conﬁrmed O
by O
noting O
that O
the O
key O
conditional B
independence I
property O
( O
13.5 O
) O
is O
not O
satisﬁed O
for O
the O
individual O
markov O
chains O
in O
the O
factorial B
hmm O
model O
, O
as O
is O
shown O
using O
d-separation B
in O
figure O
13.20. O
now O
suppose O
that O
there O
are O
m O
chains O
of O
hidden O
nodes O
and O
for O
simplicity O
suppose O
that O
all O
latent O
variables O
have O
the O
same O
number O
k O
of O
states O
. O
( O
2.190 O
) O
112 O
2. O
probability B
distributions O
1 O
( O
a O
) O
1 O
( O
b O
) O
0.5 O
0 O
0.5 O
0.3 O
0.2 O
0.5 O
0 O
0 O
0.5 O
1 O
0 O
0.5 O
1 O
figure O
2.23 O
illustration O
of O
a O
mixture O
of O
3 O
gaussians O
in O
a O
two-dimensional O
space O
. O
because O
cn O
( O
and O
hence O
its O
inverse B
) O
is O
positive B
deﬁnite I
by O
construction O
, O
and O
because O
the O
sum O
of O
two O
positive B
deﬁnite I
matrices O
is O
also O
positive B
deﬁnite I
, O
we O
see O
that O
the O
hessian O
matrix O
a O
= O
−∇∇ψ O
( O
an O
) O
is O
positive B
deﬁnite I
and O
so O
the O
posterior O
distribution O
p O
( O
an|tn O
) O
is O
log O
convex O
and O
therefore O
has O
a O
single O
mode O
that O
is O
the O
global O
section O
4.3.3 O
exercise O
6.24 O
6.4. O
gaussian O
processes O
317 O
maximum O
. O
the O
solution O
is O
obtained O
by O
exploring O
all O
possible O
input O
functions O
to O
ﬁnd O
the O
one O
that O
maximizes O
, O
or O
minimizes O
, O
the O
functional B
. O
point O
wa O
is O
a O
local B
minimum I
and O
wb O
is O
the O
global B
minimum I
. O
factor O
graphs O
are O
said O
to O
be O
bipartite B
because O
they O
consist O
of O
two O
distinct O
kinds O
of O
nodes O
, O
and O
all O
links O
go O
between O
nodes O
of O
opposite O
type O
. O
the O
probability B
problem O
of O
pattern O
recognition O
learning B
and O
the O
method O
of O
potential O
functions O
. O
exercise O
3.7 O
exercise O
3.8 O
3.3. O
bayesian O
linear B
regression I
153 O
next O
we O
compute O
the O
posterior O
distribution O
, O
which O
is O
proportional O
to O
the O
product O
of O
the O
likelihood B
function I
and O
the O
prior B
. O
amongst O
all O
distributions O
q O
( O
z O
) O
having O
the O
form O
( O
10.5 O
) O
, O
we O
now O
seek O
that O
distri- O
bution O
for O
which O
the O
lower B
bound I
l O
( O
q O
) O
is O
largest O
. O
in O
order O
to O
learn O
such O
a O
model O
from O
a O
training B
set I
, O
we O
have O
to O
determine O
the O
structure O
of O
the O
tree B
, O
including O
which O
input O
variable O
is O
chosen O
at O
each O
node B
to O
form O
the O
split O
criterion O
as O
well O
as O
the O
value O
of O
the O
threshold B
parameter I
θi O
for O
the O
split O
. O
1.37 O
( O
( O
cid:1 O
) O
) O
using O
the O
deﬁnition O
( O
1.111 O
) O
together O
with O
the O
product B
rule I
of I
probability I
, O
prove O
the O
result O
( O
1.112 O
) O
. O
marginalization O
of O
the O
logistic B
sigmoid I
model O
under O
a O
gaussian O
approximation O
to O
the O
posterior O
distribution O
will O
be O
illustrated O
in O
the O
context O
of O
variational B
inference I
in O
figure O
10.13 O
. O
now O
suppose O
we O
condition O
on O
node B
c O
, O
as O
shown O
in O
figure O
8.18. O
using O
bayes O
’ O
theorem O
, O
together O
with O
( O
8.26 O
) O
, O
we O
obtain O
p O
( O
a O
, O
b|c O
) O
= O
p O
( O
a O
, O
b O
, O
c O
) O
p O
( O
c O
) O
= O
p O
( O
a O
) O
p O
( O
c|a O
) O
p O
( O
b|c O
) O
p O
( O
c O
) O
= O
p O
( O
a|c O
) O
p O
( O
b|c O
) O
and O
so O
again O
we O
obtain O
the O
conditional B
independence I
property O
a O
⊥⊥ O
b O
| O
c. O
as O
before O
, O
we O
can O
interpret O
these O
results O
graphically O
. O
continuous O
latent O
variables O
will O
form O
the O
subject O
of O
chapter O
12. O
as O
well O
as O
providing O
a O
framework O
for O
building O
more O
complex O
probability B
dis- O
tributions O
, O
mixture B
models O
can O
also O
be O
used O
to O
cluster O
data O
. O
similarly O
, O
we O
can O
deﬁne O
a O
functional B
as O
a O
mapping O
that O
takes O
a O
function O
as O
the O
input O
and O
that O
returns O
the O
value O
of O
the O
functional B
as O
the O
output O
. O
a O
key O
quantity O
in O
pac O
learning B
is O
the O
vapnik-chervonenkis O
dimension O
, O
or O
vc O
dimension O
, O
which O
provides O
a O
measure O
of O
the O
complexity O
of O
a O
space O
of O
functions O
, O
and O
which O
allows O
the O
pac O
framework O
to O
be O
extended B
to O
spaces O
containing O
an O
inﬁnite O
number O
of O
functions O
. O
the O
error B
involved O
in O
the O
discretized O
approximation O
to O
the O
continuous O
time O
dynamics O
will O
go O
to O
zero O
, O
assuming O
a O
smooth O
function O
e O
( O
z O
) O
, O
in O
the O
limit O
 O
→ O
0. O
however O
, O
for O
a O
nonzero O
 O
as O
used O
in O
practice O
, O
some O
residual O
error B
will O
remain O
. O
focussing O
just O
on O
the O
random O
variables O
for O
the O
moment O
, O
we O
see O
that O
the O
joint O
distribution O
is O
given O
by O
the O
product O
of O
the O
prior B
p O
( O
w O
) O
and O
n O
conditional B
distributions O
p O
( O
tn|w O
) O
for O
n O
= O
1 O
, O
. O
then O
in O
the O
ﬁrst O
phase O
, O
we O
propagate O
messages O
from O
the O
leaf O
node B
x1 O
to O
the O
root B
node I
using O
µxn→fn O
, O
n+1 O
( O
xn O
) O
= O
µfn−1 O
, O
n→xn O
( O
xn O
) O
µfn−1 O
, O
n→xn O
( O
xn O
) O
= O
max O
xn−1 O
ln O
fn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
+ O
µxn−1→f O
n−1 O
, O
n O
( O
xn O
) O
( O
cid:8 O
) O
( O
cid:9 O
) O
which O
follow O
from O
applying O
( O
8.94 O
) O
and O
( O
8.93 O
) O
to O
this O
particular O
graph O
. O
for O
this O
reason O
, O
it O
is O
com- O
mon O
practice O
to O
grow O
a O
large O
tree O
, O
using O
a O
stopping O
criterion O
based O
on O
the O
number O
of O
data O
points O
associated O
with O
the O
leaf O
nodes O
, O
and O
then O
prune O
back O
the O
resulting O
tree B
. O
to O
do O
this O
, O
we O
consider O
a O
small O
sphere O
centred O
on O
the O
point O
x O
at O
which O
we O
wish O
to O
estimate O
the O
2.5. O
nonparametric B
methods I
125 O
figure O
2.26 O
illustration O
of O
k-nearest-neighbour O
den- O
sity O
estimation O
using O
the O
same O
data O
set O
as O
in O
figures O
2.25 O
and O
2.24. O
we O
see O
that O
the O
parameter O
k O
governs O
the O
degree O
of O
smoothing O
, O
so O
that O
a O
small O
value O
of O
k O
leads O
to O
a O
very O
noisy O
density B
model O
( O
top O
panel O
) O
, O
whereas O
a O
large O
value O
( O
bot- O
tom O
panel O
) O
smoothes O
out O
the O
bimodal O
na- O
ture O
of O
the O
true O
distribution O
( O
shown O
by O
the O
green O
curve O
) O
from O
which O
the O
data O
set O
was O
generated O
. O
( O
b.6 O
) O
( O
b.7 O
) O
( O
b.8 O
) O
( O
b.9 O
) O
the O
beta O
is O
the O
conjugate B
prior I
for O
the O
bernoulli O
distribution O
, O
for O
which O
a O
and O
b O
can O
be O
interpreted O
as O
the O
effective O
prior O
number O
of O
observations O
of O
x O
= O
1 O
and O
x O
= O
0 O
, O
respectively O
. O
we O
therefore O
consider O
k O
linear B
regression I
models O
, O
each O
governed O
by O
its O
own O
weight B
parameter I
wk O
. O
as O
we O
shall O
see O
, O
there O
is O
a O
trade-off O
between O
bias B
and O
variance B
, O
with O
very O
ﬂexible O
models O
having O
low O
bias B
and O
high O
variance B
, O
and O
relatively O
rigid O
models O
having O
high O
bias B
and O
low O
variance B
. O
if O
the O
prior B
is O
broad O
so O
that O
α0 O
→ O
0 O
, O
then O
e O
[ O
πk O
] O
→ O
0 O
and O
the O
component O
plays O
no O
role O
in O
the O
model O
, O
whereas O
if O
. O
however O
, O
it O
will O
have O
signiﬁcance O
provided O
a O
( O
x O
) O
takes O
a O
simple O
functional B
form O
. O
however O
, O
we O
can O
easily O
adapt O
gaussian O
processes O
to O
classiﬁcation B
problems O
by O
transforming O
the O
output O
of O
the O
gaussian O
process O
using O
an O
appropriate O
nonlinear O
activation B
function I
. O
( O
10.21 O
) O
( O
cid:24 O
) O
n O
( O
cid:2 O
) O
n=1 O
− O
τ O
2 O
τ O
2π O
( O
cid:18 O
) O
n/2 O
( O
cid:17 O
) O
p O
( O
µ|τ O
) O
= O
n O
( O
cid:10 O
) O
exp O
we O
now O
introduce O
conjugate B
prior I
distributions O
for O
µ O
and O
τ O
given O
by O
µ|µ0 O
, O
( O
λ0τ O
) O
−1 O
( O
10.22 O
) O
( O
10.23 O
) O
where O
gam O
( O
τ|a0 O
, O
b0 O
) O
is O
the O
gamma B
distribution I
deﬁned O
by O
( O
2.146 O
) O
. O
this O
is O
l O
w O
646 O
13. O
sequential B
data I
straightforward O
since O
, O
again O
using O
bayes O
’ O
theorem O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:2 O
) O
p O
( O
zn+1|xn O
) O
= O
= O
= O
= O
= O
p O
( O
zn+1|zn O
, O
xn O
) O
p O
( O
zn|xn O
) O
dzn O
p O
( O
zn+1|zn O
) O
p O
( O
zn|xn O
) O
dzn O
p O
( O
zn+1|zn O
) O
p O
( O
zn|xn O
, O
xn−1 O
) O
dzn O
p O
( O
zn+1|zn O
) O
p O
( O
xn|zn O
) O
p O
( O
zn|xn−1 O
) O
dzn O
( O
cid:6 O
) O
p O
( O
xn|zn O
) O
p O
( O
zn|xn−1 O
) O
dzn O
n O
p O
( O
zn+1|z O
( O
l O
) O
w O
( O
l O
) O
n O
) O
where O
we O
have O
made O
use O
of O
the O
conditional B
independence I
properties O
l O
p O
( O
zn+1|zn O
, O
xn O
) O
= O
p O
( O
zn+1|zn O
) O
p O
( O
xn|zn O
, O
xn−1 O
) O
= O
p O
( O
xn|zn O
) O
( O
13.119 O
) O
( O
13.120 O
) O
( O
13.121 O
) O
which O
follow O
from O
the O
application O
of O
the O
d-separation B
criterion O
to O
the O
graph O
in O
fig- O
ure O
13.5. O
the O
distribution O
given O
by O
( O
13.119 O
) O
is O
a O
mixture B
distribution I
, O
and O
samples O
can O
be O
drawn O
by O
choosing O
a O
component O
l O
with O
probability B
given O
by O
the O
mixing O
coef- O
ﬁcients O
w O
( O
l O
) O
and O
then O
drawing O
a O
sample O
from O
the O
corresponding O
component O
. O
5.4.4 O
5.4.5 O
exact B
evaluation I
of O
the O
hessian O
. O
data O
set O
corresponds O
to O
the O
special O
case O
in O
which O
the O
parameters O
ajk O
are O
the O
same O
for O
all O
values O
of O
j O
, O
so O
that O
the O
conditional B
distribution O
p O
( O
zn|zn−1 O
) O
is O
independent B
of O
zn−1 O
. O
note O
that O
while O
slack O
variables O
allow O
for O
overlapping O
class O
distributions O
, O
this O
framework O
is O
still O
sensitive O
to O
outliers B
because O
the O
penalty O
for O
misclassiﬁcation O
increases O
linearly O
with O
ξ. O
our O
goal O
is O
now O
to O
maximize O
the O
margin B
while O
softly O
penalizing O
points O
that O
lie O
on O
the O
wrong O
side O
of O
the O
margin B
boundary O
. O
the O
technique O
of O
variational B
inference I
, O
to O
be O
discussed O
in O
chapter O
10 O
, O
has O
been O
applied O
to O
bayesian O
neural O
networks O
using O
a O
factorized O
gaussian O
approximation O
278 O
5. O
neural O
networks O
to O
the O
posterior O
distribution O
( O
hinton O
and O
van O
camp O
, O
1993 O
) O
and O
also O
using O
a O
full- O
covariance B
gaussian O
( O
barber O
and O
bishop O
, O
1998a O
; O
barber O
and O
bishop O
, O
1998b O
) O
. O
( O
10.67 O
) O
notice O
the O
similarity O
to O
the O
corresponding O
result O
for O
the O
responsibilities O
in O
maximum B
likelihood I
em O
, O
which O
from O
( O
9.13 O
) O
can O
be O
written O
in O
the O
form O
( O
cid:12 O
) O
−1 O
2 O
( O
cid:13 O
) O
( O
xn O
− O
µk O
) O
tλk O
( O
xn O
− O
µk O
) O
rnk O
∝ O
πk|λk|1/2 O
exp O
( O
10.68 O
) O
where O
we O
have O
used O
the O
precision O
in O
place O
of O
the O
covariance B
to O
highlight O
the O
similarity O
to O
( O
10.67 O
) O
. O
if O
the O
transition B
probability I
for O
the O
markov O
chain O
is O
given O
by O
t O
( O
z O
, O
z O
( O
cid:4 O
) O
) O
, O
and O
the O
sample O
set O
is O
given O
by O
z O
( O
1 O
) O
, O
. O
combining O
this O
term O
with O
the O
remaining O
terms O
from O
2.3. O
the O
gaussian O
distribution O
89 O
( O
2.70 O
) O
that O
depend O
on O
xa O
, O
we O
obtain O
1 O
2 O
[ O
λbbµb O
− O
λba O
( O
xa O
− O
µa O
) O
] O
t O
λ O
bb O
[ O
λbbµb O
− O
λba O
( O
xa O
− O
µa O
) O
] O
−1 O
a O
( O
λaaµa O
+ O
λabµb O
) O
+ O
const O
−1 O
xt O
a O
λaaxa O
+ O
xt O
2 O
a O
( O
λaa O
− O
λabλ O
= O
−1 O
−1 O
xt O
bb O
λba O
) O
xa O
2 O
a O
( O
λaa O
− O
λabλ O
−1 O
bb O
λba O
) O
−1µa O
+ O
const O
+xt O
( O
2.87 O
) O
where O
‘ O
const O
’ O
denotes O
quantities O
independent B
of O
xa O
. O
each O
basis B
function I
φj O
( O
xn O
) O
, O
evaluated O
at O
the O
n O
data O
points O
, O
can O
also O
be O
represented O
as O
a O
vector O
in O
the O
same O
space O
, O
denoted O
by O
ϕj O
, O
as O
illustrated O
in O
figure O
3.2. O
note O
that O
ϕj O
corresponds O
to O
the O
jth O
column O
of O
φ O
, O
whereas O
φ O
( O
xn O
) O
corresponds O
to O
the O
nth O
row O
of O
φ. O
if O
the O
number O
m O
of O
basis O
functions O
is O
smaller O
than O
the O
number O
n O
of O
data O
points O
, O
then O
the O
m O
vectors O
φj O
( O
xn O
) O
will O
span O
a O
linear O
subspace O
s O
of O
dimensionality O
m. O
we O
deﬁne O
y O
to O
be O
an O
n-dimensional O
vector O
whose O
nth O
element O
is O
given O
by O
y O
( O
xn O
, O
w O
) O
, O
where O
n O
= O
1 O
, O
. O
( O
10.7 O
) O
here O
the O
notation O
ei O
( O
cid:9 O
) O
=j O
[ O
··· O
] O
denotes O
an O
expectation B
with O
respect O
to O
the O
q O
distributions O
over O
all O
variables O
zi O
for O
i O
( O
cid:9 O
) O
= O
j O
, O
so O
that O
( O
cid:25 O
) O
( O
cid:25 O
) O
ln O
p O
( O
x O
, O
z O
) O
− O
qj O
ln O
qj O
dzj O
+ O
const O
qj O
ln O
qj O
dzj O
+ O
const O
ln O
p O
( O
x O
, O
z O
) O
qi O
dzi O
dzj O
− O
ln O
qi O
dz O
i O
qi O
i O
qj O
= O
= O
( O
10.6 O
) O
( O
cid:6 O
) O
( O
cid:14 O
) O
i O
( O
cid:9 O
) O
=j O
i O
( O
cid:9 O
) O
=j O
( O
cid:6 O
) O
ei O
( O
cid:9 O
) O
=j O
[ O
ln O
p O
( O
x O
, O
z O
) O
] O
= O
ln O
p O
( O
x O
, O
z O
) O
qi O
dzi O
. O
elements O
of O
information B
theory I
. O
5.34 O
( O
( O
cid:12 O
) O
) O
www O
derive O
the O
result O
( O
5.155 O
) O
for O
the O
derivative B
of O
the O
error B
function I
with O
respect O
to O
the O
network O
output O
activations O
controlling O
the O
mixing O
coefﬁcients O
in O
the O
mixture B
density I
network I
. O
as O
a O
simple O
example O
, O
suppose O
we O
wish O
to O
ﬁnd O
the O
stationary B
point O
of O
the O
function O
f O
( O
x1 O
, O
x2 O
) O
= O
1 O
− O
x2 O
2 O
subject O
to O
the O
constraint O
g O
( O
x1 O
, O
x2 O
) O
= O
x1 O
+ O
x2 O
− O
1 O
= O
0 O
, O
as O
illustrated O
in O
figure O
e.2 O
. O
in O
some O
applications O
, O
the O
loopy B
belief I
propagation I
algorithm O
can O
give O
poor O
re- O
sults O
, O
whereas O
in O
other O
applications O
it O
has O
proven O
to O
be O
very O
effective O
. O
, O
tn O
+1 O
will O
be O
given O
by O
p O
( O
tn O
+1 O
) O
= O
n O
( O
tn O
+1|0 O
, O
cn O
+1 O
) O
( O
6.64 O
) O
where O
cn O
+1 O
is O
an O
( O
n O
+ O
1 O
) O
× O
( O
n O
+ O
1 O
) O
covariance B
matrix I
with O
elements O
given O
by O
( O
6.62 O
) O
. O
al- O
though O
this O
can O
not O
be O
used O
for O
real-time O
prediction O
, O
it O
plays O
a O
key O
role O
in O
learning B
the O
parameters O
of O
the O
model O
. O
( O
13.5 O
) O
610 O
13. O
sequential B
data I
the O
joint O
distribution O
for O
this O
model O
is O
given O
by O
p O
( O
x1 O
, O
. O
thus O
, O
the O
sum-product B
algorithm I
arises O
as O
a O
special O
case O
of O
expectation B
propa- O
gation O
if O
we O
use O
an O
approximating O
distribution O
that O
is O
fully O
factorized O
. O
similarly O
, O
by O
making O
use O
of O
the O
results O
( O
2.81 O
) O
and O
( O
2.82 O
) O
show O
that O
the O
conditional B
distribution O
p O
( O
y|x O
) O
is O
given O
by O
( O
2.100 O
) O
. O
( O
1.64 O
) O
p O
( O
t|x O
, O
wml O
, O
βml O
) O
= O
n O
( O
cid:10 O
) O
( O
cid:17 O
) O
( O
cid:18 O
) O
( O
m O
+1 O
) O
/2 O
now O
let O
us O
take O
a O
step O
towards O
a O
more O
bayesian O
approach O
and O
introduce O
a O
prior B
distribution O
over O
the O
polynomial O
coefﬁcients O
w. O
for O
simplicity O
, O
let O
us O
consider O
a O
gaussian O
distribution O
of O
the O
form O
( O
cid:20 O
) O
( O
cid:19 O
) O
− O
α O
2 O
exp O
wtw O
( O
1.65 O
) O
p O
( O
w|α O
) O
= O
n O
( O
w|0 O
, O
α O
−1i O
) O
= O
α O
2π O
where O
α O
is O
the O
precision O
of O
the O
distribution O
, O
and O
m O
+1 O
is O
the O
total O
number O
of O
elements O
in O
the O
vector O
w O
for O
an O
m O
th O
order O
polynomial O
. O
these O
two O
approaches O
to O
determining O
α O
should O
of O
course O
converge O
to O
the O
same O
result O
( O
assuming O
they O
ﬁnd O
the O
same O
local B
maximum O
of O
the O
evidence B
function I
) O
. O
the O
notion O
behind O
the O
terminology O
is O
that O
differ- O
ent O
components O
can O
model O
the O
distribution O
in O
different O
regions O
of O
input O
space O
( O
they O
k O
( O
cid:2 O
) O
14.5. O
conditional O
mixture O
models O
673 O
figure O
14.10 O
illustration O
of O
a O
mixture O
of O
logistic O
regression B
models O
. O
similarly O
, O
evaluate O
the O
corresponding O
probability B
given O
also O
the O
observation O
that O
the O
battery O
is O
ﬂat O
, O
and O
note O
that O
this O
second O
probability O
is O
lower O
. O
if O
we O
introduce O
hyperpriors O
over O
α O
and O
β O
, O
the O
predictive B
distribution I
is O
obtained O
by O
marginalizing O
over O
w O
, O
α O
and O
β O
so O
that O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
p O
( O
t|t O
) O
= O
p O
( O
t|w O
, O
β O
) O
p O
( O
w|t O
, O
α O
, O
β O
) O
p O
( O
α O
, O
β|t O
) O
dw O
dα O
dβ O
( O
3.74 O
) O
where O
p O
( O
t|w O
, O
β O
) O
is O
given O
by O
( O
3.8 O
) O
and O
p O
( O
w|t O
, O
α O
, O
β O
) O
is O
given O
by O
( O
3.49 O
) O
with O
mn O
and O
sn O
deﬁned O
by O
( O
3.53 O
) O
and O
( O
3.54 O
) O
respectively O
. O
however O
, O
an O
alternative O
approach O
is O
to O
use O
the O
functional B
form O
of O
the O
generalized B
linear I
model I
explicitly O
and O
to O
determine O
its O
parameters O
directly O
by O
using O
maximum B
likelihood I
. O
if O
the O
forward B
problem I
involves O
a O
many-to-one O
mapping O
, O
then O
the O
inverse B
problem I
will O
have O
multiple O
solu- O
tions O
. O
from O
the O
sum O
and O
product O
rules O
, O
the O
marginal B
density O
is O
given O
by O
p O
( O
x O
) O
= O
p O
( O
k O
) O
p O
( O
x|k O
) O
( O
2.191 O
) O
k O
( O
cid:2 O
) O
k=1 O
which O
is O
equivalent O
to O
( O
2.188 O
) O
in O
which O
we O
can O
view O
πk O
= O
p O
( O
k O
) O
as O
the O
prior B
prob- O
ability O
of O
picking O
the O
kth O
component O
, O
and O
the O
density B
n O
( O
x|µk O
, O
σk O
) O
= O
p O
( O
x|k O
) O
as O
the O
probability B
of O
x O
conditioned O
on O
k. O
as O
we O
shall O
see O
in O
later O
chapters O
, O
an O
impor- O
tant O
role O
is O
played O
by O
the O
posterior O
probabilities O
p O
( O
k|x O
) O
, O
which O
are O
also O
known O
as O
responsibilities O
. O
5.5 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
maximizing O
likelihood O
for O
a O
multiclass B
neural O
network O
model O
in O
which O
the O
network O
outputs O
have O
the O
interpretation O
yk O
( O
x O
, O
w O
) O
= O
p O
( O
tk O
= O
1|x O
) O
is O
equivalent O
to O
the O
minimization O
of O
the O
cross-entropy B
error I
function I
( O
5.24 O
) O
. O
first O
note O
that O
the O
likelihood B
function I
p O
( O
t|w O
) O
deﬁned O
by O
( O
3.10 O
) O
is O
the O
exponential O
of O
a O
quadratic O
function O
of O
w. O
the O
corresponding O
conjugate B
prior I
is O
therefore O
given O
by O
a O
gaussian O
distribution O
of O
the O
form O
p O
( O
w O
) O
= O
n O
( O
w|m0 O
, O
s0 O
) O
( O
3.48 O
) O
having O
mean B
m0 O
and O
covariance B
s0 O
. O
when O
the O
size O
n O
of O
the O
data O
set O
is O
large O
the O
predictive B
distribution I
( O
10.81 O
) O
reduces O
to O
a O
mixture O
of O
gaussians O
. O
exercises O
7 O
sparse O
kernel O
machines O
7.1 O
maximum B
margin I
classiﬁers O
. O
( O
c O
− O
an O
) O
ξn O
= O
0 O
( O
7.62 O
) O
( O
7.63 O
) O
( O
7.64 O
) O
( O
7.65 O
) O
( O
7.66 O
) O
( O
7.67 O
) O
( O
7.68 O
) O
an O
= O
( O
cid:1 O
) O
an O
= O
0. O
we O
again O
have O
a O
sparse O
solution O
, O
and O
the O
only O
terms O
that O
have O
to O
be O
7.1. O
maximum B
margin I
classiﬁers O
343 O
evaluated O
in O
the O
predictive O
model O
( O
7.64 O
) O
are O
those O
that O
involve O
the O
support O
vectors O
. O
initially O
, O
we O
shall O
motivate O
the O
em O
algorithm O
by O
giving O
a O
relatively O
informal O
treatment O
in O
the O
context O
of O
the O
gaussian O
mixture B
model I
. O
550 O
11. O
sampling B
methods I
during O
the O
evolution O
of O
this O
dynamical B
system I
, O
the O
value O
of O
the O
hamiltonian O
h O
is O
constant O
, O
as O
is O
easily O
seen O
by O
differentiation O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
i O
i O
dh O
dτ O
= O
= O
∂h O
∂zi O
dzi O
dτ O
∂h O
∂zi O
∂h O
∂ri O
+ O
∂h O
∂ri O
− O
∂h O
∂ri O
dri O
dτ O
∂h O
∂zi O
( O
cid:13 O
) O
( O
cid:13 O
) O
= O
0 O
. O
because O
the O
model O
has O
linear-gaussian O
conditional B
distributions O
, O
we O
can O
write O
the O
transition O
and O
emission O
distributions O
in O
the O
general O
form O
p O
( O
zn|zn−1 O
) O
= O
n O
( O
zn|azn−1 O
, O
γ O
) O
p O
( O
xn|zn O
) O
= O
n O
( O
xn|czn O
, O
σ O
) O
. O
in O
fact O
, O
the O
issues O
of O
nonlinearity O
and O
non-gaussianity O
are O
related O
because O
a O
general O
probability B
density O
can O
be O
obtained O
from O
a O
simple O
fixed O
reference O
density B
, O
such O
as O
a O
gaussian O
, O
by O
making O
a O
nonlinear O
change O
of O
variables O
. O
historically O
various O
‘ O
information O
criteria O
’ O
have O
been O
proposed O
that O
attempt O
to O
correct O
for O
the O
bias B
of O
maximum B
likelihood I
by O
the O
addition O
of O
a O
penalty O
term O
to O
compensate O
for O
the O
over-ﬁtting B
of O
more O
complex O
models O
. O
we O
can O
similarly O
ﬁnd O
a O
recursion O
relation O
for O
the O
quantities O
β O
( O
zn O
) O
by O
making O
use O
of O
the O
conditional B
independence I
properties O
( O
13.27 O
) O
and O
( O
13.28 O
) O
giving O
β O
( O
zn O
) O
= O
p O
( O
xn+1 O
, O
. O
p O
( O
t|x O
, O
w O
, O
β O
) O
= O
n O
( O
cid:2 O
) O
taking O
the O
negative O
logarithm O
, O
we O
obtain O
the O
error B
function I
β O
2 O
n=1 O
{ O
y O
( O
xn O
, O
w O
) O
− O
tn O
} O
2 O
− O
n O
2 O
ln O
β O
+ O
n O
2 O
ln O
( O
2π O
) O
( O
5.13 O
) O
which O
can O
be O
used O
to O
learn O
the O
parameters O
w O
and O
β. O
in O
section O
5.7 O
, O
we O
shall O
dis- O
cuss O
the O
bayesian O
treatment O
of O
neural O
networks O
, O
while O
here O
we O
consider O
a O
maximum B
likelihood I
approach O
. O
the O
interpretation O
of O
this O
summa- O
tion O
over O
h O
is O
that O
just O
one O
model O
is O
responsible O
for O
generating O
the O
whole O
data O
set O
, O
and O
the O
probability B
distribution O
over O
h O
simply O
reﬂects O
our O
uncertainty O
as O
to O
which O
model O
that O
is O
. O
for O
the O
binary O
coding O
scheme O
, O
this O
conditional B
expectation I
is O
given O
by O
the O
vector O
of O
posterior O
class O
probabilities O
. O
10.2.5 O
induced O
factorizations O
in O
deriving O
these O
variational B
update O
equations O
for O
the O
gaussian O
mixture B
model I
, O
we O
assumed O
a O
particular O
factorization B
of O
the O
variational B
posterior O
distribution O
given O
by O
( O
10.42 O
) O
. O
in O
addition O
, O
we O
have O
also O
found O
an O
expres- O
sion B
for O
the O
bias B
value O
w0 O
given O
by O
( O
4.34 O
) O
. O
we O
have O
seen O
that O
variational B
message O
passing O
and O
expectation B
propagation I
op- O
timize O
two O
different O
forms O
of O
the O
kullback-leibler O
divergence O
. O
( O
2.104 O
) O
l O
( O
cid:16 O
) O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
x O
y O
( O
cid:15 O
) O
( O
cid:15 O
) O
exercise O
2.29 O
the O
covariance B
matrix I
is O
found O
by O
taking O
the O
inverse B
of O
the O
precision O
, O
which O
can O
be O
done O
using O
the O
matrix O
inversion O
formula O
( O
2.76 O
) O
to O
give O
cov O
[ O
z O
] O
= O
r−1 O
= O
−1 O
−1at O
−1 O
l−1 O
+ O
aλ O
λ O
λ O
aλ O
−1at O
. O
an O
even O
simpler O
approach O
is O
( O
c O
) O
in O
which O
we O
use O
the O
training B
data O
to O
ﬁnd O
a O
discriminant B
function I
f O
( O
x O
) O
that O
maps O
each O
x O
directly O
onto O
a O
class O
label O
, O
thereby O
combining O
the O
inference B
and O
decision O
stages O
into O
a O
single O
learning B
problem O
. O
14.3 O
boosting B
. O
12.4. O
nonlinear O
latent B
variable I
models O
591 O
12.4. O
nonlinear O
latent B
variable I
models O
in O
this O
chapter O
, O
we O
have O
focussed O
on O
the O
simplest O
class O
of O
models O
having O
continuous O
latent O
variables O
, O
namely O
those O
based O
on O
linear-gaussian O
distributions O
. O
in O
a O
bayesian O
neural B
network I
, O
the O
prior B
distribution O
over O
the O
parameter O
vector O
w O
, O
in O
conjunction O
with O
the O
network O
function O
f O
( O
x O
, O
w O
) O
, O
produces O
a O
prior B
distribution O
over O
functions O
from O
y O
( O
x O
) O
where O
y O
is O
the O
vector O
of O
network O
outputs O
. O
for O
a O
model O
governed O
by O
a O
set O
of O
parameters O
w O
, O
the O
model B
evidence I
is O
given O
, O
from O
the O
sum O
and O
product O
rules O
of O
probability B
, O
by O
p O
( O
d|mi O
) O
= O
p O
( O
d|w O
, O
mi O
) O
p O
( O
w|mi O
) O
dw O
. O
if O
the O
graph O
is O
constructed O
using O
distributions O
from O
the O
exponential B
family I
, O
and O
if O
the O
parent-child O
relationships O
preserve O
conjugacy O
, O
then O
the O
full O
conditional B
distri- O
butions O
arising O
in O
gibbs O
sampling O
will O
have O
the O
same O
functional B
form O
as O
the O
orig- O
546 O
11. O
sampling B
methods I
figure O
11.12 O
the O
gibbs O
sampling O
method O
requires O
samples O
to O
be O
drawn O
from O
the O
conditional B
distribution O
of O
a O
variable O
condi- O
tioned O
on O
the O
remaining O
variables O
. O
an O
introduction O
to O
variational B
methods O
for O
graphical O
models O
. O
regularized O
max- O
imum O
likelihood O
can O
be O
interpreted O
as O
a O
map O
( O
maximum B
posterior I
) O
approach O
in O
which O
the O
regularizer O
can O
be O
viewed O
as O
the O
logarithm O
of O
a O
prior B
parameter O
distribu- O
tion O
. O
thus O
the O
marginal B
distribution O
of O
x O
is O
a O
gaussian O
mixture O
of O
the O
form O
( O
9.7 O
) O
. O
2.27 O
( O
( O
cid:12 O
) O
) O
let O
x O
and O
z O
be O
two O
independent B
random O
vectors O
, O
so O
that O
p O
( O
x O
, O
z O
) O
= O
p O
( O
x O
) O
p O
( O
z O
) O
. O
one O
of O
the O
simplest O
of O
these O
is O
the O
mean B
, O
corresponding O
to O
the O
conditional B
average O
of O
the O
target O
data O
, O
and O
is O
given O
by O
( O
cid:6 O
) O
k O
( O
cid:2 O
) O
k=1 O
e O
[ O
t|x O
] O
= O
tp O
( O
t|x O
) O
dt O
= O
πk O
( O
x O
) O
µk O
( O
x O
) O
( O
5.158 O
) O
5.7. O
bayesian O
neural O
networks O
277 O
where O
we O
have O
used O
( O
5.148 O
) O
. O
this O
set O
is O
distinct O
from O
the O
set O
of O
distributions O
such O
that O
for O
each O
distribution O
there O
exists O
an O
undirected B
graph I
that O
is O
a O
perfect B
map I
. O
the O
similarity O
of O
the O
logistic B
sigmoid I
and O
the O
probit B
function I
, O
for O
this O
choice O
of O
λ O
, O
is O
illustrated O
in O
figure O
4.9. O
the O
advantage O
of O
using O
a O
probit B
function I
is O
that O
its O
convolution O
with O
a O
gaussian O
can O
be O
expressed O
analytically O
in O
terms O
of O
another O
probit B
function I
. O
we O
have O
therefore O
been O
able O
to O
exploit O
the O
many O
conditional B
independence I
properties O
of O
this O
simple O
graph O
in O
order O
to O
obtain O
an O
efﬁcient O
calcula- O
tion O
. O
notice O
that O
the O
entropy B
of O
this O
variable O
is O
given O
by O
h O
[ O
x O
] O
= O
−8 O
× O
1 O
8 O
log2 O
1 O
8 O
= O
3 O
bits B
. O
components O
that O
provide O
insufﬁcient O
contribution O
exercise O
10.22 O
section O
3.4 O
exercise O
10.23 O
section O
7.2.2 O
10.2. O
illustration O
: O
variational B
mixture O
of O
gaussians O
485 O
to O
explaining O
the O
data O
will O
have O
their O
mixing O
coefﬁcients O
driven O
to O
zero O
during O
the O
optimization O
, O
and O
so O
they O
are O
effectively O
removed O
from O
the O
model O
through O
automatic B
relevance I
determination I
. O
to O
understand O
this O
, O
let O
us O
examine O
how O
the O
number O
of O
computer O
operations O
required O
to O
evaluate O
the O
derivatives O
of O
the O
error B
function I
scales O
with O
the O
total O
number O
w O
of O
weights O
and O
biases O
in O
the O
network O
. O
to O
ﬁnd O
an O
invariant O
measure O
of O
the O
mean B
, O
we O
note O
that O
the O
observations O
can O
be O
viewed O
as O
points O
on O
the O
unit O
circle O
and O
can O
therefore O
be O
described O
instead O
by O
two-dimensional O
unit O
vectors O
x1 O
, O
. O
this O
gives O
a O
more O
principled O
approach O
to O
multiclass B
classiﬁcation O
than O
the O
pairwise O
method O
used O
in O
the O
support B
vector I
machine I
and O
also O
provides O
probabilis- O
tic O
predictions O
for O
new O
data O
points O
. O
7.2.1 O
rvm O
for B
regression I
the O
relevance B
vector I
machine I
for O
regression B
is O
a O
linear O
model O
of O
the O
form O
studied O
in O
chapter O
3 O
but O
with O
a O
modiﬁed O
prior B
that O
results O
in O
sparse O
solutions O
. O
indeed O
, O
we O
can O
identify O
three O
distinct O
approaches O
to O
solving O
regression B
problems O
given O
, O
in O
order O
of O
decreasing O
complexity O
, O
by O
: O
( O
a O
) O
first O
solve O
the O
inference B
problem O
of O
determining O
the O
joint O
density B
p O
( O
x O
, O
t O
) O
. O
multidimensional O
stochastic O
ap- O
proximation B
methods O
. O
the O
relevance B
vector I
machine I
, O
discussed O
in O
section O
7.2 O
, O
also O
chooses O
a O
subset O
from O
a O
ﬁxed O
set O
of O
basis O
functions O
and O
typically O
results O
in O
much O
225 O
226 O
5. O
neural O
networks O
sparser O
models O
. O
kl O
( O
q||p O
) O
l O
( O
q O
, O
θ O
) O
ln O
p O
( O
x|θ O
) O
exercise O
9.24 O
section O
1.6.1 O
carefully O
the O
forms O
of O
the O
expressions O
( O
9.71 O
) O
and O
( O
9.72 O
) O
, O
and O
in O
particular O
noting O
that O
they O
differ O
in O
sign O
and O
also O
that O
l O
( O
q O
, O
θ O
) O
contains O
the O
joint O
distribution O
of O
x O
and O
z O
while O
kl O
( O
q O
( O
cid:5 O
) O
p O
) O
contains O
the O
conditional B
distribution O
of O
z O
given O
x. O
to O
verify O
the O
decomposition O
( O
9.70 O
) O
, O
we O
ﬁrst O
make O
use O
of O
the O
product B
rule I
of I
probability I
to O
give O
ln O
p O
( O
x O
, O
z|θ O
) O
= O
ln O
p O
( O
z|x O
, O
θ O
) O
+ O
ln O
p O
( O
x|θ O
) O
( O
9.73 O
) O
which O
we O
then O
substitute O
into O
the O
expression O
for O
l O
( O
q O
, O
θ O
) O
. O
if O
we O
simply O
average O
the O
measurements O
, O
the O
error B
due O
to O
random O
noise O
will O
be O
reduced O
, O
but O
unfortunately O
we O
will O
just O
obtain O
a O
single O
averaged O
estimate O
, O
in O
which O
we O
have O
averaged O
over O
the O
changing O
value O
of O
z O
, O
thereby O
introducing O
a O
new O
source O
of O
error B
. O
12.1.2 O
minimum-error O
formulation O
we O
now O
discuss O
an O
alternative O
formulation O
of O
pea O
based O
on O
projection O
error B
minimization O
. O
the O
number O
of O
messages O
that O
have O
to O
be O
computed O
is O
given O
by O
twice O
the O
number O
of O
links O
in O
the O
graph O
and O
so O
involves O
only O
twice O
the O
computation O
involved O
in O
ﬁnding O
a O
single O
marginal B
. O
the O
top O
left O
plot O
shows O
the O
initial O
parameter O
vector O
w O
shown O
as O
a O
black O
arrow O
together O
with O
the O
corresponding O
decision B
boundary I
( O
black O
line O
) O
, O
in O
which O
the O
arrow O
points O
towards O
the O
decision B
region I
which O
classiﬁed O
as O
belonging O
to O
the O
red O
class O
. O
the O
structure O
of O
this O
mixture B
density I
network I
is O
illustrated O
in O
figure O
5.20. O
the O
mixture B
density I
network I
is O
closely O
related O
to O
the O
mixture B
of I
experts I
discussed O
in O
section O
14.5.3. O
the O
principle O
difference O
is O
that O
in O
the O
mixture B
density I
network I
the O
same O
function O
is O
used O
to O
predict O
the O
parameters O
of O
all O
of O
the O
component O
densities O
as O
well O
as O
the O
mixing O
co- O
efﬁcients O
, O
and O
so O
the O
nonlinear O
hidden O
units O
are O
shared O
amongst O
the O
input-dependent O
functions O
. O
use O
this O
result O
to O
show O
that O
, O
if O
the O
decision O
regions O
of O
a O
two-class O
classiﬁcation B
problem O
are O
chosen O
to O
minimize O
the O
probability B
of O
misclassiﬁcation O
, O
this O
probability B
will O
satisfy O
( O
cid:6 O
) O
p O
( O
mistake O
) O
( O
cid:1 O
) O
{ O
p O
( O
x O
, O
c1 O
) O
p O
( O
x O
, O
c2 O
) O
} O
1/2 O
dx O
. O
large B
margin I
dags O
for O
multiclass O
clas- O
siﬁcation O
. O
by O
forming O
mixtures O
of O
von O
mises O
distributions O
, O
we O
obtain O
a O
ﬂexible O
framework O
for O
modelling O
periodic O
variables O
that O
can O
handle O
multimodality B
. O
slice B
sampling I
. O
in O
( O
b O
) O
, O
the O
joint O
distri- O
bution O
is O
given O
by O
a O
general O
form O
p O
( O
x O
) O
= O
f O
( O
x1 O
, O
x2 O
, O
x3 O
) O
, O
whereas O
in O
( O
c O
) O
, O
it O
is O
given O
by O
the O
more O
speciﬁc O
factorization B
p O
( O
x O
) O
= O
fa O
( O
x1 O
, O
x2 O
) O
fb O
( O
x1 O
, O
x3 O
) O
fc O
( O
x2 O
, O
x3 O
) O
. O
taking O
the O
gradient O
of O
the O
error B
function I
with O
respect O
to O
w O
, O
we O
obtain O
∇e O
( O
w O
) O
= O
( O
yn O
− O
tn O
) O
φn O
( O
4.91 O
) O
n=1 O
where O
we O
have O
made O
use O
of O
( O
4.88 O
) O
. O
( O
5.157 O
) O
276 O
5. O
neural O
networks O
figure O
5.21 O
( O
a O
) O
plot O
of O
the O
mixing O
coefﬁcients O
πk O
( O
x O
) O
as O
a O
function O
of O
x O
for O
the O
three O
kernel O
functions O
in O
a O
mixture B
density I
network I
trained O
on O
the O
data O
shown O
in O
figure O
5.19. O
the O
model O
has O
three O
gaussian O
compo- O
nents O
, O
and O
uses O
a O
two-layer O
multi- O
layer O
perceptron B
with O
ﬁve O
‘ O
tanh O
’ O
sig- O
moidal O
units O
in O
the O
hidden O
layer O
, O
and O
nine O
outputs O
( O
corresponding O
to O
the O
3 O
means O
and O
3 O
variances O
of O
the O
gaus- O
sian O
components O
and O
the O
3 O
mixing O
coefﬁcients O
) O
. O
practical O
methods O
of O
optimiza- O
tern O
classiﬁcation B
( O
second O
ed. O
) O
. O
one O
simple O
extension O
of O
the O
linear B
dynamical I
system I
is O
to O
use O
a O
gaussian O
mixture B
as O
the O
initial O
distribution O
for O
z1 O
. O
at O
each O
step O
of O
the O
gibbs O
sampling O
algorithm O
, O
the O
conditional B
distribution O
for O
a O
particular O
component O
zi O
has O
some O
mean B
µi O
and O
some O
variance B
σ2 O
i O
. O
for O
example O
, O
if O
the O
goal O
is O
real-time O
face B
detection I
in O
a O
high-resolution O
video O
stream O
, O
the O
computer O
must O
handle O
huge O
numbers O
of O
pixels O
per O
second O
, O
and O
presenting O
these O
directly O
to O
a O
complex O
pattern O
recognition O
algorithm O
may O
be O
computationally O
infeasi- O
ble O
. O
as O
a O
consequence O
, O
the O
solution O
for O
wmap O
found O
by O
maximiz- O
ing O
the O
log O
posterior O
will O
depend O
on O
the O
initialization O
of O
w. O
solutions O
that O
differ O
only O
as O
a O
consequence O
of O
the O
interchange O
and O
sign O
reversal O
symmetries B
in O
the O
hidden O
units O
are O
identical O
so O
far O
as O
predictions O
are O
concerned O
, O
and O
it O
is O
irrelevant O
which O
of O
the O
equivalent O
solutions O
is O
found O
. O
given O
this O
restriction O
, O
we O
can O
make O
a O
precise O
relationship O
between O
factorization B
and O
conditional B
independence I
. O
to O
understand O
the O
difference O
, O
note O
that O
if O
a O
single O
gaussian O
collapses O
onto O
a O
data O
point O
it O
will O
contribute O
multiplicative O
factors O
to O
the O
likelihood B
function I
arising O
from O
the O
other O
data O
points O
and O
these O
factors O
will O
go O
to O
zero O
exponentially O
fast O
, O
giving O
an O
overall O
likelihood O
that O
goes O
to O
zero O
rather O
than O
inﬁnity O
. O
we O
close O
this O
chapter O
by O
showing O
how O
the O
k-nearest-neighbour O
technique O
for O
density O
estimation O
can O
be O
extended B
to O
the O
problem O
of O
classiﬁcation B
. O
to O
gain O
some O
insight O
into O
the O
nature O
of O
the O
exponential O
error O
function O
, O
we O
ﬁrst O
consider O
the O
expected O
error B
given O
by O
( O
cid:6 O
) O
ex O
, O
t O
[ O
exp O
{ O
−ty O
( O
x O
) O
} O
] O
= O
exp O
{ O
−ty O
( O
x O
) O
} O
p O
( O
t|x O
) O
p O
( O
x O
) O
dx O
. O
the O
weight O
parameters O
were O
found O
using O
scaled O
conjugate B
gradients O
, O
and O
the O
hyperparameter B
α O
was O
optimized O
using O
the O
evidence O
framework O
. O
however O
, O
the O
models O
explored O
in O
this O
chapter O
are O
equally O
applicable O
to O
all O
605 O
606 O
13. O
sequential B
data I
figure O
13.1 O
example O
of O
a O
spectro- O
gram O
of O
the O
spoken O
words O
“ O
bayes O
’ O
theo- O
rem O
” O
showing O
a O
plot O
of O
the O
intensity O
of O
the O
spectral O
coefﬁcients O
versus O
time O
index O
. O
( O
1.35 O
) O
we O
shall O
make O
extensive O
use O
of O
this O
result O
when O
we O
discuss O
sampling B
methods I
in O
chapter O
11. O
the O
approximation O
in O
( O
1.35 O
) O
becomes O
exact O
in O
the O
limit O
n O
→ O
∞ O
. O
if O
we O
have O
a O
univariate O
gaussian O
n O
( O
x|µ O
, O
τ O
−1 O
) O
together O
with O
a O
gamma O
prior O
gam O
( O
τ|a O
, O
b O
) O
and O
we O
integrate O
out O
the O
precision O
, O
we O
obtain O
the O
marginal B
distribution O
of O
x O
in O
the O
form O
section O
2.3.6 O
exercise O
2.46 O
figure O
2.15 O
plot O
of O
student O
’ O
s O
t-distribution O
( O
2.159 O
) O
for O
µ O
= O
0 O
and O
λ O
= O
1 O
for O
various O
values O
of O
ν. O
the O
limit O
ν O
→ O
∞ O
corresponds O
to O
a O
gaussian O
distribution O
with O
mean B
µ O
and O
precision O
λ O
. O
the O
framework O
of O
ordered B
over-relaxation I
( O
neal O
, O
1999 O
) O
generalizes O
this O
approach O
to O
non- O
gaussian O
distributions O
. O
this O
step O
leaves O
the O
desired O
distribution O
invariant O
because O
if O
zi O
has O
mean B
µi O
( O
cid:4 O
) O
and O
variance B
σ2 O
i. O
the O
effect O
of O
over-relaxation B
is O
to O
encourage O
directed B
motion O
through O
state O
space O
when O
the O
variables O
are O
highly O
correlated O
. O
the O
general O
framework O
for O
achieving O
this O
is O
called O
d-separation B
, O
where O
the O
‘ O
d O
’ O
stands O
for O
‘ O
directed B
’ O
( O
pearl O
, O
1988 O
) O
. O
show O
that O
this O
leads O
to O
a O
stochastic B
k-means O
algorithm O
in O
which O
, O
for O
each O
data O
point O
xn O
, O
the O
nearest O
prototype O
µk O
is O
updated O
using O
( O
9.5 O
) O
. O
exercise O
14.1 O
14.2. O
committees O
section O
3.2 O
the O
simplest O
way O
to O
construct O
a O
committee B
is O
to O
average O
the O
predictions O
of O
a O
set O
of O
individual O
models O
. O
10.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
starting O
from O
the O
joint O
distribution O
( O
10.41 O
) O
, O
and O
applying O
the O
general O
result O
( O
10.9 O
) O
, O
show O
that O
the O
optimal O
variational B
distribution O
q O
( O
cid:1 O
) O
( O
z O
) O
over O
the O
latent O
variables O
for O
the O
bayesian O
mixture O
of O
gaussians O
is O
given O
by O
( O
10.48 O
) O
by O
verifying O
the O
steps O
given O
in O
the O
text O
. O
, O
bd O
) O
t O
and O
the O
covariance B
matrix I
is O
diagonal B
of O
the O
form O
diag O
( O
v1 O
, O
. O
a O
family O
of O
approximate O
al- O
gorithms O
for O
bayesian O
inference B
. O
by O
basing O
a O
regression B
model O
on O
a O
heavy-tailed O
distribution O
such O
as O
a O
t-distribution O
, O
we O
obtain O
a O
more O
robust O
model O
. O
suppose O
the O
initial O
weight B
vector I
w O
( O
0 O
) O
is O
chosen O
to O
be O
at O
the O
origin O
and O
is O
updated O
using O
simple O
gradient B
descent I
w O
( O
τ O
) O
= O
w O
( O
τ−1 O
) O
− O
ρ∇e O
( O
5.196 O
) O
where O
τ O
denotes O
the O
step O
number O
, O
and O
ρ O
is O
the O
learning O
rate O
( O
which O
is O
assumed O
to O
be O
small O
) O
. O
principal B
component I
analysis I
principal O
compooem O
analy O
, O
; O
'' O
or O
rca O
. O
in O
the O
particular O
context O
of O
the O
hidden O
markov O
model O
, O
this O
is O
known O
as O
the O
forward-backward B
algorithm I
( O
rabiner O
, O
1989 O
) O
, O
or O
the O
baum-welch O
algorithm O
( O
baum O
, O
1972 O
) O
. O
a O
bayesian O
treatment O
of O
the O
hme O
has O
been O
given O
by O
bishop O
and O
svens´en O
( O
2003 O
) O
based O
on O
variational B
inference I
. O
∇e O
( O
w O
) O
= O
h O
= O
∇∇e O
( O
w O
) O
= O
( O
wtφn O
− O
tn O
) O
φn O
= O
φtφw O
− O
φtt O
φnφt O
n O
= O
φtφ O
( O
4.93 O
) O
( O
4.94 O
) O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n=1 O
section O
3.1.1 O
where O
φ O
is O
the O
n O
× O
m O
design B
matrix I
, O
whose O
nth O
row O
is O
given O
by O
φt O
( O
cid:27 O
) O
raphson O
update O
then O
takes O
the O
form O
φtφw O
( O
old O
) O
− O
φtt O
w O
( O
new O
) O
= O
w O
( O
old O
) O
− O
( O
φtφ O
) O
−1 O
( O
cid:26 O
) O
n. O
the O
newton- O
= O
( O
φtφ O
) O
−1φtt O
which O
we O
recognize O
as O
the O
standard O
least-squares O
solution O
. O
for O
the O
annular O
conﬁguration O
, O
the O
relationship O
between O
phase O
fraction O
and O
path O
length O
is O
nonlinear O
and O
so O
the O
manifold B
will O
be O
nonlinear O
. O
show O
that O
the O
mean B
and O
covariance B
of O
the O
mixture B
distribution I
are O
given O
by O
( O
9.49 O
) O
and O
( O
9.50 O
) O
. O
410 O
8. O
graphical O
models O
x1 O
x2 O
x3 O
x1 O
x2 O
x3 O
x4 O
( O
a O
) O
x4 O
( O
b O
) O
figure O
8.52 O
flow O
of O
messages O
for O
the O
sum-product B
algorithm I
applied O
to O
the O
example O
graph O
in O
figure O
8.51 O
. O
we O
can O
also O
use O
the O
result O
of O
a O
clustering B
algorithm O
to O
perform O
data O
compres- O
sion B
. O
this O
effect O
can O
be O
understood O
qualitatively O
in O
terms O
of O
the O
automatic O
trade-off O
in O
a O
bayesian O
model O
between O
ﬁtting O
the O
data O
and O
the O
complexity O
of O
the O
model O
, O
in O
which O
the O
complexity O
penalty O
arises O
from O
components O
whose O
parameters O
are O
pushed O
away O
from O
their O
prior B
values O
. O
alterna- O
tively O
we O
can O
employ O
a O
powerful O
framework O
called O
expectation B
maximization I
, O
which O
will O
be O
discussed O
at O
length O
in O
chapter O
9 O
. O
show O
that O
y O
also O
has O
a O
gaussian O
distribution O
, O
and O
find O
expressions O
for O
its O
mean B
and O
covariance B
. O
it O
also O
gives O
bounds O
for O
the O
computational O
cost O
of O
learning B
, O
although O
we O
do O
not O
consider O
these O
here O
. O
for O
the O
more O
general O
case O
of O
m O
7.2. O
relevance B
vector I
machines O
351 O
basis O
vectors O
ϕ1 O
, O
. O
1 O
2 O
3 O
4 O
appendix O
d. O
calculus B
of I
variations I
we O
can O
think O
of O
a O
function O
y O
( O
x O
) O
as O
being O
an O
operator O
that O
, O
for O
any O
input O
value O
x O
, O
returns O
an O
output O
value O
y. O
in O
the O
same O
way O
, O
we O
can O
deﬁne O
a O
functional B
f O
[ O
y O
] O
to O
be O
an O
operator O
that O
takes O
a O
function O
y O
( O
x O
) O
and O
returns O
an O
output O
value O
f O
. O
one O
way O
to O
set O
the O
values O
of O
these O
parameters O
is O
to O
use O
maximum B
likelihood I
. O
then O
in O
the O
back-tracking B
step O
, O
having O
found O
xmax O
, O
we O
can O
then O
use O
these O
stored O
values O
to O
as- O
sign O
consistent B
maximizing O
states O
xmax O
m O
. O
exercise O
14.3 O
14.3. O
boosting B
boosting O
is O
a O
powerful O
technique O
for O
combining O
multiple O
‘ O
base O
’ O
classiﬁers O
to O
produce O
a O
form O
of O
committee B
whose O
performance O
can O
be O
signiﬁcantly O
better O
than O
that O
of O
any O
of O
the O
base O
classiﬁers O
. O
the O
naive O
bayes O
assumption O
is O
helpful O
when O
the O
dimensionality O
d O
of O
the O
input O
space O
is O
high O
, O
making O
density B
estimation I
in O
the O
full O
d-dimensional O
space O
more O
chal- O
lenging O
. O
1 O
0.5 O
0 O
0 O
ξ O
1.5 O
x O
3 O
0.4 O
λξ O
− O
g O
( O
λ O
) O
0.2 O
0 O
−1 O
−0.5 O
λ O
0 O
494 O
10. O
approximate O
inference B
y O
f O
( O
x O
) O
y O
−g O
( O
λ O
) O
f O
( O
x O
) O
x O
λx O
x O
λx O
− O
g O
( O
λ O
) O
figure O
10.11 O
in O
the O
left-hand O
plot O
the O
red O
curve O
shows O
a O
convex B
function I
f O
( O
x O
) O
, O
and O
the O
blue O
line O
represents O
the O
linear O
function O
λx O
, O
which O
is O
a O
lower B
bound I
on O
f O
( O
x O
) O
because O
f O
( O
x O
) O
> O
λx O
for O
all O
x. O
for O
the O
given O
value O
of O
slope O
λ O
the O
contact O
point O
of O
the O
tangent O
line O
having O
the O
same O
slope O
is O
found O
by O
minimizing O
with O
respect O
to O
x O
the O
discrepancy O
( O
shown O
by O
the O
green O
dashed O
lines O
) O
given O
by O
f O
( O
x O
) O
− O
λx O
. O
12.25 O
( O
** O
) O
iii O
! O
i O
consider O
a O
linear-gaussian O
latent-variable O
model O
having O
a O
latent O
space O
distribution O
p O
( O
z O
) O
= O
n O
( O
xio O
, O
i O
) O
and O
a O
conditional B
distribution O
for O
the O
observed O
vari O
( O
cid:173 O
) O
able O
p O
( O
xlz O
) O
= O
n O
( O
xlwz O
+ O
il O
, O
< O
p O
) O
where O
< O
p O
is O
an O
arbitrary O
symmetric O
, O
positive O
( O
cid:173 O
) O
definite O
noise O
covariance B
matrix I
. O
samples O
from O
the O
gamma B
distribution I
can O
be O
obtained O
by O
sampling O
from O
the O
cauchy O
and O
then O
applying O
the O
rejection O
sam- O
pling O
criterion O
. O
here O
we O
consider O
two O
techniques O
called O
rejection B
sampling I
and O
importance B
sampling I
. O
or O
density B
modelling O
, O
then O
there O
can O
be O
benefits O
in O
exploiling O
this O
manifold B
struclure O
. O
3.4 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
linear O
model O
of O
the O
form O
d O
( O
cid:2 O
) O
y O
( O
x O
, O
w O
) O
= O
w0 O
+ O
wixi O
i=1 O
together O
with O
a O
sum-of-squares B
error I
function O
of O
the O
form O
n O
( O
cid:2 O
) O
n=1 O
ed O
( O
w O
) O
= O
1 O
2 O
{ O
y O
( O
xn O
, O
w O
) O
− O
tn O
} O
2 O
. O
in O
a O
practical O
application O
, O
therefore O
, O
it O
will O
be O
wise O
to O
keep O
aside O
an O
independent B
test O
set O
of O
data O
on O
which O
to O
evaluate O
the O
overall O
performance O
of O
the O
ﬁnal O
system O
. O
once O
we O
are O
told O
that O
the O
fruit O
is O
an O
orange O
, O
we O
can O
then O
use O
bayes O
’ O
theorem O
to O
compute O
the O
probability B
p O
( O
b|f O
) O
, O
which O
we O
shall O
call O
the O
posterior B
probability I
because O
it O
is O
the O
probability B
obtained O
after O
we O
have O
observed O
f O
. O
unfortunately O
, O
this O
will O
not O
help O
here O
because O
we O
are O
forming O
sums O
of O
products O
of O
small O
numbers O
( O
we O
are O
in O
fact O
im- O
plicitly O
summing O
over O
all O
possible O
paths O
through O
the O
lattice B
diagram I
of O
figure O
13.7 O
) O
. O
the O
maximum B
margin I
hyperplane O
is O
deﬁned O
by O
the O
location O
of O
the O
support O
vectors O
. O
2.3.7 O
student O
’ O
s O
t-distribution O
we O
have O
seen O
that O
the O
conjugate B
prior I
for O
the O
precision O
of O
a O
gaussian O
is O
given O
by O
a O
gamma B
distribution I
. O
thus O
the O
optimization O
of O
the O
variational B
posterior O
distribution O
involves O
cycling O
between O
two O
stages O
analogous O
to O
the O
e O
and O
m O
steps O
of O
the O
maximum B
likelihood I
em O
algorithm O
. O
7.13 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
the O
evidence O
framework O
for O
rvm O
regression B
, O
we O
obtained O
the O
re-estimation O
formulae O
( O
7.87 O
) O
and O
( O
7.88 O
) O
by O
maximizing O
the O
marginal B
likelihood I
given O
by O
( O
7.85 O
) O
. O
this O
is O
known O
as O
a O
dual O
formulation O
because O
, O
by O
noting O
that O
the O
solution O
for O
a O
can O
be O
expressed O
as O
a O
linear O
combination O
of O
the O
elements O
of O
φ O
( O
x O
) O
, O
we O
recover O
the O
original O
formulation O
in O
terms O
of O
the O
parameter O
vector O
w. O
note O
that O
the O
prediction O
at O
x O
is O
given O
by O
a O
linear O
combination O
of O
the O
target O
values O
from O
the O
training B
set I
. O
for O
each O
feature B
map I
in O
the O
convolutional B
layer O
, O
there O
is O
a O
plane O
of O
units O
in O
the O
subsampling B
layer O
and O
each O
unit O
takes O
inputs O
from O
a O
small O
receptive O
ﬁeld O
in O
the O
corresponding O
feature B
map I
of O
the O
convolutional B
layer O
. O
we O
can O
readily O
generalize O
this O
result O
to O
arbitrary O
tree-structured O
factor O
graphs O
by O
substituting O
the O
expression O
( O
8.59 O
) O
for O
the O
factor B
graph I
expansion O
into O
( O
8.89 O
) O
and O
again O
exchanging O
maximizations O
with O
products O
. O
similarly O
, O
the O
quantity O
p O
( O
y O
|x O
) O
is O
a O
conditional B
probability I
and O
is O
verbalized O
as O
“ O
the O
probability B
of O
y O
given O
x O
” O
, O
whereas O
the O
quantity O
p O
( O
x O
) O
is O
a O
marginal B
probability I
1.2. O
probability B
theory O
15 O
and O
is O
simply O
“ O
the O
probability B
of O
x O
” O
. O
we O
cycle O
through O
the O
training B
patterns O
in O
turn O
, O
and O
for O
each O
pattern O
xn O
we O
evaluate O
the O
perceptron B
function O
( O
4.52 O
) O
. O
( O
4.133 O
) O
( O
cid:13 O
) O
( O
cid:12 O
) O
−1 O
2 O
( O
cid:13 O
) O
( O
cid:12 O
) O
q O
( O
z O
) O
= O
|a|1/2 O
( O
2π O
) O
m/2 O
exp O
the O
distribution O
q O
( O
z O
) O
is O
proportional O
to O
f O
( O
z O
) O
and O
the O
appropriate O
normalization O
coef- O
ﬁcient O
can O
be O
found O
by O
inspection O
, O
using O
the O
standard O
result O
( O
2.43 O
) O
for O
a O
normalized O
multivariate O
gaussian O
, O
giving O
−1 O
2 O
( O
4.134 O
) O
where O
|a| O
denotes O
the O
determinant O
of O
a. O
this O
gaussian O
distribution O
will O
be O
well O
deﬁned O
provided O
its O
precision B
matrix I
, O
given O
by O
a O
, O
is O
positive B
deﬁnite I
, O
which O
implies O
that O
the O
stationary B
point O
z0 O
must O
be O
a O
local B
maximum O
, O
not O
a O
minimum O
or O
a O
saddle O
point O
. O
an O
introduction O
to O
probability B
theory O
and O
its O
applications O
( O
second O
ed O
. O
the O
image O
segmentation O
problem O
is O
in O
general O
extremely O
difﬁcult O
k O
= O
2 O
k O
= O
3 O
k O
= O
10 O
original O
image O
9.1. O
k-means O
clustering B
429 O
figure O
9.3 O
two O
examples O
of O
the O
application O
of O
the O
k-means O
clustering B
algorithm O
to O
image O
segmentation O
show- O
ing O
the O
initial O
images O
together O
with O
their O
k-means O
segmentations O
obtained O
using O
various O
values O
of O
k. O
this O
also O
illustrates O
of O
the O
use O
of O
vector B
quantization I
for O
data B
compression I
, O
in O
which O
smaller O
values O
of O
k O
give O
higher O
compression O
at O
the O
expense O
of O
poorer O
image O
quality O
. O
the O
distribution O
( O
2.154 O
) O
is O
called O
the O
normal-gamma O
or O
gaussian-gamma O
distribution O
and O
is O
plotted O
in O
figure O
2.14. O
note O
that O
this O
is O
not O
simply O
the O
product O
of O
an O
independent B
gaussian O
prior B
over O
µ O
and O
a O
gamma O
prior O
over O
λ O
, O
because O
the O
precision O
of O
µ O
is O
a O
linear O
function O
of O
λ. O
even O
if O
we O
chose O
a O
prior B
in O
which O
µ O
and O
λ O
were O
independent B
, O
the O
posterior O
distribution O
would O
exhibit O
a O
coupling O
between O
the O
precision O
of O
µ O
and O
the O
value O
of O
λ O
. O
an O
alternative O
way O
to O
reduce O
the O
number O
of O
independent B
parameters O
in O
a O
model O
is O
by O
sharing O
parameters O
( O
also O
known O
as O
tying O
of O
parameters O
) O
. O
finally O
, O
we O
note O
that O
if O
the O
joint O
distribution O
of O
two O
variables O
factorizes O
into O
the O
product O
of O
the O
marginals O
, O
so O
that O
p O
( O
x O
, O
y O
) O
= O
p O
( O
x O
) O
p O
( O
y O
) O
, O
then O
x O
and O
y O
are O
said O
to O
be O
independent B
. O
the O
issue O
of O
choosing O
an O
appropriate O
distribution O
relates O
to O
the O
problem O
of O
model O
selec- O
tion O
that O
has O
already O
been O
encountered O
in O
the O
context O
of O
polynomial B
curve I
ﬁtting I
in O
chapter O
1 O
and O
that O
is O
a O
central O
issue O
in O
pattern O
recognition O
. O
the O
green O
contours O
corresponding O
to O
1 O
, O
2 O
, O
and O
3 O
standard O
deviations O
for O
a O
correlated O
gaussian O
distribution O
p O
( O
z O
) O
over O
two O
variables O
z1 O
and O
z2 O
, O
and O
the O
red O
contours O
represent O
the O
corresponding O
levels O
for O
an O
q O
( O
z O
) O
approximating O
over O
the O
same O
variables O
given O
by O
the O
product O
of O
two O
independent B
univariate O
gaussian O
distributions O
whose O
parameters O
are O
obtained O
by O
minimization O
of O
the O
kullback- O
leibler O
divergence O
kl O
( O
q O
( O
cid:3 O
) O
p O
) O
, O
and O
the O
reverse O
kullback-leibler O
( O
b O
) O
divergence O
kl O
( O
p O
( O
cid:3 O
) O
q O
) O
. O
( O
5.16 O
) O
exercise O
5.2 O
following O
the O
same O
argument O
as O
for O
a O
single O
target O
variable O
, O
we O
see O
that O
the O
maximum B
likelihood I
weights O
are O
determined O
by O
minimizing O
the O
sum-of-squares B
error I
function O
( O
5.11 O
) O
. O
however O
, O
in O
general O
this O
will O
not O
be O
the O
case O
for O
the O
marginal B
likelihood I
function O
for O
the O
ob- O
served O
data O
. O
now O
, O
using O
the O
results O
( O
2.59 O
) O
, O
and O
( O
2.62 O
) O
, O
show O
that O
e O
[ O
xnxm O
] O
= O
µµt O
+ O
inmς O
( O
2.291 O
) O
where O
xn O
denotes O
a O
data O
point O
sampled O
from O
a O
gaussian O
distribution O
with O
mean B
µ O
and O
covariance B
σ O
, O
and O
inm O
denotes O
the O
( O
n O
, O
m O
) O
element O
of O
the O
identity O
matrix O
. O
ensemble O
learning B
for O
multi-layer O
networks O
. O
next O
, O
prove O
the O
results O
( O
10.220 O
) O
– O
( O
10.222 O
) O
by O
using O
( O
10.207 O
) O
and O
completing B
the I
square I
in O
the O
exponential O
. O
in O
practice O
, O
however O
, O
the O
class-conditional O
distributions O
may O
overlap O
, O
in O
which O
case O
exact O
separation O
of O
the O
training B
data O
can O
lead O
to O
poor O
generalization B
. O
as O
with O
the O
bayesian O
logistic B
regression I
model O
of O
section O
4.5 O
, O
if O
we O
are O
only O
interested O
in O
the O
decision B
boundary I
correspond- O
ing O
to O
p O
( O
tn O
+1|tn O
) O
= O
0.5 O
, O
then O
we O
need O
only O
consider O
the O
mean B
and O
we O
can O
ignore O
the O
effect O
of O
the O
variance B
. O
fortunately O
, O
we O
can O
address O
problems O
such O
as O
this O
much O
more O
sys- O
tematically O
by O
deﬁning O
a O
probabilistic O
model O
that O
captures O
the O
time O
evolution O
and O
measurement O
processes O
and O
then O
applying O
the O
inference B
and O
learning B
methods O
devel- O
oped O
in O
earlier O
chapters O
. O
( O
4.148 O
) O
we O
can O
evaluate O
p O
( O
a O
) O
by O
noting O
that O
the O
delta O
function O
imposes O
a O
linear O
constraint O
on O
w O
and O
so O
forms O
a O
marginal B
distribution O
from O
the O
joint O
distribution O
q O
( O
w O
) O
by O
inte- O
grating O
out O
all O
directions O
orthogonal O
to O
φ. O
because O
q O
( O
w O
) O
is O
gaussian O
, O
we O
know O
from O
section O
2.3.2 O
that O
the O
marginal B
distribution O
will O
also O
be O
gaussian O
. O
we O
saw O
that O
the O
simple O
polynomial O
kernel O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
contains O
only O
if O
we O
consider O
the O
slightly O
generalized B
kernel O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
with O
c O
> O
0 O
, O
then O
the O
corresponding O
feature O
mapping O
φ O
( O
x O
) O
contains O
con- O
terms O
of O
degree O
two O
. O
this O
is O
equivalent O
to O
minimizing O
the O
regularized O
error O
function O
e O
( O
w O
) O
= O
− O
ln O
p O
( O
d|w O
) O
+ O
α O
2 O
wtw O
( O
5.182 O
) O
and O
can O
be O
achieved O
using O
error B
backpropagation I
combined O
with O
standard O
optimiza- O
tion O
algorithms O
, O
as O
discussed O
in O
section O
5.3. O
having O
found O
a O
solution O
wmap O
for O
the O
weight B
vector I
, O
the O
next O
step O
is O
to O
eval- O
uate O
the O
hessian O
matrix O
h O
comprising O
the O
second O
derivatives O
of O
the O
negative O
log O
likelihood O
function O
. O
instead O
of O
mod- O
elling O
the O
density B
of O
data O
, O
however O
, O
these O
methods O
aim O
to O
ﬁnd O
a O
smooth O
boundary O
enclosing O
a O
region O
of O
high O
density B
. O
our O
probabilistic O
model O
speciﬁes O
the O
joint O
distribution O
p O
( O
x O
, O
z O
) O
, O
and O
our O
goal O
is O
to O
ﬁnd O
an O
approximation O
for O
the O
posterior O
distribution O
p O
( O
z|x O
) O
as O
well O
as O
for O
the O
model B
evidence I
p O
( O
x O
) O
. O
the O
conditional B
distribution O
of O
targets O
given O
inputs O
is O
then O
a O
bernoulli O
distribution O
of O
the O
form O
1 O
+ O
exp O
( O
−a O
) O
p O
( O
t|x O
, O
w O
) O
= O
y O
( O
x O
, O
w O
) O
t O
{ O
1 O
− O
y O
( O
x O
, O
w O
) O
} O
1−t O
. O
( O
1.121 O
) O
exercise O
1.41 O
58 O
1. O
introduction O
thus O
we O
can O
view O
the O
mutual B
information I
as O
the O
reduction O
in O
the O
uncertainty O
about O
x O
by O
virtue O
of O
being O
told O
the O
value O
of O
y O
( O
or O
vice O
versa O
) O
. O
before O
we O
observe O
any O
data O
, O
the O
prior B
probability O
of O
the O
fuel O
tank O
being O
empty O
is O
p O
( O
f O
= O
0 O
) O
= O
0.1. O
now O
suppose O
that O
we O
observe O
the O
fuel O
gauge O
and O
discover O
that O
it O
reads O
empty O
, O
i.e. O
, O
g O
= O
0 O
, O
corresponding O
to O
the O
middle O
graph O
in O
figure O
8.21. O
we O
can O
use O
bayes O
’ O
theorem O
to O
evaluate O
the O
posterior B
probability I
of O
the O
fuel O
tank O
being O
empty O
. O
in O
section O
3.3 O
, O
we O
developed O
a O
bayesian O
solution O
for O
a O
simple O
linear B
regression I
model O
under O
the O
assumption O
of O
gaussian O
noise O
. O
various O
other O
extensions O
of O
gaus- O
exercise O
6.23 O
figure O
6.8 O
illustration O
of O
gaussian O
process O
re- O
gression O
applied O
to O
the O
sinusoidal B
data I
set O
in O
figure O
a.6 O
in O
which O
the O
three O
right-most O
data O
points O
have O
been O
omitted O
. O
3.1.2 O
geometry O
of O
least O
squares O
3.1.3 O
3.1.4 O
regularized B
least I
squares I
. O
we O
have O
already O
seen O
many O
advantages O
of O
using O
probabilistic O
predictions O
in O
section O
1.5.4. O
here O
it O
will O
also O
provide O
us O
with O
a O
clearer O
motivation O
both O
for O
the O
choice O
of O
output O
unit O
nonlinearity O
and O
the O
choice O
of O
error B
function I
. O
thus O
we O
have O
class-conditional O
distributions O
of O
the O
form O
p O
( O
x|ck O
) O
= O
ki O
( O
1 O
− O
µki O
) O
1−xi O
µxi O
( O
4.81 O
) O
d O
( O
cid:14 O
) O
i=1 O
exercise O
4.10 O
section O
2.3.7 O
section O
8.2.2 O
which O
contain O
d O
independent B
parameters O
for O
each O
class O
. O
xi O
of O
xi O
as O
well O
as O
on O
the O
co-parents B
, O
in O
other O
words O
variables O
corresponding O
to O
parents O
of O
node B
xk O
other O
than O
node B
xi O
. O
1 O
4 O
× O
4 O
10 O
+ O
3 O
4 O
× O
6 O
10 O
( O
1.22 O
) O
1.2. O
probability B
theory O
17 O
suppose O
instead O
we O
are O
told O
that O
a O
piece O
of O
fruit O
has O
been O
selected O
and O
it O
is O
an O
orange O
, O
and O
we O
would O
like O
to O
know O
which O
box O
it O
came O
from O
. O
( O
6.36 O
) O
an O
application O
of O
fisher O
kernels O
to O
document B
retrieval I
is O
given O
by O
hofmann O
( O
2000 O
) O
. O
recall O
that O
the O
results O
for O
the O
conditional B
distribution O
are O
most O
easily O
expressed O
in O
terms O
of O
the O
partitioned B
precision O
matrix O
, O
using O
( O
2.73 O
) O
and O
( O
2.75 O
) O
. O
−5 O
0 O
θ O
5 O
x O
10 O
10.7.1 O
example O
: O
the O
clutter B
problem I
following O
minka O
( O
2001b O
) O
, O
we O
illustrate O
the O
ep O
algorithm O
using O
a O
simple O
exam- O
ple O
in O
which O
the O
goal O
is O
to O
infer O
the O
mean B
θ O
of O
a O
multivariate O
gaussian O
distribution O
over O
a O
variable O
x O
given O
a O
set O
of O
observations O
drawn O
from O
that O
distribution O
. O
( O
13.2 O
) O
section O
8.2 O
exercise O
13.1 O
n=2 O
from O
the O
d-separation B
property O
, O
we O
see O
that O
the O
conditional B
distribution O
for O
observa- O
tion O
xn O
, O
given O
all O
of O
the O
observations O
up O
to O
time O
n O
, O
is O
given O
by O
p O
( O
xn|x1 O
, O
. O
learning B
to O
ﬁnd O
pre-images O
. O
sequential B
learning I
is O
also O
appropriate O
for O
real- O
time O
applications O
in O
which O
the O
data O
observations O
are O
arriving O
in O
a O
continuous O
stream O
, O
and O
predictions O
must O
be O
made O
before O
all O
of O
the O
data O
points O
are O
seen O
. O
by O
−1 O
as O
the O
sum O
of O
a O
sym- O
writing O
the O
precision B
matrix I
( O
inverse B
covariance O
matrix O
) O
σ O
metric O
and O
an O
anti-symmetric O
matrix O
, O
show O
that O
the O
anti-symmetric O
term O
does O
not O
appear O
in O
the O
exponent O
of O
the O
gaussian O
, O
and O
hence O
that O
the O
precision B
matrix I
may O
be O
taken O
to O
be O
symmetric O
without O
loss O
of O
generality O
. O
we O
then O
ask O
if O
there O
exists O
a O
path O
that O
connects O
any O
node B
in O
a O
to O
any O
node B
in O
b. O
if O
there O
are O
no O
such O
paths O
, O
then O
the O
conditional B
independence I
property O
must O
hold O
. O
for O
batch O
optimization O
, O
there O
are O
more O
efﬁcient O
methods O
, O
such O
as O
conjugate B
gra- O
dients O
and O
quasi-newton O
methods O
, O
which O
are O
much O
more O
robust O
and O
much O
faster O
than O
simple O
gradient B
descent I
( O
gill O
et O
al. O
, O
1981 O
; O
fletcher O
, O
1987 O
; O
nocedal O
and O
wright O
, O
1999 O
) O
. O
in O
fact O
, O
the O
concept O
of O
entropy B
has O
much O
earlier O
origins O
in O
physics O
where O
it O
was O
introduced O
in O
the O
context O
of O
equilibrium O
thermodynamics O
and O
later O
given O
a O
deeper O
interpretation O
as O
a O
measure O
of O
disorder O
through O
developments O
in O
statistical O
mechanics O
. O
so O
far O
, O
we O
have O
assumed O
that O
the O
variance B
of O
the O
gaussian O
distribution O
over O
the O
data O
is O
known O
and O
our O
goal O
is O
to O
infer O
the O
mean B
. O
denoting O
these O
coefﬁcients O
by O
the O
vector O
w O
, O
show O
that O
the O
dual O
of O
the O
dual O
formulation O
is O
given O
by O
the O
original O
representation O
in O
terms O
of O
the O
parameter O
vector O
w. O
6.2 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
this O
exercise O
, O
we O
develop O
a O
dual O
formulation O
of O
the O
perceptron B
learning O
algorithm O
. O
1.5.4 O
inference B
and O
decision O
we O
have O
broken O
the O
classiﬁcation B
problem O
down O
into O
two O
separate O
stages O
, O
the O
inference B
stage O
in O
which O
we O
use O
training B
data O
to O
learn O
a O
model O
for O
p O
( O
ck|x O
) O
, O
and O
the O
exercise O
1.24 O
1.5. O
decision B
theory I
43 O
subsequent O
decision O
stage O
in O
which O
we O
use O
these O
posterior O
probabilities O
to O
make O
op- O
timal O
class O
assignments O
. O
this O
can O
be O
done O
by O
expressing O
the O
mixing O
coefﬁcients O
in O
terms O
of O
a O
set O
of O
auxiliary O
variables O
{ O
ηj O
} O
using O
the O
softmax B
function I
given O
by O
( O
cid:5 O
) O
m O
πj O
= O
exp O
( O
ηj O
) O
k=1 O
exp O
( O
ηk O
) O
. O
sam- O
ples O
of O
the O
regression B
function I
y O
( O
x O
, O
w O
) O
obtained O
by O
drawing O
samples O
of O
w O
from O
this O
posterior O
distribution O
are O
shown O
in O
the O
right-hand O
plot O
. O
, O
xm O
) O
= O
σ O
w0 O
+ O
wixi O
= O
σ O
( O
wtx O
) O
( O
8.10 O
) O
i=1 O
where O
σ O
( O
a O
) O
= O
( O
1+exp O
( O
−a O
) O
) O
−1 O
is O
the O
logistic B
sigmoid I
, O
x O
= O
( O
x0 O
, O
x1 O
, O
. O
by O
adapting O
these O
parameters O
to O
a O
data O
set O
using O
maximum B
likelihood I
, O
it O
becomes O
possible O
to O
detect O
input O
variables O
that O
have O
little O
effect O
on O
the O
predictive B
distribution I
, O
because O
the O
corresponding O
values O
of O
ηi O
will O
be O
small O
. O
i O
would O
also O
like O
to O
thank O
asela O
gunawardana O
for O
plotting O
the O
spectrogram B
in O
figure O
13.1 O
, O
and O
bernhard O
sch¨olkopf O
for O
permission O
to O
use O
his O
kernel O
pca O
code O
to O
plot O
fig- O
ure O
12.17. O
preface O
ix O
many O
people O
have O
helped O
by O
proofreading O
draft O
material O
and O
providing O
com- O
ments O
and O
suggestions O
, O
including O
shivani O
agarwal O
, O
c´edric O
archambeau O
, O
arik O
azran O
, O
andrew O
blake O
, O
hakan O
cevikalp O
, O
michael O
fourman O
, O
brendan O
frey O
, O
zoubin O
ghahra- O
mani O
, O
thore O
graepel O
, O
katherine O
heller O
, O
ralf O
herbrich O
, O
geoffrey O
hinton O
, O
adam O
jo- O
hansen O
, O
matthew O
johnson O
, O
michael O
jordan O
, O
eva O
kalyvianaki O
, O
anitha O
kannan O
, O
julia O
lasserre O
, O
david O
liu O
, O
tom O
minka O
, O
ian O
nabney O
, O
tonatiuh O
pena O
, O
yuan O
qi O
, O
sam O
roweis O
, O
balaji O
sanjiya O
, O
toby O
sharp O
, O
ana O
costa O
e O
silva O
, O
david O
spiegelhalter O
, O
jay O
stokes O
, O
tara O
symeonides O
, O
martin O
szummer O
, O
marshall O
tappen O
, O
ilkay O
ulusoy O
, O
chris O
williams O
, O
john O
winn O
, O
and O
andrew O
zisserman O
. O
instead O
, O
we O
make O
a O
linear O
approximation O
for O
the O
output O
unit O
activation O
in O
the O
form O
a O
( O
x O
, O
w O
) O
( O
cid:7 O
) O
amap O
( O
x O
) O
+ O
bt O
( O
w O
− O
wmap O
) O
( O
5.186 O
) O
where O
amap O
( O
x O
) O
= O
a O
( O
x O
, O
wmap O
) O
, O
and O
the O
vector O
b O
≡ O
∇a O
( O
x O
, O
wmap O
) O
can O
be O
found O
by O
backpropagation B
. O
the O
problem O
can O
be O
resolved O
by O
mod- O
elling O
state O
duration O
directly O
in O
which O
the O
diagonal B
coefﬁcients O
akk O
are O
all O
set O
to O
zero O
, O
and O
each O
state O
k O
is O
explicitly O
associated O
with O
a O
probability B
distribution O
p O
( O
t|k O
) O
of O
pos- O
sible O
duration O
times O
. O
which O
represent O
the O
independent B
noise O
variances O
for O
each O
of O
the O
variables O
, O
are O
called O
llniqllenesses O
. O
the O
proposal B
distribution I
itself O
is O
chosen O
to O
be O
sufﬁciently O
simple O
that O
it O
is O
straightforward O
to O
draw O
samples O
from O
it O
directly O
. O
although O
this O
graph O
looks O
messy O
, O
we O
can O
again O
appeal O
to O
d-separation B
to O
see O
that O
in O
fact O
it O
still O
has O
a O
simple O
probabilistic O
structure O
. O
this O
approach O
is O
used O
in O
radial B
basis I
function I
networks O
and O
also O
in O
support B
vector I
and O
relevance B
vector I
machines O
. O
useful O
texts O
on O
wavelets B
include O
ogden O
( O
1997 O
) O
, O
mallat O
( O
1999 O
) O
, O
and O
vidakovic O
( O
1999 O
) O
. O
a O
popular O
generative B
model I
for O
sequences O
is O
the O
hidden O
markov O
model O
, O
which O
expresses O
the O
distribution O
p O
( O
x O
) O
as O
a O
marginalization O
over O
a O
corresponding O
sequence O
of O
hidden O
states O
z O
= O
{ O
z1 O
, O
. O
we O
begin O
by O
looking O
in O
some O
detail O
at O
the O
support B
vector I
machine I
( O
svm O
) O
, O
which O
became O
popular O
in O
some O
years O
ago O
for O
solving O
problems O
in O
classiﬁcation B
, O
regression B
, O
and O
novelty B
detection I
. O
4.19 O
( O
( O
cid:12 O
) O
) O
www O
write O
down O
expressions O
for O
the O
gradient O
of O
the O
log O
likelihood O
, O
as O
well O
as O
the O
corresponding O
hessian O
matrix O
, O
for O
the O
probit B
regression I
model O
deﬁned O
in O
sec- O
tion O
4.3.5. O
these O
are O
the O
quantities O
that O
would O
be O
required O
to O
train O
such O
a O
model O
using O
irls O
. O
thus O
any O
set O
of O
points O
in O
the O
original O
two-dimensional O
space O
x O
would O
be O
constrained O
to O
lie O
exactly O
on O
a O
two-dimensional O
nonlinear O
manifold B
embedded O
in O
the O
six-dimensional O
feature B
space I
. O
for O
the O
multivariate O
gaus- O
sian O
, O
there O
are O
d2 O
second B
order I
moments O
given O
by O
e O
[ O
xixj O
] O
, O
which O
we O
can O
group O
together O
to O
form O
the O
matrix O
e O
[ O
xxt O
] O
. O
( O
5.161 O
) O
similarly O
, O
we O
shall O
choose O
a O
prior B
distribution O
over O
the O
weights O
w O
that O
is O
gaussian O
of O
the O
form O
( O
5.162 O
) O
for O
an O
i.i.d O
. O
2.4.3 O
noninformative B
priors O
in O
some O
applications O
of O
probabilistic O
inference O
, O
we O
may O
have O
prior B
knowledge O
that O
can O
be O
conveniently O
expressed O
through O
the O
prior B
distribution O
. O
by O
comparison O
, O
the O
compressed O
images O
require O
43 O
, O
248 O
bits B
( O
k O
= O
2 O
) O
, O
86 O
, O
472 O
bits B
( O
k O
= O
3 O
) O
, O
and O
173 O
, O
040 O
bits B
( O
k O
= O
10 O
) O
, O
respectively O
, O
to O
transmit O
. O
this O
is O
now O
combined O
with O
the O
factor O
fj O
( O
θ O
) O
to O
give O
a O
distribution O
( O
10.196 O
) O
\j O
( O
θ O
) O
= O
q O
( O
θ O
) O
( O
cid:4 O
) O
fj O
( O
θ O
) O
q O
fj O
( O
θ O
) O
q O
\j O
( O
θ O
) O
1 O
zj O
section O
10.7.1 O
508 O
10. O
approximate O
inference B
1 O
0.8 O
0.6 O
0.4 O
0.2 O
0 O
−2 O
−1 O
0 O
1 O
2 O
3 O
4 O
40 O
30 O
20 O
10 O
0 O
−2 O
−1 O
0 O
1 O
2 O
3 O
4 O
figure O
10.14 O
illustration O
of O
the O
expectation B
propagation I
approximation O
using O
a O
gaussian O
distribution O
for O
the O
example O
considered O
earlier O
in O
figures O
4.14 O
and O
10.1. O
the O
left-hand O
plot O
shows O
the O
original O
distribution O
( O
yellow O
) O
along O
with O
the O
laplace O
( O
red O
) O
, O
global O
variational O
( O
green O
) O
, O
and O
ep O
( O
blue O
) O
approximations O
, O
and O
the O
right-hand O
plot O
shows O
the O
corresponding O
negative O
logarithms O
of O
the O
distributions O
. O
5.3.1 O
evaluation O
of O
error-function O
derivatives O
we O
now O
derive O
the O
backpropagation B
algorithm O
for O
a O
general O
network O
having O
ar- O
bitrary O
feed-forward O
topology O
, O
arbitrary O
differentiable O
nonlinear O
activation O
functions O
, O
and O
a O
broad O
class O
of O
error B
function I
. O
our O
goal O
is O
to O
minimize O
the O
expected O
loss O
, O
which O
we O
have O
decomposed O
into O
the O
sum O
of O
a O
( O
squared O
) O
bias B
, O
a O
variance B
, O
and O
a O
constant O
noise O
term O
. O
( O
4.150 O
) O
note O
that O
the O
distribution O
of O
a O
takes O
the O
same O
form O
as O
the O
predictive B
distribution I
( O
3.58 O
) O
for O
the O
linear B
regression I
model O
, O
with O
the O
noise O
variance B
set O
to O
zero O
. O
for O
instance O
, O
it O
would O
be O
very O
inefﬁcient O
to O
maintain O
a O
full O
precision B
matrix I
for O
the O
gaussian O
distribution O
over O
a O
set O
of O
variables O
if O
the O
optimal O
form O
for O
that O
distribution O
always O
had O
a O
diago- O
nal O
precision B
matrix I
( O
corresponding O
to O
a O
factorization B
with O
respect O
to O
the O
individual O
variables O
described O
by O
that O
gaussian O
) O
. O
the O
joint O
probability B
distribution O
p O
( O
x O
, O
t O
) O
provides O
a O
complete O
summary O
of O
the O
uncertainty O
associated O
with O
these O
variables O
. O
( O
2πσ2 O
) O
n/2 O
exp O
n=1 O
n=1 O
we O
see O
that O
the O
likelihood B
function I
takes O
the O
form O
of O
the O
exponential O
of O
a O
quad- O
ratic O
form O
in O
µ. O
thus O
if O
we O
choose O
a O
prior B
p O
( O
µ O
) O
given O
by O
a O
gaussian O
, O
it O
will O
be O
a O
− O
1 O
2σ2 O
( O
xn O
− O
µ O
) O
2 O
. O
jn O
= O
vnat O
( O
pn O
) O
ξ O
( O
zn−1 O
, O
zn O
) O
= O
( O
cn O
) O
can O
be O
obtained O
from O
( O
13.65 O
) O
in O
the O
form O
for O
the O
em O
algorithm O
, O
we O
also O
require O
the O
pairwise O
posterior O
marginals O
, O
which O
−1 O
( O
cid:1 O
) O
α O
( O
zn−1 O
) O
p O
( O
xn|zn O
) O
p O
( O
zn|z−1 O
) O
( O
cid:1 O
) O
β O
( O
zn O
) O
n O
( O
zn−1|µn−1 O
, O
vn−1 O
) O
n O
( O
zn|azn−1 O
, O
γ O
) O
n O
( O
xn|czn O
, O
σ O
) O
n O
( O
zn| O
( O
cid:1 O
) O
µn O
, O
( O
cid:1 O
) O
vn O
) O
substituting O
for O
( O
cid:1 O
) O
α O
( O
zn O
) O
using O
( O
13.84 O
) O
and O
rearranging O
, O
we O
see O
that O
ξ O
( O
zn−1 O
, O
zn O
) O
is O
a O
cn O
( O
cid:1 O
) O
α O
( O
zn O
) O
gaussian O
with O
mean B
given O
with O
components O
γ O
( O
zn−1 O
) O
and O
γ O
( O
zn O
) O
, O
and O
a O
covariance B
between O
zn O
and O
zn−1 O
given O
by O
( O
13.103 O
) O
= O
. O
we O
have O
already O
highlighted O
the O
fact O
that O
the O
support B
vector I
machine I
does O
not O
provide O
probabilistic O
outputs O
but O
instead O
makes O
classiﬁcation B
decisions O
for O
new O
in- O
put O
vectors O
. O
one O
way O
to O
address O
this O
prob- O
lem O
is O
to O
use O
restricted O
forms O
of O
the O
covariance B
matrix I
. O
we O
have O
already O
discussed O
some O
of O
the O
beneﬁts O
of O
determining O
probabilities O
in O
sec- O
tion O
1.5.4. O
an O
alternative O
sparse O
kernel O
technique O
, O
known O
as O
the O
relevance B
vector I
machine I
( O
rvm O
) O
, O
is O
based O
on O
a O
bayesian O
formulation O
and O
provides O
posterior O
proba- O
bilistic O
outputs O
, O
as O
well O
as O
having O
typically O
much O
sparser O
solutions O
than O
the O
svm O
. O
conditional B
independence I
for O
statistical O
operations O
. O
we O
now O
see O
that O
there O
is O
a O
considerable O
simpliﬁcation O
if O
we O
choose O
a O
particular O
form O
for O
the O
link B
function I
f O
−1 O
( O
y O
) O
given O
by O
which O
gives O
f O
( O
ψ O
( O
y O
) O
) O
= O
y O
and O
hence O
f O
we O
have O
a O
= O
ψ O
and O
hence O
f O
function O
reduces O
to O
( O
cid:4 O
) O
( O
a O
) O
ψ O
−1 O
( O
y O
) O
= O
ψ O
( O
y O
) O
f O
( O
4.123 O
) O
−1 O
( O
y O
) O
, O
( O
cid:4 O
) O
( O
y O
) O
= O
1. O
in O
this O
case O
, O
the O
gradient O
of O
the O
error B
( O
cid:4 O
) O
( O
y O
) O
= O
1. O
also O
, O
because O
a O
= O
f O
( O
cid:4 O
) O
( O
ψ O
) O
ψ O
∇ O
ln O
e O
( O
w O
) O
= O
1 O
s O
{ O
yn O
− O
tn O
} O
φn O
. O
the O
ellipse O
shows O
a O
con- O
tour O
of O
constant O
error B
, O
and O
wml O
denotes O
the O
minimum O
of O
the O
er- O
ror O
function O
. O
they O
also O
allow O
us O
to O
be O
more O
explicit O
about O
the O
details O
of O
the O
factorization B
, O
as O
we O
shall O
see O
. O
it O
is O
worth O
emphasizing O
that O
we O
did O
not O
assume O
that O
q O
( O
zi O
) O
is O
gaussian O
, O
but O
rather O
we O
derived O
this O
result O
by O
variational B
optimization O
of O
the O
kl O
divergence O
over O
all O
possible O
distributions O
q O
( O
zi O
) O
. O
( O
6.29 O
) O
( O
cid:2 O
) O
i O
298 O
6. O
kernel O
methods O
section O
9.2 O
section O
13.2 O
exercise O
6.13 O
this O
is O
equivalent O
, O
up O
to O
an O
overall O
multiplicative O
constant O
, O
to O
a O
mixture B
distribution I
in O
which O
the O
components O
factorize O
, O
with O
the O
index O
i O
playing O
the O
role O
of O
a O
‘ O
latent O
’ O
variable O
. O
probability B
and O
the O
weighing O
of O
ev- O
idence O
. O
( O
13.21 O
) O
γ O
( O
znk O
) O
exercise O
13.8 O
section O
8.4 O
n O
( O
cid:2 O
) O
n=1 O
d O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
i=1 O
k=1 O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
γ O
( O
znk O
) O
xni O
for O
the O
case O
of O
discrete O
multinomial O
observed O
variables O
, O
the O
conditional B
distribution O
of O
the O
observations O
takes O
the O
form O
p O
( O
x|z O
) O
= O
µxizk O
ik O
( O
13.22 O
) O
and O
the O
corresponding O
m-step O
equations O
are O
given O
by O
µik O
= O
. O
exercise O
2.38 O
simple O
manipulation O
involving O
completing B
the I
square I
in O
the O
exponent O
shows O
that O
the O
posterior O
distribution O
is O
given O
by O
p O
( O
µ|x O
) O
= O
n O
( O
cid:10 O
) O
µ|µn O
, O
σ2 O
n O
( O
cid:11 O
) O
( O
2.138 O
) O
( O
2.139 O
) O
( O
2.140 O
) O
( O
2.141 O
) O
( O
2.142 O
) O
where O
µn O
= O
1 O
σ2 O
n O
= O
σ2 O
0 O
+ O
σ2 O
µ0 O
+ O
n O
σ2 O
+ O
n O
σ2 O
n O
σ2 O
n O
σ2 O
1 O
σ2 O
0 O
0 O
0 O
+ O
σ2 O
µml O
in O
which O
µml O
is O
the O
maximum B
likelihood I
solution O
for O
µ O
given O
by O
the O
sample B
mean I
n O
( O
cid:2 O
) O
µml O
= O
1 O
n O
xn O
. O
( O
cid:6 O
) O
q O
( O
z O
) O
ln O
( O
cid:12 O
) O
p O
( O
x O
, O
z O
) O
q O
( O
z O
) O
p O
( O
z|x O
) O
q O
( O
z O
) O
kl O
( O
q O
( O
cid:5 O
) O
p O
) O
= O
− O
q O
( O
z O
) O
ln O
( O
10.2 O
) O
( O
10.3 O
) O
( O
10.4 O
) O
this O
differs O
from O
our O
discussion O
of O
em O
only O
in O
that O
the O
parameter O
vector O
θ O
no O
longer O
appears O
, O
because O
the O
parameters O
are O
now O
stochastic B
variables O
and O
are O
absorbed O
into O
z. O
since O
in O
this O
chapter O
we O
will O
mainly O
be O
interested O
in O
continuous O
variables O
we O
have O
used O
integrations O
rather O
than O
summations O
in O
formulating O
this O
decomposition O
. O
then O
( O
2.246 O
) O
provides O
an O
estimate O
of O
the O
density B
associated O
with O
each O
class O
( O
cid:5 O
) O
p O
( O
x|ck O
) O
= O
kk O
nkv O
similarly O
, O
the O
unconditional O
density B
is O
given O
by O
p O
( O
x O
) O
= O
k O
n O
v O
while O
the O
class O
priors O
are O
given O
by O
p O
( O
ck O
) O
= O
nk O
n O
. O
exercise O
12.28 O
independent B
component I
analysis I
12.4.1 O
we O
begin O
by O
considering O
models O
in O
which O
the O
observed O
variables O
are O
related O
linearly O
to O
the O
latent O
variables O
, O
but O
for O
which O
the O
latent O
distribution O
is O
non-gaussian O
. O
another O
form O
of O
gem O
algorithm O
, O
known O
as O
the O
expectation B
conditional I
maximization I
, O
or O
ecm O
, O
algorithm O
, O
involves O
making O
several O
constrained O
optimizations O
within O
each O
m O
step O
( O
meng O
and O
rubin O
, O
1993 O
) O
. O
as O
with O
the O
classiﬁcation B
problem O
, O
we O
can O
either O
determine O
the O
appropriate O
prob- O
abilities O
and O
then O
use O
these O
to O
make O
optimal O
decisions O
, O
or O
we O
can O
build O
models O
that O
make O
decisions O
directly O
. O
a O
graph O
is O
said O
to O
be O
a O
d O
map O
( O
for O
‘ O
dependency B
map I
’ O
) O
of O
a O
distribution O
if O
every O
conditional B
independence I
statement O
satisﬁed O
by O
the O
distribution O
is O
reﬂected O
in O
the O
graph O
. O
section O
6.4.7 O
6.3. O
radial B
basis I
function I
networks O
in O
chapter O
3 O
, O
we O
discussed O
regression B
models O
based O
on O
linear O
combinations O
of O
ﬁxed O
basis O
functions O
, O
although O
we O
did O
not O
discuss O
in O
detail O
what O
form O
those O
basis O
functions O
might O
take O
. O
if O
we O
restrict O
the O
covariance B
matrix I
to O
be O
diagonal B
, O
then O
it O
has O
only O
d O
independent B
parameters O
, O
and O
so O
the O
number O
of O
parameters O
now O
grows O
linearly O
with O
dimensionality O
. O
one O
of O
the O
most O
important O
applications O
of O
the O
kalman O
ﬁlter O
is O
to O
tracking O
, O
and O
this O
is O
illustrated O
using O
a O
simple O
example O
of O
an O
object O
moving O
in O
two O
dimensions O
in O
figure O
13.22. O
so O
far O
, O
we O
have O
solved O
the O
inference B
problem O
of O
ﬁnding O
the O
posterior O
marginal O
for O
a O
node B
zn O
given O
observations O
from O
x1 O
up O
to O
xn O
. O
we O
can O
then O
deﬁne O
an O
objective O
function O
, O
sometimes O
called O
a O
distortion B
measure I
, O
given O
by O
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
j O
= O
rnk O
( O
cid:5 O
) O
xn O
− O
µk O
( O
cid:5 O
) O
2 O
( O
9.1 O
) O
which O
represents O
the O
sum O
of O
the O
squares O
of O
the O
distances O
of O
each O
data O
point O
to O
its O
n=1 O
k=1 O
9.1. O
k-means O
clustering B
425 O
assigned O
vector O
µk O
. O
however O
, O
it O
is O
found O
empirically O
that O
often O
none O
of O
the O
available O
splits O
produces O
a O
signiﬁcant O
reduction O
in O
error B
, O
and O
yet O
after O
several O
more O
splits O
a O
substantial O
error B
reduction O
is O
found O
. O
consider O
ﬁrst O
the O
maximization O
with O
respect O
to O
α O
, O
which O
can O
be O
done O
by O
analogy O
with O
the O
linear B
regression I
case O
discussed O
in O
section O
3.5.2. O
we O
ﬁrst O
deﬁne O
the O
eigenvalue O
equation O
( O
5.177 O
) O
where O
h O
is O
the O
hessian O
matrix O
comprising O
the O
second O
derivatives O
of O
the O
sum-of- O
squares O
error B
function I
, O
evaluated O
at O
w O
= O
wmap O
. O
( O
4.111 O
) O
one O
way O
to O
motivate O
an O
alternative O
choice O
for O
the O
link B
function I
is O
to O
consider O
a O
noisy O
threshold O
model O
, O
as O
follows O
. O
if O
the O
variables O
are O
not O
independent B
, O
we O
can O
gain O
some O
idea O
of O
whether O
they O
are O
‘ O
close O
’ O
to O
being O
indepen- O
dent O
by O
considering O
the O
kullback-leibler O
divergence O
between O
the O
joint O
distribution O
and O
the O
product O
of O
the O
marginals O
, O
given O
by O
( O
cid:6 O
) O
( O
cid:6 O
) O
i O
[ O
x O
, O
y O
] O
≡ O
kl O
( O
p O
( O
x O
, O
y O
) O
( O
cid:6 O
) O
p O
( O
x O
) O
p O
( O
y O
) O
) O
= O
− O
p O
( O
x O
, O
y O
) O
ln O
( O
cid:15 O
) O
( O
cid:16 O
) O
p O
( O
x O
) O
p O
( O
y O
) O
p O
( O
x O
, O
y O
) O
dx O
dy O
( O
1.120 O
) O
which O
is O
called O
the O
mutual B
information I
between O
the O
variables O
x O
and O
y. O
from O
the O
properties O
of O
the O
kullback-leibler O
divergence O
, O
we O
see O
that O
i O
( O
x O
, O
y O
) O
( O
cid:2 O
) O
0 O
with O
equal- O
ity O
if O
, O
and O
only O
if O
, O
x O
and O
y O
are O
independent B
. O
the O
largest O
entropy B
would O
arise O
from O
a O
uniform B
distribution I
that O
would O
give O
h O
= O
− O
ln O
( O
1/30 O
) O
= O
3.40. O
exercise O
1.29 O
from O
which O
we O
ﬁnd O
that O
all O
of O
the O
p O
( O
xi O
) O
are O
equal O
and O
are O
given O
by O
p O
( O
xi O
) O
= O
1/m O
where O
m O
is O
the O
total O
number O
of O
states O
xi O
. O
we O
can O
if O
we O
wish O
view O
the O
sum-product B
algorithm I
in O
a O
slightly O
different O
form O
by O
eliminating O
messages O
from O
variable O
nodes O
to O
factor O
nodes O
and O
simply O
considering O
messages O
that O
are O
sent O
out O
by O
factor O
nodes O
. O
10.1.4 O
model B
comparison I
as O
well O
as O
performing O
inference B
over O
the O
hidden O
variables O
z O
, O
we O
may O
also O
wish O
to O
compare O
a O
set O
of O
candidate O
models O
, O
labelled O
by O
the O
index O
m O
, O
and O
having O
prior B
probabilities O
p O
( O
m O
) O
. O
in O
1812 O
he O
published O
the O
ﬁrst O
edition O
of O
th´eorie O
analytique O
des O
probabilit´es O
, O
in O
which O
laplace O
states O
that O
“ O
probability B
theory O
is O
nothing O
but O
common O
sense O
reduced O
to O
calculation O
” O
. O
n=1 O
k=1 O
q O
( O
cid:1 O
) O
( O
z O
) O
= O
rznk O
nk O
n=1 O
k=1 O
( O
10.46 O
) O
( O
10.47 O
) O
( O
10.48 O
) O
exercise O
10.12 O
requiring O
that O
this O
distribution O
be O
normalized O
, O
and O
noting O
that O
for O
each O
value O
of O
n O
the O
quantities O
znk O
are O
binary O
and O
sum O
to O
1 O
over O
all O
values O
of O
k O
, O
we O
obtain O
where O
10.2. O
illustration O
: O
variational B
mixture O
of O
gaussians O
477 O
k O
( O
cid:2 O
) O
rnk O
= O
ρnk O
ρnj O
. O
( O
10.115 O
) O
n O
( O
cid:2 O
) O
( O
cid:26 O
) O
n=1 O
( O
cid:27 O
) O
( O
cid:27 O
) O
( O
cid:27 O
) O
( O
cid:27 O
) O
( O
cid:21 O
) O
thus O
we O
see O
that O
this O
decomposes O
into O
a O
sum O
of O
independent B
terms O
, O
one O
for O
each O
value O
of O
n O
, O
and O
hence O
the O
solution O
for O
q O
( O
cid:1 O
) O
( O
z O
) O
will O
factorize O
over O
n O
so O
that O
q O
( O
cid:1 O
) O
( O
z O
) O
= O
n O
q O
( O
cid:1 O
) O
( O
zn O
) O
. O
if O
we O
transform O
from O
cartesian O
to O
polar O
coordinates O
, O
and O
then O
integrate O
out O
the O
directional O
variables O
, O
we O
obtain O
an O
expression O
for O
the O
density B
p O
( O
r O
) O
as O
a O
function O
of O
radius O
r O
from O
the O
origin O
. O
, O
tn O
} O
, O
we O
can O
construct O
the O
corresponding O
likelihood B
function I
n O
( O
cid:14 O
) O
n=1 O
p O
( O
tn|xn O
, O
w O
, O
β O
) O
. O
8.4.2 O
trees O
8.4.3 O
8.4.4 O
the O
sum-product B
algorithm I
. O
this O
problem O
is O
known O
as O
section O
10.1 O
9.2. O
mixtures O
of O
gaussians O
435 O
identiﬁability B
( O
casella O
and O
berger O
, O
2002 O
) O
and O
is O
an O
important O
issue O
when O
we O
wish O
to O
interpret O
the O
parameter O
values O
discovered O
by O
a O
model O
. O
( O
5.90 O
) O
5.4. O
the O
hessian O
matrix O
253 O
again O
, O
by O
using O
a O
symmetrical O
central B
differences I
formulation O
, O
we O
ensure O
that O
the O
residual O
errors O
are O
o O
( O
2 O
) O
rather O
than O
o O
( O
 O
) O
. O
we O
have O
already O
seen O
that O
least-squares O
solutions O
lack O
robustness B
to O
outliers B
, O
and O
this O
applies O
equally O
to O
the O
classiﬁcation B
application O
, O
as O
illustrated O
in O
figure O
4.4. O
here O
we O
see O
that O
the O
additional O
data O
points O
in O
the O
right- O
hand O
ﬁgure O
produce O
a O
signiﬁcant O
change O
in O
the O
location O
of O
the O
decision B
boundary I
, O
even O
though O
these O
point O
would O
be O
correctly O
classiﬁed O
by O
the O
original O
decision O
bound- O
ary O
in O
the O
left-hand O
ﬁgure O
. O
appendix O
a O
section O
13.3 O
this O
takes O
the O
same O
form O
as O
( O
7.85 O
) O
in O
the O
regression B
case O
, O
and O
so O
we O
can O
apply O
the O
same O
analysis O
of O
sparsity B
and O
obtain O
the O
same O
fast O
learning O
algorithm O
in O
which O
we O
fully O
optimize O
a O
single O
hyperparameter B
αi O
at O
each O
step O
. O
if O
we O
con O
( O
cid:173 O
) O
sider O
the O
general O
case O
of O
an O
m O
-dimensional O
projection O
space O
, O
the O
optimal O
linear O
pro O
( O
cid:173 O
) O
jection O
for O
which O
the O
variance B
of O
the O
projected O
data O
is O
maximized O
is O
now O
defined O
by O
the O
m O
eigenvectors O
u O
1 O
, O
... O
, O
u O
m O
of O
the O
data O
covariance O
matrix O
s O
corresponding O
to O
the O
m O
largest O
eigenvalues O
> O
'1 O
, O
... O
, O
am O
. O
these O
are O
called O
chain O
graphs O
( O
lauritzen O
and O
wermuth O
, O
1989 O
; O
frydenberg O
, O
1990 O
) O
, O
and O
contain O
the O
directed B
and O
undirected B
graphs O
considered O
so O
far O
as O
special O
cases O
. O
y O
> O
0 O
y O
= O
0 O
y O
< O
0 O
x2 O
r1 O
r2 O
w O
x O
y O
( O
x O
) O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
x1 O
x⊥ O
−w0 O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
an O
arbitrary O
point O
x O
and O
let O
x⊥ O
be O
its O
orthogonal O
projection O
onto O
the O
decision B
surface I
, O
so O
that O
x O
= O
x⊥ O
+ O
r O
w O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
. O
on O
the O
right O
the O
distribution O
is O
given O
by O
a O
linear O
combination O
of O
two O
gaussians O
which O
has O
been O
ﬁtted O
to O
the O
data O
by O
maximum B
likelihood I
using O
techniques O
discussed O
chap- O
ter O
9 O
, O
and O
which O
gives O
a O
better O
rep- O
resentation O
of O
the O
data O
. O
in O
particular O
, O
we O
can O
determine O
the O
parameters O
θ O
of O
the O
model O
by O
maximizing O
the O
likelihood B
function I
l O
( O
θ O
) O
= O
p O
( O
x|u O
, O
θ O
) O
where O
u O
is O
a O
matrix O
whose O
rows O
are O
given O
by O
ut O
n. O
as O
a O
consequence O
of O
the O
conditional B
independence I
property O
( O
13.5 O
) O
this O
likelihood B
function I
can O
be O
maximized O
efﬁciently O
using O
an O
em O
algorithm O
in O
which O
the O
e O
step O
involves O
forward O
and O
backward O
recursions O
. O
this O
is O
simply O
the O
result O
that O
we O
derived O
previously O
and O
that O
shows O
that O
the O
optimal O
least O
squares O
predictor O
is O
given O
by O
the O
conditional B
mean O
. O
this O
noise O
might O
arise O
from O
intrinsically O
stochastic B
( O
i.e O
. O
the O
corresponding O
conditional B
distribution O
( O
6.48 O
) O
is O
given O
by O
a O
gaus- O
sian O
mixture B
, O
and O
is O
shown O
, O
together O
with O
the O
conditional B
mean O
, O
for O
the O
sinusoidal O
synthetic O
data O
set O
in O
figure O
6.3. O
an O
obvious O
extension O
of O
this O
model O
is O
to O
allow O
for O
more O
ﬂexible O
forms O
of O
gaus- O
sian O
components O
, O
for O
instance O
having O
different O
variance B
parameters O
for O
the O
input O
and O
target O
variables O
. O
l O
z1 O
the O
conditional B
distributions O
are O
gaussian O
, O
which O
represents O
a O
more O
general O
class O
of O
distributions O
than O
the O
multivariate O
gaussian O
because O
, O
for O
example O
, O
the O
non-gaussian O
distribution O
p O
( O
z O
, O
y O
) O
∝ O
exp O
( O
−z2y2 O
) O
has O
gaussian O
conditional B
distributions O
. O
( O
10.240 O
) O
exercises O
517 O
we O
recognize O
this O
as O
the O
sum-product O
rule O
in O
the O
form O
in O
which O
messages O
from O
vari- O
able O
nodes O
to O
factor O
nodes O
have O
been O
eliminated O
, O
as O
illustrated O
by O
the O
example O
shown O
in O
figure O
8.50. O
the O
quantity O
( O
cid:4 O
) O
fjm O
( O
θm O
) O
corresponds O
to O
the O
message O
µfj→θm O
( O
θm O
) O
, O
which O
factor O
node O
j O
sends O
to O
variable O
node B
m O
, O
and O
the O
product O
over O
k O
in O
( O
10.240 O
) O
is O
over O
all O
factors O
that O
depend O
on O
the O
variables O
θm O
that O
have O
variables O
( O
other O
than O
variable O
θl O
) O
in O
common O
with O
factor O
fj O
( O
θj O
) O
. O
we O
ﬁrst O
choose O
the O
initial O
latent B
variable I
z1 O
with O
probabilities O
governed O
by O
the O
parameters O
πk O
and O
then O
sample O
the O
corresponding O
observation O
x1 O
. O
to O
verify O
that O
the O
stationary B
point O
is O
indeed O
a O
maximum O
, O
we O
can O
evaluate O
the O
second O
derivative O
of O
the O
entropy B
, O
which O
gives O
∂ O
( O
cid:4 O
) O
h O
∂p O
( O
xi O
) O
∂p O
( O
xj O
) O
= O
−iij O
1 O
pi O
( O
1.100 O
) O
where O
iij O
are O
the O
elements O
of O
the O
identity O
matrix O
. O
we O
shall O
now O
go O
from O
this O
graph O
to O
the O
corresponding O
representation O
of O
the O
joint O
probability B
distribution O
written O
in O
terms O
of O
the O
product O
of O
a O
set O
of O
conditional B
dis- O
tributions O
, O
one O
for O
each O
node B
in O
the O
graph O
. O
( O
1.110 O
) O
thus O
we O
see O
again O
that O
the O
entropy B
increases O
as O
the O
distribution O
becomes O
broader O
, O
i.e. O
, O
as O
σ2 O
increases O
. O
thus O
, O
the O
vectors O
of O
weights O
which O
lead O
into O
the O
hidden O
units O
in O
figure O
12.18 O
form O
a O
basis O
set O
which O
spans O
the O
principal B
subspace I
. O
this O
is O
indeed O
the O
case O
and O
corresponds O
to O
undirected B
graphical O
models O
. O
5.41 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
following O
analogous O
steps O
to O
those O
given O
in O
section O
5.7.1 O
for B
regression I
networks O
, O
derive O
the O
result O
( O
5.183 O
) O
for O
the O
marginal B
likelihood I
in O
the O
case O
of O
a O
net- O
work O
having O
a O
cross-entropy B
error I
function I
and O
logistic-sigmoid O
output-unit O
activa- O
tion O
function O
. O
677 O
678 O
a. O
data O
sets O
figure O
a.1 O
one O
hundred O
examples O
of O
the O
mnist O
digits O
chosen O
at O
ran- O
dom O
from O
the O
training B
set I
. O
modelling O
conditional B
probability I
distributions O
for O
periodic O
variables O
. O
the O
computational O
cost O
is O
only O
twice O
that O
for O
ﬁnding O
the O
marginal B
of O
a O
single O
node B
, O
rather O
than O
n O
times O
as O
much O
. O
derivatives O
of O
the O
error B
with O
respect O
to O
the O
centres O
of O
the O
gaussians O
are O
also O
∂ O
( O
cid:4 O
) O
e O
∂µj O
( O
cid:2 O
) O
i O
= O
λ O
γj O
( O
wi O
) O
( O
µi O
− O
wj O
) O
σ2 O
j O
( O
5.142 O
) O
exercise O
5.30 O
easily O
computed O
to O
give O
which O
has O
a O
simple O
intuitive O
interpretation O
, O
because O
it O
pushes O
µj O
towards O
an O
aver- O
age O
of O
the O
weight O
values O
, O
weighted O
by O
the O
posterior O
probabilities O
that O
the O
respective O
weight O
parameters O
were O
generated O
by O
component O
j. O
similarly O
, O
the O
derivatives O
with O
respect O
to O
the O
variances O
are O
given O
by O
exercise O
5.31 O
( O
cid:15 O
) O
( O
cid:16 O
) O
∂ O
( O
cid:4 O
) O
e O
∂σj O
( O
cid:2 O
) O
i O
= O
λ O
γj O
( O
wi O
) O
− O
( O
wi O
− O
µj O
) O
2 O
σ3 O
j O
1 O
σj O
( O
5.143 O
) O
which O
drives O
σj O
towards O
the O
weighted O
average O
of O
the O
squared O
deviations O
of O
the O
weights O
around O
the O
corresponding O
centre O
µj O
, O
where O
the O
weighting O
coefﬁcients O
are O
again O
given O
by O
the O
posterior B
probability I
that O
each O
weight O
is O
generated O
by O
component O
j. O
note O
that O
in O
a O
practical O
implementation O
, O
new O
variables O
ηj O
deﬁned O
by O
σ2 O
j O
= O
exp O
( O
ηj O
) O
( O
5.144 O
) O
are O
introduced O
, O
and O
the O
minimization O
is O
performed O
with O
respect O
to O
the O
ηj O
. O
( O
c O
) O
as O
in O
( O
b O
) O
but O
showing O
a O
different O
local B
minimum I
of O
the O
kullback-leibler O
divergence O
. O
, O
yn O
is O
speciﬁed O
completely O
by O
the O
second-order O
statistics O
, O
namely O
the O
mean B
and O
the O
covariance B
. O
8.7 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
using O
the O
recursion O
relations O
( O
8.15 O
) O
and O
( O
8.16 O
) O
, O
show O
that O
the O
mean B
and O
covari- O
ance O
of O
the O
joint O
distribution O
for O
the O
graph O
shown O
in O
figure O
8.14 O
are O
given O
by O
( O
8.17 O
) O
and O
( O
8.18 O
) O
, O
respectively O
. O
section O
1.1 O
the O
goal O
in O
the O
curve B
ﬁtting I
problem O
is O
to O
be O
able O
to O
make O
predictions O
for O
the O
target O
variable O
t O
given O
some O
new O
value O
of O
the O
input O
variable O
x O
on O
the O
basis O
of O
a O
set O
of O
training B
data O
comprising O
n O
input O
values O
x O
= O
( O
x1 O
, O
. O
( O
2.115 O
) O
( O
2.116 O
) O
( O
2.117 O
) O
2.3.4 O
maximum B
likelihood I
for O
the O
gaussian O
given O
a O
data O
set O
x O
= O
( O
x1 O
, O
. O
this O
transformation O
can O
be O
exactly O
compensated O
by O
changing O
the O
sign O
of O
all O
of O
the O
weights O
leading O
out O
of O
that O
hidden B
unit I
. O
this O
is O
an O
example O
of O
a O
phenomenon O
called O
bias B
and O
is O
related O
to O
the O
problem O
of O
over-ﬁtting B
encountered O
in O
the O
context O
of O
polynomial B
curve I
ﬁtting I
. O
discuss O
the O
form O
of O
this O
gaussian O
distribution O
for O
m O
< O
d O
, O
for O
m O
= O
d O
, O
and O
for O
m O
> O
d. O
12.6 O
( O
* O
) O
imm O
draw O
a O
directed B
probabilistic O
graph O
for O
the O
probabilistic O
pca O
model O
described O
in O
section O
12.2 O
in O
which O
the O
components O
of O
the O
observed B
variable I
x O
are O
shown O
explicitly O
as O
separate O
nodes O
. O
the O
number O
in O
the O
top O
left O
of O
each O
diagram O
shows O
the O
num- O
ber O
of O
iterations O
of O
variational B
infer- O
ence O
. O
13.2.2 O
the O
forward-backward B
algorithm I
next O
we O
seek O
an O
efﬁcient O
procedure O
for O
evaluating O
the O
quantities O
γ O
( O
znk O
) O
and O
ξ O
( O
zn−1 O
, O
j O
, O
znk O
) O
, O
corresponding O
to O
the O
e O
step O
of O
the O
em O
algorithm O
. O
2 O
p O
( O
x|µ O
) O
= O
1 O
− O
µ O
2 O
1 O
+ O
µ O
2.3 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
in O
this O
exercise O
, O
we O
prove O
that O
the O
binomial B
distribution I
( O
2.9 O
) O
is O
nor- O
malized O
. O
9.2.2 O
em O
for O
gaussian O
mixtures O
an O
elegant O
and O
powerful O
method O
for O
ﬁnding O
maximum B
likelihood I
solutions O
for O
models O
with O
latent O
variables O
is O
called O
the O
expectation-maximization O
algorithm O
, O
or O
em O
algorithm O
( O
dempster O
et O
al. O
, O
1977 O
; O
mclachlan O
and O
krishnan O
, O
1997 O
) O
. O
write O
down O
the O
backpropagation B
equations O
for O
δkr O
, O
and O
hence O
derive O
a O
set O
of O
back- O
propagation O
equations O
for O
the O
evaluation O
of O
the O
φkr O
. O
atld O
is O
given O
by O
, O
, O
( O
, O
, O
) O
_ O
n O
{ O
xllf O
, O
c O
) O
( O
ius O
) O
12.2. O
probabilistic O
pea O
573 O
where O
the O
d O
x O
d O
covariance B
matrix I
c O
is O
defined O
by O
21. O
c O
= O
wwt O
+ O
0- O
( O
12.36 O
) O
this O
result O
can O
also O
be O
derived O
more O
directly O
by O
noting O
that O
the O
predictive B
distribution I
will O
be O
gaussian O
and O
then O
evaluating O
its O
mean B
and O
covariance B
using O
( O
12.33 O
) O
. O
the O
plots O
in O
figure O
3.8 O
only O
show O
the O
point-wise O
predictive O
variance O
as O
a O
func- O
tion O
of O
x. O
in O
order O
to O
gain O
insight O
into O
the O
covariance B
between O
the O
predictions O
at O
different O
values O
of O
x O
, O
we O
can O
draw O
samples O
from O
the O
posterior O
distribution O
over O
w O
, O
and O
then O
plot O
the O
corresponding O
functions O
y O
( O
x O
, O
w O
) O
, O
as O
shown O
in O
figure O
3.9 O
. O
due O
to O
the O
use O
of O
local O
receptive O
ﬁelds O
, O
the O
number O
of O
weights O
in O
the O
network O
is O
smaller O
than O
if O
the O
network O
were O
fully B
connected I
. O
14.7 O
( O
( O
cid:12 O
) O
) O
by O
making O
a O
variational B
minimization O
of O
the O
expected O
exponential O
error O
function O
given O
by O
( O
14.27 O
) O
with O
respect O
to O
all O
possible O
functions O
y O
( O
x O
) O
, O
show O
that O
the O
minimizing O
function O
is O
given O
by O
( O
14.28 O
) O
. O
show O
that O
the O
sequence O
of O
latent B
variable I
values O
obtained O
by O
maximizing O
each O
of O
these O
posterior O
distributions O
individually O
is O
the O
same O
as O
the O
most O
probable O
sequence O
of O
latent O
values O
. O
, O
xk O
) O
over O
k O
variables O
that O
factorizes O
according O
to O
( O
8.5 O
) O
corresponding O
to O
a O
directed B
acyclic I
graph I
. O
because O
the O
error B
function I
( O
5.153 O
) O
is O
composed O
of O
a O
sum O
of O
terms O
, O
one O
for O
each O
training B
data O
point O
, O
we O
can O
consider O
the O
derivatives O
for O
a O
particular O
pattern O
n O
and O
then O
ﬁnd O
the O
derivatives O
of O
e O
by O
summing O
over O
all O
patterns O
. O
we O
start O
with O
some O
initial O
parameter O
value O
θold O
, O
and O
in O
the O
ﬁrst O
e O
step O
we O
evaluate O
the O
poste- O
rior O
distribution O
over O
latent O
variables O
, O
which O
gives O
rise O
to O
a O
lower B
bound I
l O
( O
θ O
, O
θ O
( O
old O
) O
) O
whose O
value O
equals O
the O
log O
likelihood O
at O
θ O
( O
old O
) O
, O
as O
shown O
by O
the O
blue O
curve O
. O
on O
the O
geometry O
of O
feedforward O
neural B
network I
error O
surfaces O
. O
2.50 O
( O
( O
cid:12 O
) O
) O
show O
that O
in O
the O
limit O
ν O
→ O
∞ O
, O
the O
multivariate O
student O
’ O
s O
t-distribution O
( O
2.162 O
) O
reduces O
to O
a O
gaussian O
with O
mean B
µ O
and O
precision O
λ O
. O
if O
we O
convert O
a O
directed B
tree O
into O
an O
undirected B
graph I
, O
we O
see O
that O
the O
moralization B
step O
will O
not O
add O
any O
links O
as O
all O
nodes O
have O
at O
most O
one O
parent O
, O
and O
as O
a O
consequence O
the O
corresponding O
moralized O
graph O
will O
be O
an O
undirected B
tree O
. O
however O
, O
bayesian O
methods O
are O
intrinsically O
well O
suited O
to O
sequential B
learning I
in O
which O
the O
data O
points O
are O
processed O
one O
at O
a O
time O
and O
then O
discarded O
. O
from O
( O
3.90 O
) O
we O
see O
that O
the O
value O
of O
α O
that O
maximizes O
the O
marginal B
likelihood I
satisﬁes O
α O
= O
γ O
n O
mn O
mt O
. O
note O
that O
as O
long O
as O
the O
regularization B
parameter O
satisﬁes O
λ O
> O
0 O
, O
its O
precise O
value O
plays O
no O
role O
. O
13.27 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
linear B
dynamical I
system I
of O
the O
form O
discussed O
in O
sec- O
tion O
13.3 O
in O
which O
the O
amplitude O
of O
the O
observation O
noise O
goes O
to O
zero O
, O
so O
that O
σ O
= O
0. O
show O
that O
the O
posterior O
distribution O
for O
zn O
has O
mean B
xn O
and O
zero O
variance B
. O
the O
factor B
analysis I
model O
is O
explaining O
the O
observed O
covariance O
structure O
of O
the O
data O
by O
representing O
the O
independent B
vari O
( O
cid:173 O
) O
ance O
associated O
with O
each O
coordinate O
in O
the O
matrix O
1j O
. O
in O
particular O
, O
we O
shall O
choose O
a O
sum-of-squares B
error I
of O
the O
form O
1 O
n O
e O
( O
w O
) O
= O
`` O
2 O
l O
ily O
( O
xn O
, O
w O
) O
- O
xn O
11 O
2 O
• O
n=l O
( O
12.91 O
) O
if O
the O
hidden O
units O
have O
linear O
activations O
functions O
, O
then O
it O
can O
be O
shown O
that O
the O
error B
function I
has O
a O
unique O
global B
minimum I
, O
and O
that O
at O
this O
minimum O
the O
network O
performs O
a O
projection O
onto O
the O
m O
-dimensional O
subspace O
which O
is O
spanned O
by O
the O
first O
m O
principal O
components O
of O
the O
data O
( O
bourlard O
and O
kamp O
, O
1988 O
; O
baldi O
and O
hornik O
, O
1989 O
) O
. O
( O
3.109 O
) O
3.7 O
( O
( O
cid:12 O
) O
) O
by O
using O
the O
technique O
of O
completing B
the I
square I
, O
verify O
the O
result O
( O
3.49 O
) O
for O
the O
posterior O
distribution O
of O
the O
parameters O
w O
in O
the O
linear O
basis O
function O
model O
in O
which O
mn O
and O
sn O
are O
deﬁned O
by O
( O
3.50 O
) O
and O
( O
3.51 O
) O
respectively O
. O
the O
root O
of O
the O
regres- O
sion B
function O
corresponds O
to O
the O
maximum O
like- O
lihood O
estimator O
µml O
. O
the O
laplace O
approximation O
is O
based O
on O
a O
local B
quadratic O
expansion O
around O
a O
mode O
of O
the O
posterior O
distribution O
over O
weights O
. O
6.26 O
( O
( O
cid:12 O
) O
) O
using O
the O
result O
( O
2.115 O
) O
, O
derive O
the O
expressions O
( O
6.87 O
) O
and O
( O
6.88 O
) O
for O
the O
mean B
and O
variance B
of O
the O
posterior O
distribution O
p O
( O
an O
+1|tn O
) O
in O
the O
gaussian O
process O
clas- O
siﬁcation O
model O
. O
each O
< O
lata O
poinils O
plollfld O
at O
tile O
mean B
ot O
its O
posm'k O
> O
< O
dislribution O
in O
..tent O
s O
; O
> O
ace O
, O
tile O
`` O
'' O
, O
'' O
ineanty O
mlhe O
gtm O
1tlod8i._lha O
sepamlion O
betwoon O
the O
groups O
of O
data O
points O
to O
be O
... O
..n O
`` O
'' O
'' O
. O
, O
xn O
, O
zn+1|zn O
) O
622 O
13. O
sequential B
data I
figure O
13.13 O
illustration O
of O
the O
backward O
recursion O
( O
13.38 O
) O
for O
evaluation O
of O
the O
β O
variables O
. O
however O
, O
a O
more O
interesting O
, O
and O
more O
common O
, O
approach O
is O
to O
use O
the O
same O
set O
of O
basis O
functions O
to O
model O
all O
of O
the O
components O
of O
the O
target B
vector I
so O
that O
y O
( O
x O
, O
w O
) O
= O
wtφ O
( O
x O
) O
( O
3.31 O
) O
where O
y O
is O
a O
k-dimensional O
column O
vector O
, O
w O
is O
an O
m O
× O
k O
matrix O
of O
parameters O
, O
and O
φ O
( O
x O
) O
is O
an O
m-dimensional O
column O
vector O
with O
elements O
φj O
( O
x O
) O
, O
with O
φ0 O
( O
x O
) O
= O
1 O
as O
before O
. O
if O
we O
consider O
two O
nodes O
xi O
and O
xj O
that O
are O
not O
connected O
by O
a O
link B
, O
then O
these O
variables O
must O
be O
conditionally O
independent B
given O
all O
other O
nodes O
in O
the O
graph O
. O
( O
2.105 O
) O
( O
cid:16 O
) O
92 O
2. O
probability B
distributions O
similarly O
, O
we O
can O
ﬁnd O
the O
mean B
of O
the O
gaussian O
distribution O
over O
z O
by O
identify- O
ing O
the O
linear O
terms O
in O
( O
2.102 O
) O
, O
which O
are O
given O
by O
xtλµ O
− O
xtatlb O
+ O
ytlb O
= O
λµ O
− O
atlb O
lb O
. O
for O
the O
varia- O
tional O
gaussian O
mixture B
model I
the O
expected O
values O
of O
the O
mixing O
coefﬁcients O
in O
the O
posterior O
distribution O
are O
given O
by O
section O
10.4.1 O
section O
3.4 O
exercise O
10.15 O
( O
10.69 O
) O
consider O
a O
component O
for O
which O
nk O
( O
cid:7 O
) O
0 O
and O
αk O
( O
cid:7 O
) O
α0 O
. O
prior B
to O
boltzmann O
, O
the O
concept O
of O
en- O
tropy O
was O
already O
known O
from O
classical B
thermodynamics O
where O
it O
quantiﬁes O
the O
fact O
that O
when O
we O
take O
energy O
from O
a O
system O
, O
not O
all O
of O
that O
energy O
is O
typically O
available O
to O
do O
useful O
work O
. O
we O
shall O
see O
that O
the O
least O
squares O
approach O
to O
ﬁnding O
the O
model O
parameters O
represents O
a O
speciﬁc O
case O
of O
maximum B
likelihood I
( O
discussed O
in O
section O
1.2.5 O
) O
, O
and O
that O
the O
over-ﬁtting B
problem O
can O
be O
understood O
as O
a O
general O
property O
of O
maximum B
likelihood I
. O
this O
result O
is O
easily O
generalized B
to O
the O
multiclass B
case O
where O
again O
the O
maximum B
likelihood I
estimate O
of O
the O
prior B
probability O
associated O
with O
class O
ck O
is O
given O
by O
the O
fraction O
of O
the O
training B
set I
points O
assigned O
to O
that O
class O
. O
( O
5.57 O
) O
( O
cid:2 O
) O
n O
in O
the O
above O
derivation O
we O
have O
implicitly O
assumed O
that O
each O
hidden O
or O
output O
unit O
in O
the O
network O
has O
the O
same O
activation B
function I
h O
( O
· O
) O
. O
now O
let O
us O
suppose O
that O
the O
mean B
is O
known O
and O
we O
wish O
to O
infer O
the O
variance B
. O
first O
show O
that O
n O
( O
d O
, O
m O
) O
satisﬁes O
m O
( O
cid:2 O
) O
n O
( O
d O
, O
m O
) O
= O
n O
( O
d O
, O
m O
) O
( O
1.138 O
) O
where O
n O
( O
d O
, O
m O
) O
is O
the O
number O
of O
independent B
parameters O
in O
the O
term O
of O
order O
m. O
now O
make O
use O
of O
the O
result O
( O
1.137 O
) O
, O
together O
with O
proof O
by O
induction O
, O
to O
show O
that O
m=0 O
n O
( O
d O
, O
m O
) O
= O
( O
d O
+ O
m O
) O
! O
d O
! O
m O
! O
. O
in O
a O
bayesian O
setting O
, O
however O
, O
we O
marginalize O
over O
all O
possible O
484 O
10. O
approximate O
inference B
figure O
10.7 O
plot O
of O
the O
variational B
lower O
bound O
l O
versus O
the O
number O
k O
of O
com- O
ponents O
in O
the O
gaussian O
mixture B
model I
, O
for O
the O
old O
faithful O
data O
, O
showing O
a O
distinct O
peak O
at O
k O
= O
2 O
components O
. O
note O
that O
this O
simply O
multiplies O
the O
error B
function I
by O
a O
factor O
of O
2 O
and O
so O
is O
equivalent O
to O
using O
the O
original O
error B
function I
. O
finally O
, O
the O
technique O
of O
reinforcement B
learning I
( O
sutton O
and O
barto O
, O
1998 O
) O
is O
con- O
cerned O
with O
the O
problem O
of O
ﬁnding O
suitable O
actions O
to O
take O
in O
a O
given O
situation O
in O
order O
to O
maximize O
a O
reward O
. O
for O
instance O
, O
to O
separate O
two O
classes O
whose O
optimal O
decision B
boundary I
runs O
at O
45 O
degrees O
to O
the O
axes O
would O
need O
a O
large O
number O
of O
axis-parallel O
splits O
of O
the O
input O
space O
as O
compared O
to O
a O
single O
non-axis-aligned O
split O
. O
an O
invariant O
form O
for O
the O
prior B
probability O
in O
estimation O
problems O
. O
suppose O
the O
true O
regression B
function I
that O
we O
are O
trying O
to O
predict O
is O
given O
by O
h O
( O
x O
) O
, O
so O
that O
the O
output O
of O
each O
of O
the O
models O
can O
be O
written O
as O
the O
true O
value O
plus O
an O
error B
in O
the O
form O
ym O
( O
x O
) O
= O
h O
( O
x O
) O
+ O
m O
( O
x O
) O
. O
this O
is O
in O
contrast O
to O
directed B
graphs O
in O
which O
each O
factor O
represents O
the O
conditional B
distribu- O
tion O
of O
the O
corresponding O
variable O
, O
conditioned O
on O
the O
state O
of O
its O
parents O
. O
by O
differentiating O
( O
1.27 O
) O
, O
show O
that O
the O
location O
( O
cid:1 O
) O
y O
of O
the O
maximum O
of O
the O
density B
in O
y O
is O
not O
in O
general O
related O
to O
the O
location O
( O
cid:1 O
) O
x O
of O
the O
maximum O
of O
the O
density B
over O
x O
by O
the O
simple O
functional B
relation O
( O
cid:1 O
) O
x O
= O
g O
( O
( O
cid:1 O
) O
y O
) O
as O
a O
consequence O
of O
the O
jacobian O
factor O
. O
µml O
p O
( O
z|µ O
) O
µ O
as O
a O
speciﬁc O
example O
, O
we O
consider O
once O
again O
the O
sequential B
estimation I
of O
the O
mean B
of O
a O
gaussian O
distribution O
, O
in O
which O
case O
the O
parameter O
θ O
( O
n O
) O
is O
the O
estimate O
( O
n O
) O
ml O
of O
the O
mean B
of O
the O
gaussian O
, O
and O
the O
random O
variable O
z O
is O
given O
by O
µ O
1 O
σ2 O
( O
x O
− O
µml O
) O
. O
the O
eigenvalues O
λi O
measure O
the O
curvature O
of O
the O
likelihood B
function I
, O
and O
so O
in O
figure O
3.15 O
the O
eigenvalue O
λ1 O
is O
small O
compared O
with O
λ2 O
( O
because O
a O
smaller O
curvature O
corresponds O
to O
a O
greater O
elongation O
of O
the O
contours O
of O
the O
likelihood O
func- O
tion O
) O
. O
9.22 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
maximization O
of O
the O
expected O
complete-data O
log O
likelihood O
deﬁned O
by O
( O
9.66 O
) O
, O
derive O
the O
m O
step O
equations O
( O
9.67 O
) O
and O
( O
9.68 O
) O
for O
re-estimating O
the O
hyperpa- O
rameters O
of O
the O
relevance B
vector I
machine I
for O
regression B
. O
the O
average O
amount O
of O
information O
that O
they O
transmit O
in O
the O
process O
is O
obtained O
by O
taking O
the O
expectation B
of O
( O
1.92 O
) O
with O
respect O
to O
the O
distribution O
p O
( O
x O
) O
and O
is O
given O
by O
( O
cid:2 O
) O
h O
[ O
x O
] O
= O
− O
p O
( O
x O
) O
log2 O
p O
( O
x O
) O
. O
now O
sup- O
pose O
we O
restrict O
attention O
to O
approximations O
in O
which O
the O
factors O
themselves O
factorize O
with O
respect O
to O
the O
individual O
variables O
so O
that O
q O
( O
x O
) O
∝ O
( O
cid:4 O
) O
fa1 O
( O
x1 O
) O
( O
cid:4 O
) O
fa2 O
( O
x2 O
) O
( O
cid:4 O
) O
fb2 O
( O
x2 O
) O
( O
cid:4 O
) O
fb3 O
( O
x3 O
) O
( O
cid:4 O
) O
fc2 O
( O
x2 O
) O
( O
cid:4 O
) O
fc4 O
( O
x4 O
) O
( O
10.227 O
) O
which O
corresponds O
to O
the O
factor B
graph I
shown O
on O
the O
right O
in O
figure O
10.18. O
because O
the O
individual O
factors O
are O
factorized O
, O
the O
overall O
distribution O
q O
( O
x O
) O
is O
itself O
fully O
fac- O
torized O
. O
, O
m. O
the O
committee B
prediction O
is O
given O
by O
ym O
( O
x O
) O
. O
( O
13.72 O
) O
using O
bayes O
’ O
theorem O
this O
can O
be O
expressed O
in O
terms O
of O
the O
sequence O
probabilities O
associated O
with O
the O
hidden O
markov O
models O
r O
( O
cid:2 O
) O
r=1 O
( O
cid:24 O
) O
r O
( O
cid:2 O
) O
r=1 O
ln O
( O
cid:25 O
) O
( O
cid:5 O
) O
m O
p O
( O
xr|θr O
) O
p O
( O
mr O
) O
l=1 O
p O
( O
xr|θl O
) O
p O
( O
lr O
) O
where O
p O
( O
m O
) O
is O
the O
prior B
probability O
of O
class O
m. O
optimization O
of O
this O
cost B
function I
is O
more O
complex O
than O
for O
maximum O
likelihood O
( O
kapadia O
, O
1998 O
) O
, O
and O
in O
particular O
( O
13.73 O
) O
632 O
13. O
sequential B
data I
figure O
13.17 O
section O
of O
an O
autoregressive O
hidden O
markov O
model O
, O
in O
which O
the O
distribution O
of O
the O
observation O
xn O
depends O
on O
a O
subset O
of O
the O
previous O
observations O
as O
well O
as O
on O
the O
hidden O
state O
zn O
. O
the O
marginal B
distribution O
over O
z O
is O
given O
by O
( O
cid:6 O
) O
ep O
( O
z O
) O
( O
cid:6 O
) O
( O
cid:1 O
) O
p O
( O
z O
, O
u O
) O
du O
= O
if O
0 O
( O
cid:1 O
) O
u O
( O
cid:1 O
) O
( O
cid:4 O
) O
p O
( O
z O
) O
( O
cid:4 O
) O
p O
( O
z O
) O
1/zp O
0 O
otherwise O
= O
p O
( O
z O
) O
du O
= O
1 O
zp O
0 O
zp O
( O
11.52 O
) O
values O
. O
bayesian O
parameter O
estimation O
via O
variational B
methods O
. O
although O
exact O
integration O
is O
intractable O
, O
we O
can O
use O
variational B
methods O
to O
ﬁnd O
a O
tractable O
approximation O
. O
correspondingly O
, O
the O
m O
step O
represents O
a O
re-estimation O
of O
the O
principal O
580 O
12. O
contlnljoljs O
i O
'' O
ht O
; O
i'it O
vi\ O
riarles O
fig O
'' O
.. O
12.11 O
probabilistic O
pca O
visoo O
, O
zsbon O
01 O
a O
portion O
0i1he O
`` O
'' O
! O
low O
data O
setlo O
< O
ihe O
! O
irsl O
100 O
( O
lata O
» O
einls O
, O
the O
left.. O
, O
... O
nd O
plot O
oiiows O
ihe O
i'o O
'' O
leoo O
< O
mean B
proj9c1ions O
oilhfi O
( O
lata O
poims O
on O
lhe O
principal B
subspace I
. O
2. O
initialize O
using O
one O
basis B
function I
ϕ1 O
, O
with O
hyperparameter B
α1 O
set O
using O
( O
7.101 O
) O
, O
with O
the O
remaining O
hyperparameters O
αj O
for O
j O
( O
cid:9 O
) O
= O
i O
initialized O
to O
inﬁnity O
, O
so O
that O
only O
ϕ1 O
is O
included O
in O
the O
model O
. O
we O
shall O
see O
later O
how O
to O
give O
a O
probabilistic O
interpretation O
to O
a O
neural B
network I
. O
the O
maximum B
margin I
solution O
can O
be O
motivated O
us- O
ing O
computational O
learning B
theory O
, O
also O
known O
as O
statistical B
learning I
theory I
. O
for O
instance O
, O
the O
region O
x1 O
( O
cid:1 O
) O
θ1 O
is O
further O
subdivided O
according O
to O
whether O
x2 O
( O
cid:1 O
) O
θ2 O
or O
x2 O
> O
θ2 O
, O
giving O
rise O
to O
the O
regions O
denoted O
a O
and O
b. O
the O
recursive O
subdivision O
can O
be O
described O
by O
the O
traversal O
of O
the O
binary O
tree O
shown O
in O
figure O
14.6. O
for O
any O
new O
input O
x O
, O
we O
determine O
which O
region O
it O
falls O
into O
by O
starting O
at O
the O
top O
of O
the O
tree B
at O
the O
root B
node I
and O
following O
a O
path O
down O
to O
a O
speciﬁc O
leaf O
node B
according O
to O
the O
decision O
criteria O
at O
each O
node B
. O
( O
1.150 O
) O
1.22 O
( O
( O
cid:1 O
) O
) O
www O
given O
a O
loss B
matrix I
with O
elements O
lkj O
, O
the O
expected O
risk O
is O
minimized O
if O
, O
for O
each O
x O
, O
we O
choose O
the O
class O
that O
minimizes O
( O
1.81 O
) O
. O
however O
, O
if O
the O
variance B
parameter O
is O
large O
, O
then O
the O
rejection O
rate O
will O
be O
high O
because O
, O
in O
the O
kind O
of O
complex O
problems O
we O
are O
considering O
, O
many O
of O
the O
proposed O
steps O
will O
be O
to O
states O
for O
which O
the O
probability B
p O
( O
z O
) O
is O
low O
. O
statistics O
and O
probability B
letters O
4 O
, O
53–56 O
. O
finally O
, O
show O
that O
the O
probability B
− O
32 O
2σ2 O
( O
cid:15 O
) O
( O
cid:16 O
) O
( O
1.148 O
) O
( O
1.149 O
) O
we O
therefore O
see O
that O
most O
of O
the O
probability B
mass O
in O
a O
high-dimensional O
gaussian O
distribution O
is O
located O
at O
a O
different O
radius O
from O
the O
region O
of O
high O
probability B
density O
. O
adaptive B
rejection I
sampling I
for O
gibbs O
sampling O
. O
if O
our O
goal O
is O
data B
compression I
. O
this O
can O
be O
done O
, O
for O
instance O
, O
using O
the O
exact O
method O
of O
sec- O
tion O
5.4.5 O
, O
or O
using O
the O
outer B
product I
approximation I
given O
by O
( O
5.85 O
) O
. O
in O
particular O
, O
there O
is O
no O
‘ O
over-ﬁtting B
’ O
associated O
with O
highly O
ﬂexible O
dis- O
tributions O
. O
any O
regularizer O
should O
be O
consistent B
with O
this O
property O
, O
otherwise O
it O
arbitrarily O
favours O
one O
solution O
over O
another O
, O
equivalent O
one O
. O
under O
( O
cid:1 O
) O
p O
( O
z O
, O
u O
) O
invariant O
, O
which O
can O
be O
achieved O
by O
ensuring O
that O
detailed O
balance O
is O
in O
practice O
, O
it O
can O
be O
difﬁcult O
to O
sample O
directly O
from O
a O
slice O
through O
the O
distribu- O
tion O
and O
so O
instead O
we O
deﬁne O
a O
sampling O
scheme O
that O
leaves O
the O
uniform B
distribution I
straightforward O
. O
as O
we O
have O
seen O
there O
is O
a O
close O
similarity O
between O
the O
variational B
solution O
for O
the O
bayesian O
mixture O
of O
gaussians O
and O
the O
em O
algorithm O
for O
maximum O
likelihood O
. O
once O
we O
know O
the O
posterior O
distribution O
over O
models O
, O
the O
predictive B
distribution I
is O
given O
, O
from O
the O
sum O
and O
product O
rules O
, O
by O
l O
( O
cid:2 O
) O
( O
cid:6 O
) O
p O
( O
t|x O
, O
d O
) O
= O
p O
( O
t|x O
, O
mi O
, O
d O
) O
p O
( O
mi|d O
) O
. O
for O
the O
moment O
, O
however O
, O
we O
simply O
note O
that O
in O
applying O
maximum B
likelihood I
to O
gaussian O
mixture B
models O
we O
must O
take O
steps O
to O
avoid O
ﬁnding O
such O
pathological O
solutions O
and O
instead O
seek O
local B
maxima O
of O
the O
likelihood B
function I
that O
are O
well O
behaved O
. O
to O
ﬁnd O
a O
conjugate B
prior I
, O
we O
consider O
the O
dependence O
of O
the O
likelihood B
function I
on O
µ O
and O
λ O
( O
cid:13 O
) O
n O
( O
cid:14 O
) O
n=1 O
( O
cid:15 O
) O
( O
cid:15 O
) O
( O
cid:12 O
) O
( O
cid:16 O
) O
1/2 O
( O
cid:16 O
) O
( O
cid:30 O
) O
n O
exp O
− O
λ O
2 O
( O
cid:24 O
) O
p O
( O
x|µ O
, O
λ O
) O
= O
( O
cid:29 O
) O
∝ O
λ1/2 O
exp O
( O
xn O
− O
µ O
) O
2 O
n O
( O
cid:2 O
) O
n=1 O
exp O
λµ O
xn O
− O
λ O
2 O
( O
cid:25 O
) O
x2 O
n O
. O
the O
choice O
of O
basis O
for O
the O
logarithm O
is O
arbitrary O
, O
and O
for O
the O
moment O
we O
shall O
adopt O
the O
convention O
prevalent O
in O
information B
theory I
of O
using O
logarithms O
to O
the O
base O
of O
2. O
in O
this O
case O
, O
as O
we O
shall O
see O
shortly O
, O
the O
units O
of O
h O
( O
x O
) O
are O
bits B
( O
‘ O
binary O
digits O
’ O
) O
. O
first O
of O
all O
, O
the O
data O
vectors O
{ O
xn O
} O
typically O
lie O
close O
to O
a O
non- O
linear O
manifold O
whose O
intrinsic B
dimensionality I
is O
smaller O
than O
that O
of O
the O
input O
space O
as O
a O
result O
of O
strong O
correlations O
between O
the O
input O
variables O
. O
the O
relevance B
vector I
machine I
is O
a O
speciﬁc O
instance O
of O
this O
model O
, O
which O
is O
in- O
tended O
to O
mirror O
the O
structure O
of O
the O
support B
vector I
machine I
. O
williams O
( O
1998 O
) O
has O
given O
explicit O
forms O
for O
the O
covariance B
in O
the O
case O
of O
two O
speciﬁc O
choices O
for O
the O
hidden B
unit I
activation O
function O
( O
probit O
and O
gaussian O
) O
. O
following O
the O
discussions O
in O
section O
1.2.5 O
and O
3.1 O
, O
we O
assume O
that O
t O
has O
a O
gaussian O
distribution O
with O
an O
x- O
dependent O
mean B
, O
which O
is O
given O
by O
the O
output O
of O
the O
neural B
network I
, O
so O
that O
p O
( O
t|x O
, O
w O
) O
= O
n O
( O
cid:10 O
) O
t|y O
( O
x O
, O
w O
) O
, O
β O
−1 O
( O
cid:11 O
) O
( O
5.12 O
) O
where O
β O
is O
the O
precision O
( O
inverse B
variance O
) O
of O
the O
gaussian O
noise O
. O
the O
latter O
problem O
can O
be O
alleviated O
by O
organizing O
the O
pairwise O
classiﬁers O
into O
a O
directed B
acyclic I
graph I
( O
not O
to O
be O
confused O
with O
a O
probabilistic B
graphical I
model I
) O
leading O
to O
the O
dagsvm O
( O
platt O
et O
al. O
, O
2000 O
) O
. O
the O
technique O
of O
backpropagation B
can O
also O
be O
applied O
to O
the O
calculation O
of O
other O
deriva- O
tives O
. O
here O
we O
shall O
con- O
sider O
a O
network O
having O
a O
single O
logistic B
sigmoid I
output O
corresponding O
to O
a O
two-class O
classiﬁcation B
problem O
. O
3 O
2 O
1 O
0 O
−1 O
−2 O
−2 O
−1 O
0 O
1 O
2 O
symmetries B
, O
and O
thus O
any O
given O
weight B
vector I
will O
be O
one O
of O
a O
set O
2m O
equivalent O
weight O
vectors O
. O
( O
4.74 O
) O
tn O
lnn O
( O
xn|µ1 O
, O
σ O
) O
= O
−1 O
2 O
n=1 O
n=1 O
setting O
the O
derivative B
with O
respect O
to O
µ1 O
to O
zero O
and O
rearranging O
, O
we O
obtain O
( O
4.75 O
) O
which O
is O
simply O
the O
mean B
of O
all O
the O
input O
vectors O
xn O
assigned O
to O
class O
c1 O
. O
each O
gaussian O
density B
n O
( O
x|µk O
, O
σk O
) O
is O
called O
a O
component O
of O
the O
mixture B
and O
has O
its O
own O
mean B
µk O
and O
covariance B
σk O
. O
asymptotic O
theory B
for O
prin- O
cipal O
component O
analysis O
. O
clearly O
, O
simple O
weight B
decay I
( O
5.112 O
) O
, O
that O
treats O
all O
weights O
and O
biases O
on O
an O
equal O
footing O
, O
does O
not O
satisfy O
this O
property O
. O
however O
, O
when O
the O
mixing O
coefﬁcients O
are O
input O
dependent O
, O
this O
hierarchical B
model O
becomes O
nontrivial O
. O
for O
discrete O
variables O
, O
a O
simple O
approach O
is O
called O
uniform B
sampling I
. O
describe O
how O
the O
parameters O
w O
can O
be O
learned O
from O
data O
using O
maximum B
likelihood I
. O
learning B
internal O
representations O
by O
er- O
ror O
propagation O
. O
we O
shall O
take O
the O
marginal B
and O
conditional B
distributions O
to O
be O
p O
( O
x O
) O
= O
n O
( O
cid:10 O
) O
p O
( O
y|x O
) O
= O
n O
( O
cid:10 O
) O
( O
cid:11 O
) O
( O
cid:11 O
) O
−1 O
x|µ O
, O
λ O
y|ax O
+ O
b O
, O
l−1 O
where O
µ O
, O
a O
, O
and O
b O
are O
parameters O
governing O
the O
means O
, O
and O
λ O
and O
l O
are O
precision O
matrices O
. O
using O
the O
technique O
of O
proof O
by O
induction O
, O
we O
can O
show O
from O
( O
1.114 O
) O
that O
a O
convex B
function I
f O
( O
x O
) O
satisﬁes O
( O
cid:22 O
) O
m O
( O
cid:2 O
) O
( O
cid:23 O
) O
m O
( O
cid:2 O
) O
i=1 O
f O
λixi O
( O
cid:5 O
) O
i O
λi O
= O
1 O
, O
for O
any O
set O
of O
points O
{ O
xi O
} O
. O
at O
each O
stage O
there O
is O
a O
larger O
degree O
of O
invariance B
to O
input O
trans- O
formations O
compared O
to O
the O
previous O
layer O
. O
( O
12.3 O
) O
appendix O
e O
we O
now O
maximize O
the O
projected O
variance B
ufsul O
with O
respect O
to O
ul O
. O
the O
presence O
of O
this O
normalization O
constant O
is O
one O
of O
the O
major O
limitations O
of O
undirected B
graphs O
. O
in O
the O
speech B
recognition I
context O
, O
warping O
of O
the O
time O
axis O
is O
associated O
with O
natural O
variations O
in O
the O
speed O
of O
speech O
, O
and O
again O
the O
hidden O
markov O
model O
can O
accommodate O
such O
a O
distortion O
and O
not O
penalize O
it O
too O
heavily O
. O
the O
perceptron B
criterion I
is O
therefore O
given O
by O
ep O
( O
w O
) O
= O
− O
wtφntn O
( O
4.54 O
) O
( O
cid:2 O
) O
n∈m O
frank O
rosenblatt O
1928–1969 O
rosenblatt O
’ O
s O
perceptron B
played O
an O
important O
role O
in O
the O
history O
of O
ma- O
chine O
learning B
. O
the O
number O
of O
parameters O
can O
then O
be O
much O
smaller O
than O
in O
a O
completely O
general O
model O
( O
for O
ex- O
ample O
it O
may O
grow O
linearly O
with O
m O
) O
, O
although O
this O
is O
achieved O
at O
the O
expense O
of O
a O
restricted O
family O
of O
conditional B
distributions O
. O
of O
course O
, O
we O
could O
always O
trivially O
convert O
any O
distribution O
over O
a O
directed B
graph O
into O
one O
over O
an O
undirected B
graph I
by O
simply O
using O
a O
fully B
connected I
undirected O
graph O
. O
finally O
, O
a O
two-stage O
message B
passing I
algorithm O
, O
essentially O
equivalent O
to O
the O
sum-product B
algorithm I
, O
can O
now O
be O
applied O
to O
this O
junction O
tree O
in O
order O
to O
ﬁnd O
marginals O
and O
conditionals O
. O
the O
function O
k O
( O
u O
) O
is O
an O
example O
of O
a O
kernel B
function I
, O
and O
in O
this O
context O
is O
also O
called O
a O
parzen O
window O
. O
, O
tn O
) O
t. O
taking O
the O
log O
of O
both O
sides O
, O
and O
substituting O
for O
the O
prior B
distribution O
using O
( O
4.140 O
) O
, O
and O
for O
the O
likelihood B
function I
using O
( O
4.89 O
) O
, O
we O
obtain O
p O
( O
w|t O
) O
∝ O
p O
( O
w O
) O
p O
( O
t|w O
) O
( O
w O
− O
m0 O
) O
ts O
0 O
( O
w O
− O
m0 O
) O
−1 O
n O
( O
cid:2 O
) O
ln O
p O
( O
w|t O
) O
= O
−1 O
2 O
+ O
n=1 O
{ O
tn O
ln O
yn O
+ O
( O
1 O
− O
tn O
) O
ln O
( O
1 O
− O
yn O
) O
} O
+ O
const O
( O
4.142 O
) O
where O
yn O
= O
σ O
( O
wtφn O
) O
. O
natural O
gradient O
works O
efﬁ- O
10 O
, O
ciently O
in O
learning B
. O
the O
minimum O
value O
of O
( O
bias B
) O
2 O
+ O
variance B
occurs O
around O
ln O
λ O
= O
−0.31 O
, O
which O
is O
close O
to O
the O
value O
that O
gives O
the O
minimum O
error O
on O
the O
test O
data O
. O
note O
that O
the O
optimal O
solution O
for O
q O
( O
cid:1 O
) O
( O
z O
) O
depends O
on O
moments O
evaluated O
with O
respect O
to O
the O
distributions O
of O
other O
variables O
, O
and O
so O
again O
the O
variational B
update O
equations O
are O
coupled O
and O
must O
be O
solved O
iteratively O
. O
figure O
14.10 O
shows O
an O
example O
of O
the O
mixture O
of O
logistic O
regression B
models O
applied O
to O
a O
simple O
classiﬁcation B
problem O
. O
it O
can O
therefore O
be O
viewed O
as O
an O
inﬁnite O
mixture B
692 O
b. O
probability B
distributions O
of O
gaussians O
having O
the O
same O
mean B
but O
different O
variances O
. O
in O
practice O
, O
however O
, O
it O
is O
often O
worth O
investing O
substantial O
computational O
resources O
during O
the O
training B
phase O
in O
order O
to O
obtain O
a O
compact O
model O
that O
is O
fast O
at O
processing O
new O
data O
. O
it O
is O
useful O
to O
distinguish O
between O
stationary B
and O
nonstationary O
sequential O
dis- O
tributions O
. O
for O
instance O
, O
if O
we O
put O
a O
uniform O
prior O
distribution O
over O
the O
mean B
of O
a O
gaussian O
, O
then O
the O
posterior O
distribution O
for O
the O
mean B
, O
once O
we O
have O
observed O
at O
least O
one O
data O
point O
, O
will O
be O
proper O
. O
speciﬁcally O
, O
the O
algorithm O
uses O
a O
markov O
chain O
consisting O
of O
alternate O
stochastic B
updates O
of O
the O
momentum B
variable I
r O
and O
hamiltonian O
dynamical O
updates O
using O
the O
leapfrog O
algorithm O
. O
156 O
3. O
linear O
models O
for B
regression I
posterior O
distribution O
would O
become O
a O
delta O
function O
centred O
on O
the O
true O
parameter O
values O
, O
shown O
by O
the O
white O
cross O
. O
this O
is O
a O
more O
restricted O
form O
of O
conditional B
distribution O
than O
the O
general O
case O
but O
is O
now O
governed O
by O
a O
number O
of O
parameters O
that O
grows O
linearly O
with O
m. O
in O
this O
sense O
, O
it O
is O
analogous O
to O
the O
choice O
of O
a O
restrictive O
form O
of O
covariance B
matrix I
( O
for O
example O
, O
a O
diagonal B
matrix O
) O
in O
a O
multivariate O
gaussian O
distribution O
. O
the O
optimal O
solution O
is O
the O
one O
which O
minimizes O
the O
loss B
function I
. O
( O
7.79 O
) O
next O
we O
introduce O
a O
prior B
distribution O
over O
the O
parameter O
vector O
w O
and O
as O
in O
chapter O
3 O
, O
we O
shall O
consider O
a O
zero-mean O
gaussian O
prior B
. O
this O
situation O
typically O
arises O
when O
one O
of O
the O
factors O
is O
the O
prior B
p O
( O
θ O
) O
, O
and O
so O
we O
see O
that O
the O
prior B
factor O
can O
be O
incorporated O
once O
exactly O
and O
does O
not O
need O
to O
be O
reﬁned O
. O
4.2.4 O
exponential B
family I
as O
we O
have O
seen O
, O
for O
both O
gaussian O
distributed O
and O
discrete O
inputs O
, O
the O
posterior O
class O
probabilities O
are O
given O
by O
generalized O
linear O
models O
with O
logistic B
sigmoid I
( O
k O
= O
4.3. O
probabilistic O
discriminative O
models O
203 O
2 O
classes O
) O
or O
softmax O
( O
k O
( O
cid:2 O
) O
2 O
classes O
) O
activation O
functions O
. O
the O
conditional B
distribution O
of O
the O
target O
variable O
, O
for O
a O
probabilistic O
mixture O
of O
k O
logistic B
regression I
models O
, O
is O
given O
by O
k O
( O
cid:2 O
) O
( O
cid:10 O
) O
k=1 O
p O
( O
t|φ O
, O
θ O
) O
= O
πkyt O
k O
[ O
1 O
− O
yk O
] O
1−t O
( O
cid:11 O
) O
where O
φ O
is O
the O
feature O
vector O
, O
yk O
= O
σ O
denotes O
the O
adjustable O
parameters O
namely O
{ O
πk O
} O
and O
{ O
wk O
} O
. O
this O
gives O
an O
alternative O
representation O
of O
the O
transitions O
between O
latent O
states O
, O
known O
as O
a O
lattice O
or O
trellis B
diagram I
, O
and O
which O
is O
shown O
for O
the O
case O
of O
the O
hidden O
markov O
model O
in O
figure O
13.7. O
the O
speciﬁcation O
of O
the O
probabilistic O
model O
is O
completed O
by O
deﬁning O
the O
con- O
ditional O
distributions O
of O
the O
observed O
variables O
p O
( O
xn|zn O
, O
φ O
) O
, O
where O
φ O
is O
a O
set O
of O
pa- O
rameters O
governing O
the O
distribution O
. O
if O
the O
number O
m O
of O
basis O
functions O
is O
smaller O
than O
the O
number O
n O
of O
data O
points O
, O
it O
will O
be O
computationally O
more O
efﬁcient O
to O
work O
in O
the O
basis B
function I
310 O
6. O
kernel O
methods O
figure O
6.7 O
illustration O
of O
the O
mechanism O
of O
gaussian O
process O
regression B
for O
the O
case O
of O
one O
training B
point O
and O
one O
test O
point O
, O
in O
which O
the O
red O
el- O
lipses O
show O
contours O
of O
the O
joint O
dis- O
tribution O
p O
( O
t1 O
, O
t2 O
) O
. O
x1 O
x2 O
x1 O
x2 O
x1 O
x2 O
8.4. O
inference B
in O
graphical O
models O
401 O
f O
x3 O
( O
b O
) O
fc O
fa O
fb O
x3 O
( O
c O
) O
x3 O
( O
a O
) O
figure O
8.42 O
( O
a O
) O
a O
directed B
graph O
with O
the O
factorization B
p O
( O
x1 O
) O
p O
( O
x2 O
) O
p O
( O
x3|x1 O
, O
x2 O
) O
. O
let O
us O
denote O
a O
clique B
by O
c O
and O
the O
set O
of O
variables O
in O
that O
clique B
by O
xc O
. O
observe O
that O
a O
message O
has O
passed O
once O
in O
each O
direction O
across O
each O
link B
in O
the O
graph O
. O
( O
3.67 O
) O
i=1 O
this O
is O
an O
example O
of O
a O
mixture B
distribution I
in O
which O
the O
overall O
predictive O
distribu- O
tion O
is O
obtained O
by O
averaging O
the O
predictive O
distributions O
p O
( O
t|x O
, O
mi O
, O
d O
) O
of O
individual O
models O
, O
weighted O
by O
the O
posterior O
probabilities O
p O
( O
mi|d O
) O
of O
those O
models O
. O
by O
inspection O
of O
the O
form O
of O
the O
multinomial B
distribution I
, O
we O
see O
that O
the O
conjugate B
prior I
is O
given O
by O
µαk−1 O
k O
( O
2.37 O
) O
p O
( O
µ|α O
) O
∝ O
k O
( O
cid:14 O
) O
k=1 O
( O
cid:5 O
) O
where O
0 O
( O
cid:1 O
) O
µk O
( O
cid:1 O
) O
1 O
and O
k O
µk O
= O
1. O
here O
α1 O
, O
. O
figure O
13.16 O
shows O
a O
fragment O
of O
the O
hidden O
markov O
model O
expanded O
as O
lattice B
diagram I
. O
again O
, O
we O
see O
that O
a O
particular O
graph O
is O
de- O
scribing O
a O
whole O
family O
of O
probability B
distributions O
. O
we O
shall O
assume O
that O
the O
conditional B
variance O
of O
z O
is O
ﬁnite O
so O
that O
( O
2.128 O
) O
and O
we O
shall O
also O
, O
without O
loss O
of O
generality O
, O
consider O
the O
case O
where O
f O
( O
θ O
) O
> O
0 O
for O
θ O
> O
θ O
( O
cid:1 O
) O
and O
f O
( O
θ O
) O
< O
0 O
for O
θ O
< O
θ O
( O
cid:1 O
) O
, O
as O
is O
the O
case O
in O
figure O
2.10. O
the O
robbins-monro O
procedure O
then O
deﬁnes O
a O
sequence O
of O
successive O
estimates O
of O
the O
root O
θ O
( O
cid:1 O
) O
given O
by O
e O
( O
z O
− O
f O
) O
2 O
| O
θ O
< O
∞ O
θ O
( O
n O
) O
= O
θ O
( O
n−1 O
) O
+ O
an−1z O
( O
θ O
( O
n−1 O
) O
) O
( O
2.129 O
) O
where O
z O
( O
θ O
( O
n O
) O
) O
is O
an O
observed O
value O
of O
z O
when O
θ O
takes O
the O
value O
θ O
( O
n O
) O
. O
two O
cases O
are O
particularly O
worthy O
of O
note O
, O
namely O
when O
the O
parent O
and O
child B
node I
each O
correspond O
to O
discrete O
variables O
and O
when O
they O
each O
correspond O
to O
gaussian O
variables O
, O
because O
in O
these O
two O
cases O
the O
relationship O
can O
be O
extended B
hierarchically O
to O
construct O
arbitrarily O
complex O
directed O
acyclic O
graphs O
. O
similarly O
, O
show O
that O
the O
covariance B
matrix I
of O
y O
is O
given O
by O
the O
sum O
of O
the O
covariance B
matrices O
of O
x O
and O
z. O
conﬁrm O
that O
this O
result O
agrees O
with O
that O
of O
exercise O
1.10 O
. O
for O
the O
univariate O
gaussian O
, O
we O
have O
p O
( O
x|µ O
, O
σ2 O
) O
= O
= O
1 O
( O
2πσ2 O
) O
1/2 O
exp O
( O
2πσ2 O
) O
1/2 O
exp O
1 O
− O
1 O
2σ2 O
( O
x O
− O
µ O
) O
2 O
− O
1 O
2σ2 O
x2 O
+ O
µ O
σ2 O
x O
− O
1 O
2σ2 O
µ2 O
( O
cid:13 O
) O
( O
2.218 O
) O
( O
2.219 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:13 O
) O
116 O
2. O
probability B
distributions O
exercise O
2.57 O
exercise O
2.58 O
which O
, O
after O
some O
simple O
rearrangement O
, O
can O
be O
cast O
in O
the O
standard O
exponential O
family O
form O
( O
2.194 O
) O
with O
η O
= O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
µ/σ2 O
−1/2σ2 O
x O
x2 O
u O
( O
x O
) O
= O
h O
( O
x O
) O
= O
( O
2π O
) O
−1/2 O
g O
( O
η O
) O
= O
( O
−2η2 O
) O
1/2 O
exp O
( O
2.220 O
) O
( O
2.221 O
) O
( O
2.222 O
) O
( O
2.223 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
. O
158 O
3. O
linear O
models O
for B
regression I
t O
1 O
0 O
−1 O
t O
1 O
0 O
−1 O
0 O
0 O
t O
1 O
0 O
−1 O
x O
1 O
0 O
x O
1 O
t O
1 O
0 O
−1 O
x O
1 O
0 O
x O
1 O
figure O
3.9 O
plots O
of O
the O
function O
y O
( O
x O
, O
w O
) O
using O
samples O
from O
the O
posterior O
distributions O
over O
w O
corresponding O
to O
the O
plots O
in O
figure O
3.8. O
if O
we O
used O
localized O
basis O
functions O
such O
as O
gaussians O
, O
then O
in O
regions O
away O
from O
the O
basis B
function I
centres O
, O
the O
contribution O
from O
the O
second O
term O
in O
the O
predic- O
−1 O
. O
the O
amount O
of O
information O
can O
be O
viewed O
as O
the O
‘ O
degree O
of O
surprise O
’ O
on O
learning B
the O
value O
of O
x. O
if O
we O
are O
told O
that O
a O
highly O
improbable O
event O
has O
just O
occurred O
, O
we O
will O
have O
received O
more O
information O
than O
if O
we O
were O
told O
that O
some O
very O
likely O
event O
has O
just O
occurred O
, O
and O
if O
we O
knew O
that O
the O
event O
was O
certain O
to O
happen O
we O
would O
receive O
no O
information O
. O
also O
shown O
are O
the O
classiﬁcations O
of O
the O
input O
space O
given O
by O
the O
k-nearest-neighbour O
algorithm O
for O
various O
values O
of O
k. O
exercises O
127 O
an O
interesting O
property O
of O
the O
nearest-neighbour O
( O
k O
= O
1 O
) O
classiﬁer O
is O
that O
, O
in O
the O
limit O
n O
→ O
∞ O
, O
the O
error B
rate O
is O
never O
more O
than O
twice O
the O
minimum O
achievable O
error B
rate O
of O
an O
optimal O
classiﬁer O
, O
i.e. O
, O
one O
that O
uses O
the O
true O
class O
distributions O
( O
cover O
and O
hart O
, O
1967 O
) O
. O
atσa O
2.21 O
( O
( O
cid:12 O
) O
) O
show O
that O
a O
real O
, O
symmetric O
matrix O
of O
size O
d× O
d O
has O
d O
( O
d O
+ O
1 O
) O
/2 O
independent B
parameters O
. O
blind B
source I
separation I
, O
2 O
: O
problems O
statement O
. O
it O
is O
interesting O
to O
compare O
max-sum O
with O
the O
iterated B
conditional I
modes I
( O
icm O
) O
algorithm O
described O
on O
page O
389. O
each O
step O
in O
icm O
is O
computationally O
simpler O
be- O
cause O
the O
‘ O
messages O
’ O
that O
are O
passed O
from O
one O
node B
to O
the O
next O
comprise O
a O
single O
value O
consisting O
of O
the O
new O
state O
of O
the O
node B
for O
which O
the O
conditional B
distribution O
is O
maximized O
. O
because O
a O
standard O
network O
trained O
by O
least O
squares O
is O
approximating O
the O
conditional B
mean O
, O
we O
see O
that O
a O
mixture B
density I
network I
can O
reproduce O
the O
conventional O
least-squares O
result O
as O
a O
special O
case O
. O
consider O
the O
variance B
of O
the O
predictive B
distribution I
along O
some O
direction O
specified O
by O
the O
unit O
vector O
v O
, O
where O
vtv O
= O
1 O
, O
which O
is O
given O
by O
vtcv O
. O
the O
image O
( O
a O
vector O
of O
pixel O
intensities O
) O
has O
a O
probability B
distribution O
that O
is O
dependent O
on O
the O
identity O
of O
the O
object O
as O
well O
as O
on O
its O
position O
and O
orientation O
. O
5.2 O
network O
training B
. O
this O
is O
easily O
veriﬁed O
by O
inspection O
of O
the O
directed B
graph O
in O
figure O
9.6 O
( O
cid:2 O
) O
and O
making O
use O
of O
the O
d-separation B
criterion O
. O
points O
at O
which O
the O
gradient O
vanishes O
are O
called O
stationary B
points O
, O
and O
may O
be O
further O
classiﬁed O
into O
minima O
, O
maxima O
, O
and O
saddle O
points O
. O
398 O
8. O
graphical O
models O
now O
suppose O
we O
wish O
to O
evaluate O
the O
marginals O
p O
( O
xn O
) O
for O
every O
node B
n O
∈ O
{ O
1 O
, O
. O
markov O
chain O
sampling O
for O
dirichlet O
process O
mixture B
models O
. O
, O
xn O
} O
, O
we O
can O
determine O
the O
param- O
eters O
of O
an O
hmm O
using O
maximum B
likelihood I
. O
using O
the O
result O
( O
3.49 O
) O
for O
linear O
regression B
models O
, O
we O
see O
that O
the O
posterior O
distribution O
for O
the O
weights O
is O
again O
gaussian O
and O
takes O
the O
form O
p O
( O
w|t O
, O
x O
, O
α O
, O
β O
) O
= O
n O
( O
w|m O
, O
σ O
) O
( O
7.81 O
) O
where O
the O
mean B
and O
covariance B
are O
given O
by O
( O
cid:10 O
) O
( O
cid:11 O
) O
−1 O
m O
= O
βσφtt O
σ O
= O
( O
7.82 O
) O
( O
7.83 O
) O
where O
φ O
is O
the O
n O
× O
m O
design B
matrix I
with O
elements O
φni O
= O
φi O
( O
xn O
) O
, O
and O
a O
= O
diag O
( O
αi O
) O
. O
( O
13.38 O
) O
zn+1 O
note O
that O
in O
this O
case O
we O
have O
a O
backward O
message B
passing I
algorithm O
that O
evaluates O
β O
( O
zn O
) O
in O
terms O
of O
β O
( O
zn+1 O
) O
. O
how- O
ever O
, O
there O
are O
no O
marginal B
distributions O
associated O
with O
such O
nodes O
. O
the O
class O
of O
density B
model O
given O
by O
( O
2.249 O
) O
is O
called O
a O
kernel B
density I
estimator I
, O
or O
parzen O
estimator O
. O
thus O
, O
if O
we O
transform O
our O
uniformly O
distributed O
variable O
z O
using O
y O
= O
−λ O
−1 O
ln O
( O
1 O
− O
z O
) O
, O
then O
y O
will O
have O
an O
exponential B
distribution I
. O
it O
therefore O
follows O
from O
( O
9.70 O
) O
that O
l O
( O
q O
, O
θ O
) O
( O
cid:1 O
) O
ln O
p O
( O
x|θ O
) O
, O
in O
other O
words O
that O
l O
( O
q O
, O
θ O
) O
is O
a O
lower B
bound I
on O
ln O
p O
( O
x|θ O
) O
. O
, O
k. O
the O
value O
of O
µk O
gives O
the O
probability B
of O
the O
random O
variable O
taking O
state O
k O
, O
and O
so O
these O
parameters O
are O
subject O
to O
the O
constraints O
0 O
( O
cid:1 O
) O
µk O
( O
cid:1 O
) O
1 O
k O
µk O
= O
1. O
the O
conjugate B
prior I
distribution O
for O
the O
parameters O
{ O
µk O
} O
is O
the O
and O
dirichlet O
. O
, O
xn−1 O
) O
such O
as O
a O
neural B
network I
. O
( O
6.99 O
) O
by O
minimizing O
e O
with O
respect O
to O
the O
function O
y O
( O
z O
) O
using O
the O
calculus B
of I
variations I
( O
appendix O
d O
) O
, O
show O
that O
optimal O
solution O
for O
y O
( O
x O
) O
is O
given O
by O
a O
nadaraya-watson O
kernel B
regression I
solution O
of O
the O
form O
( O
6.45 O
) O
with O
a O
kernel O
of O
the O
form O
( O
6.46 O
) O
. O
to O
do O
this O
we O
again O
return O
to O
the O
concept O
of O
a O
graphical B
model I
as O
a O
ﬁlter O
, O
corre- O
sponding O
to O
figure O
8.25. O
consider O
the O
set O
of O
all O
possible O
distributions O
deﬁned O
over O
a O
ﬁxed O
set O
of O
variables O
corresponding O
to O
the O
nodes O
of O
a O
particular O
undirected B
graph I
. O
robustness B
is O
also O
an O
important O
property O
for B
regression I
problems O
. O
however O
, O
this O
is O
also O
one O
of O
its O
great O
weaknesses O
because O
the O
computa- O
tional O
cost O
of O
evaluating O
the O
density B
grows O
linearly O
with O
the O
size O
of O
the O
data O
set O
. O
because O
the O
error B
function I
is O
a O
quadratic O
function O
of O
the O
coefﬁcients O
w O
, O
its O
derivatives O
with O
respect O
to O
the O
coefﬁcients O
will O
be O
linear O
in O
the O
elements O
of O
w O
, O
and O
so O
the O
minimization O
of O
the O
error B
function I
has O
a O
unique O
solution O
, O
denoted O
by O
w O
( O
cid:1 O
) O
, O
which O
can O
be O
found O
in O
closed O
form O
. O
to O
understand O
the O
structure O
of O
this O
model O
, O
imagine O
a O
mixture B
distribution I
in O
which O
each O
component O
in O
the O
mixture B
is O
itself O
a O
mixture B
distribution I
. O
each O
e O
or O
m O
step O
in O
this O
incremental O
algorithm O
is O
increasing O
the O
value O
of O
l O
( O
q O
, O
θ O
) O
and O
, O
as O
we O
have O
shown O
above O
, O
if O
the O
algorithm O
converges O
to O
a O
local B
( O
or O
global O
) O
maximum O
of O
l O
( O
q O
, O
θ O
) O
, O
this O
will O
correspond O
to O
a O
local B
( O
or O
global O
) O
maximum O
of O
the O
log O
likelihood O
function O
ln O
p O
( O
x|θ O
) O
. O
5.5. O
regularization B
in O
neural O
networks O
the O
number O
of O
input O
and O
outputs O
units O
in O
a O
neural B
network I
is O
generally O
determined O
by O
the O
dimensionality O
of O
the O
data O
set O
, O
whereas O
the O
number O
m O
of O
hidden O
units O
is O
a O
free O
parameter O
that O
can O
be O
adjusted O
to O
give O
the O
best O
predictive O
performance O
. O
at O
both O
small O
and O
large O
values O
of O
x O
, O
where O
the O
conditional B
probability I
density O
of O
the O
target O
data O
is O
unimodal O
, O
only O
one O
of O
the O
ker- O
nels O
has O
a O
high O
value O
for O
its O
prior B
probability O
, O
while O
at O
intermediate O
val- O
ues O
of O
x O
, O
where O
the O
conditional B
den- O
sity O
is O
trimodal O
, O
the O
three O
mixing O
co- O
efﬁcients O
have O
comparable O
values O
. O
x1 O
x2 O
x1 O
x2 O
x1 O
f O
x3 O
( O
b O
) O
x3 O
( O
a O
) O
x2 O
fb O
fa O
x3 O
( O
c O
) O
figure O
8.41 O
( O
a O
) O
an O
undirected B
graph I
with O
a O
single O
clique B
potential O
ψ O
( O
x1 O
, O
x2 O
, O
x3 O
) O
. O
ridge O
regres- O
sion B
: O
biased O
estimation O
for O
nonorthogonal O
prob- O
lems O
. O
the O
accuracy O
of O
the O
ﬁnite B
differences I
method O
can O
be O
improved O
signiﬁcantly O
by O
using O
symmetrical O
central B
differences I
of O
the O
form O
 O
+ O
o O
( O
 O
) O
= O
en O
( O
wji O
+ O
 O
) O
− O
en O
( O
wji O
− O
 O
) O
2 O
∂en O
∂wji O
+ O
o O
( O
2 O
) O
. O
we O
shall O
give O
an O
example O
of O
the O
sum-product B
algorithm I
applied O
to O
a O
graph O
of O
linear-gaussian O
variables O
when O
we O
consider O
linear O
dynamical O
systems O
. O
in O
the O
case O
of O
classiﬁcation B
, O
there O
are O
various O
179 O
180 O
4. O
linear O
models O
for O
classification O
ways O
of O
using O
target O
values O
to O
represent O
class O
labels O
. O
in O
the O
basic O
metropolis O
algorithm O
( O
metropolis O
et O
al. O
, O
1953 O
) O
, O
we O
assume O
that O
the O
proposal B
distribution I
is O
symmetric O
, O
that O
is O
q O
( O
za|zb O
) O
= O
q O
( O
zb|za O
) O
for O
all O
values O
of O
za O
and O
zb O
. O
prob- O
ability O
theory B
provides O
a O
consistent B
framework O
for O
the O
quantiﬁcation O
and O
manipula- O
tion O
of O
uncertainty O
and O
forms O
one O
of O
the O
central O
foundations O
for O
pattern O
recognition O
. O
we O
may O
then O
seek O
a O
form O
of O
prior B
distribution O
, O
called O
a O
noninformative B
prior I
, O
which O
is O
intended O
to O
have O
as O
little O
inﬂu- O
ence O
on O
the O
posterior O
distribution O
as O
possible O
( O
jeffries O
, O
1946 O
; O
box O
and O
tao O
, O
1973 O
; O
bernardo O
and O
smith O
, O
1994 O
) O
. O
in O
order O
to O
evaluate O
the O
predictive B
distribution I
( O
6.76 O
) O
, O
we O
seek O
a O
gaussian O
approximation O
to O
the O
posterior O
distribution O
over O
an O
+1 O
, O
which O
, O
using O
bayes O
’ O
theorem O
, O
is O
given O
by O
( O
cid:6 O
) O
p O
( O
an O
+1 O
, O
an|tn O
) O
dan O
( O
cid:6 O
) O
1 O
p O
( O
an O
+1|tn O
) O
= O
p O
( O
an O
+1 O
, O
an O
) O
p O
( O
tn|an O
+1 O
, O
an O
) O
dan O
p O
( O
an O
+1|an O
) O
p O
( O
an O
) O
p O
( O
tn|an O
) O
dan O
p O
( O
tn O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
1 O
p O
( O
tn O
) O
p O
( O
an O
+1|an O
) O
p O
( O
an|tn O
) O
dan O
( O
6.77 O
) O
= O
= O
= O
section O
2.3 O
section O
10.1 O
section O
10.7 O
section O
4.4 O
316 O
6. O
kernel O
methods O
where O
we O
have O
used O
p O
( O
tn|an O
+1 O
, O
an O
) O
= O
p O
( O
tn|an O
) O
. O
at O
any O
point O
wc O
, O
the O
local B
gradient O
of O
the O
error B
surface O
is O
given O
by O
the O
vector O
∇e O
. O
figure O
10.17 O
compares O
the O
performance O
of O
ep O
with O
variational B
bayes O
( O
mean B
ﬁeld I
theory I
) O
and O
the O
laplace O
approximation O
on O
the O
clutter B
problem I
. O
, O
n. O
13.2 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
joint O
probability B
distribution O
( O
13.2 O
) O
corresponding O
to O
the O
directed B
graph O
of O
figure O
13.3. O
using O
the O
sum O
and O
product O
rules O
of O
probability B
, O
verify O
that O
this O
joint O
distribution O
satisﬁes O
the O
conditional B
independence I
property O
( O
13.3 O
) O
for O
n O
= O
2 O
, O
. O
as O
an O
aside O
it O
is O
worth O
noting O
that O
, O
if O
we O
deﬁne O
conjugate B
( O
gamma O
) O
prior B
distri- O
butions O
over O
α O
and O
β O
, O
then O
the O
marginalization O
over O
these O
hyperparameters O
in O
( O
3.74 O
) O
can O
be O
performed O
analytically O
to O
give O
a O
student O
’ O
s O
t-distribution O
over O
w O
( O
see O
sec- O
tion O
2.3.7 O
) O
. O
these O
units O
perform O
subsampling B
. O
in O
order O
to O
develop O
a O
bayesian O
treatment O
for O
this O
problem O
, O
we O
need O
to O
introduce O
a O
prior B
distribution O
p O
( O
µ O
) O
over O
the O
parameter O
µ. O
here O
we O
consider O
a O
form O
of O
prior B
distribution O
that O
has O
a O
simple O
interpretation O
as O
well O
as O
some O
useful O
analytical O
properties O
. O
derive O
an O
expression O
for O
the O
differential B
entropy I
of O
the O
variable O
x O
= O
x1 O
+ O
x2 O
. O
how- O
ever O
, O
it O
may O
converge O
to O
a O
local B
rather O
than O
global B
minimum I
of O
j. O
the O
convergence O
properties O
of O
the O
k-means O
algorithm O
were O
studied O
by O
macqueen O
( O
1967 O
) O
. O
the O
decomposition O
( O
9.70 O
) O
is O
illustrated O
in O
fig- O
ure O
9.11. O
the O
em O
algorithm O
is O
a O
two-stage O
iterative O
optimization O
technique O
for O
ﬁnding O
maximum B
likelihood I
solutions O
. O
4.5.2 O
predictive B
distribution I
the O
predictive B
distribution I
for O
class O
c1 O
, O
given O
a O
new O
feature O
vector O
φ O
( O
x O
) O
, O
is O
obtained O
by O
marginalizing O
with O
respect O
to O
the O
posterior O
distribution O
p O
( O
w|t O
) O
, O
which O
is O
itself O
approximated O
by O
a O
gaussian O
distribution O
q O
( O
w O
) O
so O
that O
p O
( O
c1|φ O
, O
t O
) O
= O
p O
( O
c1|φ O
, O
w O
) O
p O
( O
w|t O
) O
dw O
( O
cid:7 O
) O
( O
4.145 O
) O
with O
the O
corresponding O
probability B
for O
class O
c2 O
given O
by O
p O
( O
c2|φ O
, O
t O
) O
= O
1− O
p O
( O
c1|φ O
, O
t O
) O
. O
for O
the O
mean B
, O
make O
use O
of O
the O
matrix O
identity O
( O
c.6 O
) O
, O
and O
for O
the O
variance B
, O
make O
use O
of O
the O
matrix O
identity O
( O
c.7 O
) O
. O
note O
that O
the O
bound O
given O
by O
( O
10.149 O
) O
applies O
only O
to O
the O
two-class O
problem O
and O
so O
this O
approach O
does O
not O
directly O
generalize O
to O
classiﬁcation B
problems O
with O
k O
> O
2 O
classes O
. O
thus O
, O
for O
this O
prior B
there O
is O
the O
same O
probability B
mass O
in O
the O
range O
1 O
( O
cid:1 O
) O
σ O
( O
cid:1 O
) O
10 O
as O
in O
the O
range O
10 O
( O
cid:1 O
) O
σ O
( O
cid:1 O
) O
100 O
and O
in O
100 O
( O
cid:1 O
) O
σ O
( O
cid:1 O
) O
1000 O
. O
a O
fully O
bayesian O
treatment O
, O
based O
on O
variational B
inference I
( O
bishop O
and O
winn O
, O
2000 O
) O
, O
allows O
the O
number O
of O
components O
in O
the O
mixture B
, O
as O
well O
as O
the O
effective O
dimensionalities O
of O
the O
individual O
models O
, O
to O
be O
inferred O
from O
the O
data O
. O
from O
this O
density B
function O
we O
can O
calculate O
more O
speciﬁc O
quantities O
that O
may O
be O
of O
interest O
in O
different O
applications O
. O
controlling O
the O
sensitivity O
of O
support B
vector I
machines O
. O
we O
capture O
our O
assumptions O
about O
w O
, O
before O
observing O
the O
data O
, O
in O
the O
form O
of O
a O
prior B
probability O
distribution O
p O
( O
w O
) O
. O
an O
important O
property O
of O
support B
vector I
machines O
is O
that O
the O
determination O
of O
the O
model O
parameters O
corresponds O
to O
a O
convex O
optimization O
prob- O
lem O
, O
and O
so O
any O
local B
solution O
is O
also O
a O
global O
optimum O
. O
( O
5.167 O
) O
similarly O
, O
the O
predictive B
distribution I
is O
obtained O
by O
marginalizing O
with O
respect O
to O
this O
posterior O
distribution O
p O
( O
t|x O
, O
d O
) O
= O
( O
cid:6 O
) O
p O
( O
t|x O
, O
w O
) O
q O
( O
w|d O
) O
dw O
. O
( O
c O
) O
a O
different O
factor B
graph I
representing O
the O
same O
distribution O
with O
factors O
fa O
( O
x1 O
) O
= O
p O
( O
x1 O
) O
, O
fb O
( O
x2 O
) O
= O
p O
( O
x2 O
) O
and O
fc O
( O
x1 O
, O
x2 O
, O
x3 O
) O
= O
p O
( O
x3|x1 O
, O
x2 O
) O
. O
the O
joint O
distribution O
corresponding O
to O
a O
directed B
graph O
can O
be O
written O
using O
the O
decomposition O
p O
( O
x O
) O
= O
p O
( O
xi|pai O
) O
( O
10.122 O
) O
where O
xi O
denotes O
the O
variable O
( O
s O
) O
associated O
with O
node B
i O
, O
and O
pai O
denotes O
the O
parent O
set O
corresponding O
to O
node B
i. O
note O
that O
xi O
may O
be O
a O
latent B
variable I
or O
it O
may O
belong O
to O
the O
set O
of O
observed O
variables O
. O
for O
some O
problems O
, O
known O
as O
heteroscedas- O
tic O
, O
the O
noise O
variance B
itself O
will O
also O
depend O
on O
x. O
to O
model O
this O
, O
we O
can O
extend O
the O
312 O
6. O
kernel O
methods O
for O
gaussian O
processes O
, O
figure O
6.9 O
samples O
from O
the O
ard O
prior B
in O
which O
the O
kernel B
function I
is O
given O
by O
( O
6.71 O
) O
. O
returning O
to O
the O
polynomial O
regression O
problem O
, O
we O
can O
plot O
the O
model B
evidence I
against O
the O
order O
of O
the O
polynomial O
, O
as O
shown O
in O
figure O
3.14. O
here O
we O
have O
assumed O
a O
prior B
of O
the O
form O
( O
1.65 O
) O
with O
the O
parameter O
α O
ﬁxed O
at O
α O
= O
5 O
× O
10−3 O
. O
it O
is O
sometimes O
also O
convenient O
to O
think O
of O
the O
prior B
distribution O
for O
a O
scale B
parameter I
in O
terms O
of O
the O
density B
of O
the O
log O
of O
the O
parameter O
. O
we O
see O
that O
, O
for O
small O
noise O
amplitudes O
, O
tikhonov O
regularization B
is O
related O
to O
the O
addition O
of O
random O
noise O
to O
the O
inputs O
, O
which O
has O
been O
shown O
to O
improve O
generalization B
in O
appropriate O
circumstances O
( O
sietsma O
and O
dow O
, O
1991 O
) O
. O
if O
the O
graph O
is O
fully B
connected I
then O
we O
have O
a O
completely O
general O
distribution O
having O
km O
− O
1 O
parameters O
, O
whereas O
if O
there O
are O
no O
links O
in O
the O
graph O
the O
joint O
distribution O
factorizes O
into O
the O
product O
of O
the O
marginals O
, O
and O
the O
total O
number O
of O
parameters O
is O
m O
( O
k O
− O
1 O
) O
. O
as O
indicated O
for O
the O
case O
d O
= O
3 O
and O
m O
= O
2 O
in O
figure O
12.20. O
such O
a O
network O
effectively O
perfonns O
a O
nonlinear O
principal B
component I
analysis I
. O
( O
4.130 O
) O
the O
laplace O
approximation O
is O
illustrated O
in O
figure O
4.14. O
note O
that O
the O
gaussian O
approximation O
will O
only O
be O
well O
deﬁned O
if O
its O
precision O
a O
> O
0 O
, O
in O
other O
words O
the O
stationary B
point O
z0 O
must O
be O
a O
local B
maximum O
, O
so O
that O
the O
second O
derivative O
of O
f O
( O
z O
) O
at O
the O
point O
z0 O
is O
negative O
. O
this O
pair O
of O
random O
numbers O
has O
uniform B
distribution I
under O
the O
curve O
of O
the O
function O
kq O
( O
z O
) O
. O
we O
can O
apply O
the O
k-means O
algorithm O
to O
the O
problem O
of O
lossy B
data I
compression I
as O
follows O
. O
the O
result O
will O
be O
the O
same O
irrespective O
of O
which O
node B
is O
chosen O
as O
the O
root O
. O
we O
can O
, O
however O
, O
obtain O
a O
much O
more O
efﬁcient O
algorithm O
by O
exploiting O
the O
con- O
ditional O
independence O
properties O
of O
the O
graphical B
model I
. O
the O
blue O
curve O
shows O
a O
sample O
func- O
tion O
from O
the O
gaussian O
process O
prior B
over O
functions O
, O
and O
the O
red O
points O
show O
the O
values O
of O
yn O
obtained O
by O
evaluating O
the O
function O
at O
a O
set O
of O
in- O
put O
values O
{ O
xn O
} O
. O
an O
alternative O
approach O
to O
obtaining O
re-estimation O
equations O
for O
ξ O
is O
to O
note O
that O
in O
the O
integral O
over O
w O
in O
the O
deﬁnition O
( O
10.159 O
) O
of O
the O
lower B
bound I
l O
( O
ξ O
) O
, O
the O
integrand O
has O
a O
gaussian-like O
form O
and O
so O
the O
integral O
can O
be O
evaluated O
analytically O
. O
we O
approximate O
this O
using O
a O
fully O
factorized O
distribution O
of O
the O
form O
( O
cid:14 O
) O
i O
( O
cid:14 O
) O
( O
cid:4 O
) O
fik O
( O
θk O
) O
q O
( O
θ O
) O
∝ O
( O
10.237 O
) O
i O
k O
where O
θk O
corresponds O
to O
an O
individual O
variable O
node B
. O
in O
the O
simplest O
case O
, O
the O
mixing O
coefﬁ- O
cients O
are O
independent B
of O
the O
input O
variables O
. O
4.1.1 O
two O
classes O
the O
simplest O
representation O
of O
a O
linear B
discriminant I
function O
is O
obtained O
by O
tak- O
ing O
a O
linear O
function O
of O
the O
input O
vector O
so O
that O
y O
( O
x O
) O
= O
wtx O
+ O
w0 O
( O
4.4 O
) O
where O
w O
is O
called O
a O
weight B
vector I
, O
and O
w0 O
is O
a O
bias B
( O
not O
to O
be O
confused O
with O
bias B
in O
the O
statistical O
sense O
) O
. O
it O
is O
interesting O
to O
contrast O
the O
neural B
network I
solution O
to O
this O
problem O
with O
the O
corresponding O
approach O
based O
on O
a O
linear O
classiﬁcation O
model O
of O
the O
kind O
discussed O
in O
chapter O
4. O
suppose O
that O
we O
are O
using O
a O
standard O
two-layer O
network O
of O
the O
kind O
shown O
in O
figure O
5.1. O
we O
see O
that O
the O
weight O
parameters O
in O
the O
ﬁrst O
layer O
of O
the O
network O
are O
shared O
between O
the O
various O
outputs O
, O
whereas O
in O
the O
linear O
model O
each O
classiﬁcation B
problem O
is O
solved O
independently O
. O
in O
a O
practical O
application O
, O
however O
, O
we O
must O
often O
make O
a O
speciﬁc O
prediction O
for O
the O
value O
of O
t O
, O
or O
more O
generally O
take O
a O
speciﬁc O
action O
based O
on O
our O
understanding O
of O
the O
values O
t O
is O
likely O
to O
take O
, O
and O
this O
aspect O
is O
the O
subject O
of O
decision B
theory I
. O
our O
starting O
point O
is O
the O
likelihood B
function I
for O
the O
gaussian O
mixture B
model I
, O
il- O
lustrated O
by O
the O
graphical B
model I
in O
figure O
9.6. O
for O
each O
observation O
xn O
we O
have O
a O
corresponding O
latent B
variable I
zn O
comprising O
a O
1-of-k O
binary O
vector O
with O
ele- O
ments O
znk O
for O
k O
= O
1 O
, O
. O
it O
is O
important O
to O
observe O
that O
the O
moral O
graph O
in O
this O
example O
is O
fully B
connected I
and O
so O
exhibits O
no O
conditional B
independence I
properties O
, O
in O
contrast O
to O
the O
original O
directed B
graph O
. O
new O
support B
vector I
algorithms O
. O
let O
us O
work O
out O
the O
computational O
cost O
of O
evaluating O
the O
required O
marginal B
using O
this O
re-ordered O
expression O
. O
in O
fact O
, O
these O
two O
criteria O
are O
related O
, O
as O
we O
shall O
discuss O
in O
the O
context O
of O
curve B
ﬁtting I
. O
in O
a O
practical O
architecture O
, O
there O
may O
be O
several O
pairs O
of O
convolutional B
and O
sub- O
sampling O
layers O
. O
2.23 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
diagonalizing O
the O
coordinate O
system O
using O
the O
eigenvector O
expansion O
( O
2.45 O
) O
, O
show O
that O
the O
volume O
contained O
within O
the O
hyperellipsoid O
corresponding O
to O
a O
constant O
132 O
2. O
probability B
distributions O
mahalanobis O
distance O
∆ O
is O
given O
by O
vd|σ|1/2∆d O
( O
2.286 O
) O
where O
vd O
is O
the O
volume O
of O
the O
unit O
sphere O
in O
d O
dimensions O
, O
and O
the O
mahalanobis O
distance O
is O
deﬁned O
by O
( O
2.44 O
) O
. O
, O
zn O
} O
in O
which O
c O
becomes O
the O
identity O
matrix O
and O
where O
the O
transition O
prob- O
ability O
a O
= O
0 O
because O
the O
observations O
are O
independent B
. O
in O
section O
1.5.5 O
we O
saw O
that O
the O
function O
that O
minimizes O
the O
sum-of-squares B
error I
is O
given O
by O
the O
condi- O
tional O
average O
e O
[ O
t|x O
] O
of O
the O
target O
values O
t. O
from O
( O
5.131 O
) O
we O
see O
that O
the O
regularized O
error O
will O
equal O
the O
unregularized O
sum-of-squares O
plus O
terms O
which O
are O
o O
( O
ξ O
) O
, O
and O
so O
the O
network O
function O
that O
minimizes O
the O
total O
error B
will O
have O
the O
form O
y O
( O
x O
) O
= O
e O
[ O
t|x O
] O
+ O
o O
( O
ξ O
) O
. O
unlike O
the O
svm O
it O
also O
produces O
probabilistic O
outputs O
, O
although O
this O
is O
at O
the O
expense O
of O
a O
nonconvex O
optimization O
during O
training B
. O
one O
obvious O
problem O
is O
that O
the O
estimated O
density B
has O
discontinuities O
that O
are O
due O
to O
the O
bin O
edges O
rather O
than O
any O
property O
of O
the O
underlying O
distribution O
that O
generated O
the O
data O
. O
( O
6.43 O
) O
we O
now O
assume O
for O
simplicity O
that O
the O
component O
density B
functions O
have O
zero O
mean B
so O
that O
f O
( O
x O
, O
t O
) O
t O
dt O
= O
0 O
for O
all O
values O
of O
x. O
using O
a O
simple O
change O
of O
variable O
, O
we O
then O
obtain O
where O
n O
, O
m O
= O
1 O
, O
. O
1.5.4 O
inference B
and O
decision O
. O
we O
note O
here O
that O
the O
hessian O
matrix O
can O
sometimes O
be O
calculated O
indirectly O
as O
part O
of O
the O
network O
training B
algorithm O
. O
( O
cid:27 O
) O
using O
the O
form O
( O
2.194 O
) O
for O
members O
of O
the O
exponential B
family I
, O
we O
see O
that O
the O
distribution O
of O
x O
can O
be O
written O
in O
the O
form O
( O
cid:26 O
) O
p O
( O
x|λk O
) O
= O
h O
( O
x O
) O
g O
( O
λk O
) O
exp O
λt O
k O
u O
( O
x O
) O
. O
this O
is O
a O
useful O
result O
because O
in O
practice O
we O
may O
wish O
to O
use O
parametric O
forms O
for O
the O
clique B
potentials O
, O
or O
equivalently O
for O
the O
conditional B
distributions O
if O
we O
started O
from O
a O
directed B
graph O
. O
the O
plot O
on O
the O
left O
shows O
the O
predictive B
distribution I
obtained O
using O
variational B
inference I
. O
( O
7.47 O
) O
( O
7.48 O
) O
338 O
7. O
sparse O
kernel O
machines O
for O
comparison O
with O
other O
error B
functions O
, O
we O
can O
divide O
by O
ln O
( O
2 O
) O
so O
that O
the O
error B
function I
passes O
through O
the O
point O
( O
0 O
, O
1 O
) O
. O
for O
this O
example O
, O
we O
have O
chosen O
k O
= O
2 O
, O
and O
so O
in O
this O
exercise O
9.1 O
appendix O
a O
426 O
9. O
mixture B
models O
and O
em O
( O
a O
) O
−2 O
( O
d O
) O
−2 O
( O
g O
) O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
0 O
2 O
0 O
2 O
( O
b O
) O
−2 O
( O
e O
) O
−2 O
( O
h O
) O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
0 O
2 O
0 O
2 O
( O
c O
) O
−2 O
( O
f O
) O
−2 O
( O
i O
) O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
0 O
2 O
0 O
2 O
−2 O
0 O
2 O
−2 O
0 O
2 O
−2 O
0 O
2 O
figure O
9.1 O
illustration O
of O
the O
k-means O
algorithm O
using O
the O
re-scaled O
old O
faithful O
data O
set O
. O
in O
the O
case O
of O
gaussian O
emission O
densities O
this O
involves O
estimating O
the O
parameters O
using O
the O
standard O
linear O
regression B
equations O
, O
discussed O
in O
chapter O
3. O
we O
have O
seen O
that O
the O
autoregressive B
hmm O
appears O
as O
a O
natural O
extension O
of O
the O
standard O
hmm O
when O
viewed O
as O
a O
graphical B
model I
. O
1.5 O
1 O
0.5 O
0 O
−0.5 O
−1 O
−1.5 O
0 O
0.2 O
0.4 O
0.6 O
0.8 O
1 O
in O
fact O
, O
this O
model O
deﬁnes O
not O
only O
a O
conditional B
expectation I
but O
also O
a O
full O
conditional B
distribution O
given O
by O
p O
( O
t|x O
) O
= O
p O
( O
t O
, O
x O
) O
= O
( O
cid:6 O
) O
p O
( O
t O
, O
x O
) O
dt O
( O
cid:2 O
) O
( O
cid:6 O
) O
( O
cid:2 O
) O
n O
f O
( O
x O
− O
xn O
, O
t O
− O
tn O
) O
f O
( O
x O
− O
xm O
, O
t O
− O
tm O
) O
dt O
( O
6.48 O
) O
exercise O
6.18 O
m O
from O
which O
other O
expectations O
can O
be O
evaluated O
. O
exercises O
285 O
5.3 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
regression B
problem O
involving O
multiple O
target O
variables O
in O
which O
it O
is O
assumed O
that O
the O
distribution O
of O
the O
targets O
, O
conditioned O
on O
the O
input O
vector O
x O
, O
is O
a O
gaussian O
of O
the O
form O
p O
( O
t|x O
, O
w O
) O
= O
n O
( O
t|y O
( O
x O
, O
w O
) O
, O
σ O
) O
( O
5.192 O
) O
where O
y O
( O
x O
, O
w O
) O
is O
the O
output O
of O
a O
neural B
network I
with O
input O
vector O
x O
and O
weight B
vector I
w O
, O
and O
σ O
is O
the O
covariance B
of O
the O
assumed O
gaussian O
noise O
on O
the O
targets O
. O
because O
the O
factor O
p O
( O
x O
) O
is O
common O
to O
both O
terms O
, O
we O
can O
restate O
this O
result O
as O
saying O
that O
the O
minimum O
40 O
1. O
introduction O
( O
cid:1 O
) O
x O
x0 O
p O
( O
x O
, O
c1 O
) O
p O
( O
x O
, O
c2 O
) O
r1 O
x O
r2 O
figure O
1.24 O
schematic O
illustration O
of O
the O
joint O
probabilities O
p O
( O
x O
, O
ck O
) O
for O
each O
of O
two O
classes O
plotted O
against O
x O
, O
together O
with O
the O
decision B
boundary I
x O
= O
bx O
. O
2.5. O
nonparametric B
methods I
123 O
we O
can O
exploit O
the O
result O
( O
2.246 O
) O
in O
two O
different O
ways O
. O
7.2 O
relevance B
vector I
machines O
. O
contours O
of O
the O
likelihood B
function I
are O
then O
axis-aligned O
ellipses O
. O
the O
m-step O
equations O
are O
modiﬁed O
through O
the O
introduction O
of O
the O
prior B
term O
ln O
p O
( O
θ O
) O
, O
which O
typically O
requires O
only O
a O
small O
modiﬁcation O
to O
the O
standard O
maximum O
likeli- O
hood O
m-step O
equations O
. O
note O
that O
if O
x O
is O
a O
discrete O
variable O
, O
then O
p O
( O
x O
) O
is O
sometimes O
called O
a O
probability B
mass O
function O
because O
it O
can O
be O
regarded O
as O
a O
set O
of O
‘ O
probability B
masses O
’ O
concentrated O
at O
the O
allowed O
values O
of O
x. O
the O
sum O
and O
product O
rules O
of O
probability B
, O
as O
well O
as O
bayes O
’ O
theorem O
, O
apply O
equally O
to O
the O
case O
of O
probability B
densities O
, O
or O
to O
combinations O
of O
discrete O
and O
con- O
tinuous O
variables O
. O
now O
consider O
a O
prior B
distribution O
over O
w O
given O
by O
an O
isotropic B
gaussian O
of O
the O
form O
p O
( O
w O
) O
= O
n O
( O
w|0 O
, O
α O
−1i O
) O
( O
6.50 O
) O
governed O
by O
the O
hyperparameter B
α O
, O
which O
represents O
the O
precision O
( O
inverse B
variance O
) O
of O
the O
distribution O
. O
as O
we O
vary O
the O
location O
bx O
of O
the O
decision B
boundary I
, O
the O
combined O
areas O
of O
the O
blue O
and O
green O
regions O
remains O
constant O
, O
whereas O
the O
size O
of O
the O
red O
region O
varies O
. O
again O
, O
the O
error B
function I
in O
( O
1.4 O
) O
can O
be O
minimized O
exactly O
in O
closed O
form O
. O
note O
that O
these O
diagonal B
elements O
lie O
in O
the O
range O
( O
0 O
, O
1/4 O
) O
, O
and O
hence O
wn O
is O
a O
positive B
deﬁnite I
matrix I
. O
to O
summarize O
, O
principal B
component I
analysis I
involves O
evaluating O
the O
mean B
x O
and O
the O
covariance B
matrix I
s O
of O
the O
data O
set O
and O
then O
finding O
the O
m O
eigenvectors O
of O
s O
corresponding O
to O
the O
m O
largest O
eigenvalues O
. O
7.18 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
gradient O
vector O
and O
hessian O
matrix O
of O
the O
log O
poste- O
rior O
distribution O
( O
7.109 O
) O
for O
the O
classiﬁcation B
relevance O
vector O
machine O
are O
given O
by O
( O
7.110 O
) O
and O
( O
7.111 O
) O
. O
because O
the O
true O
posterior O
distribution O
is O
unimodal O
, O
as O
we O
shall O
see O
shortly O
, O
the O
expectation B
propagation I
approach O
can O
give O
good O
results O
. O
let O
us O
simply O
assume O
that O
all O
models O
are O
given O
equal O
prior B
probability O
. O
we O
can O
answer O
questions O
such O
as O
these O
, O
and O
indeed O
much O
more O
complex O
questions O
associated O
with O
problems O
in O
pattern O
recognition O
, O
once O
we O
have O
equipped O
ourselves O
with O
the O
two O
el- O
ementary O
rules O
of O
probability B
, O
known O
as O
the O
sum B
rule I
and O
the O
product B
rule I
. O
8.19 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
apply O
the O
sum-product B
algorithm I
derived O
in O
section O
8.4.4 O
to O
the O
chain-of- O
nodes O
model O
discussed O
in O
section O
8.4.1 O
and O
show O
that O
the O
results O
( O
8.54 O
) O
, O
( O
8.55 O
) O
, O
and O
( O
8.57 O
) O
are O
recovered O
as O
a O
special O
case O
. O
( O
1.113 O
) O
this O
is O
known O
as O
the O
relative B
entropy I
or O
kullback-leibler O
divergence O
, O
or O
kl O
diver- O
gence O
( O
kullback O
and O
leibler O
, O
1951 O
) O
, O
between O
the O
distributions O
p O
( O
x O
) O
and O
q O
( O
x O
) O
. O
here O
we O
shall O
focus O
on O
a O
widely O
used O
model O
known O
as O
a O
linear B
dynamical I
system I
. O
( O
c O
) O
plot O
of O
the O
contours O
of O
the O
corresponding O
con- O
ditional O
probability B
density O
of O
the O
tar- O
get O
data O
for O
the O
same O
mixture B
den- O
sity O
network O
. O
( O
12.51 O
) O
exercise O
12.14 O
section O
12.2.4 O
section O
9.4 O
the O
number O
of O
independent B
parameters O
in O
this O
model O
therefore O
only O
grows O
linearly O
with O
d O
, O
for O
fixed O
m. O
if O
we O
take O
m O
= O
d O
- O
1 O
, O
then O
we O
recover O
the O
standard O
result O
for O
a O
full O
covariance B
gaussian O
. O
( O
5.170 O
) O
with O
this O
approximation O
, O
we O
now O
have O
a O
linear-gaussian O
model O
with O
a O
gaussian O
distribution O
for O
p O
( O
w O
) O
and O
a O
gaussian O
for O
p O
( O
t|w O
) O
whose O
mean B
is O
a O
linear O
function O
of O
w O
of O
the O
form O
t|y O
( O
x O
, O
wmap O
) O
+ O
gt O
( O
w O
− O
wmap O
) O
, O
β O
−1 O
. O
1.41 O
( O
( O
cid:1 O
) O
) O
www O
using O
the O
sum O
and O
product O
rules O
of O
probability B
, O
show O
that O
the O
mutual B
information I
i O
( O
x O
, O
y O
) O
satisﬁes O
the O
relation O
( O
1.121 O
) O
. O
learning B
of O
word O
stress O
in O
a O
sub-optimal O
second B
order I
backpropagation O
neural B
network I
. O
σ2 O
( O
x O
) O
= O
β O
( O
5.173 O
) O
we O
see O
that O
the O
predictive B
distribution I
p O
( O
t|x O
, O
d O
) O
is O
a O
gaussian O
whose O
mean B
is O
given O
by O
the O
network O
function O
y O
( O
x O
, O
wmap O
) O
with O
the O
parameter O
set O
to O
their O
map O
value O
. O
this O
decision B
boundary I
is O
also O
plotted O
on O
the O
left-hand O
ﬁgure O
. O
( O
10.155 O
) O
500 O
10. O
approximate O
inference B
where O
this O
is O
a O
quadratic O
function O
of O
w O
, O
and O
so O
we O
can O
obtain O
the O
corresponding O
variational B
approximation O
to O
the O
posterior O
distribution O
by O
identifying O
the O
linear O
and O
quadratic O
terms O
in O
w O
, O
giving O
a O
gaussian O
variational B
posterior O
of O
the O
form O
q O
( O
w O
) O
= O
n O
( O
w|mn O
, O
sn O
) O
( O
cid:22 O
) O
n O
( O
cid:2 O
) O
mn O
= O
sn O
s O
−1 O
n O
= O
s O
−1 O
0 O
+ O
2 O
s O
( O
tn O
− O
1/2 O
) O
φn O
−1 O
0 O
m0 O
+ O
n O
( O
cid:2 O
) O
n=1 O
λ O
( O
ξn O
) O
φnφt O
n. O
( O
cid:23 O
) O
( O
10.156 O
) O
( O
10.157 O
) O
( O
10.158 O
) O
n=1 O
as O
with O
the O
laplace O
framework O
, O
we O
have O
again O
obtained O
a O
gaussian O
approximation O
to O
the O
posterior O
distribution O
. O
because O
zn O
is O
a O
k-dimensional O
vec- O
tor O
with O
all O
elements O
equal O
to O
0 O
except O
for O
a O
single O
element O
having O
the O
value O
1 O
, O
the O
complete-data O
log O
likelihood O
function O
is O
simply O
a O
sum O
of O
k O
independent B
contribu- O
tions O
, O
one O
for O
each O
mixture B
component I
. O
( O
2000 O
) O
gave O
a O
different O
and O
very O
simple O
interpretation O
of O
boosting B
in O
terms O
of O
the O
sequential O
minimization O
of O
an O
exponential O
error O
function O
. O
we O
choose O
a O
very O
simple O
energy B
function I
for O
these O
cliques O
of O
the O
form O
−ηxiyi O
where O
η O
is O
a O
positive O
constant O
. O
, O
m O
, O
is O
shown O
in O
figure O
8.12. O
another O
way O
of O
controlling O
the O
exponential O
growth O
in O
the O
number O
of O
parameters O
in O
models O
of O
discrete O
variables O
is O
to O
use O
parameterized O
models O
for O
the O
conditional B
distributions O
instead O
of O
complete O
tables O
of O
conditional B
probability I
values O
. O
furthermore O
, O
the O
exponential O
error O
does O
not O
generalize O
to O
classiﬁcation B
problems O
having O
k O
> O
2 O
classes O
, O
again O
in O
contrast O
to O
the O
cross-entropy O
for O
a O
probabilistic O
model O
, O
which O
is O
easily O
generalized B
to O
give O
( O
4.108 O
) O
. O
a O
general O
symmetric O
covariance B
matrix I
σ O
will O
have O
d O
( O
d O
+ O
1 O
) O
/2 O
independent B
parameters O
, O
and O
there O
are O
another O
d O
independent B
parameters O
in O
µ O
, O
giv- O
ing O
d O
( O
d O
+ O
3 O
) O
/2 O
parameters O
in O
total O
. O
2 O
tr O
it O
is O
straightforward O
to O
introduce O
a O
prior B
over O
θ O
and O
to O
maximize O
the O
log O
poste- O
rior O
using O
gradient-based O
methods O
. O
to O
do O
this O
we O
assume O
that O
the O
variational B
distribution O
has O
the O
factorization B
deﬁned O
by O
( O
10.42 O
) O
and O
( O
10.55 O
) O
with O
factors O
given O
by O
( O
10.48 O
) O
, O
( O
10.57 O
) O
, O
and O
( O
10.59 O
) O
. O
derive O
the O
lower B
bound I
( O
10.144 O
) O
on O
the O
logistic B
sigmoid I
function O
directly O
by O
making O
a O
ﬁrst B
order I
taylor O
series O
expan- O
sion B
of O
the O
function O
f O
( O
x O
) O
in O
the O
variable O
x2 O
centred O
on O
the O
value O
ξ2 O
. O
556 O
11. O
sampling B
methods I
exercises O
11.1 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
ﬁnite O
sample O
estimator O
( O
cid:1 O
) O
f O
deﬁned O
by O
( O
11.2 O
) O
has O
mean B
equal O
to O
e O
[ O
f O
] O
and O
variance B
given O
by O
( O
11.3 O
) O
. O
exercise O
10.32 O
10.6.2 O
optimizing O
the O
variational B
parameters O
we O
now O
have O
a O
normalized O
gaussian O
approximation O
to O
the O
posterior O
distribution O
, O
which O
we O
shall O
use O
shortly O
to O
evaluate O
the O
predictive B
distribution I
for O
new O
data O
points O
. O
`` O
nli~e O
probabili O
, O
tic O
l'ca.lllcre O
i O
' O
no O
longer O
a O
closed-form O
maximum B
likelihood I
solution O
for O
\v O
. O
vibes O
: O
a O
variational B
inference I
engine O
for O
bayesian O
networks O
. O
if O
we O
set O
the O
derivative B
of O
ln O
p O
( O
d|µ O
) O
with O
respect O
to O
µ O
equal O
to O
zero O
, O
we O
obtain O
the O
maximum B
likelihood I
estimator O
µml O
= O
1 O
n O
xn O
n=1 O
( O
2.7 O
) O
jacob O
bernoulli O
1654–1705 O
jacob O
bernoulli O
, O
also O
known O
as O
jacques O
or O
james O
bernoulli O
, O
was O
a O
swiss O
mathematician O
and O
was O
the O
ﬁrst O
of O
many O
in O
the O
bernoulli O
family O
to O
pursue O
a O
career O
in O
science O
and O
mathematics O
. O
the O
right-hand O
plot O
shows O
the O
logistic B
sigmoid I
again O
in O
red O
together O
with O
the O
gaussian O
lower B
bound I
( O
10.144 O
) O
shown O
in O
blue O
. O
we O
see O
that O
both O
can O
be O
seen O
as O
continuous O
approximations O
to O
the O
ideal O
misclassiﬁcation O
error B
func- O
tion O
. O
( O
11.46 O
) O
( O
τ O
) O
2 O
next O
we O
replace O
z O
distribution O
( O
τ O
) O
2 O
by O
a O
value O
z O
( O
τ O
+1 O
) O
2 O
obtained O
by O
sampling O
from O
the O
conditional B
p O
( O
z2|z O
( O
τ O
+1 O
) O
1 O
( O
τ O
) O
3 O
) O
, O
z O
( O
11.47 O
) O
so O
that O
the O
new O
value O
for O
z1 O
is O
used O
straight O
away O
in O
subsequent O
sampling O
steps O
. O
the O
mechanism O
of O
sparsity B
in O
the O
context O
of O
the O
relevance B
vector I
machine I
. O
thus O
we O
see O
that O
, O
conditioned O
on O
c O
, O
the O
joint O
distribution O
of O
a O
and O
b O
factorizes O
into O
the O
prod- O
uct O
of O
the O
marginal B
distribution O
of O
a O
and O
the O
marginal B
distribution O
of O
b O
( O
again O
both O
conditioned O
on O
c O
) O
. O
input O
values O
from O
a O
patch O
are O
linearly O
combined O
using O
the O
weights O
and O
the O
bias B
, O
and O
the O
result O
transformed O
by O
a O
sigmoidal O
nonlinearity O
using O
( O
5.1 O
) O
. O
note O
that O
this O
algorithm O
is O
in O
fact O
independent B
of O
which O
node B
was O
designated O
as O
the O
root O
, O
exercise O
8.20 O
408 O
8. O
graphical O
models O
figure O
8.50 O
the O
sum-product B
algorithm I
can O
be O
viewed O
purely O
in O
terms O
of O
messages O
sent O
out O
by O
factor O
nodes O
to O
other O
factor O
nodes O
. O
the O
nonlinear O
functions O
h O
( O
· O
) O
are O
generally O
chosen O
to O
be O
sigmoidal O
functions O
such O
as O
the O
logistic B
sigmoid I
or O
the O
‘ O
tanh O
’ O
function O
. O
also O
separately O
infer O
the O
prior B
class O
probabilities O
p O
( O
ck O
) O
. O
this O
arises O
because O
the O
maximum B
likelihood I
so- O
lution O
occurs O
when O
the O
hyperplane O
corresponding O
to O
σ O
= O
0.5 O
, O
equivalent O
to O
wtφ O
= O
0 O
, O
separates O
the O
two O
classes O
and O
the O
magnitude O
of O
w O
goes O
to O
inﬁnity O
. O
this O
can O
be O
achieved O
by O
using O
a O
mixture B
model I
for O
p O
( O
t|x O
) O
in O
which O
both O
the O
mixing O
coefﬁcients O
as O
well O
as O
the O
component O
densities O
are O
ﬂexible O
functions O
of O
the O
input O
vector O
x O
, O
giving O
rise O
to O
the O
mixture B
density I
network I
. O
next O
, O
we O
consider O
the O
determination O
of O
these O
parameters O
using O
maximum B
likelihood I
( O
ghahra- O
mani O
and O
hinton O
, O
1996b O
) O
. O
on O
the O
left O
is O
the O
result O
of O
using O
the O
simple O
approximation O
( O
5.185 O
) O
based O
on O
a O
point O
estimate O
wmap O
of O
the O
parameters O
, O
in O
which O
the O
green O
curve O
shows O
the O
y O
= O
0.5 O
decision B
boundary I
, O
and O
the O
other O
contours O
correspond O
to O
output O
probabilities O
of O
y O
= O
0.1 O
, O
0.3 O
, O
0.7 O
, O
and O
0.9. O
on O
the O
right O
is O
the O
corresponding O
result O
obtained O
using O
( O
5.190 O
) O
. O
if O
we O
consider O
covariance B
matrices O
that O
are O
diagonal B
, O
so O
that O
σ O
= O
diag O
( O
σ2 O
i O
) O
, O
we O
then O
have O
a O
total O
of O
2d O
inde- O
pendent O
parameters O
in O
the O
density B
model O
. O
if O
we O
assume O
a O
completely O
ﬂexible O
function O
y O
( O
x O
) O
, O
we O
can O
do O
this O
formally O
using O
the O
calculus B
of I
variations I
to O
give O
( O
cid:6 O
) O
{ O
y O
( O
x O
) O
− O
t O
} O
p O
( O
x O
, O
t O
) O
dt O
= O
0 O
. O
( O
1.88 O
) O
solving O
for O
y O
( O
x O
) O
, O
and O
using O
the O
sum O
and O
product O
rules O
of O
probability B
, O
we O
obtain O
δe O
[ O
l O
] O
δy O
( O
x O
) O
= O
2 O
( O
cid:6 O
) O
( O
cid:6 O
) O
y O
( O
x O
) O
= O
tp O
( O
x O
, O
t O
) O
dt O
= O
p O
( O
x O
) O
tp O
( O
t|x O
) O
dt O
= O
et O
[ O
t|x O
] O
( O
1.89 O
) O
figure O
1.28 O
the O
regression B
function I
y O
( O
x O
) O
, O
which O
minimizes O
the O
expected O
squared O
loss O
, O
is O
given O
by O
the O
mean B
of O
the O
conditional B
distri- O
bution O
p O
( O
t|x O
) O
. O
we O
illustrate O
the O
bernoulli O
mixture B
model I
in O
figure O
9.10 O
by O
using O
it O
to O
model O
handwritten O
digits O
. O
it O
is O
easy O
to O
show O
that O
the O
representation O
on O
the O
right- O
hand O
side O
of O
( O
8.5 O
) O
is O
always O
correctly O
normalized O
provided O
the O
individual O
conditional B
distributions O
are O
normalized O
. O
using O
the O
orthogonality O
property O
rrt O
= O
i O
, O
we O
see O
that O
the O
quantity O
wwt O
that O
appears O
in O
the O
covariance B
matrix I
c O
takes O
the O
form O
( O
12.39 O
) O
and O
hence O
is O
independent B
of O
r. O
thus O
there O
is O
a O
whole O
family O
of O
matrices O
w O
all O
of O
which O
give O
rise O
to O
the O
same O
predictive B
distribution I
. O
( O
6.7 O
) O
setting O
the O
gradient O
of O
j O
( O
a O
) O
with O
respect O
to O
a O
to O
zero O
, O
we O
obtain O
the O
following O
solu- O
tion O
a O
= O
( O
k O
+ O
λin O
) O
−1 O
t. O
( O
6.8 O
) O
294 O
6. O
kernel O
methods O
if O
we O
substitute O
this O
back O
into O
the O
linear B
regression I
model O
, O
we O
obtain O
the O
following O
prediction O
for O
a O
new O
input O
x O
y O
( O
x O
) O
= O
wtφ O
( O
x O
) O
= O
atφφ O
( O
x O
) O
= O
k O
( O
x O
) O
t O
( O
k O
+ O
λin O
) O
−1 O
t O
( O
6.9 O
) O
where O
we O
have O
deﬁned O
the O
vector O
k O
( O
x O
) O
with O
elements O
kn O
( O
x O
) O
= O
k O
( O
xn O
, O
x O
) O
. O
( O
5.141 O
) O
5.5. O
regularization B
in O
neural O
networks O
271 O
the O
effect O
of O
the O
regularization B
term O
is O
therefore O
to O
pull O
each O
weight O
towards O
the O
centre O
of O
the O
jth O
gaussian O
, O
with O
a O
force O
proportional O
to O
the O
posterior B
probability I
of O
that O
gaussian O
for O
the O
given O
weight O
. O
, O
zm O
sampling O
from O
the O
conditional B
distributions O
p O
( O
zi|pai O
) O
. O
for O
large O
d O
, O
the O
total O
number O
of O
parameters O
exercise O
2.21 O
84 O
2. O
probability B
distributions O
figure O
2.8 O
contours O
of O
constant O
probability B
density O
for O
a O
gaussian O
distribution O
in O
two O
dimensions O
in O
which O
the O
covariance B
matrix I
is O
( O
a O
) O
of O
general O
form O
, O
( O
b O
) O
diagonal B
, O
in O
which O
the O
elliptical O
contours O
are O
aligned O
with O
the O
coordinate O
axes O
, O
and O
( O
c O
) O
proportional O
to O
the O
identity O
matrix O
, O
in O
which O
the O
contours O
are O
concentric O
circles O
. O
to O
do O
this O
, O
we O
ﬁrst O
deﬁne O
the O
kinetic B
energy I
by O
k O
( O
r O
) O
= O
( O
cid:5 O
) O
r O
( O
cid:5 O
) O
2 O
= O
1 O
2 O
1 O
2 O
r2 O
i O
. O
( O
9.10 O
) O
similarly O
, O
the O
conditional B
distribution O
of O
x O
given O
a O
particular O
value O
for O
z O
is O
a O
gaussian O
k=1 O
p O
( O
x|zk O
= O
1 O
) O
= O
n O
( O
x|µk O
, O
σk O
) O
k O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
k=1 O
which O
can O
also O
be O
written O
in O
the O
form O
p O
( O
x|z O
) O
= O
n O
( O
x|µk O
, O
σk O
) O
zk O
. O
similarly O
, O
if O
x O
is O
a O
point O
on O
the O
decision B
surface I
, O
then O
y O
( O
x O
) O
= O
0 O
, O
and O
so O
the O
normal O
distance O
from O
the O
origin O
to O
the O
decision B
surface I
is O
given O
by O
( O
4.5 O
) O
wtx O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
= O
− O
w0 O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
. O
for O
m O
hidden O
units O
, O
there O
will O
be O
m O
such O
‘ O
sign-ﬂip O
’ O
232 O
5. O
neural O
networks O
figure O
5.4 O
example O
of O
the O
solution O
of O
a O
simple O
two- O
class O
classiﬁcation B
problem O
involving O
synthetic O
data O
using O
a O
neural B
network I
having O
two O
inputs O
, O
two O
hidden O
units O
with O
‘ O
tanh O
’ O
activation O
functions O
, O
and O
a O
single O
output O
having O
a O
logistic B
sigmoid I
activa- O
tion O
function O
. O
we O
saw O
that O
, O
for O
independent O
variables O
x O
and O
y O
for O
which O
p O
( O
x O
, O
y O
) O
= O
p O
( O
x O
) O
p O
( O
y O
) O
, O
the O
entropy B
functions O
are O
additive O
, O
so O
that O
h O
( O
x O
, O
y O
) O
= O
h O
( O
x O
) O
+ O
h O
( O
y O
) O
. O
consider O
a O
small O
region O
r O
of O
phase B
space I
that O
, O
under O
a O
sequence O
of O
l O
leapfrog O
iterations O
of O
step O
size O
 O
, O
maps O
to O
a O
region O
r O
( O
cid:4 O
) O
. O
section O
10.1.4 O
exercise O
10.21 O
10.2.4 O
determining O
the O
number O
of O
components O
we O
have O
seen O
that O
the O
variational B
lower O
bound O
can O
be O
used O
to O
determine O
a O
pos- O
terior O
distribution O
over O
the O
number O
k O
of O
components O
in O
the O
mixture B
model I
. O
in O
particular O
, O
we O
shall O
show O
that O
the O
maximum B
likelihood I
approach O
systematically O
underestimates O
the O
variance B
of O
the O
distribution O
. O
we O
can O
similarly O
deﬁne O
re-scaled O
variables O
( O
cid:1 O
) O
β O
( O
zn O
) O
using O
( O
cid:23 O
) O
( O
cid:1 O
) O
β O
( O
zn O
) O
tities O
( O
cid:1 O
) O
β O
( O
zn O
) O
are O
simply O
the O
ratio O
of O
two O
conditional B
probabilities O
which O
will O
again O
remain O
within O
machine O
precision O
because O
, O
from O
( O
13.35 O
) O
, O
the O
quan- O
we O
have O
to O
evaluate O
and O
store O
cn O
, O
which O
is O
easily O
done O
because O
it O
is O
the O
coefﬁcient O
n O
( O
cid:14 O
) O
β O
( O
zn O
) O
= O
cm O
m=n+1 O
( O
13.59 O
) O
( O
13.60 O
) O
( O
cid:22 O
) O
zn−1 O
p O
( O
xn+1 O
, O
. O
then O
, O
by O
maximizing O
the O
bound O
with O
respect O
to O
these O
parameters O
, O
derive O
the O
re-estimation O
equations O
for O
the O
factors O
in O
the O
variational B
distribution O
, O
and O
show O
that O
these O
are O
the O
same O
as O
those O
obtained O
in O
section O
10.2.1 O
. O
10.6.2 O
optimizing O
the O
variational B
parameters O
. O
note O
that O
the O
bias B
of O
the O
maximum B
likelihood I
solution O
becomes O
less O
signiﬁcant O
as O
the O
number O
n O
of O
data O
points O
increases O
, O
and O
in O
the O
limit O
n O
→ O
∞ O
the O
maximum B
likelihood I
solution O
for O
the O
variance B
equals O
the O
true O
variance B
of O
the O
distribution O
that O
generated O
the O
data O
. O
we O
shall O
.iew O
factor B
analysis I
as O
a O
form O
of O
lalent O
`` O
ariable O
densily O
model O
. O
these O
can O
arise O
, O
for O
example O
, O
with O
inverse B
problems O
in O
which O
the O
distribution O
can O
be O
multimodal O
, O
in O
which O
case O
the O
gaussian O
assumption O
can O
lead O
to O
very O
poor O
predic- O
tions O
. O
consider O
, O
for O
the O
sake O
of O
illustration O
, O
a O
somewhat O
artiﬁcial O
problem O
in O
which O
we O
wish O
to O
sample O
from O
a O
zero-mean O
mul- O
tivariate O
gaussian O
distribution O
with O
covariance B
σ2 O
pi O
, O
where O
i O
is O
the O
unit O
matrix O
, O
by O
rejection B
sampling I
from O
a O
proposal B
distribution I
that O
is O
itself O
a O
zero-mean O
gaussian O
distribution O
having O
covariance B
σ2 O
p O
in O
order O
that O
there O
exists O
a O
k O
such O
that O
kq O
( O
z O
) O
( O
cid:2 O
) O
p O
( O
z O
) O
. O
the O
functional B
form O
of O
the O
model O
was O
the O
same O
as O
for O
the O
perceptron B
, O
but O
a O
different O
approach O
to O
training B
was O
adopted O
( O
widrow O
and O
hoff O
, O
1960 O
; O
widrow O
and O
lehr O
, O
1990 O
) O
. O
for O
k O
> O
2 O
classes O
, O
we O
again O
make O
use O
of O
the O
probabilistic O
approach O
in O
section O
4.3.4 O
in O
which O
there O
are O
k O
linear O
models O
of O
the O
form O
ak O
= O
wt O
k O
x O
( O
7.120 O
) O
356 O
7. O
sparse O
kernel O
machines O
2 O
0 O
−2 O
−2 O
0 O
2 O
figure O
7.12 O
example O
of O
the O
relevance B
vector I
machine I
applied O
to O
a O
synthetic O
data O
set O
, O
in O
which O
the O
left-hand O
plot O
shows O
the O
decision B
boundary I
and O
the O
data O
points O
, O
with O
the O
relevance O
vectors O
indicated O
by O
circles O
. O
our O
discussion O
will O
also O
form O
the O
basis O
for O
the O
derivation O
of O
the O
variational B
inference I
framework O
. O
similarly O
, O
maximizing O
( O
2.181 O
) O
with O
respect O
to O
m O
, O
and O
making O
use O
of O
i O
i1 O
( O
m O
) O
( O
abramowitz O
and O
stegun O
, O
1965 O
) O
, O
we O
have O
a O
( O
m O
) O
= O
1 O
n O
cos O
( O
θn O
− O
θml O
0 O
) O
( O
2.185 O
) O
where O
we O
have O
substituted O
for O
the O
maximum B
likelihood I
solution O
for O
θml O
that O
we O
are O
performing O
a O
joint O
optimization O
over O
θ O
and O
m O
) O
, O
and O
we O
have O
deﬁned O
0 O
( O
recalling O
n O
( O
cid:2 O
) O
n=1 O
a O
( O
m O
) O
= O
i1 O
( O
m O
) O
i0 O
( O
m O
) O
. O
also O
show O
that O
in O
the O
m O
step O
, O
the O
initial O
probability B
and O
transition B
probability I
parameters O
are O
re-estimated O
using O
modiﬁed O
forms O
of O
( O
13.18 O
) O
and O
( O
13.19 O
) O
given O
by O
( O
r O
) O
1k O
) O
r=1 O
γ O
( O
z O
r O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
r O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
r O
( O
cid:2 O
) O
r O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
γ O
( O
z O
n=2 O
r=1 O
r=1 O
j=1 O
( O
r O
) O
1j O
) O
πk O
= O
ajk O
= O
ξ O
( O
z O
( O
r O
) O
n−1 O
, O
j O
, O
z O
( O
r O
) O
n O
, O
k O
) O
ξ O
( O
z O
( O
r O
) O
n−1 O
, O
j O
, O
z O
( O
r O
) O
n O
, O
l O
) O
r=1 O
l=1 O
n=2 O
exercises O
649 O
( O
13.124 O
) O
( O
13.125 O
) O
where O
, O
for O
notational O
convenience O
, O
we O
have O
assumed O
that O
the O
sequences O
are O
of O
the O
same O
length O
( O
the O
generalization B
to O
sequences O
of O
different O
lengths O
is O
straightforward O
) O
. O
to O
ﬁnd O
the O
conditional B
distribution O
p O
( O
tn O
+1|t O
) O
, O
we O
begin O
by O
writing O
down O
the O
joint O
distribution O
p O
( O
tn O
+1 O
) O
, O
where O
tn O
+1 O
denotes O
the O
vector O
( O
t1 O
, O
. O
a O
single O
attenuation O
measurement O
alone O
is O
not O
sufﬁcient O
because O
there O
are O
two O
degrees B
of I
freedom I
corresponding O
to O
the O
fraction O
of O
oil O
and O
the O
fraction O
of O
water O
( O
the O
fraction O
of O
gas O
is O
redundant O
because O
the O
three O
fractions O
must O
add O
to O
one O
) O
. O
the O
technique O
of O
importance B
sampling I
provides O
a O
framework O
for O
approximating O
expectations O
di- O
rectly O
but O
does O
not O
itself O
provide O
a O
mechanism O
for O
drawing O
samples O
from O
distribution O
p O
( O
z O
) O
. O
490 O
10. O
approximate O
inference B
figure O
10.9 O
plot O
of O
the O
lower B
bound I
l O
ver- O
sus O
the O
order O
m O
of O
the O
polyno- O
mial O
, O
for O
a O
polynomial O
model O
, O
in O
which O
a O
set O
of O
10 O
data O
points O
is O
generated O
from O
a O
polynomial O
with O
m O
= O
3 O
sampled O
over O
the O
inter- O
val O
( O
−5 O
, O
5 O
) O
with O
additive O
gaussian O
noise O
of O
variance B
0.09. O
the O
value O
of O
the O
bound O
gives O
the O
log O
prob- O
ability O
of O
the O
model O
, O
and O
we O
see O
that O
the O
value O
of O
the O
bound O
peaks O
at O
m O
= O
3 O
, O
corresponding O
to O
the O
true O
model O
from O
which O
the O
data O
set O
was O
generated O
. O
( O
1.53 O
) O
section O
1.2.5 O
n=1 O
when O
viewed O
as O
a O
function O
of O
µ O
and O
σ2 O
, O
this O
is O
the O
likelihood B
function I
for O
the O
gaus- O
sian O
and O
is O
interpreted O
diagrammatically O
in O
figure O
1.14. O
one O
common O
criterion O
for O
determining O
the O
parameters O
in O
a O
probability B
distribu- O
tion O
using O
an O
observed O
data O
set O
is O
to O
ﬁnd O
the O
parameter O
values O
that O
maximize O
the O
likelihood B
function I
. O
the O
reader O
is O
encouraged O
to O
work O
through O
this O
example O
in O
detail O
as O
it O
provides O
many O
insights O
into O
the O
practical O
appli- O
cation O
of O
variational B
methods O
. O
kernel B
substitution I
in O
( O
6.24 O
) O
to O
replace O
xtx O
( O
cid:4 O
) O
obtain O
the O
gaussian O
kernel O
is O
not O
restricted O
to O
the O
use O
of O
euclidean O
distance O
. O
section O
4.3.3 O
exercise O
14.16 O
14.5.3 O
mixtures O
of O
experts O
in O
section O
14.5.1 O
, O
we O
considered O
a O
mixture O
of O
linear O
regression B
models O
, O
and O
in O
section O
14.5.2 O
we O
discussed O
the O
analogous O
mixture O
of O
linear O
classiﬁers O
. O
12.28 O
( O
* O
* O
) O
iii O
! O
i O
use O
the O
transformation O
property O
( O
1.27 O
) O
of O
a O
probability B
density O
under O
a O
change O
of O
variable O
to O
show O
that O
any O
density B
p O
( O
y O
) O
can O
be O
obtained O
from O
a O
fixed O
density B
q O
( O
x O
) O
that O
is O
everywhere O
nonzero O
by O
making O
a O
nonlinear O
change O
of O
variable O
y O
= O
f O
( O
x O
) O
in O
which O
f O
( O
x O
) O
is O
a O
monotonic O
function O
so O
that O
0 O
: O
: O
: O
; O
j O
' O
( O
x O
) O
< O
00. O
write O
down O
the O
differential B
equation O
satisfied O
by O
f O
( O
x O
) O
and O
draw O
a O
diagram O
illustrating O
the O
transformation O
of O
the O
density B
. O
there O
may O
be O
several O
feature O
maps O
in O
a O
given O
convolutional B
layer O
for O
each O
plane O
of O
units O
in O
the O
previous O
subsampling B
layer O
, O
so O
that O
the O
gradual O
reduction O
in O
spatial O
resolution O
is O
then O
compensated O
by O
an O
increas- O
ing O
number O
of O
features O
. O
we O
shall O
deﬁne O
the O
joint O
distribution O
p O
( O
x O
, O
z O
) O
in O
terms O
of O
a O
marginal B
dis- O
tribution O
p O
( O
z O
) O
and O
a O
conditional B
distribution O
p O
( O
x|z O
) O
, O
corresponding O
to O
the O
graphical B
model I
in O
figure O
9.4. O
the O
marginal B
distribution O
over O
z O
is O
speciﬁed O
in O
terms O
of O
the O
mixing O
coefﬁcients O
πk O
, O
such O
that O
( O
cid:5 O
) O
k O
( O
cid:2 O
) O
k=1 O
p O
( O
zk O
= O
1 O
) O
= O
πk O
9.2. O
mixtures O
of O
gaussians O
431 O
figure O
9.4 O
graphical O
representation O
of O
a O
mixture B
model I
, O
in O
which O
the O
joint O
distribution O
is O
expressed O
in O
the O
form O
p O
( O
x O
, O
z O
) O
= O
p O
( O
z O
) O
p O
( O
x|z O
) O
. O
this O
in O
turn O
deﬁnes O
a O
non-gaussian O
process O
over O
tn O
+1 O
, O
and O
by O
conditioning O
on O
the O
training B
data O
tn O
we O
obtain O
the O
required O
predictive O
distri- O
bution O
. O
the O
jacobian O
can O
also O
be O
evaluated O
using O
an O
alternative O
forward B
propagation I
formalism O
, O
which O
can O
be O
derived O
in O
an O
analogous O
way O
to O
the O
backpropagation B
approach O
given O
here O
. O
if O
the O
variables O
are O
discrete O
, O
and O
if O
the O
conditional B
distri- O
butions O
are O
represented O
by O
general O
conditional B
probability I
tables O
, O
then O
the O
number O
of O
parameters O
in O
such O
a O
model O
will O
have O
km−1 O
( O
k O
− O
1 O
) O
parameters O
. O
in O
the O
univariate O
case O
, O
we O
considered O
the O
second B
order I
moment O
given O
by O
e O
[ O
x2 O
] O
. O
0.15 O
0.12 O
0.09 O
0.06 O
0.03 O
0 O
−3 O
( O
bias B
) O
2 O
variance B
( O
bias B
) O
2 O
+ O
variance B
test O
error B
−2 O
−1 O
0 O
1 O
2 O
ln O
λ O
ﬁt O
a O
model O
with O
24 O
gaussian O
basis O
functions O
by O
minimizing O
the O
regularized O
error O
function O
( O
3.27 O
) O
to O
give O
a O
prediction O
function O
y O
( O
l O
) O
( O
x O
) O
as O
shown O
in O
figure O
3.5. O
the O
top O
row O
corresponds O
to O
a O
large O
value O
of O
the O
regularization B
coefﬁcient O
λ O
that O
gives O
low O
variance B
( O
because O
the O
red O
curves O
in O
the O
left O
plot O
look O
similar O
) O
but O
high O
bias B
( O
because O
the O
two O
curves O
in O
the O
right O
plot O
are O
very O
different O
) O
. O
now O
consider O
how O
to O
determine O
the O
structure O
of O
the O
decision B
tree I
. O
the O
conjugate B
prior I
for O
µ O
is O
the O
gaussian O
, O
and O
the O
conjugate B
prior I
for O
τ O
is O
the O
gamma B
distribution I
. O
this O
requires O
the O
second O
derivatives O
of O
ψ O
( O
an O
) O
, O
which O
we O
also O
require O
for O
the O
laplace O
approximation O
anyway O
, O
and O
which O
are O
given O
by O
∇∇ψ O
( O
an O
) O
= O
−wn O
− O
c O
−1 O
n O
( O
6.82 O
) O
where O
wn O
is O
a O
diagonal B
matrix O
with O
elements O
σ O
( O
an O
) O
( O
1− O
σ O
( O
an O
) O
) O
, O
and O
we O
have O
used O
the O
result O
( O
4.88 O
) O
for O
the O
derivative B
of O
the O
logistic B
sigmoid I
function O
. O
w O
( O
m+1 O
) O
= O
w O
( O
m O
) O
n O
n O
( O
14.26 O
) O
because O
the O
term O
exp O
( O
−αm/2 O
) O
is O
independent B
of O
n O
, O
we O
see O
that O
it O
weights O
all O
data O
points O
by O
the O
same O
factor O
and O
so O
can O
be O
discarded O
. O
−1 O
n O
, O
and O
hence O
( O
3.84 O
) O
is O
equivalent O
to O
the O
previous O
( O
3.84 O
) O
using O
( O
3.54 O
) O
, O
we O
see O
that O
a O
= O
s O
deﬁnition O
( O
3.53 O
) O
, O
and O
therefore O
represents O
the O
mean B
of O
the O
posterior O
distribution O
. O
we O
can O
obtain O
a O
sequential B
learning I
algorithm O
by O
applying O
the O
technique O
of O
stochastic B
gradient I
descent I
, O
also O
known O
as O
sequential B
gradient I
descent I
, O
as O
follows O
. O
the O
process O
of O
evaluating O
( O
8.97 O
) O
will O
also O
give O
the O
value O
xmax O
for O
the O
most O
probable O
value O
of O
the O
root B
node I
variable O
, O
deﬁned O
by O
xmax O
= O
arg O
max O
x O
µfs→x O
( O
x O
) O
( O
8.98 O
) O
at O
this O
point O
, O
we O
might O
be O
tempted O
simply O
to O
continue O
with O
the O
message B
passing I
al- O
gorithm O
and O
send O
messages O
from O
the O
root O
back O
out O
to O
the O
leaves O
, O
using O
( O
8.93 O
) O
and O
( O
8.94 O
) O
, O
then O
apply O
( O
8.98 O
) O
to O
all O
of O
the O
remaining O
variable O
nodes O
. O
thus O
p O
( O
r O
) O
δr O
is O
the O
probability B
mass O
inside O
a O
thin O
shell O
of O
thickness O
δr O
located O
at O
radius O
r. O
this O
distribution O
is O
plotted O
, O
for O
various O
values O
of O
d O
, O
in O
figure O
1.23 O
, O
and O
we O
see O
that O
for O
large O
d O
the O
probability B
mass O
of O
the O
gaussian O
is O
concentrated O
in O
a O
thin O
shell O
. O
in O
the O
terminology O
of O
statistics O
, O
this O
model O
is O
known O
as O
logistic B
regression I
, O
although O
it O
should O
be O
emphasized O
that O
this O
is O
a O
model O
for O
classiﬁcation B
rather O
than O
regression B
. O
( O
1.54 O
) O
n O
( O
cid:2 O
) O
n=1 O
maximizing O
( O
1.54 O
) O
with O
respect O
to O
µ O
, O
we O
obtain O
the O
maximum B
likelihood I
solution O
given O
by O
µml O
= O
( O
1.55 O
) O
which O
is O
the O
sample B
mean I
, O
i.e. O
, O
the O
mean B
of O
the O
observed O
values O
{ O
xn O
} O
. O
recall O
that O
our O
goal O
is O
to O
calculate O
the O
marginal B
for O
variable O
node B
x O
, O
and O
that O
this O
marginal B
is O
given O
by O
the O
product O
of O
incoming O
messages O
along O
all O
of O
the O
links O
arriving O
at O
that O
node B
. O
1.2. O
probability B
theory O
13 O
figure O
1.10 O
we O
can O
derive O
the O
sum O
and O
product O
rules O
of O
probability B
by O
considering O
two O
random O
variables O
, O
x O
, O
which O
takes O
the O
values O
{ O
xi O
} O
where O
i O
= O
1 O
, O
. O
we O
can O
achieve O
this O
by O
introducing O
additional O
latent O
variables O
to O
permit O
a O
rich O
class O
of O
models O
to O
be O
constructed O
out O
of O
simple O
components O
, O
as O
we O
did O
with O
mixture B
distributions O
in O
chapter O
9 O
and O
with O
continuous O
latent B
variable I
models O
in O
chapter O
12. O
for O
each O
observation O
xn O
, O
we O
introduce O
a O
corresponding O
latent B
variable I
zn O
( O
which O
may O
be O
of O
different O
type O
or O
dimensionality O
to O
the O
observed B
variable I
) O
. O
we O
can O
express O
the O
category O
of O
a O
digit O
using O
target B
vector I
t O
, O
which O
represents O
the O
identity O
of O
the O
corresponding O
digit O
. O
in O
the O
case O
of O
a O
gaussian O
emission O
density O
, O
we O
can O
use O
the O
linear- O
gaussian O
framework O
in O
which O
the O
conditional B
distribution O
for O
xn O
given O
the O
values O
of O
the O
previous O
observations O
, O
and O
the O
value O
of O
zn O
, O
is O
a O
gaussian O
whose O
mean B
is O
a O
linear O
combination O
of O
the O
values O
of O
the O
conditioning O
variables O
. O
suppose O
our O
observed B
variable I
consists O
of O
a O
d-dimensional O
vector O
x O
= O
( O
x1 O
, O
. O
( O
9.17 O
) O
( O
9.18 O
) O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n=1 O
436 O
9. O
mixture B
models O
and O
em O
section O
2.3.4 O
appendix O
e O
we O
can O
interpret O
nk O
as O
the O
effective O
number O
of O
points O
assigned O
to O
cluster O
k. O
note O
carefully O
the O
form O
of O
this O
solution O
. O
the O
predictive B
distribution I
for O
the O
synthetic O
sinusoidal O
regression B
problem O
is O
illustrated O
in O
figure O
1.17 O
. O
the O
simplest O
regularizer O
is O
the O
quadratic O
, O
giving O
a O
regularized O
error O
5.5. O
regularization B
in O
neural O
networks O
257 O
1 O
0 O
−1 O
m O
= O
1 O
1 O
0 O
−1 O
m O
= O
3 O
1 O
0 O
−1 O
m O
= O
10 O
0 O
1 O
0 O
1 O
0 O
1 O
figure O
5.9 O
examples O
of O
two-layer O
networks O
trained O
on O
10 O
data O
points O
drawn O
from O
the O
sinusoidal B
data I
set O
. O
7.1.5 O
computational B
learning I
theory I
. O
making O
use O
of O
( O
2.105 O
) O
and O
( O
2.108 O
) O
we O
see O
that O
the O
mean B
and O
covariance B
of O
the O
marginal B
distribution O
p O
( O
y O
) O
are O
given O
by O
e O
[ O
y O
] O
= O
aµ O
+ O
b O
cov O
[ O
y O
] O
= O
l−1 O
+ O
aλ O
−1at O
. O
0 O
) O
= O
n O
( O
µ|µ0 O
, O
σ2 O
( O
cid:18 O
) O
as O
a O
second O
example O
, O
consider O
a O
density B
of O
the O
form O
p O
( O
x|σ O
) O
= O
1 O
σ O
f O
x O
σ O
( O
2.236 O
) O
exercise O
2.59 O
where O
σ O
> O
0. O
note O
that O
this O
will O
be O
a O
normalized O
density O
provided O
f O
( O
x O
) O
is O
correctly O
normalized O
. O
exercise O
10.39 O
similarly O
, O
we O
compute O
the O
mean B
and O
variance B
of O
qnew O
( O
θ O
) O
by O
ﬁnding O
the O
mean B
and O
variance B
of O
q O
\n O
( O
θ O
) O
fn O
( O
θ O
) O
to O
give O
\n O
v O
m O
= O
m\n O
+ O
ρn O
v\n O
+ O
1 O
\n O
) O
2 O
\n O
− O
ρn O
( O
v O
v\n O
+ O
1 O
v O
= O
v O
( O
xn O
− O
m\n O
) O
+ O
ρn O
( O
1 O
− O
ρn O
) O
( O
v O
\n O
) O
2 O
( O
cid:5 O
) O
xn O
− O
m\n O
( O
cid:5 O
) O
2 O
d O
( O
v\n O
+ O
1 O
) O
2 O
( O
10.217 O
) O
( O
10.218 O
) O
where O
the O
quantity O
we O
use O
( O
10.207 O
) O
to O
compute O
the O
reﬁned O
factor O
( O
cid:4 O
) O
fn O
( O
θ O
) O
whose O
parameters O
are O
given O
by O
has O
a O
simple O
interpretation O
as O
the O
probability B
of O
the O
point O
xn O
not O
being O
clutter O
. O
in O
the O
m O
step O
, O
we O
then O
maximize O
the O
expected O
complete-data O
log O
likelihood O
which O
is O
given O
by O
q O
( O
ξ O
, O
ξold O
) O
= O
e O
[ O
ln O
h O
( O
w O
, O
ξ O
) O
p O
( O
w O
) O
] O
( O
10.160 O
) O
where O
the O
expectation B
is O
taken O
with O
respect O
to O
the O
posterior O
distribution O
q O
( O
w O
) O
evalu- O
ated O
using O
ξold O
. O
if O
the O
tree B
is O
condensed O
, O
so O
that O
any O
clique B
that O
is O
a O
subset O
of O
another O
clique B
is O
absorbed O
into O
the O
larger O
clique B
, O
this O
gives O
a O
junction O
tree O
. O
for O
instance O
, O
in O
the O
chain O
example O
of O
figure O
8.10 O
, O
we O
can O
arrange O
that O
all O
of O
the O
conditional B
distributions O
p O
( O
xi|xi−1 O
) O
, O
for O
i O
= O
2 O
, O
. O
( O
10.173 O
) O
504 O
10. O
approximate O
inference B
with O
this O
factorization B
we O
can O
appeal O
to O
the O
general O
result O
( O
10.9 O
) O
to O
ﬁnd O
expressions O
for O
the O
optimal O
factors O
. O
6.19 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
another O
viewpoint O
on O
kernel B
regression I
comes O
from O
a O
consideration O
of O
re- O
gression O
problems O
in O
which O
the O
input O
variables O
as O
well O
as O
the O
target O
variables O
are O
corrupted O
with O
additive O
noise O
. O
note O
that O
we O
are O
working O
in O
terms O
of O
precision O
matrices O
rather O
than O
covariance B
matrices O
as O
this O
somewhat O
simpliﬁes O
the O
mathemat- O
ics O
. O
if O
the O
factor B
graph I
was O
derived O
from O
a O
directed B
graph O
, O
then O
the O
joint O
distribution O
is O
already O
correctly O
nor- O
malized O
, O
and O
so O
the O
marginals O
obtained O
by O
the O
sum-product B
algorithm I
will O
similarly O
be O
normalized O
correctly O
. O
using O
( O
c.19 O
) O
, O
the O
derivative B
of O
the O
log O
likelihood O
with O
respect O
to O
µ O
is O
given O
by O
ln O
p O
( O
x|µ O
, O
σ O
) O
= O
∂ O
∂µ O
−1 O
( O
xn O
− O
µ O
) O
σ O
( O
2.120 O
) O
and O
setting O
this O
derivative B
to O
zero O
, O
we O
obtain O
the O
solution O
for O
the O
maximum B
likelihood I
estimate O
of O
the O
mean B
given O
by O
µml O
= O
1 O
n O
xn O
( O
2.121 O
) O
94 O
2. O
probability B
distributions O
exercise O
2.34 O
which O
is O
the O
mean B
of O
the O
observed O
set O
of O
data O
points O
. O
several O
widely O
used O
techniques O
are O
examples O
of O
linear-gaussian O
models O
, O
such O
as O
probabilistic O
principal O
component O
analysis O
, O
factor B
analysis I
, O
and O
linear O
dy- O
namical O
systems O
( O
roweis O
and O
ghahramani O
, O
1999 O
) O
. O
6.4.1 O
linear B
regression I
revisited O
. O
µ1 O
x1 O
µ1 O
x1 O
µ2 O
x2 O
x2 O
µ O
369 O
µm O
xm O
xm O
ter O
µi O
representing O
the O
probability B
p O
( O
xi O
= O
1 O
) O
, O
giving O
m O
parameters O
in O
total O
for O
the O
parent O
nodes O
. O
because O
we O
are O
dealing O
with O
mixture B
distributions O
, O
it O
is O
convenient O
to O
view O
the O
mixing O
coefﬁcients O
πk O
( O
x O
) O
as O
x-dependent O
prior B
probabilities O
and O
to O
introduce O
the O
corresponding O
posterior O
probabilities O
given O
by O
( O
cid:5 O
) O
k O
γk O
( O
t|x O
) O
= O
πknnk O
l=1 O
πlnnl O
( O
5.154 O
) O
where O
nnk O
denotes O
n O
( O
tn|µk O
( O
xn O
) O
, O
σ2 O
k O
( O
xn O
) O
) O
. O
the O
presence O
of O
the O
sum O
prevents O
the O
logarithm O
from O
acting O
directly O
on O
the O
joint O
distribution O
, O
resulting O
in O
complicated O
expressions O
for O
the O
maximum B
likelihood I
solution O
. O
l=1 O
αlyl O
( O
x O
) O
m O
( O
cid:2 O
) O
1 O
2 O
660 O
14. O
combining B
models I
m O
= O
2 O
−1 O
0 O
1 O
2 O
m O
= O
10 O
m O
= O
1 O
−1 O
0 O
1 O
2 O
m O
= O
6 O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
2 O
0 O
−2 O
m O
= O
3 O
−1 O
0 O
1 O
2 O
m O
= O
150 O
−1 O
0 O
1 O
2 O
−1 O
0 O
1 O
2 O
−1 O
0 O
1 O
2 O
figure O
14.2 O
illustration O
of O
boosting B
in O
which O
the O
base O
learners O
consist O
of O
simple O
thresholds O
applied O
to O
one O
or O
other O
of O
the O
axes O
. O
at O
each O
step O
there O
will O
be O
some O
number O
of O
candidate O
regions O
in O
input O
space O
that O
can O
be O
split O
, O
corresponding O
to O
the O
addition O
of O
a O
pair O
of O
leaf O
nodes O
to O
the O
existing O
tree B
. O
now O
consider O
the O
eigenvector O
equation O
for O
the O
covariance B
matrix I
σui O
= O
λiui O
( O
2.45 O
) O
exercise O
2.17 O
exercise O
2.18 O
where O
i O
= O
1 O
, O
. O
the O
model O
has O
multiple O
markov O
chains O
of O
continuous O
linear-gaussian O
latent O
vari- O
ables O
, O
each O
of O
which O
is O
analogous O
to O
the O
latent O
chain O
of O
the O
linear B
dynamical I
system I
discussed O
earlier O
, O
together O
with O
a O
markov O
chain O
of O
discrete O
variables O
of O
the O
form O
used O
in O
a O
hidden O
markov O
model O
. O
in O
the O
m O
step O
we O
maximize O
the O
expected O
complete-data O
log O
likelihood O
, O
which O
is O
deﬁned O
by O
ew O
[ O
ln O
p O
( O
t|x O
, O
w O
, O
β O
) O
p O
( O
w|α O
) O
] O
( O
9.66 O
) O
exercise O
9.22 O
where O
the O
expectation B
is O
taken O
with O
respect O
to O
the O
posterior O
distribution O
computed O
using O
the O
‘ O
old O
’ O
parameter O
values O
. O
having O
found O
a O
mode O
wmap O
, O
we O
can O
then O
build O
a O
local B
gaussian O
approximation O
by O
evaluating O
the O
matrix O
of O
second O
derivatives O
of O
the O
negative O
log O
posterior O
distribu- O
tion O
. O
it O
can O
be O
cast O
in O
a O
particularly O
simple O
and O
general O
form O
if O
we O
ﬁrst O
introduce O
a O
new O
graphical O
construction O
called O
a O
factor B
graph I
( O
frey O
, O
1998 O
; O
kschischnang O
et O
al. O
, O
2001 O
) O
. O
if O
, O
however O
, O
we O
wish O
to O
compare O
different O
values O
of O
k O
, O
then O
we O
need O
to O
take O
account O
of O
this O
multimodality B
. O
locally B
linear I
embedding I
, O
or O
lle O
( O
roweis O
and O
saul O
, O
2000 O
) O
first O
computes O
the O
set O
of O
coefficients O
that O
best O
reconstructs O
each O
data O
point O
from O
its O
neighbours O
. O
this O
can O
be O
done O
with O
more O
efficient O
techniques O
, O
such O
as O
the O
power B
method I
( O
golub O
and O
van O
loan O
, O
1996 O
) O
, O
that O
scale O
like O
o O
( O
md O
2 O
) O
, O
or O
alternatively O
we O
can O
make O
use O
of O
the O
em O
algorithm O
. O
show O
that O
a O
gaussian O
ap- O
proximation B
to O
the O
posterior O
distribution O
can O
be O
maintained O
through O
the O
use O
of O
the O
lower B
bound I
( O
10.151 O
) O
, O
in O
which O
the O
distribution O
is O
initialized O
using O
the O
prior B
, O
and O
as O
each O
data O
point O
is O
absorbed O
its O
corresponding O
variational B
parameter O
ξn O
is O
optimized O
. O
the O
joint O
distribution O
is O
given O
by O
we O
seek O
an O
approximation O
q O
( O
x O
) O
that O
has O
the O
same O
factorization B
, O
so O
that O
p O
( O
x O
) O
= O
fa O
( O
x1 O
, O
x2 O
) O
fb O
( O
x2 O
, O
x3 O
) O
fc O
( O
x2 O
, O
x4 O
) O
. O
because O
we O
will O
typically O
need O
to O
detect O
multiple O
features O
in O
order O
to O
build O
an O
effective O
model O
, O
there O
will O
generally O
be O
multiple O
feature O
maps O
in O
the O
convolutional B
layer O
, O
each O
having O
its O
own O
set O
of O
weight O
and O
bias B
parameters O
. O
for O
example O
, O
the O
akaike O
information B
criterion I
, O
or O
aic O
( O
akaike O
, O
1974 O
) O
, O
chooses O
the O
model O
for O
which O
the O
quan- O
tity O
( O
1.73 O
) O
is O
largest O
. O
we O
therefore O
see O
that O
maximizing O
likelihood O
is O
equivalent O
, O
so O
far O
as O
determining O
w O
is O
concerned O
, O
to O
minimizing O
the O
sum-of-squares B
error I
function O
deﬁned O
by O
( O
1.2 O
) O
. O
finally O
, O
show O
that O
the O
form O
of O
the O
model O
is O
preserved O
in O
two O
cases O
: O
( O
i O
) O
a O
is O
a O
diagonal B
matrix O
and O
< O
p O
is O
a O
diagonal B
matrix O
. O
here O
m O
! O
, O
pronounced O
‘ O
factorial B
m O
’ O
, O
denotes O
the O
product O
m O
× O
( O
m O
− O
1 O
) O
× O
. O
the O
neural B
network I
in O
figure O
5.20 O
can O
, O
for O
example O
, O
be O
a O
two-layer O
network O
having O
sigmoidal O
( O
‘ O
tanh O
’ O
) O
hidden O
units O
. O
10 O
approximate O
inference B
10.1 O
variational B
inference I
. O
thus O
the O
maximum B
margin I
solution O
is O
found O
by O
solving O
arg O
max O
( O
7.3 O
) O
where O
we O
have O
taken O
the O
factor O
1/ O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
outside O
the O
optimization O
over O
n O
because O
w O
wtφ O
( O
xn O
) O
+ O
b O
tn O
w O
, O
b O
1 O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
min O
n O
328 O
7. O
sparse O
kernel O
machines O
( O
cid:10 O
) O
( O
cid:11 O
) O
( O
cid:10 O
) O
does O
not O
depend O
on O
n. O
direct O
solution O
of O
this O
optimization O
problem O
would O
be O
very O
complex O
, O
and O
so O
we O
shall O
convert O
it O
into O
an O
equivalent O
problem O
that O
is O
much O
easier O
to O
solve O
. O
also O
, O
it O
can O
be O
difﬁcult O
to O
know O
whether O
a O
sampling O
scheme O
is O
generating O
independent B
samples O
from O
the O
required O
distribution O
. O
note O
carefully O
the O
structure O
of O
the O
message B
passing I
equation O
. O
for O
any O
given O
path O
, O
the O
corresponding O
probability B
is O
given O
by O
the O
product O
of O
the O
elements O
of O
the O
tran- O
sition O
matrix O
ajk O
, O
corresponding O
to O
the O
probabil- O
ities O
p O
( O
zn+1|zn O
) O
for O
each O
segment O
of O
the O
path O
, O
along O
with O
the O
emission O
densities O
p O
( O
xn|k O
) O
asso- O
ciated O
with O
each O
node B
on O
the O
path O
. O
the O
goal O
of O
segmentation O
is O
to O
partition O
an O
image O
into O
regions O
each O
of O
which O
has O
a O
reasonably O
homogeneous B
visual O
appearance O
or O
which O
corresponds O
to O
objects O
or O
parts O
of O
objects O
( O
forsyth O
and O
ponce O
, O
2003 O
) O
. O
in O
order O
to O
understand O
the O
importance O
of O
gradient O
information O
, O
it O
is O
useful O
to O
consider O
a O
local B
approximation O
to O
the O
error B
function I
based O
on O
a O
taylor O
expansion O
. O
by O
making O
use O
of O
e O
[ O
i O
] O
= O
0 O
and O
e O
[ O
ij O
] O
= O
δijσ2 O
, O
show O
that O
minimizing O
ed O
averaged O
over O
the O
noise O
distribution O
is O
equivalent O
to O
minimizing O
the O
sum-of-squares B
error I
for O
noise-free O
input O
variables O
with O
the O
addition O
of O
a O
weight-decay O
regularization B
term O
, O
in O
which O
the O
bias B
parameter I
w0 O
is O
omitted O
from O
the O
regularizer O
. O
we O
can O
use O
the O
result O
( O
4.137 O
) O
to O
obtain O
a O
more O
accurate O
estimate O
of O
the O
model B
evidence I
starting O
from O
the O
laplace O
approximation O
, O
as O
we O
illustrate O
in O
the O
context O
of O
neural O
networks O
in O
section O
5.7 O
. O
show O
that O
in O
the O
limit O
 O
→ O
0 O
, O
maximizing O
the O
expected O
complete- O
data O
log O
likelihood O
for O
this O
model O
, O
given O
by O
( O
9.40 O
) O
, O
is O
equivalent O
to O
minimizing O
the O
distortion B
measure I
j O
for O
the O
k-means O
algorithm O
given O
by O
( O
9.1 O
) O
. O
averages O
of O
random O
variables O
tend O
to O
a O
gaussian O
, O
by O
the O
central B
limit I
theorem I
, O
and O
the O
sum O
of O
two O
gaussian O
variables O
is O
again O
gaussian O
. O
in O
this O
case O
, O
station- O
arity O
simply O
requires O
that O
∂g/∂y O
( O
x O
) O
= O
0 O
for O
all O
values O
of O
x. O
if O
we O
are O
optimizing O
a O
functional B
with O
respect O
to O
a O
probability B
distribution O
, O
then O
we O
need O
to O
maintain O
the O
normalization O
constraint O
on O
the O
probabilities O
. O
the O
contribution O
to O
the O
error B
associated O
with O
a O
particular O
misclassiﬁed O
pattern O
is O
a O
linear O
function O
of O
w O
in O
regions O
of O
w O
space O
where O
the O
pattern O
is O
misclassiﬁed O
and O
zero O
in O
regions O
where O
it O
is O
correctly O
classiﬁed O
. O
statistical O
factor O
analysis O
and O
related O
methods O
: O
theory B
and O
applications O
. O
the O
corresponding O
cumulative B
distribution I
function I
is O
given O
by O
( O
cid:6 O
) O
a O
φ O
( O
a O
) O
= O
−∞ O
n O
( O
θ|0 O
, O
1 O
) O
dθ O
( O
4.114 O
) O
which O
is O
known O
as O
the O
probit B
function I
. O
the O
trace O
tr O
( O
a O
) O
of O
a O
matrix O
a O
is O
deﬁned O
as O
the O
sum O
of O
the O
elements O
on O
the O
leading O
diagonal B
. O
the O
technique O
of O
slice B
sampling I
( O
neal O
, O
2003 O
) O
provides O
an O
adaptive O
step O
size O
that O
is O
automatically O
adjusted O
to O
match O
the O
characteristics O
of O
the O
distribution O
. O
one O
simple O
choice O
of O
error B
function I
, O
which O
is O
widely O
used O
, O
is O
given O
by O
the O
sum O
of O
the O
squares O
of O
the O
errors O
between O
the O
predictions O
y O
( O
xn O
, O
w O
) O
for O
each O
data O
point O
xn O
and O
the O
corresponding O
target O
values O
tn O
, O
so O
that O
we O
minimize O
e O
( O
w O
) O
= O
1 O
2 O
{ O
y O
( O
xn O
, O
w O
) O
− O
tn O
} O
2 O
( O
1.2 O
) O
n O
( O
cid:2 O
) O
n=1 O
where O
the O
factor O
of O
1/2 O
is O
included O
for O
later O
convenience O
. O
by O
deﬁnition O
, O
the O
two O
sets O
of O
points O
will O
be O
linearly O
αnxn O
n O
sets O
of O
points O
can O
not O
be O
linearly B
separable I
, O
and O
conversely O
that O
if O
they O
are O
linearly B
separable I
, O
their O
convex O
hulls O
do O
not O
intersect O
. O
show O
that O
, O
after O
τ O
steps O
, O
the O
components O
of O
the O
weight B
vector I
parallel O
to O
the O
eigenvectors O
of O
h O
can O
be O
written O
( O
τ O
) O
j O
= O
{ O
1 O
− O
( O
1 O
− O
ρηj O
) O
τ O
} O
w O
( O
cid:1 O
) O
w O
j O
( O
5.197 O
) O
where O
wj O
= O
wtuj O
, O
and O
uj O
and O
ηj O
are O
the O
eigenvectors O
and O
eigenvalues O
, O
respectively O
, O
of O
h O
so O
that O
( O
5.198 O
) O
show O
that O
as O
τ O
→ O
∞ O
, O
this O
gives O
w O
( O
τ O
) O
→ O
w O
( O
cid:1 O
) O
as O
expected O
, O
provided O
|1 O
− O
ρηj| O
< O
1. O
now O
suppose O
that O
training B
is O
halted O
after O
a O
ﬁnite O
number O
τ O
of O
steps O
. O
indeed O
, O
from O
the O
discussion O
in O
sec- O
tion O
5.1.1 O
we O
see O
that O
for O
any O
point O
w O
that O
is O
a O
local B
minimum I
, O
there O
will O
be O
other O
points O
in O
weight O
space O
that O
are O
equivalent O
minima O
. O
ab O
( O
2.15 O
) O
( O
2.16 O
) O
the O
parameters O
a O
and O
b O
are O
often O
called O
hyperparameters O
because O
they O
control O
the O
distribution O
of O
the O
parameter O
µ. O
figure O
2.2 O
shows O
plots O
of O
the O
beta B
distribution I
for O
various O
values O
of O
the O
hyperparameters O
. O
if O
the O
differential B
operator O
is O
isotropic B
then O
the O
green O
’ O
s O
functions O
depend O
only O
on O
the O
radial O
distance O
from O
the O
corresponding O
data O
point O
. O
exercise O
13.12 O
we O
have O
seen O
in O
earlier O
chapters O
that O
the O
maximum B
likelihood I
approach O
is O
most O
effective O
when O
the O
number O
of O
data O
points O
is O
large O
in O
relation O
to O
the O
number O
of O
parame- O
ters O
. O
the O
vertical O
green O
line O
in O
the O
right O
plot O
shows O
the O
decision B
boundary I
in O
x O
that O
gives O
the O
minimum O
misclassiﬁcation O
rate O
. O
for O
graphical O
models O
, O
this O
conditional B
distribution O
is O
a O
function O
only O
of O
the O
states O
of O
the O
nodes O
in O
the O
markov O
blanket O
. O
probability B
of O
making O
a O
mistake O
is O
obtained O
if O
each O
value O
of O
x O
is O
assigned O
to O
the O
class O
for O
which O
the O
posterior B
probability I
p O
( O
ck|x O
) O
is O
largest O
. O
suppose O
that O
for O
each O
path O
we O
evaluate O
its O
probability B
by O
summing O
up O
products O
of O
transition O
and O
emission O
probabilities O
as O
we O
work O
our O
way O
forward O
along O
each O
path O
through O
the O
lattice O
. O
( O
13.32 O
) O
note O
that O
the O
denominator O
p O
( O
x O
) O
is O
implicitly O
conditioned O
on O
the O
parameters O
θold O
of O
the O
hmm O
and O
hence O
represents O
the O
likelihood B
function I
. O
this O
chapter O
also O
provides O
a O
self-contained O
introduction O
to O
three O
important O
tools O
that O
will O
be O
used O
throughout O
the O
book O
, O
namely O
probability B
theory O
, O
decision B
theory I
, O
and O
infor- O
mation B
theory O
. O
in O
practice O
, O
suitable O
analytically O
speciﬁed O
importance B
sampling I
distributions O
can O
not O
readily O
be O
found O
for O
the O
kinds O
of O
complex O
models O
considered O
in O
this O
book O
. O
( O
e.5 O
) O
( O
e.6 O
) O
( O
e.7 O
) O
( O
e.8 O
) O
e. O
lagrange O
multipliers O
709 O
figure O
e.2 O
a O
simple O
example O
of O
the O
use O
of O
lagrange O
multipli- O
ers O
in O
which O
the O
aim O
is O
to O
maximize O
f O
( O
x1 O
, O
x2 O
) O
= O
1 O
− O
x2 O
2 O
subject O
to O
the O
constraint O
g O
( O
x1 O
, O
x2 O
) O
= O
0 O
where O
g O
( O
x1 O
, O
x2 O
) O
= O
x1 O
+ O
x2 O
− O
1. O
the O
circles O
show O
contours O
of O
the O
function O
f O
( O
x1 O
, O
x2 O
) O
, O
and O
the O
diagonal B
line O
shows O
the O
constraint O
surface O
g O
( O
x1 O
, O
x2 O
) O
= O
0 O
. O
exercise O
13.27 O
exercise O
13.28 O
if O
we O
consider O
a O
situation O
in O
which O
the O
measurement O
noise O
is O
small O
compared O
to O
the O
rate O
at O
which O
the O
latent B
variable I
is O
evolving O
, O
then O
we O
ﬁnd O
that O
the O
posterior O
distribution O
for O
zn O
depends O
only O
on O
the O
current O
measurement O
xn O
, O
in O
accordance O
with O
the O
intuition O
from O
our O
simple O
example O
at O
the O
start O
of O
the O
section O
. O
maximization O
of O
the O
log O
likelihood O
can O
be O
done O
using O
efﬁcient O
gradient-based O
optimization O
algorithms O
such O
as O
conjugate B
gradients O
( O
fletcher O
, O
1987 O
; O
nocedal O
and O
wright O
, O
1999 O
; O
bishop O
and O
nabney O
, O
2008 O
) O
. O
a O
generalized B
optimization O
of O
the O
k-d O
tree B
for O
fast O
nearest-neighbour O
search O
. O
9.7 O
( O
( O
cid:12 O
) O
) O
www O
verify O
that O
maximization O
of O
the O
complete-data O
log O
likelihood O
( O
9.36 O
) O
for O
a O
gaussian O
mixture B
model I
leads O
to O
the O
result O
that O
the O
means O
and O
covariances O
of O
each O
component O
are O
ﬁtted O
independently O
to O
the O
corresponding O
group O
of O
data O
points O
, O
and O
the O
mixing O
coefﬁcients O
are O
given O
by O
the O
fractions O
of O
points O
in O
each O
group O
. O
we O
can O
similarly O
evaluate O
the O
variance B
of O
the O
density B
function O
about O
the O
condi- O
( O
cid:8 O
) O
( O
cid:5 O
) O
t O
− O
e O
[ O
t|x O
] O
( O
cid:5 O
) O
2 O
|x O
( O
cid:9 O
) O
⎧⎨⎩σ2 O
k O
( O
cid:2 O
) O
πk O
( O
x O
) O
k O
( O
x O
) O
+ O
k=1 O
’ O
’ O
’ O
’ O
’ O
µk O
( O
x O
) O
− O
k O
( O
cid:2 O
) O
l=1 O
’ O
’ O
’ O
’ O
’ O
2 O
πl O
( O
x O
) O
µl O
( O
x O
) O
( O
5.159 O
) O
⎫⎬⎭ O
( O
5.160 O
) O
exercise O
5.37 O
tional O
average O
, O
to O
give O
s2 O
( O
x O
) O
= O
e O
= O
where O
we O
have O
used O
( O
5.148 O
) O
and O
( O
5.158 O
) O
. O
, O
xd O
) O
t O
are O
assumed O
to O
be O
independent B
. O
if O
, O
in O
the O
absence O
of O
weight B
decay I
, O
the O
weight B
vector I
starts O
at O
the O
origin O
and O
proceeds O
during O
training B
along O
a O
path O
that O
follows O
the O
local B
negative O
gradient O
vec- O
tor O
, O
then O
the O
weight B
vector I
will O
move O
initially O
parallel O
to O
the O
w2 O
axis O
through O
a O
point O
corresponding O
roughly O
to O
( O
cid:4 O
) O
w O
and O
then O
move O
towards O
the O
minimum O
of O
the O
error B
func- O
eigenvalues O
of O
the O
hessian O
. O
in O
fact O
, O
the O
bayesian O
paradigm O
leads O
very O
naturally O
to O
a O
sequential O
view O
of O
the O
inference B
problem O
. O
figure O
3.17 O
plot O
of O
the O
10 O
parameters O
wi O
from O
the O
gaussian O
basis B
function I
model O
versus O
the O
effective O
num- O
ber O
of O
parameters O
γ O
, O
in O
which O
the O
hyperparameter B
α O
is O
varied O
in O
the O
range O
0 O
( O
cid:1 O
) O
α O
( O
cid:1 O
) O
∞ O
causing O
γ O
to O
vary O
in O
the O
range O
0 O
( O
cid:1 O
) O
γ O
( O
cid:1 O
) O
m. O
wi O
2 O
1 O
0 O
−1 O
−2 O
0 O
8 O
4 O
5 O
2 O
6 O
3 O
1 O
7 O
9 O
0 O
2 O
4 O
6 O
8 O
γ O
10 O
3.6. O
limitations O
of O
fixed O
basis O
functions O
throughout O
this O
chapter O
, O
we O
have O
focussed O
on O
models O
comprising O
a O
linear O
combina- O
tion O
of O
ﬁxed O
, O
nonlinear O
basis O
functions O
. O
recall O
from O
section O
4.3.6 O
that O
there O
is O
a O
natural O
pairing O
of O
the O
error B
function I
( O
given O
by O
the O
negative O
log O
likelihood O
) O
and O
the O
output O
unit O
activation B
function I
. O
we O
therefore O
need O
a O
way O
to O
modify O
the O
support B
vector I
machine I
so O
as O
to O
allow O
some O
of O
the O
training B
points O
to O
be O
misclassiﬁed O
. O
suppose O
that O
one O
of O
the O
components O
of O
the O
mixture B
model I
, O
let O
us O
say O
the O
jth O
component O
, O
has O
its O
mean B
µj O
exactly O
equal O
to O
one O
of O
the O
data O
figure O
9.6 O
graphical O
representation O
of O
a O
gaussian O
mixture B
model I
for O
a O
set O
of O
n O
i.i.d O
. O
here O
γ O
( O
x O
) O
is O
the O
gamma B
function I
deﬁned O
by O
( O
1.141 O
) O
while O
k O
( O
cid:2 O
) O
α0 O
= O
αk O
. O
for O
the O
nonlinear O
neural B
network I
, O
however O
, O
it O
ignores O
the O
fact O
that O
changes O
in O
α O
will O
cause O
changes O
in O
the O
hessian O
h O
, O
which O
in O
turn O
will O
change O
the O
eigenvalues O
. O
one O
of O
the O
simplest O
ways O
of O
choosing O
basis B
function I
centres O
is O
to O
use O
a O
randomly O
chosen O
subset O
of O
the O
data O
points O
. O
this O
allows O
factor O
graphs O
to O
be O
more O
speciﬁc O
about O
the O
402 O
8. O
graphical O
models O
( O
a O
) O
( O
b O
) O
( O
c O
) O
figure O
8.43 O
( O
a O
) O
a O
directed B
polytree O
. O
given O
by O
ttle O
, O
'ari. O
, O
ionallower O
bour.d O
, O
cun O
lhen O
be O
c O
< O
> O
mpun O
: O
d O
for O
a O
r.nge O
of O
different O
' O
'' O
tue' O
'' O
f O
; O
\i O
ar.d O
itie O
' O
'' O
iue O
giving O
iht O
largest O
marginal B
likelihood I
`` O
, O
iecloo_ O
l1ere O
we O
consider O
. O
( O
3.8 O
) O
section O
1.5.5 O
recall O
that O
, O
if O
we O
assume O
a O
squared O
loss B
function I
, O
then O
the O
optimal O
prediction O
, O
for O
a O
new O
value O
of O
x O
, O
will O
be O
given O
by O
the O
conditional B
mean O
of O
the O
target O
variable O
. O
from O
bayes O
’ O
theorem O
( O
1.82 O
) O
, O
we O
see O
that O
the O
posterior O
probabilities O
are O
proportional O
to O
the O
prior B
probabilities O
, O
which O
we O
can O
interpret O
as O
the O
fractions O
of O
points O
in O
each O
class O
. O
we O
therefore O
determine O
the O
network O
parameters O
w O
by O
minimizing O
an O
error B
function I
which O
captures O
the O
degree O
of O
mismatch O
between O
the O
input O
vectors O
and O
their O
reconstructions O
. O
8.28 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
the O
concept O
of O
a O
pending B
message I
in O
the O
sum-product B
algorithm I
for O
a O
factor B
graph I
was O
deﬁned O
in O
section O
8.4.7. O
show O
that O
if O
the O
graph O
has O
one O
or O
more O
cycles O
, O
there O
will O
always O
be O
at O
least O
one O
pending B
message I
irrespective O
of O
how O
long O
the O
algorithm O
runs O
. O
it O
is O
also O
worth O
noting O
that O
the O
k-means O
algorithm O
itself O
is O
often O
used O
to O
initialize O
the O
parameters O
in O
a O
gaussian O
mixture B
model I
before O
applying O
the O
em O
algorithm O
. O
the O
model O
contains O
a O
binary O
latent O
variable O
z O
that O
indicates O
which O
component O
of O
the O
mixture B
is O
responsible O
for O
generating O
the O
corresponding O
data O
point O
. O
the O
right O
plot O
shows O
the O
corresponding O
projection O
based O
on O
the O
fisher O
linear B
discriminant I
, O
showing O
the O
greatly O
improved O
class O
separation O
. O
consider O
three O
variables O
a O
, O
b O
, O
and O
c O
, O
and O
suppose O
that O
the O
conditional B
distribution O
of O
a O
, O
given O
b O
and O
c O
, O
is O
such O
that O
it O
does O
not O
depend O
on O
the O
value O
of O
b O
, O
so O
that O
p O
( O
a|b O
, O
c O
) O
= O
p O
( O
a|c O
) O
. O
in O
the O
case O
of O
probability B
distributions O
deﬁned O
by O
an O
undirected B
graph I
, O
there O
is O
no O
one-pass O
sampling O
strategy O
that O
will O
sample O
even O
from O
the O
prior B
distribution O
with O
no O
observed O
variables O
. O
if O
the O
weights O
were O
constant O
, O
then O
the O
parameters O
of O
the O
mixture B
model I
could O
be O
determined O
by O
using O
the O
em O
algorithm O
discussed O
in O
chapter O
9. O
however O
, O
the O
dis- O
tribution O
of O
weights O
is O
itself O
evolving O
during O
the O
learning B
process O
, O
and O
so O
to O
avoid O
nu- O
merical O
instability O
, O
a O
joint O
optimization O
is O
performed O
simultaneously O
over O
the O
weights O
and O
the O
mixture-model O
parameters O
. O
to O
do O
this O
we O
introduce O
a O
gaussian O
process O
prior B
over O
the O
vector O
an O
+1 O
, O
which O
has O
compo- O
nents O
a O
( O
x1 O
) O
, O
. O
observed O
nodes O
are O
easily O
handled O
within O
the O
sum-product B
algorithm I
as O
follows O
. O
4. O
the O
hessian O
plays O
a O
central O
role O
in O
the O
laplace O
approximation O
for O
a O
bayesian O
neural B
network I
( O
see O
section O
5.7 O
) O
. O
λ O
thus O
in O
the O
yj O
coordinate O
system O
, O
the O
gaussian O
distribution O
takes O
the O
form O
d O
( O
cid:14 O
) O
j=1 O
p O
( O
y O
) O
= O
p O
( O
x O
) O
|j| O
= O
1 O
( O
2πλj O
) O
1/2 O
exp O
− O
y2 O
j O
2λj O
( O
cid:12 O
) O
( O
cid:13 O
) O
( O
2.55 O
) O
( O
2.56 O
) O
e O
[ O
x O
] O
= O
1 O
( O
2π O
) O
d/2 O
1 O
|σ|1/2 O
1 O
1 O
which O
is O
the O
product O
of O
d O
independent B
univariate O
gaussian O
distributions O
. O
in O
p. O
p. O
bonissone O
, O
m. O
hen- O
rion B
, O
l. O
n. O
kanal O
, O
and O
j. O
f. O
lemmer O
( O
eds O
. O
the O
cor- O
responding O
decision B
boundary I
is O
therefore O
deﬁned O
by O
the O
relation O
y O
( O
x O
) O
= O
0 O
, O
which O
corresponds O
to O
a O
( O
d O
− O
1 O
) O
-dimensional O
hyperplane O
within O
the O
d-dimensional O
input O
space O
. O
suppose O
that O
in O
an O
undirected B
graph I
we O
identify O
three O
sets O
of O
nodes O
, O
denoted O
a O
, O
b O
, O
and O
c O
, O
and O
that O
we O
consider O
the O
conditional B
independence I
property O
a O
⊥⊥ O
b O
| O
c. O
( O
8.37 O
) O
to O
test O
whether O
this O
property O
is O
satisﬁed O
by O
a O
probability B
distribution O
deﬁned O
by O
a O
graph O
we O
consider O
all O
possible O
paths O
that O
connect O
nodes O
in O
set O
a O
to O
nodes O
in O
set O
b. O
if O
all O
such O
paths O
pass O
through O
one O
or O
more O
nodes O
in O
set O
c O
, O
then O
all O
such O
paths O
are O
‘ O
blocked O
’ O
and O
so O
the O
conditional B
independence I
property O
holds O
. O
bootstrap B
methods O
: O
another O
look O
at O
the O
jackknife O
. O
ideally O
, O
this O
should O
rely O
only O
on O
the O
training B
data O
and O
should O
allow O
multiple O
hyperparameters O
and O
model O
types O
to O
be O
compared O
in O
a O
single O
training B
run O
. O
11.1.1 O
standard O
distributions O
11.1.2 O
rejection B
sampling I
. O
the O
second O
half O
of O
the O
network O
defines O
an O
arbitrary O
functional B
mapping O
from O
the O
m O
-dimensional O
space O
back O
into O
the O
original O
d-dimensional O
input O
space O
. O
later O
we O
shall O
modify O
the O
technique O
to O
allow O
the O
most O
probable O
state O
to O
be O
found O
, O
giving O
rise O
to O
the O
max-sum B
algorithm I
. O
for O
instance O
, O
the O
parameters O
might O
be O
partitioned B
into O
groups O
, O
and O
the O
m O
step O
is O
broken O
down O
into O
multiple O
steps O
each O
of O
which O
involves O
optimizing O
one O
of O
the O
subset O
with O
the O
remainder O
held O
ﬁxed O
. O
thus O
, O
by O
completing B
the I
square I
with O
respect O
to O
xb O
, O
we O
can O
integrate O
out O
xb O
and O
the O
only O
term O
remaining O
from O
the O
contributions O
on O
the O
left-hand O
side O
of O
( O
2.84 O
) O
that O
depends O
on O
xa O
is O
the O
last O
term O
on O
the O
right-hand O
side O
of O
( O
2.84 O
) O
in O
which O
m O
is O
given O
by O
( O
2.85 O
) O
. O
recall O
that O
the O
ratio O
α/β O
is O
analogous O
to O
a O
regularization B
parameter O
. O
we O
can O
gain O
further O
insight O
into O
bayesian O
model B
comparison I
and O
understand O
how O
the O
marginal B
likelihood I
can O
favour O
models O
of O
intermediate O
complexity O
by O
con- O
sidering O
figure O
3.13. O
here O
the O
horizontal O
axis O
is O
a O
one-dimensional O
representation O
of O
the O
space O
of O
possible O
data O
sets O
, O
so O
that O
each O
point O
on O
this O
axis O
corresponds O
to O
a O
speciﬁc O
data O
set O
. O
here O
we O
show O
that O
this O
approach O
is O
closely O
related O
to O
the O
technique O
of O
tangent B
propagation I
( O
bishop O
, O
1995b O
; O
leen O
, O
1995 O
) O
. O
12.11 O
12.12 O
( O
* O
* O
) O
for O
( O
y2 O
> O
0 O
show O
that O
the O
posterior O
mean O
in O
the O
probabilistic O
pca O
model O
is O
shifted O
towards O
the O
origin O
relative B
to O
the O
orthogonal O
projection O
. O
for O
the O
perceptron B
, O
however O
, O
it O
is O
more O
convenient O
to O
use O
target O
values O
t O
= O
+1 O
for O
class O
c1 O
and O
t O
= O
−1 O
for O
class O
c2 O
, O
which O
matches O
the O
choice O
of O
activation B
function I
. O
, O
tn O
) O
t. O
as O
usual O
, O
it O
is O
convenient O
to O
maximize O
the O
log O
of O
the O
likelihood B
function I
. O
4.7 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
logistic B
sigmoid I
function O
( O
4.59 O
) O
satisﬁes O
the O
property O
σ O
( O
−a O
) O
= O
1 O
− O
σ O
( O
a O
) O
and O
that O
its O
inverse B
is O
given O
by O
σ O
−1 O
( O
y O
) O
= O
ln O
{ O
y/ O
( O
1 O
− O
y O
) O
} O
. O
in O
the O
case O
of O
a O
directed B
polytree O
, O
conversion O
to O
an O
undirected B
graph I
results O
in O
loops O
due O
to O
the O
moralization B
step O
, O
whereas O
conversion O
to O
a O
factor B
graph I
again O
results O
in O
a O
tree B
, O
as O
illustrated O
in O
figure O
8.43. O
in O
fact O
, O
local B
cycles O
in O
a O
directed B
graph O
due O
to O
links O
connecting O
parents O
of O
a O
node B
can O
be O
removed O
on O
conversion O
to O
a O
factor B
graph I
by O
deﬁning O
the O
appropriate O
factor O
function O
, O
as O
shown O
in O
figure O
8.44. O
we O
have O
seen O
that O
multiple O
different O
factor O
graphs O
can O
represent O
the O
same O
di- O
rected O
or O
undirected B
graph I
. O
neural O
networks O
and O
principal B
component I
analysis I
: O
learning B
from O
examples O
without O
local B
minima O
. O
note O
that O
this O
is O
broader O
and O
shifted O
relative B
to O
the O
blue O
curve O
( O
which O
is O
shown O
dashed O
in O
the O
centre O
plot O
for O
comparison O
) O
. O
this O
technique O
is O
called O
maximum B
posterior I
, O
or O
simply O
map O
. O
these O
concepts O
are O
illustrated O
by O
the O
undirected B
graph I
over O
four O
variables O
shown O
in O
figure O
8.29. O
this O
graph O
has O
ﬁve O
cliques O
of O
two O
nodes O
given O
by O
{ O
x1 O
, O
x2 O
} O
, O
{ O
x2 O
, O
x3 O
} O
, O
{ O
x3 O
, O
x4 O
} O
, O
{ O
x4 O
, O
x2 O
} O
, O
and O
{ O
x1 O
, O
x3 O
} O
, O
as O
well O
as O
two O
maximal O
cliques O
given O
by O
{ O
x1 O
, O
x2 O
, O
x3 O
} O
and O
{ O
x2 O
, O
x3 O
, O
x4 O
} O
. O
maximizing O
this O
evidence B
function I
with O
respect O
to O
α O
again O
leads O
to O
the O
re-estimation O
equation O
given O
by O
( O
5.178 O
) O
. O
we O
therefore O
take O
our O
prior B
distribution O
to O
be O
p O
( O
µ O
) O
= O
n O
( O
cid:10 O
) O
µ|µ0 O
, O
σ2 O
0 O
( O
cid:11 O
) O
and O
the O
posterior O
distribution O
is O
given O
by O
p O
( O
µ|x O
) O
∝ O
p O
( O
x|µ O
) O
p O
( O
µ O
) O
. O
sparse O
greedy O
gaussian O
process O
regression B
. O
for O
the O
maximization O
with O
respect O
to O
πk O
, O
we O
need O
to O
introduce O
a O
k O
πk O
= O
1. O
following O
analogous O
lagrange O
multiplier O
to O
enforce O
the O
constraint O
steps O
to O
those O
used O
for O
the O
mixture O
of O
gaussians O
, O
we O
then O
obtain O
( O
cid:5 O
) O
πk O
= O
nk O
n O
( O
9.60 O
) O
which O
represents O
the O
intuitively O
reasonable O
result O
that O
the O
mixing B
coefﬁcient I
for O
com- O
ponent O
k O
is O
given O
by O
the O
effective O
fraction O
of O
points O
in O
the O
data O
set O
explained O
by O
that O
component O
. O
the O
formulation O
of O
linear B
regression I
in O
terms O
of O
a O
kernel B
function I
suggests O
an O
alternative O
approach O
to O
regression B
as O
follows O
. O
once O
the O
algorithm O
has O
converged O
, O
or O
once O
it O
has O
been O
stopped O
if O
convergence O
is O
not O
observed O
, O
the O
( O
approximate O
) O
local B
marginals O
can O
be O
computed O
using O
the O
product O
of O
the O
most O
recently O
received O
incoming O
messages O
to O
each O
variable O
node B
or O
factor O
node O
on O
every O
link B
. O
in O
practice O
, O
the O
errors O
are O
typically O
highly O
correlated O
, O
and O
the O
reduc- O
tion O
in O
overall O
error B
is O
generally O
small O
. O
this O
leads O
to O
the O
technique O
of O
tangent B
propagation I
, O
discussed O
in O
section O
5.5.4 O
. O
from O
( O
2.188 O
) O
the O
log O
of O
the O
likelihood B
function I
is O
given O
by O
n O
( O
cid:2 O
) O
( O
cid:24 O
) O
k O
( O
cid:2 O
) O
( O
cid:25 O
) O
πkn O
( O
xn|µk O
, O
σk O
) O
n=1 O
k=1 O
ln O
p O
( O
x|π O
, O
µ O
, O
σ O
) O
= O
ln O
( O
2.192 O
) O
( O
2.193 O
) O
2.4. O
the O
exponential B
family I
113 O
where O
x O
= O
{ O
x1 O
, O
. O
if O
our O
aim O
is O
to O
minimize O
the O
chance O
of O
assigning O
x O
to O
the O
wrong O
class O
, O
then O
intuitively O
we O
would O
choose O
the O
class O
having O
the O
higher O
posterior B
probability I
. O
4 O
linear O
models O
for O
classiﬁcation O
in O
the O
previous O
chapter O
, O
we O
explored O
a O
class O
of O
regression B
models O
having O
particularly O
simple O
analytical O
and O
computational O
properties O
. O
it O
has O
a O
great O
merit O
that O
there O
is O
no O
compu- O
tation O
involved O
in O
the O
‘ O
training B
’ O
phase O
because O
this O
simply O
requires O
storage O
of O
the O
training B
set I
. O
from O
( O
5.28 O
) O
, O
the O
corresponding O
local B
approximation O
to O
the O
gradient O
is O
given O
by O
for O
points O
w O
that O
are O
sufﬁciently O
close O
to O
( O
cid:1 O
) O
w O
, O
these O
expressions O
will O
give O
reasonable O
( O
5.31 O
) O
approximations O
for O
the O
error B
and O
its O
gradient O
. O
section O
9.2 O
we O
therefore O
turn O
to O
the O
expectation B
maximization I
algorithm O
to O
ﬁnd O
an O
efﬁcient O
framework O
for O
maximizing O
the O
likelihood B
function I
in O
hidden O
markov O
models O
. O
if O
we O
denote O
the O
number O
of O
observations O
of O
x O
= O
1 O
( O
heads O
) O
within O
this O
data O
set O
by O
m O
, O
then O
we O
can O
write O
( O
2.7 O
) O
in O
the O
form O
µml O
= O
m O
n O
( O
2.8 O
) O
so O
that O
the O
probability B
of O
landing O
heads O
is O
given O
, O
in O
this O
maximum B
likelihood I
frame- O
work O
, O
by O
the O
fraction O
of O
observations O
of O
heads O
in O
the O
data O
set O
. O
the O
goal O
is O
to O
ﬁnd O
a O
kernel O
that O
measures O
the O
similarity O
of O
two O
input O
vectors O
x O
and O
x O
( O
cid:4 O
) O
induced O
by O
the O
generative B
model I
. O
verify O
that O
, O
when O
the O
loss B
matrix I
is O
given O
by O
lkj O
= O
1 O
− O
ikj O
, O
where O
ikj O
are O
the O
elements O
of O
the O
identity O
matrix O
, O
this O
reduces O
to O
the O
criterion O
of O
choosing O
the O
class O
having O
the O
largest O
posterior B
probability I
. O
a O
further O
specialization O
involves O
homogeneous B
kernels O
, O
also O
known O
as O
ra- O
dial O
basis O
functions O
, O
which O
depend O
only O
on O
the O
magnitude O
of O
the O
distance O
( O
typically O
euclidean O
) O
between O
the O
arguments O
so O
that O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
k O
( O
( O
cid:5 O
) O
x O
− O
x O
( O
cid:4 O
) O
( O
cid:5 O
) O
) O
. O
recall O
that O
for O
localized O
basis O
functions O
, O
the O
predictive O
variance O
for O
linear O
regression B
models O
becomes O
small O
in O
regions O
of O
input O
space O
where O
there O
are O
no O
basis O
functions O
. O
( O
cid:2 O
) O
i O
a O
matrix O
h O
is O
said O
to O
be O
positive B
deﬁnite I
if O
, O
and O
only O
if O
, O
vthv O
> O
0 O
for O
all O
v. O
( O
5.36 O
) O
( O
5.37 O
) O
w2 O
the O
error B
figure O
5.6 O
in O
the O
neighbourhood O
of O
a O
min- O
imum O
w O
( O
cid:1 O
) O
, O
function O
can O
be O
approximated O
by O
a O
quadratic O
. O
we O
can O
readily O
verify O
the O
following O
decomposition O
based O
on O
this O
variational B
distribution O
( O
cid:13 O
) O
p O
( O
z O
, O
m|x O
) O
q O
( O
z|m O
) O
q O
( O
m O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:13 O
) O
( O
10.34 O
) O
. O
we O
say O
that O
node B
y O
is O
a O
de- O
scendant O
of O
node B
x O
if O
there O
is O
a O
path O
from O
x O
to O
y O
in O
which O
each O
step O
of O
the O
path O
follows O
the O
directions O
of O
the O
arrows O
. O
show O
that O
if O
the O
message B
passing I
algorithm O
of O
section O
8.4.1 O
is O
applied O
to O
the O
evalu- O
ation O
of O
p O
( O
x2|x3 O
, O
x5 O
) O
, O
the O
result O
will O
be O
independent B
of O
the O
value O
of O
x5 O
. O
this O
is O
most O
easily O
done O
by O
substituting O
the O
expressions O
for O
the O
gaussian O
prior B
q O
( O
w O
) O
= O
n O
( O
w|m0 O
, O
s0 O
) O
, O
together O
with O
the O
lower B
bound I
h O
( O
w O
, O
ξ O
) O
on O
the O
likelihood B
function I
, O
into O
the O
integral O
( O
10.159 O
) O
which O
deﬁnes O
l O
( O
ξ O
) O
. O
, O
n O
, O
where O
( O
7.33 O
) O
are O
known O
as O
box B
constraints I
. O
this O
issue O
is O
addressed O
by O
nearest-neighbour B
methods I
for O
density B
estimation I
. O
this O
assumption O
allowed O
us O
to O
express O
the O
likelihood B
function I
as O
the O
product O
over O
all O
data O
points O
of O
the O
prob- O
ability O
distribution O
evaluated O
at O
each O
data O
point O
. O
the O
derivative B
of O
a O
vector O
a O
with O
respect O
to O
a O
scalar O
x O
is O
itself O
a O
vector O
whose O
components O
are O
given O
by O
∂a O
∂x O
= O
∂ai O
∂x O
( O
c.16 O
) O
with O
an O
analogous O
deﬁnition O
for O
the O
derivative B
of O
a O
matrix O
. O
this O
bound O
is O
a O
convex B
function I
having O
a O
unique O
maximum O
( O
for O
mixture O
components O
from O
the O
exponential B
family I
) O
. O
for O
comparison O
, O
the O
green O
line O
denotes O
the O
optimal O
decision B
boundary I
computed O
from O
the O
distributions O
used O
to O
generate O
the O
data O
. O
l O
( O
cid:2 O
) O
( O
cid:1 O
) O
f O
= O
1 O
l O
var O
[ O
( O
cid:1 O
) O
f O
] O
= O
( O
cid:8 O
) O
exercise O
11.1 O
by O
( O
cid:9 O
) O
( O
f O
− O
e O
[ O
f O
] O
) O
2 O
1 O
l O
e O
( O
11.3 O
) O
is O
the O
variance B
of O
the O
function O
f O
( O
z O
) O
under O
the O
distribution O
p O
( O
z O
) O
. O
if O
the O
data O
are O
assumed O
to O
be O
drawn O
independently O
from O
the O
distribution O
( O
1.60 O
) O
, O
then O
the O
likelihood B
function I
is O
given O
by O
n O
( O
cid:14 O
) O
n O
( O
cid:10 O
) O
n=1 O
( O
cid:11 O
) O
p O
( O
t|x O
, O
w O
, O
β O
) O
= O
tn|y O
( O
xn O
, O
w O
) O
, O
β O
−1 O
. O
this O
is O
equivalent O
to O
choosing O
the O
mean B
of O
the O
prior B
over O
weight O
values O
p O
( O
w|α O
) O
to O
be O
zero O
in O
the O
basis B
function I
viewpoint O
. O
consider O
ﬁrst O
the O
distribution O
over O
α. O
keeping O
only O
terms O
that O
have O
a O
functional B
dependence O
on O
α O
, O
we O
have O
ln O
q O
( O
cid:1 O
) O
( O
α O
) O
= O
ln O
p O
( O
α O
) O
+ O
ew O
[ O
ln O
p O
( O
w|α O
) O
] O
+ O
const O
= O
( O
a0 O
− O
1 O
) O
ln O
α O
− O
b0α O
+ O
m O
2 O
ln O
α O
− O
α O
2 O
e O
[ O
wtw O
] O
+ O
const O
. O
if O
we O
are O
given O
an O
expression O
for O
the O
joint O
distribution O
over O
a O
set O
of O
variables O
in O
terms O
of O
a O
product O
of O
conditional B
distributions O
( O
i.e. O
, O
the O
mathematical O
representation O
underlying O
a O
directed B
graph O
) O
, O
then O
we O
could O
in O
principle O
test O
whether O
any O
poten- O
tial O
conditional B
independence I
property O
holds O
by O
repeated O
application O
of O
the O
sum O
and O
product O
rules O
of O
probability B
. O
we O
now O
derive O
the O
em O
algorithm O
for O
maximizing O
the O
likelihood B
function I
for O
the O
mixture O
of O
bernoulli O
distributions O
. O
( O
2.109 O
) O
( O
2.110 O
) O
a O
special O
case O
of O
this O
result O
is O
when O
a O
= O
i O
, O
in O
which O
case O
it O
reduces O
to O
the O
convolu- O
tion O
of O
two O
gaussians O
, O
for O
which O
we O
see O
that O
the O
mean B
of O
the O
convolution O
is O
the O
sum O
of O
the O
mean B
of O
the O
two O
gaussians O
, O
and O
the O
covariance B
of O
the O
convolution O
is O
the O
sum O
of O
their O
covariances O
. O
our O
measure O
of O
information O
content O
will O
therefore O
depend O
on O
the O
probability B
distribution O
p O
( O
x O
) O
, O
and O
we O
therefore O
look O
for O
a O
quantity O
h O
( O
x O
) O
that O
is O
a O
monotonic O
function O
of O
the O
probability B
p O
( O
x O
) O
and O
that O
expresses O
the O
information O
content O
. O
8.2.1 O
three O
example O
graphs O
8.2.2 O
d-separation B
. O
in O
bishop O
and O
james O
( O
1993 O
) O
, O
statistical O
machine O
learning B
techniques O
were O
used O
to O
predict O
the O
volume O
fractions O
and O
also O
the O
geometrical O
conﬁguration O
of O
the O
phases O
shown O
in O
figure O
a.2 O
, O
from O
the O
12-dimensional O
vector O
of O
measurements O
. O
for O
classiﬁcation O
problems O
, O
the O
process O
of O
growing O
and O
pruning O
the O
tree B
is O
sim- O
ilar O
, O
except O
that O
the O
sum-of-squares B
error I
is O
replaced O
by O
a O
more O
appropriate O
measure O
666 O
14. O
combining B
models I
of O
performance O
. O
it O
should O
be O
noted O
that O
the O
term O
backpropagation B
is O
used O
in O
the O
neural O
com- O
puting O
literature O
to O
mean B
a O
variety O
of O
different O
things O
. O
speciﬁcally O
, O
we O
shall O
see O
that O
many O
algorithms O
can O
be O
expressed O
in O
terms O
of O
the O
propagation O
of O
local B
messages O
around O
the O
graph O
. O
where O
l O
is O
a O
d O
x O
d O
diagonal B
matrix O
with O
elements O
ai O
, O
and O
u O
is O
a O
d O
x O
d O
orthog O
( O
cid:173 O
) O
onal O
matrix O
with O
columns O
given O
by O
ui O
. O
furthermore O
, O
a O
maximal B
clique I
is O
a O
clique B
such O
that O
it O
is O
not O
possible O
to O
include O
any O
other O
nodes O
from O
the O
graph O
in O
the O
set O
without O
it O
ceasing O
to O
be O
a O
clique B
. O
furthermore O
, O
we O
can O
use O
variational B
meth- O
ods O
to O
give O
a O
fully O
bayesian O
treatment O
of O
the O
hmm O
in O
which O
we O
marginalize O
over O
the O
parameter O
distributions O
( O
mackay O
, O
1997 O
) O
. O
recall O
that O
our O
goal O
is O
to O
maximize O
the O
evidence B
function I
p O
( O
t|α O
, O
β O
) O
given O
by O
( O
3.77 O
) O
with O
respect O
to O
α O
and O
β. O
because O
the O
parameter O
vector O
w O
is O
marginalized O
out O
, O
we O
can O
regard O
it O
as O
a O
latent B
variable I
, O
and O
hence O
we O
can O
optimize O
this O
marginal B
likelihood I
function O
using O
em O
. O
( O
10.187 O
) O
506 O
10. O
approximate O
inference B
we O
see O
that O
the O
optimum O
solution O
simply O
corresponds O
to O
matching O
the O
expected O
suf- O
ﬁcient O
statistics O
. O
for O
instance O
, O
consider O
an O
object B
recognition I
task O
in O
which O
each O
observed O
data O
point O
corresponds O
to O
an O
image O
( O
comprising O
a O
vector O
of O
pixel O
intensities O
) O
of O
one O
of O
the O
objects O
. O
here O
t1 O
is O
the O
training B
data O
point O
, O
and O
condition- O
ing O
on O
the O
value O
of O
t1 O
, O
correspond- O
ing O
to O
the O
vertical O
blue O
line O
, O
we O
ob- O
tain O
p O
( O
t2|t1 O
) O
shown O
as O
a O
function O
of O
t2 O
by O
the O
green O
curve O
. O
( O
10.114 O
) O
we O
shall O
also O
use O
a O
conjugate B
prior I
for O
η O
, O
which O
can O
be O
written O
as O
p O
( O
η|ν0 O
, O
v0 O
) O
= O
f O
( O
ν0 O
, O
χ0 O
) O
g O
( O
η O
) O
ν0 O
exp O
νoηtχ0 O
recall O
that O
the O
conjugate B
prior I
distribution O
can O
be O
interpreted O
as O
a O
prior B
number O
ν0 O
of O
observations O
all O
having O
the O
value O
χ0 O
for O
the O
u O
vector O
. O
by O
contrast O
, O
if O
we O
had O
ﬁtted O
gaussian O
class O
conditional B
densities O
using O
maximum B
likelihood I
, O
we O
would O
have O
used O
2m O
parameters O
for O
the O
means O
and O
m O
( O
m O
+ O
1 O
) O
/2 O
parameters O
for O
the O
( O
shared O
) O
covariance B
matrix I
. O
2.5. O
nonparametric B
methods I
121 O
∆ O
= O
0.04 O
∆ O
= O
0.08 O
∆ O
= O
0.25 O
5 O
0 O
5 O
0 O
0 O
5 O
0 O
0 O
0 O
0.5 O
0.5 O
0.5 O
1 O
1 O
1 O
in O
figure O
2.24 O
, O
we O
show O
an O
example O
of O
histogram B
density I
estimation I
. O
xn O
α O
σ2 O
tn O
n O
w O
values O
, O
for O
example O
the O
variables O
{ O
tn O
} O
from O
the O
training B
set I
in O
the O
case O
of O
polynomial B
curve I
ﬁtting I
. O
in O
the O
quadratic O
approximation O
to O
the O
error B
function I
, O
given O
in O
( O
5.28 O
) O
, O
the O
error B
surface O
is O
speciﬁed O
by O
the O
quantities O
b O
and O
h O
, O
which O
contain O
a O
total O
of O
w O
( O
w O
+ O
3 O
) O
/2 O
independent B
elements O
( O
because O
the O
matrix O
h O
is O
symmetric O
) O
, O
where O
w O
is O
the O
dimensionality O
of O
w O
( O
i.e. O
, O
the O
total O
number O
of O
adaptive O
parameters O
in O
the O
network O
) O
. O
however O
, O
an O
advantage O
of O
a O
gaussian O
processes O
viewpoint O
is O
that O
we O
can O
consider O
covariance B
functions O
that O
can O
only O
be O
expressed O
in O
terms O
of O
an O
inﬁnite O
number O
of O
basis O
functions O
. O
this O
simply O
corresponds O
to O
approximations O
that O
curve O
upwards O
instead O
of O
downwards O
and O
are O
not O
necessarily O
problematic O
provided O
the O
overall O
approximate O
posterior O
q O
( O
θ O
) O
has O
posi- O
tive O
variance B
. O
9 O
mixture B
models O
and O
em O
if O
we O
deﬁne O
a O
joint O
distribution O
over O
observed O
and O
latent O
variables O
, O
the O
correspond- O
ing O
distribution O
of O
the O
observed O
variables O
alone O
is O
obtained O
by O
marginalization O
. O
similarly O
, O
the O
probability B
that O
x O
takes O
the O
value O
xi O
irrespective O
of O
the O
value O
of O
y O
is O
written O
as O
p O
( O
x O
= O
xi O
) O
and O
is O
given O
by O
the O
fraction O
of O
the O
total O
number O
of O
points O
that O
fall O
in O
column O
i O
, O
so O
that O
. O
wishart O
the O
wishart O
distribution O
is O
the O
conjugate B
prior I
for O
the O
precision B
matrix I
of O
a O
multi- O
variate O
gaussian O
. O
if O
we O
already O
have O
a O
strategy O
for O
sampling O
from O
a O
joint O
distribution O
p O
( O
u O
, O
v O
) O
, O
then O
it O
is O
straightforward O
to O
obtain O
samples O
from O
the O
marginal B
distribution O
p O
( O
u O
) O
simply O
by O
ignoring O
the O
values O
for O
v O
in O
each O
sample O
. O
2.22 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
inverse B
of O
a O
symmetric O
matrix O
is O
itself O
symmetric O
. O
this O
is O
consistent B
with O
our O
earlier O
discussion O
of O
sparsity B
in O
the O
rvm O
, O
because O
a O
basis B
function I
φi O
( O
x O
) O
centred O
on O
a O
data O
point O
near O
the O
boundary O
will O
have O
a O
vector O
ϕi O
that O
is O
poorly O
aligned O
with O
the O
training B
data O
vector O
t. O
one O
of O
the O
potential O
advantages O
of O
the O
relevance B
vector I
machine I
compared O
with O
the O
svm O
is O
that O
it O
makes O
probabilistic O
predictions O
. O
show O
that O
the O
quantities O
ak O
given O
by O
( O
4.63 O
) O
, O
which O
appear O
in O
the O
argument O
to O
the O
softmax B
function I
describing O
the O
posterior O
class O
probabilities O
, O
are O
linear O
functions O
of O
the O
components O
of O
φ. O
note O
that O
this O
represents O
an O
example O
of O
the O
naive O
bayes O
model O
which O
is O
discussed O
in O
section O
8.2.2 O
. O
the O
log O
likelihood O
can O
be O
maximized O
using O
gradient-based O
optimization O
giving O
rise O
to O
a O
particular O
version O
of O
independent B
component I
analysis I
. O
here O
the O
learning B
algorithm O
is O
not O
given O
examples O
of O
optimal O
outputs O
, O
in O
contrast O
to O
supervised B
learning I
, O
but O
must O
instead O
discover O
them O
by O
a O
process O
of O
trial O
and O
error B
. O
diagnostic O
tests O
for O
convergence O
of O
markov O
chain O
monte O
carlo O
algorithms O
are O
summarized O
in O
robert O
and O
casella O
( O
1999 O
) O
, O
and O
some O
practical O
guidance O
on O
the O
use O
of O
sampling B
methods I
in O
the O
context O
of O
machine O
learning O
is O
given O
in O
bishop O
and O
nabney O
( O
2008 O
) O
. O
the O
distribution O
has O
period O
2π O
so O
that O
p O
( O
θ O
+ O
2π O
) O
= O
p O
( O
θ O
) O
for O
all O
θ. O
care O
must O
be O
taken O
in O
interpret- O
ing O
this O
distribution O
because O
simple O
expectations O
will O
be O
dependent O
on O
the O
( O
arbitrary O
) O
choice O
of O
origin O
for O
the O
variable O
θ. O
the O
parameter O
θ0 O
is O
analogous O
to O
the O
mean B
of O
a O
univariate O
gaussian O
, O
and O
the O
parameter O
m O
> O
0 O
, O
known O
as O
the O
concentration O
param- O
eter O
, O
is O
analogous O
to O
the O
precision O
( O
inverse B
variance O
) O
. O
in O
this O
way O
, O
the O
two-dimensional O
nonlinear O
manifold B
for O
the O
laminar O
conﬁguration O
is O
broken O
into O
six O
distinct O
segments O
. O
using O
the O
conditional B
independence I
property O
( O
13.24 O
) O
, O
together O
with O
the O
product B
rule I
of I
probability I
, O
we O
obtain O
γ O
( O
zn O
) O
= O
p O
( O
x1 O
, O
. O
also O
, O
show O
that O
the O
normalization O
constant O
zn O
, O
deﬁned O
by O
( O
10.206 O
) O
, O
is O
given O
for O
the O
clutter B
problem I
by O
( O
10.216 O
) O
. O
( O
10.159 O
) O
as O
with O
the O
optimization O
of O
the O
hyperparameter B
α O
in O
the O
linear B
regression I
model O
of O
section O
3.5 O
, O
there O
are O
two O
approaches O
to O
determining O
the O
ξn O
. O
note O
that O
in O
this O
example O
, O
the O
prior B
probability O
of O
selecting O
the O
red O
box O
was O
4/10 O
, O
so O
that O
we O
were O
more O
likely O
to O
select O
the O
blue O
box O
than O
the O
red O
one O
. O
for O
each O
pattern O
in O
the O
training B
set I
, O
we O
shall O
suppose O
that O
we O
have O
supplied O
the O
corresponding O
input O
vector O
to O
the O
network O
and O
calculated O
the O
activations O
of O
all O
of O
the O
hidden O
and O
output O
units O
in O
the O
network O
by O
successive O
application O
of O
( O
5.48 O
) O
and O
( O
5.49 O
) O
. O
hence O
show O
that O
the O
error B
function I
is O
a O
concave B
function I
of O
w O
and O
that O
it O
has O
a O
unique O
minimum O
. O
( O
8.57 O
) O
xn+1 O
this O
recursive O
message B
passing I
is O
illustrated O
in O
figure O
8.38. O
the O
normalization O
con- O
stant O
z O
is O
easily O
evaluated O
by O
summing O
the O
right-hand O
side O
of O
( O
8.54 O
) O
over O
all O
states O
of O
xn O
, O
an O
operation O
that O
requires O
only O
o O
( O
k O
) O
computation O
. O
the O
marginal B
distribution O
p O
( O
x1 O
) O
is O
governed O
by O
k O
− O
1 O
parameters O
, O
as O
before O
, O
similarly O
, O
the O
conditional B
distribution O
p O
( O
x2|x1 O
) O
requires O
the O
speciﬁcation O
of O
k O
− O
1 O
parameters O
for O
each O
of O
the O
k O
possible O
values O
of O
x1 O
. O
14.3.2 O
error B
functions O
for O
boosting O
the O
exponential O
error O
function O
that O
is O
minimized O
by O
the O
adaboost O
algorithm O
differs O
from O
those O
considered O
in O
previous O
chapters O
. O
7.16 O
( O
( O
cid:12 O
) O
) O
by O
taking O
the O
second O
derivative O
of O
the O
log O
marginal O
likelihood O
( O
7.97 O
) O
for O
the O
regression B
rvm O
with O
respect O
to O
the O
hyperparameter B
αi O
, O
show O
that O
the O
stationary B
point O
given O
by O
( O
7.101 O
) O
is O
a O
maximum O
of O
the O
marginal B
likelihood I
. O
ditional O
distribution O
for O
xj O
given O
by O
p O
( O
xj|paj O
) O
together O
with O
any O
other O
conditional B
distributions O
that O
have O
xj O
in O
the O
conditioning O
set O
. O
by O
contrast O
, O
in O
the O
direction O
w2 O
the O
eigenvalue O
λ2 O
is O
large O
compared O
with O
α O
and O
so O
the O
quantity O
λ2/ O
( O
λ2 O
+α O
) O
is O
close O
to O
unity O
, O
and O
the O
map O
value O
of O
w2 O
is O
close O
to O
its O
maximum B
likelihood I
value O
. O
using O
bayes O
’ O
theorem O
, O
the O
posterior O
distribution O
for O
w O
is O
proportional O
to O
the O
product O
of O
the O
prior B
distribution O
and O
the O
likelihood B
function I
p O
( O
w|x O
, O
t O
, O
α O
, O
β O
) O
∝ O
p O
( O
t|x O
, O
w O
, O
β O
) O
p O
( O
w|α O
) O
. O
here O
the O
latent B
variable I
z O
characterizes O
which O
of O
the O
k O
components O
of O
the O
mixture B
is O
responsible O
for O
generating O
each O
data O
point O
. O
, O
θn O
} O
of O
a O
periodic B
variable I
. O
section O
2.3 O
another O
example O
of O
conditional B
independence I
and O
d-separation B
is O
provided O
by O
the O
concept O
of O
i.i.d O
. O
an O
alternative O
way O
to O
view O
the O
conditional B
independence I
test O
is O
to O
imagine O
re- O
moving O
all O
nodes O
in O
set O
c O
from O
the O
graph O
together O
with O
any O
links O
that O
connect O
to O
those O
nodes O
. O
642 O
13. O
sequential B
data I
13.3.2 O
learning B
in O
lds O
so O
far O
, O
we O
have O
considered O
the O
inference B
problem O
for O
linear O
dynamical O
systems O
, O
assuming O
that O
the O
model O
parameters O
θ O
= O
{ O
a O
, O
γ O
, O
c O
, O
σ O
, O
µ0 O
, O
v0 O
} O
are O
known O
. O
if O
we O
weren O
’ O
t O
familiar O
with O
the O
rules O
of O
ordinary O
calculus O
, O
we O
could O
evaluate O
a O
conventional O
derivative B
dy/ O
dx O
by O
making O
a O
small O
change O
 O
to O
the O
variable O
x O
and O
then O
expanding O
in O
powers O
of O
 O
, O
so O
that O
y O
( O
x O
+ O
 O
) O
= O
y O
( O
x O
) O
+ O
( O
d.1 O
) O
and O
ﬁnally O
taking O
the O
limit O
 O
→ O
0. O
similarly O
, O
for O
a O
function O
of O
several O
variables O
y O
( O
x1 O
, O
. O
dimension O
reduction O
by O
local B
principal O
component O
analysis O
. O
if O
every O
conditional B
indepen- O
dence O
statement O
implied O
by O
a O
graph O
is O
satisﬁed O
by O
a O
speciﬁc O
distribution O
, O
then O
the O
graph O
is O
said O
to O
be O
an O
i O
map O
( O
for O
‘ O
independence B
map I
’ O
) O
of O
that O
distribution O
. O
recall O
from O
( O
2.188 O
) O
that O
the O
gaussian O
mixture B
distribution I
can O
be O
written O
as O
a O
linear O
superposition O
of O
gaussians O
in O
the O
form O
p O
( O
x O
) O
= O
πkn O
( O
x|µk O
, O
σk O
) O
. O
exercises O
3.1 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
‘ O
tanh O
’ O
function O
and O
the O
logistic B
sigmoid I
function O
( O
3.6 O
) O
are O
related O
by O
tanh O
( O
a O
) O
= O
2σ O
( O
2a O
) O
− O
1. O
hence O
show O
that O
a O
general O
linear O
combination O
of O
logistic B
sigmoid I
functions O
of O
the O
form O
m O
( O
cid:2 O
) O
j=1 O
m O
( O
cid:2 O
) O
j=1 O
( O
cid:18 O
) O
( O
cid:18 O
) O
( O
cid:17 O
) O
( O
cid:17 O
) O
x O
− O
µj O
s O
x O
− O
µj O
s O
( O
3.100 O
) O
( O
3.101 O
) O
( O
3.102 O
) O
is O
equivalent O
to O
a O
linear O
combination O
of O
‘ O
tanh O
’ O
functions O
of O
the O
form O
y O
( O
x O
, O
w O
) O
= O
w0 O
+ O
wjσ O
y O
( O
x O
, O
u O
) O
= O
u0 O
+ O
uj O
tanh O
and O
ﬁnd O
expressions O
to O
relate O
the O
new O
parameters O
{ O
u1 O
, O
. O
this O
result O
also O
shows O
that O
the O
differential B
entropy I
, O
unlike O
the O
discrete O
entropy B
, O
can O
be O
negative O
, O
because O
h O
( O
x O
) O
< O
0 O
in O
( O
1.110 O
) O
for O
σ2 O
< O
1/ O
( O
2πe O
) O
. O
√ O
2x2 O
, O
x2 O
1 O
, O
2z2 O
1 O
+ O
2x1z1x2z2 O
+ O
x2 O
√ O
2 O
2z2 O
, O
z2 O
1 O
, O
2 O
) O
( O
1 O
, O
√ O
2z1 O
, O
√ O
2z1z2 O
, O
z2 O
2 O
) O
t O
( O
7.42 O
) O
this O
kernel B
function I
therefore O
represents O
an O
inner O
product O
in O
a O
feature B
space I
having O
six O
dimensions O
, O
in O
which O
the O
mapping O
from O
input O
space O
to O
feature B
space I
is O
described O
by O
the O
vector O
function O
φ O
( O
x O
) O
. O
3.3.1 O
3.3.2 O
3.3.3 O
equivalent B
kernel I
. O
consider O
1.6. O
information B
theory I
51 O
the O
number O
of O
different O
ways O
of O
allocating O
the O
objects O
to O
the O
bins O
. O
( O
12.42 O
) O
exercise O
12.8 O
574 O
12. O
continuous O
latent O
variables O
figure O
12.10 O
the O
probabilistic O
pea O
model O
for O
a O
data O
set O
of O
n O
obser O
( O
cid:173 O
) O
vations O
of O
x O
can O
be O
expressed O
as O
a O
directed B
graph O
in O
which O
each O
observation O
x O
n O
is O
associated O
with O
a O
value O
zn O
of O
the O
latent B
variable I
. O
again O
, O
because O
l O
( O
q O
, O
θ O
) O
is O
a O
lower B
bound I
on O
the O
log O
likelihood O
function O
, O
each O
complete O
em O
cycle O
of O
the O
gem O
algorithm O
is O
guaranteed O
to O
increase O
the O
value O
of O
the O
log O
likelihood O
( O
unless O
the O
parameters O
already O
correspond O
to O
a O
local B
maximum O
) O
. O
st O
( O
x|µ O
, O
λ O
, O
ν O
) O
= O
e O
[ O
x O
] O
= O
µ O
1 O
λ O
mode O
[ O
x O
] O
= O
µ. O
var O
[ O
x O
] O
= O
γ O
( O
ν/2 O
+ O
1/2 O
) O
γ O
( O
ν/2 O
) O
for O
ν O
> O
1 O
ν O
ν O
− O
2 O
for O
ν O
> O
2 O
( O
cid:15 O
) O
( O
cid:16 O
) O
1/2 O
( O
cid:29 O
) O
λ O
πν O
1 O
+ O
λ O
( O
x O
− O
µ O
) O
2 O
ν O
( O
cid:30 O
) O
−ν/2−1/2 O
( O
b.64 O
) O
( O
b.65 O
) O
( O
b.66 O
) O
( O
b.67 O
) O
here O
ν O
> O
0 O
is O
called O
the O
number O
of O
degrees B
of I
freedom I
of O
the O
distribution O
. O
with O
this O
more O
compact O
notation O
, O
we O
can O
write O
the O
two O
fundamental O
rules O
of O
probability B
theory O
in O
the O
following O
form O
. O
in O
contrast O
to O
the O
factors O
in O
the O
joint O
distribution O
for O
a O
directed B
graph O
, O
the O
po- O
tentials O
in O
an O
undirected B
graph I
do O
not O
have O
a O
speciﬁc O
probabilistic O
interpretation O
. O
note O
that O
, O
if O
both O
w O
and O
β O
are O
treated O
as O
unknown O
, O
then O
we O
can O
introduce O
a O
conjugate B
prior I
distribution O
p O
( O
w O
, O
β O
) O
that O
, O
from O
the O
discussion O
in O
section O
2.3.6 O
, O
will O
be O
given O
by O
a O
gaussian-gamma O
distribution O
( O
denison O
et O
al. O
, O
2002 O
) O
. O
construct O
a O
joint O
distribution O
p O
( O
x O
, O
y O
) O
over O
these O
variables O
having O
the O
property O
that O
the O
value O
( O
cid:1 O
) O
x O
that O
maximizes O
the O
marginal B
p O
( O
x O
) O
, O
along O
with O
the O
value O
( O
cid:1 O
) O
y O
that O
maximizes O
the O
marginal B
p O
( O
y O
) O
, O
together O
have O
probability B
zero O
under O
the O
joint O
distribution O
, O
so O
that O
p O
( O
( O
cid:1 O
) O
x O
, O
( O
cid:1 O
) O
y O
) O
= O
0 O
. O
14.1 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
set O
models O
of O
the O
form O
p O
( O
t|x O
, O
zh O
, O
θh O
, O
h O
) O
in O
which O
x O
is O
the O
input O
vector O
, O
t O
is O
the O
target B
vector I
, O
h O
indexes O
the O
different O
models O
, O
zh O
is O
a O
latent O
vari- O
able O
for O
model O
h O
, O
and O
θh O
is O
the O
set O
of O
parameters O
for O
model O
h. O
suppose O
the O
models O
have O
prior B
probabilities O
p O
( O
h O
) O
and O
that O
we O
are O
given O
a O
training B
set I
x O
= O
{ O
x1 O
, O
. O
similarly O
, O
the O
integra- O
tion O
in O
( O
1.68 O
) O
can O
also O
be O
performed O
analytically O
with O
the O
result O
that O
the O
predictive B
distribution I
is O
given O
by O
a O
gaussian O
of O
the O
form O
p O
( O
t|x O
, O
x O
, O
t O
) O
= O
n O
( O
cid:10 O
) O
t|m O
( O
x O
) O
, O
s2 O
( O
x O
) O
( O
cid:11 O
) O
where O
the O
mean B
and O
variance B
are O
given O
by O
n O
( O
cid:2 O
) O
s2 O
( O
x O
) O
= O
β O
here O
the O
matrix O
s O
is O
given O
by O
m O
( O
x O
) O
= O
βφ O
( O
x O
) O
ts O
φ O
( O
xn O
) O
tn O
−1 O
+ O
φ O
( O
x O
) O
tsφ O
( O
x O
) O
. O
this O
is O
reﬂected O
in O
the O
bayesian O
result O
for O
the O
variance B
that O
has O
a O
factor O
n O
− O
γ O
in O
the O
denominator O
, O
thereby O
correcting O
for O
the O
bias B
of O
the O
maximum B
likelihood I
result O
. O
from O
the O
sum B
rule I
, O
it O
then O
follows O
that O
p O
( O
b O
= O
b|f O
= O
o O
) O
= O
1 O
− O
2/3 O
= O
1/3 O
. O
exercises O
421 O
8.16 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
inference B
problem O
of O
evaluating O
p O
( O
xn|xn O
) O
for O
the O
graph O
shown O
in O
figure O
8.38 O
, O
for O
all O
nodes O
n O
∈ O
{ O
1 O
, O
. O
the O
quantities O
m O
represent O
weighted O
mea- O
sures O
of O
the O
error B
rates O
of O
each O
of O
the O
base O
classiﬁers O
on O
the O
data O
set O
. O
in O
this O
case O
, O
the O
logistic B
sigmoid I
function O
becomes O
inﬁnitely O
steep O
in O
feature B
space I
, O
corresponding O
to O
a O
heaviside O
step O
function O
, O
so O
that O
every O
training B
point O
from O
each O
class O
k O
is O
assigned O
a O
posterior B
probability I
p O
( O
ck|x O
) O
= O
1. O
furthermore O
, O
there O
is O
typically O
a O
continuum O
of O
such O
solutions O
because O
any O
separating O
hyperplane O
will O
give O
rise O
to O
the O
same O
pos- O
terior O
probabilities O
at O
the O
training B
data O
points O
, O
as O
will O
be O
seen O
later O
in O
figure O
10.13. O
maximum B
likelihood I
provides O
no O
way O
to O
favour O
one O
such O
solution O
over O
another O
, O
and O
which O
solution O
is O
found O
in O
practice O
will O
depend O
on O
the O
choice O
of O
optimization O
algo- O
rithm O
and O
on O
the O
parameter O
initialization O
. O
however O
, O
not O
all O
choices O
of O
class-conditional O
density B
give O
rise O
to O
such O
a O
simple O
form O
for O
the O
posterior O
probabilities O
( O
for O
instance O
, O
if O
the O
class-conditional O
densities O
are O
modelled O
using O
gaussian O
mixtures O
) O
. O
3.1.1 O
maximum B
likelihood I
and O
least O
squares O
in O
chapter O
1 O
, O
we O
ﬁtted O
polynomial O
functions O
to O
data O
sets O
by O
minimizing O
a O
sum- O
of-squares O
error B
function I
. O
we O
noted O
that O
the O
mean B
of O
the O
conditional B
distribution O
p O
( O
xa|xb O
) O
was O
a O
linear O
function O
of O
xb O
. O
in O
section O
12.4.2 O
, O
we O
show O
that O
networks O
of O
linear O
units O
give O
rise O
to O
principal B
component I
analysis I
. O
( O
6.55 O
) O
for O
the O
speciﬁc O
case O
of O
a O
gaussian O
process O
deﬁned O
by O
the O
linear B
regression I
model O
( O
6.49 O
) O
with O
a O
weight O
prior O
( O
6.50 O
) O
, O
the O
kernel B
function I
is O
given O
by O
( O
6.54 O
) O
. O
the O
matrix O
wij O
then O
has O
i O
− O
1 O
entries O
on O
the O
ith O
row O
and O
hence O
is O
a O
lower O
triangular O
matrix O
( O
with O
no O
entries O
on O
the O
leading O
diagonal B
) O
. O
in O
practice O
, O
ten O
or O
twenty O
independent B
samples O
may O
sufﬁce O
to O
estimate O
an O
expectation B
to O
sufﬁcient O
accuracy O
. O
the O
backpropagation B
procedure O
can O
therefore O
be O
summarized O
as O
follows O
. O
8.24 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
marginal B
distribution O
for O
the O
variables O
xs O
in O
a O
factor O
fs O
( O
xs O
) O
in O
a O
tree-structured O
factor B
graph I
, O
after O
running O
the O
sum-product O
message O
passing O
algo- O
rithm O
, O
can O
be O
written O
as O
the O
product O
of O
the O
message O
arriving O
at O
the O
factor O
node O
along O
all O
its O
links O
, O
times O
the O
local B
factor O
f O
( O
xs O
) O
, O
in O
the O
form O
( O
8.72 O
) O
. O
it O
is O
convenient O
, O
however O
, O
to O
introduce O
here O
one O
of O
the O
most O
important O
probability B
distributions O
for O
continuous O
variables O
, O
called O
the O
normal O
or O
gaussian O
distribution O
. O
in O
particular O
, O
we O
place O
no O
restriction O
on O
the O
functional B
forms O
of O
the O
individual O
factors O
qi O
( O
zi O
) O
. O
4.1. O
discriminant O
functions O
195 O
1 O
0.5 O
0 O
−0.5 O
−1 O
−1 O
1 O
0.5 O
0 O
−0.5 O
−1 O
−1 O
1 O
0.5 O
0 O
−0.5 O
−1 O
−1 O
1 O
0.5 O
0 O
−0.5 O
−1 O
−1 O
−0.5 O
0 O
0.5 O
1 O
−0.5 O
0 O
0.5 O
1 O
−0.5 O
0 O
0.5 O
1 O
−0.5 O
0 O
0.5 O
1 O
figure O
4.7 O
illustration O
of O
the O
convergence O
of O
the O
perceptron B
learning O
algorithm O
, O
showing O
data O
points O
from O
two O
classes O
( O
red O
and O
blue O
) O
in O
a O
two-dimensional O
feature B
space I
( O
φ1 O
, O
φ2 O
) O
. O
7.2. O
relevance B
vector I
machines O
support B
vector I
machines O
have O
been O
used O
in O
a O
variety O
of O
classiﬁcation O
and O
regres- O
sion B
applications O
. O
as O
we O
shall O
see O
, O
this O
form O
of O
kl O
divergence O
exercise O
10.2 O
468 O
10. O
approximate O
inference B
figure O
10.2 O
comparison O
of O
the O
the O
two O
alternative O
forms O
for O
kullback-leibler O
divergence O
. O
functional B
approximation O
by O
feed-forward O
networks O
: O
a O
least-squares O
approach O
to O
generalisation O
. O
1.5.3 O
the O
reject B
option I
we O
have O
seen O
that O
classiﬁcation B
errors O
arise O
from O
the O
regions O
of O
input O
space O
where O
the O
largest O
of O
the O
posterior O
probabilities O
p O
( O
ck|x O
) O
is O
signiﬁcantly O
less O
than O
unity O
, O
or O
equivalently O
where O
the O
joint O
distributions O
p O
( O
x O
, O
ck O
) O
have O
comparable O
values O
. O
we O
now O
show O
that O
pca O
can O
also O
be O
expressed O
as O
the O
maximum B
likelihood I
solution O
of O
a O
probabilistic O
latent O
variable O
model O
. O
this O
has O
its O
origins O
with O
valiant O
( O
1984 O
) O
who O
formulated O
the O
probably B
approximately I
correct I
, O
or O
pac O
, O
learning B
framework O
. O
to O
see O
this O
, O
consider O
two O
points O
xa O
and O
xb O
both O
of O
which O
lie O
inside O
decision B
region I
rk O
, O
as O
illustrated O
in O
figure O
4.3. O
any O
point O
( O
cid:1 O
) O
x O
that O
lies O
on O
the O
line O
connecting O
xa O
and O
xb O
can O
be O
expressed O
in O
the O
form O
( O
cid:1 O
) O
x O
= O
λxa O
+ O
( O
1 O
− O
λ O
) O
xb O
( O
4.11 O
) O
184 O
4. O
linear O
models O
for O
classification O
figure O
4.3 O
illustration O
of O
the O
decision O
regions O
for O
a O
mul- O
ticlass O
linear B
discriminant I
, O
with O
the O
decision O
if O
two O
points O
xa O
boundaries O
shown O
in O
red O
. O
tipping O
( O
1999 O
) O
uses O
variational B
inference I
in O
a O
model O
with O
a O
two-dimensional O
latent O
space O
, O
allowing O
a O
binary O
data O
set O
to O
be O
visualized O
analogously O
to O
the O
use O
of O
pca O
to O
visualize O
continuous O
data O
. O
638 O
13. O
sequential B
data I
13.3.1 O
inference B
in O
lds O
we O
now O
turn O
to O
the O
problem O
of O
ﬁnding O
the O
marginal B
distributions O
for O
the O
latent O
variables O
conditional B
on O
the O
observation O
sequence O
. O
note O
that O
after O
each O
e O
step O
, O
the O
matrix O
rk O
will O
change O
and O
so O
we O
will O
have O
to O
solve O
the O
normal B
equations I
afresh O
in O
the O
subsequent O
m O
step O
. O
first O
we O
take O
the O
log O
of O
the O
logistic O
function O
and O
then O
decompose O
it O
so O
that O
−x O
) O
= O
− O
ln O
ln O
σ O
( O
x O
) O
= O
− O
ln O
( O
1 O
+ O
e O
−x/2 O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
= O
x/2 O
− O
ln O
( O
ex/2 O
+ O
e O
( O
10.138 O
) O
we O
now O
note O
that O
the O
function O
f O
( O
x O
) O
= O
− O
ln O
( O
ex/2 O
+ O
e O
−x/2 O
) O
is O
a O
convex B
function I
of O
the O
variable O
x2 O
, O
as O
can O
again O
be O
veriﬁed O
by O
ﬁnding O
the O
second O
derivative O
. O
( O
cid:16 O
) O
( O
2.65 O
) O
( O
2.66 O
) O
( O
2.67 O
) O
( O
2.69 O
) O
86 O
2. O
probability B
distributions O
evaluated O
from O
the O
joint O
distribution O
p O
( O
x O
) O
= O
p O
( O
xa O
, O
xb O
) O
simply O
by O
ﬁxing O
xb O
to O
the O
observed O
value O
and O
normalizing O
the O
resulting O
expression O
to O
obtain O
a O
valid O
probability B
distribution O
over O
xa O
. O
if O
we O
assume O
that O
the O
gaussian O
prior B
distribution O
over O
parameters O
is O
broad O
, O
and O
that O
the O
hessian O
has O
full O
rank O
, O
then O
we O
can O
approximate O
( O
4.137 O
) O
very O
roughly O
using O
ln O
p O
( O
d O
) O
( O
cid:7 O
) O
ln O
p O
( O
d|θmap O
) O
− O
1 O
2 O
m O
ln O
n O
( O
4.139 O
) O
where O
n O
is O
the O
number O
of O
data O
points O
, O
m O
is O
the O
number O
of O
parameters O
in O
θ O
and O
we O
have O
omitted O
additive O
constants O
. O
as O
with O
hidden O
markov O
models O
, O
we O
can O
develop O
interesting O
extensions O
of O
the O
ba- O
sic O
linear B
dynamical I
system I
by O
expanding O
its O
graphical O
representation O
. O
we O
ﬁrst O
initialize O
the O
variational B
parameters O
ξold O
. O
a O
more O
systematic O
approach O
is O
called O
orthogonal B
least I
squares I
( O
chen O
et O
al. O
, O
1991 O
) O
. O
442 O
9. O
mixture B
models O
and O
em O
figure O
9.9 O
this O
shows O
the O
same O
graph O
as O
in O
figure O
9.6 O
except O
that O
we O
now O
suppose O
that O
the O
discrete O
variables O
zn O
are O
ob- O
served O
, O
as O
well O
as O
the O
data O
variables O
xn O
. O
as O
we O
have O
seen O
, O
the O
hmm O
corresponds O
to O
the O
state B
space I
model I
shown O
in O
figure O
13.5 O
in O
which O
the O
latent O
variables O
are O
discrete O
but O
with O
arbitrary O
emission B
probability I
distributions O
. O
this O
projects O
to O
a O
point O
in O
data O
space O
given O
by O
wle O
[ O
zlx O
] O
+ O
j-l. O
( O
12.48 O
) O
( O
12.49 O
) O
section O
3.3.1 O
note O
that O
this O
takes O
the O
same O
form O
as O
the O
equations O
for O
regularized O
linear B
regression I
and O
is O
a O
consequence O
of O
maximizing O
the O
likelihood B
function I
for O
a O
linear O
gaussian O
model O
. O
14.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
extend O
the O
results O
of O
section O
14.5.1 O
for O
a O
mixture O
of O
linear O
regression B
models O
to O
the O
case O
of O
multiple O
target O
values O
described O
by O
a O
vector O
t. O
to O
do O
this O
, O
make O
use O
of O
the O
results O
of O
section O
3.1.5 O
. O
in O
prac- O
tice O
, O
it O
is O
more O
convenient O
to O
maximize O
the O
log O
of O
the O
likelihood B
function I
. O
the O
performance O
of O
a O
particular O
learning B
algorithm O
is O
then O
assessed O
by O
taking O
the O
average O
over O
this O
ensemble O
of O
data O
sets O
. O
first O
of O
all O
, O
the O
singularities B
that O
arise O
in O
maximum B
likelihood I
when O
a O
gaussian O
com- O
ponent O
‘ O
collapses O
’ O
onto O
a O
speciﬁc O
data O
point O
are O
absent O
in O
the O
bayesian O
treatment O
. O
13.10 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
applying O
the O
sum O
and O
product O
rules O
of O
probability B
, O
verify O
that O
the O
condi- O
tional O
independence O
properties O
( O
13.24 O
) O
– O
( O
13.31 O
) O
are O
satisﬁed O
by O
the O
joint O
distribution O
for O
the O
hidden O
markov O
model O
deﬁned O
by O
( O
13.6 O
) O
. O
note O
that O
some O
solutions O
ﬁnd O
suboptimal O
local B
maxima O
, O
but O
that O
this O
hap- O
pens O
infrequently O
. O
show O
that O
the O
hierarchical O
mixture O
can O
not O
in O
general O
be O
represented O
by O
a O
single-level O
mixture B
having O
linear O
classiﬁcation O
models O
for O
the O
mixing O
coefﬁcients O
. O
given O
a O
data O
set O
of O
observations O
, O
the O
592 O
12. O
continuous O
latent O
variables O
likelihood B
function I
for O
this O
model O
is O
a O
function O
of O
the O
coefficients O
in O
the O
linear O
com O
( O
cid:173 O
) O
bination O
. O
( O
1.49 O
) O
because O
the O
parameter O
µ O
represents O
the O
average O
value O
of O
x O
under O
the O
distribution O
, O
it O
is O
referred O
to O
as O
the O
mean B
. O
stochastic B
techniques O
such O
as O
markov O
chain O
monte O
carlo O
, O
de- O
scribed O
in O
chapter O
11 O
, O
have O
enabled O
the O
widespread O
use O
of O
bayesian O
methods O
across O
many O
domains O
. O
thus O
any O
eigenvector O
will O
define O
a O
sta O
( O
cid:173 O
) O
tionary O
point O
of O
the O
distortion B
measure I
. O
here O
we O
consider O
mixtures O
of O
linear B
regression I
models O
( O
section O
14.5.1 O
) O
and O
mixtures O
of O
chapter O
9 O
14.5. O
conditional O
mixture O
models O
667 O
logistic B
regression I
models O
( O
section O
14.5.2 O
) O
. O
if O
we O
de- O
ﬁne O
a O
gaussian O
process O
over O
a O
function O
a O
( O
x O
) O
and O
then O
transform O
the O
function O
using O
a O
logistic B
sigmoid I
y O
= O
σ O
( O
a O
) O
, O
given O
by O
( O
4.59 O
) O
, O
then O
we O
will O
obtain O
a O
non-gaussian O
stochastic B
process I
over O
functions O
y O
( O
x O
) O
where O
y O
∈ O
( O
0 O
, O
1 O
) O
. O
this O
type O
of O
construction O
can O
be O
extended B
in O
principle O
to O
any O
level O
and O
is O
an O
illustration O
of O
a O
hierarchical B
bayesian O
model O
, O
of O
which O
we O
shall O
encounter O
further O
examples O
in O
later O
chapters O
. O
14.4. O
tree-based O
models O
663 O
e O
( O
z O
) O
−1 O
0 O
1 O
z O
can O
be O
addressed O
by O
basing O
the O
boosting B
algorithm O
on O
the O
absolute O
deviation O
|y O
− O
t| O
instead O
. O
it O
should O
be O
emphasized O
that O
the O
value O
of O
α O
has O
been O
determined O
purely O
by O
look- O
ing O
at O
the O
training B
data O
. O
1.33 O
( O
( O
cid:1 O
) O
( O
cid:1 O
) O
) O
suppose O
that O
the O
conditional B
entropy I
h O
[ O
y|x O
] O
between O
two O
discrete O
random O
variables O
x O
and O
y O
is O
zero O
. O
noting O
that O
p O
( O
w O
) O
does O
not O
depend O
on O
ξ O
, O
and O
substituting O
for O
h O
( O
w O
, O
ξ O
) O
we O
obtain O
q O
( O
ξ O
, O
ξold O
) O
= O
ln O
σ O
( O
ξn O
) O
− O
ξn/2 O
− O
λ O
( O
ξn O
) O
( O
φt O
n O
e O
[ O
wwt O
] O
φn O
− O
ξ2 O
n O
) O
+ O
const O
n=1 O
( O
10.161 O
) O
where O
‘ O
const O
’ O
denotes O
terms O
that O
are O
independent B
of O
ξ. O
we O
now O
set O
the O
derivative B
with O
respect O
to O
ξn O
equal O
to O
zero O
. O
classiﬁcation B
and I
regression I
trees I
. O
we O
can O
illustrate O
the O
effect O
of O
the O
resulting O
four O
hyperpa- O
rameters O
by O
drawing O
samples O
from O
the O
prior B
and O
plotting O
the O
corresponding O
network O
functions O
, O
as O
shown O
in O
figure O
5.11. O
any O
number O
of O
groups O
wk O
so O
that O
more O
generally O
, O
we O
can O
consider O
priors O
in O
which O
the O
weights O
are O
divided O
into O
where O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
2 O
k O
= O
as O
a O
special O
case O
of O
this O
prior B
, O
if O
we O
choose O
the O
groups O
to O
correspond O
to O
the O
sets O
of O
weights O
associated O
with O
each O
of O
the O
input O
units O
, O
and O
we O
optimize O
the O
marginal B
likelihood I
with O
respect O
to O
the O
corresponding O
parameters O
αk O
, O
we O
obtain O
automatic B
relevance I
determination I
as O
discussed O
in O
section O
7.2.2 O
. O
( O
1.139 O
) O
this O
can O
be O
done O
by O
ﬁrst O
proving O
that O
the O
result O
holds O
for O
m O
= O
0 O
and O
arbitrary O
d O
( O
cid:2 O
) O
1 O
, O
then O
assuming O
that O
it O
holds O
at O
order O
m O
, O
and O
hence O
showing O
that O
it O
holds O
at O
order O
m O
+ O
1. O
finally O
, O
make O
use O
of O
stirling O
’ O
s O
approximation O
in O
the O
form O
n O
! O
( O
cid:8 O
) O
nne O
−n O
( O
1.140 O
) O
for O
large O
n O
to O
show O
that O
, O
for O
d O
( O
cid:10 O
) O
m O
, O
the O
quantity O
n O
( O
d O
, O
m O
) O
grows O
like O
dm O
, O
and O
for O
m O
( O
cid:10 O
) O
d O
it O
grows O
like O
m O
d. O
consider O
a O
cubic O
( O
m O
= O
3 O
) O
polynomial O
in O
d O
dimensions O
, O
and O
evaluate O
numerically O
the O
total O
number O
of O
independent B
parameters O
for O
( O
i O
) O
d O
= O
10 O
and O
( O
ii O
) O
d O
= O
100 O
, O
which O
correspond O
to O
typical O
small-scale O
and O
medium-scale O
machine O
learning O
applications O
. O
to O
do O
this O
, O
we O
ﬁrst O
introduce O
an O
explicit O
latent O
446 O
9. O
mixture B
models O
and O
em O
exercise O
9.14 O
variable O
z O
associated O
with O
each O
instance O
of O
x. O
as O
in O
the O
case O
of O
the O
gaussian O
mixture B
, O
z O
= O
( O
z1 O
, O
. O
in O
most O
applications O
, O
we O
will O
not O
have O
any O
prior B
knowledge O
about O
the O
mean B
of O
y O
( O
x O
) O
and O
so O
by O
symmetry O
we O
take O
it O
to O
be O
zero O
. O
in O
the O
limit O
σ2 O
→ O
0 O
, O
the O
optimal O
hyperplane O
is O
shown O
to O
be O
the O
one O
having O
maximum B
margin I
. O
we O
now O
extend O
the O
bayesian O
logistic B
regression I
model O
to O
allow O
the O
value O
of O
this O
parameter O
to O
be O
inferred O
from O
the O
data O
set O
. O
because O
these O
conditional B
and O
marginal B
distributions O
together O
specify O
the O
joint O
distribution O
, O
we O
see O
that O
the O
joint O
distribution O
is O
itself O
invariant O
. O
thus O
the O
computational O
cost O
of O
solving O
this O
problem O
exactly O
would O
grow O
exponentially O
with O
the O
size O
of O
the O
data O
set O
, O
and O
so O
an O
exact O
solution O
is O
intractable O
for O
moderately O
large O
n. O
to O
apply O
ep O
to O
the O
clutter B
problem I
, O
we O
ﬁrst O
identify O
the O
factors O
f0 O
( O
θ O
) O
= O
p O
( O
θ O
) O
and O
fn O
( O
θ O
) O
= O
p O
( O
xn|θ O
) O
. O
it O
is O
worth O
emphasizing O
once O
again O
that O
maximum B
likelihood I
would O
lead O
to O
values O
of O
the O
likeli- O
hood O
function O
that O
increase O
monotonically O
with O
k O
( O
assuming O
the O
singular O
solutions O
have O
been O
avoided O
, O
and O
discounting O
the O
effects O
of O
local B
maxima O
) O
and O
so O
can O
not O
be O
used O
to O
determine O
an O
appropriate O
model O
complexity O
. O
figure O
10.7 O
shows O
a O
plot O
of O
the O
lower B
bound I
, O
including O
the O
multimodality B
fac- O
tor O
, O
versus O
the O
number O
k O
of O
components O
for O
the O
old O
faithful O
data O
set O
. O
11.2. O
markov O
chain O
monte O
carlo O
537 O
now O
suppose O
we O
move O
from O
a O
maximum B
likelihood I
approach O
to O
a O
full O
bayesian O
treatment O
in O
which O
we O
wish O
to O
sample O
from O
the O
posterior O
distribution O
over O
the O
param- O
eter O
vector O
θ. O
in O
principle O
, O
we O
would O
like O
to O
draw O
samples O
from O
the O
joint O
posterior O
p O
( O
θ O
, O
z|x O
) O
, O
but O
we O
shall O
suppose O
that O
this O
is O
computationally O
difﬁcult O
. O
because O
the O
noise O
is O
independent B
for O
each O
data O
point O
, O
the O
joint O
distribution O
of O
the O
target O
values O
t O
= O
( O
t1 O
, O
. O
this O
posterior O
can O
be O
regarded O
as O
the O
prior B
for O
the O
next O
obser- O
vation O
. O
for O
sim- O
plicity O
, O
consider O
a O
gaussian O
mixture B
whose O
components O
have O
covariance B
matrices O
given O
by O
σk O
= O
σ2 O
ki O
, O
where O
i O
is O
the O
unit O
matrix O
, O
although O
the O
conclusions O
will O
hold O
for O
general O
covariance B
matrices O
. O
in O
other O
words O
, O
this O
model O
correctly O
captures O
the O
variance B
of O
the O
data O
along O
the O
principal O
axes O
, O
and O
approximates O
the O
variance B
in O
all O
remaining O
directions O
with O
a O
single O
average O
value O
( O
j'2 O
. O
10.5 O
local B
variational O
methods O
. O
0 O
( O
θmap O
− O
m O
) O
− O
1 O
−1 O
2 O
( O
θmap O
− O
m O
) O
tv O
−1 O
0 O
4.24 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
use O
the O
results O
from O
section O
2.3.2 O
to O
derive O
the O
result O
( O
4.151 O
) O
for O
the O
marginal- O
ization O
of O
the O
logistic B
regression I
model O
with O
respect O
to O
a O
gaussian O
posterior O
distribu- O
tion O
over O
the O
parameters O
w. O
4.25 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
suppose O
we O
wish O
to O
approximate O
the O
logistic B
sigmoid I
σ O
( O
a O
) O
deﬁned O
by O
( O
4.59 O
) O
by O
a O
scaled O
probit B
function I
φ O
( O
λa O
) O
, O
where O
φ O
( O
a O
) O
is O
deﬁned O
by O
( O
4.114 O
) O
. O
in O
chapter O
1 O
, O
we O
identiﬁed O
three O
distinct O
approaches O
to O
the O
classiﬁcation B
prob- O
lem O
. O
however O
, O
the O
advantage O
of O
the O
dual O
formulation O
, O
as O
we O
shall O
see O
, O
is O
that O
it O
is O
expressed O
entirely O
in O
terms O
of O
the O
kernel B
function I
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
. O
as O
we O
have O
discussed O
in O
chapter O
1 O
, O
if O
the O
data O
set O
is O
sufﬁciently O
large O
, O
it O
may O
be O
worthwhile O
to O
use O
sequential O
algorithms O
, O
also O
known O
as O
on-line O
algorithms O
, O
exercise O
3.2 O
144 O
3. O
linear O
models O
for B
regression I
in O
which O
the O
data O
points O
are O
considered O
one O
at O
a O
time O
, O
and O
the O
model O
parameters O
up- O
dated O
after O
each O
such O
presentation O
. O
they O
ﬁrst O
model O
the O
distribution O
over O
in- O
put O
vectors O
x O
for O
each O
class O
using O
a O
parzen O
density B
estimator O
with O
gaussian O
kernels O
section O
7.1.5 O
y O
= O
1 O
y O
= O
0 O
y O
= O
−1 O
7.1. O
maximum B
margin I
classiﬁers O
327 O
y O
= O
−1 O
y O
= O
0 O
y O
= O
1 O
margin B
figure O
7.1 O
the O
margin B
is O
deﬁned O
as O
the O
perpendicular O
distance O
between O
the O
decision B
boundary I
and O
the O
closest O
of O
the O
data O
points O
, O
as O
shown O
on O
the O
left O
ﬁgure O
. O
the O
quantity O
γi O
measures O
how O
well O
the O
corresponding O
parameter O
wi O
is O
determined O
by O
the O
data O
and O
is O
deﬁned O
by O
348 O
7. O
sparse O
kernel O
machines O
γi O
= O
1 O
− O
αiσii O
( O
7.89 O
) O
in O
which O
σii O
is O
the O
ith O
diagonal B
component O
of O
the O
posterior O
covariance O
σ O
given O
by O
( O
7.83 O
) O
. O
suitable O
choices O
for O
the O
prior B
will O
remove O
the O
singularities B
of O
the O
kind O
illustrated O
in O
figure O
9.7. O
here O
we O
have O
considered O
the O
use O
of O
the O
em O
algorithm O
to O
maximize O
a O
likelihood B
function I
when O
there O
are O
discrete O
latent O
variables O
. O
thus O
, O
for O
instance O
, O
we O
see O
that O
points O
that O
are O
misclassiﬁed O
by O
the O
m O
= O
1 O
base O
learner O
are O
given O
greater O
weight O
when O
training B
the O
m O
= O
2 O
base O
learner O
. O
2.36 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
using O
an O
analogous O
procedure O
to O
that O
used O
to O
obtain O
( O
2.126 O
) O
, O
derive O
an O
expression O
for O
the O
sequential B
estimation I
of O
the O
variance B
of O
a O
univariate O
gaussian O
134 O
2. O
probability B
distributions O
distribution O
, O
by O
starting O
with O
the O
maximum B
likelihood I
expression O
n O
( O
cid:2 O
) O
n=1 O
σ2 O
ml O
= O
1 O
n O
( O
xn O
− O
µ O
) O
2 O
. O
thc O
simplest O
continuous O
latent B
variable I
model O
assumes O
gaussian O
distributions O
for O
both O
thc O
latent O
and O
observed O
variables O
and O
makes O
use O
of O
a O
linear O
, O
gaussian O
de- O
pendence O
of O
the O
observed O
variables O
on O
ihe O
slate O
of O
the O
latent O
variables O
. O
we O
begin O
by O
consid- O
ering O
the O
forward O
equations O
in O
which O
we O
treat O
zn O
as O
the O
root B
node I
, O
and O
propagate O
messages O
from O
the O
leaf O
node B
h O
( O
z1 O
) O
to O
the O
root O
. O
4.5 O
bayesian O
logistic B
regression I
. O
10.23 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
variational B
gaussian O
mixture B
model I
in O
which O
there O
is O
no O
prior B
distribution O
over O
mixing O
coefﬁcients O
{ O
πk O
} O
. O
let O
us O
summarize O
the O
em O
algorithm O
for O
ﬁnding O
the O
variational B
posterior O
distri- O
bution O
. O
this O
can O
again O
be O
done O
us- O
ing O
the O
em O
algorithm O
in O
which O
the O
e O
step O
is O
the O
same O
as O
discussed O
above O
, O
and O
the O
m O
step O
involves O
adding O
the O
log O
of O
the O
prior B
distribution O
p O
( O
θ O
) O
to O
the O
function O
q O
( O
θ O
, O
θold O
) O
before O
maximization O
and O
represents O
a O
straightforward O
application O
of O
the O
techniques O
developed O
at O
various O
points O
in O
this O
book O
. O
) O
, O
ad- O
vances O
in O
large B
margin I
classiﬁers O
, O
pp O
. O
we O
can O
determine O
the O
parameters O
of O
this O
model O
using O
maximum B
likelihood I
, O
by O
a O
straightforward O
extension O
of O
the O
ideas O
discussed O
earlier O
. O
second B
order I
derivatives O
for O
network O
pruning O
: O
optimal O
brain O
surgeon O
. O
( O
cid:6 O
) O
= O
n O
( O
cid:10 O
) O
( O
cid:11 O
) O
( O
7.90 O
) O
thus O
the O
predictive O
mean O
is O
given O
by O
( O
7.76 O
) O
with O
w O
set O
equal O
to O
the O
posterior O
mean O
m O
, O
and O
the O
variance B
of O
the O
predictive B
distribution I
is O
given O
by O
σ2 O
( O
x O
) O
= O
( O
β O
( O
cid:1 O
) O
) O
−1 O
+ O
φ O
( O
x O
) O
tσφ O
( O
x O
) O
( O
7.91 O
) O
where O
σ O
is O
given O
by O
( O
7.83 O
) O
in O
which O
α O
and O
β O
are O
set O
to O
their O
optimized O
values O
α O
( O
cid:1 O
) O
and O
β O
( O
cid:1 O
) O
. O
we O
can O
encour- O
age O
the O
weight O
values O
to O
form O
several O
groups O
, O
rather O
than O
just O
one O
group O
, O
by O
consid- O
ering O
instead O
a O
probability B
distribution O
that O
is O
a O
mixture O
of O
gaussians O
. O
an O
example O
of O
a O
regression B
problem O
would O
be O
the O
pre- O
diction O
of O
the O
yield O
in O
a O
chemical O
manufacturing O
process O
in O
which O
the O
inputs O
consist O
of O
the O
concentrations O
of O
reactants O
, O
the O
temperature O
, O
and O
the O
pressure O
. O
6.4. O
gaussian O
processes O
in O
section O
6.1 O
, O
we O
introduced O
kernels O
by O
applying O
the O
concept O
of O
duality O
to O
a O
non- O
probabilistic O
model O
for B
regression I
. O
( O
6.73 O
) O
as O
usual O
, O
we O
denote O
the O
training B
set I
inputs O
by O
x1 O
, O
. O
theory B
of O
probability B
. O
, O
k O
, O
then O
the O
em O
algorithm O
will O
hence O
show O
that O
if O
the O
parameters O
of O
this O
model O
are O
initialized O
such O
that O
all O
compo- O
converge O
after O
one O
iteration O
, O
for O
any O
choice O
of O
the O
initial O
mixing O
coefﬁcients O
, O
and O
that O
this O
solution O
has O
the O
property O
µk O
= O
x. O
note O
that O
this O
represents O
a O
degenerate O
case O
of O
the O
mixture B
model I
in O
which O
all O
of O
the O
components O
are O
identical O
, O
and O
in O
practice O
we O
try O
to O
avoid O
such O
solutions O
by O
using O
an O
appropriate O
initialization O
. O
one O
of O
the O
great O
merits O
of O
neural O
networks O
is O
that O
the O
outputs O
share O
the O
hidden O
units O
and O
so O
they O
can O
‘ O
borrow O
statistical O
strength O
’ O
from O
each O
other O
, O
that O
is O
, O
the O
weights O
associated O
with O
each O
hidden B
unit I
are O
inﬂuenced O
by O
all O
of O
the O
output O
variables O
not O
just O
by O
one O
of O
them O
. O
the O
system O
then O
explores O
the O
distribution O
along O
the O
more O
extended B
direction O
by O
means O
of O
a O
random O
walk O
, O
and O
so O
the O
number O
of O
steps O
to O
arrive O
at O
a O
state O
that O
is O
more O
or O
less O
independent B
of O
the O
original O
state O
is O
of O
order O
( O
σmax/σmin O
) O
2. O
in O
fact O
in O
two O
dimensions O
, O
the O
increase O
in O
rejection O
rate O
as O
ρ O
increases O
is O
offset O
by O
the O
larger O
steps O
sizes O
of O
those O
transitions O
that O
are O
accepted O
, O
and O
more O
generally O
for O
a O
multivariate O
gaussian O
the O
number O
of O
steps O
required O
to O
obtain O
independent B
samples O
scales O
like O
( O
σmax/σ2 O
) O
2 O
where O
σ2 O
is O
the O
second-smallest O
stan- O
dard O
deviation O
( O
neal O
, O
1993 O
) O
. O
this O
allows O
us O
to O
provide O
a O
simple O
interpretation O
of O
the O
hyperparameters O
a O
and O
b O
in O
the O
prior B
as O
an O
effective B
number I
of I
observations I
of O
x O
= O
1 O
and O
x O
= O
0 O
, O
respectively O
. O
however O
, O
when O
we O
condition O
on O
node B
c O
, O
as O
in O
figure O
8.16 O
, O
the O
conditioned O
node B
‘ O
blocks O
’ O
the O
path O
from O
a O
to O
b O
and O
causes O
a O
and O
b O
to O
become O
( O
conditionally O
) O
independent B
. O
as O
in O
the O
case O
of O
a O
standard O
mixture O
model O
, O
the O
latent O
variables O
are O
the O
discrete O
multinomial O
variables O
zn O
describing O
which O
component O
of O
the O
mixture B
is O
responsible O
for O
generating O
the O
corresponding O
observation O
xn O
. O
( O
2.256 O
) O
126 O
2. O
probability B
distributions O
x2 O
x2 O
figure O
2.27 O
( O
a O
) O
in O
the O
k-nearest- O
neighbour O
classiﬁer O
, O
a O
new O
point O
, O
shown O
by O
the O
black O
diamond O
, O
is O
clas- O
siﬁed O
according O
to O
the O
majority O
class O
membership O
of O
the O
k O
closest O
train- O
ing O
data O
points O
, O
in O
this O
case O
k O
= O
3. O
in O
the O
nearest-neighbour O
( O
k O
= O
1 O
) O
approach O
to O
classiﬁcation B
, O
the O
resulting O
decision B
boundary I
is O
composed O
of O
hyperplanes O
that O
form O
perpendicular O
bisectors O
of O
pairs O
of O
points O
from O
different O
classes O
. O
( O
13.127 O
) O
( O
13.128 O
) O
show O
that O
this O
extension O
can O
be O
re-case O
in O
the O
framework O
discussed O
in O
this O
chapter O
by O
deﬁning O
a O
state O
vector O
z O
with O
an O
additional O
component O
ﬁxed O
at O
unity O
, O
and O
then O
aug- O
menting O
the O
matrices O
a O
and O
c O
using O
extra O
columns O
corresponding O
to O
the O
parameters O
a O
and O
c. O
13.25 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
this O
exercise O
, O
we O
show O
that O
when O
the O
kalman O
ﬁlter O
equations O
are O
applied O
to O
independent B
observations O
, O
they O
reduce O
to O
the O
results O
given O
in O
section O
2.3 O
for O
the O
maximum B
likelihood I
solution O
for O
a O
single O
gaussian O
distribution O
. O
this O
is O
known O
as O
model B
selection I
. O
assessing O
ap- O
proximations O
for O
gaussian O
process O
classiﬁcation B
. O
5.5.4 O
tangent B
propagation I
. O
again O
, O
the O
derivative B
of O
the O
error B
function I
with O
re- O
spect O
to O
the O
activation O
for O
a O
particular O
output O
unit O
takes O
the O
form O
( O
5.18 O
) O
just O
as O
in O
the O
regression B
case O
. O
the O
bound O
on O
the O
sigmoid B
then O
becomes O
σ O
( O
x O
) O
( O
cid:2 O
) O
σ O
( O
ξ O
) O
exp O
( O
x O
− O
ξ O
) O
/2 O
− O
λ O
( O
ξ O
) O
( O
x2 O
− O
ξ2 O
) O
( O
cid:26 O
) O
( O
10.142 O
) O
( O
10.143 O
) O
( O
10.144 O
) O
( O
cid:27 O
) O
section O
4.5 O
section O
4.3 O
where O
λ O
( O
ξ O
) O
is O
deﬁned O
by O
( O
10.141 O
) O
. O
from O
bayes O
’ O
theorem O
the O
model B
evidence I
is O
given O
by O
( O
cid:6 O
) O
p O
( O
d O
) O
= O
p O
( O
d|θ O
) O
p O
( O
θ O
) O
dθ O
. O
( O
2.40 O
) O
we O
see O
that O
the O
posterior O
distribution O
again O
takes O
the O
form O
of O
a O
dirichlet O
distribution O
, O
conﬁrming O
that O
the O
dirichlet O
is O
indeed O
a O
conjugate B
prior I
for O
the O
multinomial O
. O
zn O
xn O
π O
µ O
σ O
n O
now O
consider O
the O
problem O
of O
maximizing O
the O
likelihood O
for O
the O
complete B
data I
set I
{ O
x O
, O
z O
} O
. O
2.4.2 O
conjugate B
priors O
we O
have O
already O
encountered O
the O
concept O
of O
a O
conjugate B
prior I
several O
times O
, O
for O
example O
in O
the O
context O
of O
the O
bernoulli O
distribution O
( O
for O
which O
the O
conjugate B
prior I
is O
the O
beta B
distribution I
) O
or O
the O
gaussian O
( O
where O
the O
conjugate B
prior I
for O
the O
mean B
is O
a O
gaussian O
, O
and O
the O
conjugate B
prior I
for O
the O
precision O
is O
the O
wishart O
distribution O
) O
. O
we O
shall O
see O
in O
figure O
10.13 O
that O
marginalization O
with O
respect O
to O
the O
prior B
distri- O
bution O
of O
the O
parameters O
in O
a O
bayesian O
approach O
for O
a O
simple O
linearly B
separable I
data O
set O
leads O
to O
a O
decision B
boundary I
that O
lies O
in O
the O
middle O
of O
the O
region O
separating O
the O
data O
points O
. O
, O
xd O
) O
t. O
this O
is O
often O
simply O
known O
as O
linear B
regression I
. O
( O
11.19 O
) O
the O
quantities O
rl O
= O
p O
( O
z O
( O
l O
) O
) O
/q O
( O
z O
( O
l O
) O
) O
are O
known O
as O
importance B
weights I
, O
and O
they O
cor- O
rect O
the O
bias B
introduced O
by O
sampling O
from O
the O
wrong O
distribution O
. O
on O
the O
right O
is O
the O
predicted O
posterior B
probability I
for O
the O
blue O
and O
red O
classes O
together O
with O
the O
gaussian O
process O
decision B
boundary I
. O
to O
do O
this O
, O
ﬁrst O
ﬁnd O
the O
distribution O
of O
x O
by O
using O
the O
relation O
p O
( O
x|x2 O
) O
p O
( O
x2 O
) O
dx2 O
( O
2.284 O
) O
( O
cid:6 O
) O
∞ O
p O
( O
x O
) O
= O
−∞ O
and O
completing B
the I
square I
in O
the O
exponent O
. O
in O
statistics O
, O
it O
provides O
an O
ex- O
ample O
of O
a O
parameter B
shrinkage I
method O
because O
it O
shrinks O
parameter O
values O
towards O
3.1. O
linear O
basis O
function O
models O
145 O
q O
= O
0.5 O
q O
= O
1 O
q O
= O
2 O
q O
= O
4 O
figure O
3.3 O
contours O
of O
the O
regularization B
term O
in O
( O
3.29 O
) O
for O
various O
values O
of O
the O
parameter O
q. O
zero O
. O
if O
we O
set O
the O
derivative B
of O
ln O
p O
( O
x|π O
, O
µ O
, O
σ O
) O
with O
respect O
to O
σk O
to O
zero O
, O
and O
follow O
a O
similar O
line O
of O
reasoning O
, O
making O
use O
of O
the O
result O
for O
the O
maximum B
likelihood I
solution O
for O
the O
covariance B
matrix I
of O
a O
single O
gaussian O
, O
we O
obtain O
γ O
( O
znk O
) O
( O
xn O
− O
µk O
) O
( O
xn O
− O
µk O
) O
t O
( O
9.19 O
) O
n O
( O
cid:2 O
) O
n=1 O
σk O
= O
1 O
nk O
which O
has O
the O
same O
form O
as O
the O
corresponding O
result O
for O
a O
single O
gaussian O
ﬁtted O
to O
the O
data O
set O
, O
but O
again O
with O
each O
data O
point O
weighted O
by O
the O
corresponding O
poste- O
rior O
probability B
and O
with O
the O
denominator O
given O
by O
the O
effective O
number O
of O
points O
associated O
with O
the O
corresponding O
component O
. O
we O
can O
not O
simply O
compute O
and O
then O
subtract O
off O
the O
mean B
, O
since O
we O
wish O
to O
avoid O
working O
directly O
in O
feature B
space I
, O
and O
so O
again O
, O
we O
formulate O
the O
algorithm O
purely O
in- O
! O
erms O
of O
the O
kernel B
function I
. O
the O
second O
data O
set O
is O
a O
classiﬁcation B
problem O
having O
two O
classes O
, O
with O
equal O
prior B
probabilities O
, O
and O
is O
shown O
in O
figure O
a.7 O
. O
the O
idea O
is O
to O
substitute O
for O
p O
( O
x O
) O
using O
the O
factor B
graph I
expression O
( O
8.59 O
) O
and O
then O
interchange O
summations O
and O
products O
in O
order O
to O
obtain O
an O
efﬁcient O
algorithm O
. O
η2 O
1 O
4η2 O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:26 O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
( O
cid:27 O
) O
2.4.1 O
maximum B
likelihood I
and O
sufﬁcient B
statistics I
let O
us O
now O
consider O
the O
problem O
of O
estimating O
the O
parameter O
vector O
η O
in O
the O
gen- O
eral O
exponential B
family I
distribution O
( O
2.194 O
) O
using O
the O
technique O
of O
maximum O
likeli- O
hood O
. O
next O
consider O
the O
posterior O
distribution O
q O
( O
cid:1 O
) O
( O
π O
) O
for O
the O
mixing O
coefﬁcients O
and O
show O
that O
this O
too O
becomes O
sharply O
peaked O
around O
the O
maximum B
likelihood I
solution O
. O
another O
variant O
of O
the O
hmm O
worthy O
of O
mention O
is O
the O
factorial O
hidden O
markov O
model O
( O
ghahramani O
and O
jordan O
, O
1997 O
) O
, O
in O
which O
there O
are O
multiple O
independent B
exercise O
13.18 O
634 O
13. O
sequential B
data I
figure O
13.19 O
a O
factorial O
hidden O
markov O
model O
com- O
prising O
two O
markov O
chains O
of O
latent O
vari- O
ables O
. O
although O
we O
can O
solve O
this O
equation O
for O
b O
using O
an O
arbitrarily O
chosen O
support B
vector I
xn O
, O
a O
numerically O
more O
n O
= O
1 O
, O
stable O
solution O
is O
obtained O
by O
ﬁrst O
multiplying O
through O
by O
tn O
, O
making O
use O
of O
t2 O
and O
then O
averaging O
these O
equations O
over O
all O
support O
vectors O
and O
solving O
for O
b O
to O
give O
( O
cid:23 O
) O
( O
cid:22 O
) O
( O
cid:2 O
) O
n∈s O
( O
cid:2 O
) O
m∈s O
b O
= O
1 O
ns O
tn O
− O
amtmk O
( O
xn O
, O
xm O
) O
( O
7.18 O
) O
n O
( O
cid:2 O
) O
where O
ns O
is O
the O
total O
number O
of O
support O
vectors O
. O
( O
13.41 O
) O
( O
cid:2 O
) O
zn O
13.2. O
hidden O
markov O
models O
623 O
thus O
we O
can O
evaluate O
the O
likelihood B
function I
by O
computing O
this O
sum O
, O
for O
any O
conve- O
nient O
choice O
of O
n. O
for O
instance O
, O
if O
we O
only O
want O
to O
evaluate O
the O
likelihood B
function I
, O
then O
we O
can O
do O
this O
by O
running O
the O
α B
recursion I
from O
the O
start O
to O
the O
end O
of O
the O
chain O
, O
and O
then O
use O
this O
result O
for O
n O
= O
n O
, O
making O
use O
of O
the O
fact O
that O
β O
( O
zn O
) O
is O
a O
vector O
of O
1s O
. O
a O
natural O
choice O
of O
error B
func- O
tion O
would O
be O
the O
total O
number O
of O
misclassiﬁed O
patterns O
. O
for O
a O
more O
comprehensive O
discussion O
of O
the O
calculus B
of I
variations I
, O
see O
sagan O
( O
1969 O
) O
. O
support B
vector I
machines O
( O
svms O
) O
, O
discussed O
in O
chapter O
7 O
, O
address O
this O
by O
ﬁrst O
deﬁning O
basis O
functions O
that O
are O
centred O
on O
the O
training B
data O
points O
and O
then O
selecting O
a O
subset O
of O
these O
during O
training B
. O
thus O
the O
mean B
of O
the O
predictive B
distribution I
at O
a O
point O
x O
is O
given O
by O
a O
linear O
combination O
of O
the O
training B
set I
target O
variables O
tn O
, O
so O
that O
we O
can O
write O
n O
( O
cid:2 O
) O
y O
( O
x O
, O
mn O
) O
= O
k O
( O
x O
, O
xn O
) O
tn O
where O
the O
function O
n=1 O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
βφ O
( O
x O
) O
tsn O
φ O
( O
x O
( O
cid:4 O
) O
) O
( O
3.61 O
) O
( O
3.62 O
) O
is O
known O
as O
the O
smoother B
matrix I
or O
the O
equivalent B
kernel I
. O
robust O
real-time O
face B
detection I
. O
this O
can O
be O
achieved O
through O
a O
mixture B
distribution I
of O
the O
form O
t O
( O
z O
( O
cid:4 O
) O
, O
z O
) O
= O
αkbk O
( O
z O
( O
cid:4 O
) O
, O
z O
) O
( O
11.42 O
) O
k O
( O
cid:2 O
) O
k=1 O
11.2. O
markov O
chain O
monte O
carlo O
541 O
( O
cid:5 O
) O
for O
some O
set O
of O
mixing O
coefﬁcients O
α1 O
, O
. O
this O
idea O
forms O
the O
basis O
of O
several O
practical O
latent B
variable I
models O
as O
we O
shall O
see O
shortly O
. O
because O
the O
objects O
can O
occur O
at O
different O
positions O
within O
the O
image O
and O
in O
different O
orientations O
, O
there O
are O
three O
degrees B
of I
freedom I
of O
variability O
between O
images O
, O
and O
a O
set O
of O
images O
will O
live O
on O
a O
three O
dimensional O
manifold B
embedded O
within O
the O
high-dimensional O
space O
. O
) O
, O
advances O
in O
kernel O
methods O
– O
support B
vector I
learning O
, O
pp O
. O
as O
discussed O
so O
far O
, O
both O
the O
k-nearest-neighbour O
method O
, O
and O
the O
kernel O
den- O
sity O
estimator O
, O
require O
the O
entire O
training B
data O
set O
to O
be O
stored O
, O
leading O
to O
expensive O
computation O
if O
the O
data O
set O
is O
large O
. O
, O
xn O
+l O
, O
and O
suppose O
we O
deﬁne O
a O
gaussian O
process O
prior B
over O
functions O
t O
( O
x O
) O
. O
10.34 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
this O
exercise O
we O
derive O
re-estimation O
equations O
for O
the O
variational B
parame- O
ters O
ξ O
in O
the O
bayesian O
logistic B
regression I
model O
of O
section O
4.5 O
by O
direct O
maximization O
of O
the O
lower B
bound I
given O
by O
( O
10.164 O
) O
. O
also O
show O
that O
the O
minimum O
expected O
lq O
loss O
for O
q O
→ O
0 O
is O
given O
by O
the O
conditional B
mode O
, O
i.e. O
, O
by O
the O
function O
y O
( O
x O
) O
equal O
to O
the O
value O
of O
t O
that O
maximizes O
p O
( O
t|x O
) O
for O
each O
x O
. O
in O
general O
, O
the O
projection O
onto O
one O
dimension O
leads O
to O
a O
considerable O
loss O
of O
infor- O
mation B
, O
and O
classes O
that O
are O
well O
separated O
in O
the O
original O
d-dimensional O
space O
may O
become O
strongly O
overlapping O
in O
one O
dimension O
. O
if O
it O
is O
the O
case O
that O
every O
conditional B
independence I
property O
of O
the O
distribution O
is O
reﬂected O
in O
the O
graph O
, O
and O
vice O
versa O
, O
then O
the O
graph O
is O
said O
to O
be O
a O
perfect B
map I
for O
figure O
8.34 O
venn O
diagram O
illustrating O
the O
set O
of O
all O
distributions O
p O
over O
a O
given O
set O
of O
variables O
, O
together O
with O
the O
set O
of O
distributions O
d O
that O
can O
be O
represented O
as O
a O
perfect B
map I
using O
a O
directed B
graph O
, O
and O
the O
set O
u O
that O
can O
be O
represented O
as O
a O
perfect B
map I
using O
an O
undirected B
graph I
. O
10.35 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
derive O
the O
result O
( O
10.164 O
) O
for O
the O
lower B
bound I
l O
( O
ξ O
) O
in O
the O
variational B
logistic O
regression B
model O
. O
, O
n. O
13.3 O
( O
( O
cid:12 O
) O
) O
by O
using O
d-separation B
, O
show O
that O
the O
distribution O
p O
( O
x1 O
, O
. O
hence O
a O
major O
draw- O
back O
of O
the O
importance B
sampling I
method O
is O
the O
potential O
to O
produce O
results O
that O
are O
arbitrarily O
in O
error B
and O
with O
no O
diagnostic O
indication O
. O
consider O
the O
illustration O
of O
a O
convex B
function I
f O
( O
x O
) O
shown O
in O
the O
left-hand O
plot O
in O
figure O
10.11. O
in O
this O
example O
, O
the O
function O
λx O
is O
a O
lower B
bound I
on O
f O
( O
x O
) O
but O
it O
is O
not O
the O
best O
lower B
bound I
that O
can O
be O
achieved O
by O
a O
linear O
function O
having O
slope O
λ O
, O
because O
the O
tightest O
bound O
is O
given O
by O
the O
tangent O
line O
. O
draw O
the O
corresponding O
directed B
graph O
. O
( O
11.62 O
) O
div O
v O
= O
= O
now O
consider O
the O
joint O
distribution O
over O
phase B
space I
whose O
total O
energy O
is O
the O
hamiltonian O
, O
i.e. O
, O
the O
distribution O
given O
by O
p O
( O
z O
, O
r O
) O
= O
exp O
( O
−h O
( O
z O
, O
r O
) O
) O
. O
this O
can O
be O
seen O
by O
noting O
that O
the O
ﬂow O
ﬁeld O
( O
rate O
of O
change O
of O
location O
in O
phase B
space I
) O
is O
given O
by O
( O
cid:15 O
) O
( O
cid:16 O
) O
dz O
dτ O
and O
that O
the O
divergence O
of O
this O
ﬁeld O
vanishes O
v O
= O
, O
dr O
dτ O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
i O
i O
∂ O
∂zi O
− O
∂ O
∂zi O
dzi O
dτ O
+ O
∂ O
∂ri O
dri O
dτ O
∂h O
∂ri O
+ O
∂ O
∂ri O
∂h O
∂zi O
( O
cid:13 O
) O
( O
cid:13 O
) O
( O
11.61 O
) O
= O
0 O
. O
we O
now O
ﬁnd O
an O
expression O
for O
the O
total O
number O
n O
( O
d O
, O
m O
) O
of O
independent B
parameters O
in O
all O
of O
the O
terms O
up O
to O
and O
including O
the O
m6th O
order O
. O
in O
a O
frequentist B
treatment O
, O
we O
choose O
speciﬁc O
values O
for O
the O
parameters O
by O
optimizing O
some O
criterion O
, O
such O
as O
the O
likelihood B
function I
. O
the O
lasso B
gives O
a O
sparse O
solution O
in O
which O
w O
( O
cid:1 O
) O
1 O
= O
0. O
w2 O
w2 O
w O
( O
cid:1 O
) O
w O
( O
cid:1 O
) O
w1 O
w1 O
for O
the O
remainder O
of O
this O
chapter O
we O
shall O
focus O
on O
the O
quadratic O
regularizer O
( O
3.27 O
) O
both O
for O
its O
practical O
importance O
and O
its O
analytical O
tractability O
. O
each O
data O
point O
is O
thereby O
assigned O
to O
the O
cluster O
having O
the O
closest O
mean B
. O
this O
is O
known O
as O
the O
reject B
option I
. O
many O
of O
his O
ideas O
were O
encapsulated O
in O
“ O
principles O
of O
neuro- O
dynamics O
: O
perceptrons O
and O
the O
theory B
of O
brain O
mech- O
anisms O
” O
published O
in O
1962. O
rosenblatt O
’ O
s O
work O
was O
criticized O
by O
marvin O
minksy O
, O
whose O
objections O
were O
published O
in O
the O
book O
“ O
perceptrons O
” O
, O
co-authored O
with O
seymour O
papert O
. O
recall O
from O
figure O
2.5 O
that O
for O
α0 O
< O
1 O
the O
prior B
favours O
solutions O
in O
which O
some O
of O
the O
mixing O
coefﬁcients O
are O
zero O
. O
14.5.1 O
mixtures O
of O
linear B
regression I
models O
one O
of O
the O
many O
advantages O
of O
giving O
a O
probabilistic O
interpretation O
to O
the O
lin- O
ear O
regression B
model O
is O
that O
it O
can O
then O
be O
used O
as O
a O
component O
in O
more O
complex O
probabilistic O
models O
. O
similarly O
, O
we O
introduce O
an O
independent B
gaussian-wishart O
prior B
governing O
the O
mean B
and O
precision O
of O
each O
gaussian O
component O
, O
given O
by O
p O
( O
µ O
, O
λ O
) O
= O
p O
( O
µ|λ O
) O
p O
( O
λ O
) O
k O
( O
cid:14 O
) O
n O
( O
cid:10 O
) O
( O
cid:11 O
) O
w O
( O
λk|w0 O
, O
ν0 O
) O
= O
µk|m0 O
, O
( O
β0λk O
) O
−1 O
( O
10.40 O
) O
section O
2.3.6 O
k=1 O
because O
this O
represents O
the O
conjugate B
prior I
distribution O
when O
both O
the O
mean B
and O
pre- O
cision O
are O
unknown O
. O
, O
n. O
show O
that O
the O
maximum B
likelihood I
solution O
wml O
for O
the O
parameter O
matrix O
w O
has O
the O
property O
that O
each O
column O
is O
given O
by O
an O
expression O
of O
the O
form O
( O
3.15 O
) O
, O
which O
was O
the O
solution O
for O
an O
isotropic B
noise O
distribution O
. O
8.2.1 O
three O
example O
graphs O
we O
begin O
our O
discussion O
of O
the O
conditional B
independence I
properties O
of O
directed B
graphs O
by O
considering O
three O
simple O
examples O
each O
involving O
graphs O
having O
just O
three O
nodes O
. O
an O
introduction O
to O
kernel- O
based O
learning B
algorithms O
. O
thus O
the O
m-step O
equations O
for O
component O
k O
correspond O
simply O
to O
ﬁtting O
a O
single O
logistic B
regression I
model O
to O
a O
weighted O
data O
set O
in O
which O
data O
point O
n O
carries O
a O
weight O
γnk O
. O
hierarchical B
gtm O
: O
constructing O
localized O
non-linear O
projec- O
tion O
manifolds O
in O
a O
principled O
way O
. O
( O
8.55 O
) O
we O
therefore O
ﬁrst O
evaluate O
xn−1 O
µα O
( O
x2 O
) O
= O
( O
cid:2 O
) O
x1 O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
( O
8.56 O
) O
and O
then O
apply O
( O
8.55 O
) O
repeatedly O
until O
we O
reach O
the O
desired O
node B
. O
conversely O
, O
consider O
the O
undirected B
graph I
over O
four O
variables O
shown O
in O
fig- O
ure O
8.36. O
this O
graph O
exhibits O
the O
properties O
a O
( O
cid:9 O
) O
⊥⊥ O
b O
| O
∅ O
, O
c O
⊥⊥ O
d O
| O
a O
∪ O
b O
and O
a O
⊥⊥ O
b O
| O
c O
∪ O
d. O
there O
is O
no O
directed B
graph O
over O
four O
variables O
that O
implies O
the O
same O
set O
of O
conditional B
independence I
properties O
. O
we O
might O
ask O
whether O
it O
is O
possible O
to O
deﬁne O
an O
alternative O
graphical O
semantics O
for O
probability O
distributions O
such O
that O
conditional B
independence I
is O
determined O
by O
simple O
graph O
separation O
. O
the O
approach O
also O
yields O
a O
lower B
bound I
on O
the O
likelihood B
function I
p O
( O
tn|θ O
) O
. O
one O
way O
to O
construct O
such O
a O
sequence O
of O
intermediate O
systems O
is O
to O
use O
an O
energy B
function I
containing O
a O
continuous O
parameter O
0 O
( O
cid:1 O
) O
α O
( O
cid:1 O
) O
1 O
that O
interpolates O
between O
the O
two O
distributions O
eα O
( O
z O
) O
= O
( O
1 O
− O
α O
) O
e1 O
( O
z O
) O
+ O
αem O
( O
z O
) O
. O
fisher O
’ O
s O
linear B
discriminant I
. O
note O
that O
a O
mixture B
model I
for O
an O
i.i.d O
. O
these O
relations O
are O
most O
easily O
proved O
using O
d-separation B
. O
we O
can O
correct O
σ. O
n O
( O
2.123 O
) O
( O
2.124 O
) O
n O
( O
cid:2 O
) O
( O
cid:4 O
) O
σ O
= O
clearly O
from O
( O
2.122 O
) O
and O
( O
2.124 O
) O
, O
the O
expectation B
of O
( O
cid:4 O
) O
σ O
is O
equal O
to O
σ. O
n=1 O
( O
xn O
− O
µml O
) O
( O
xn O
− O
µml O
) O
t. O
1 O
n O
− O
1 O
( O
2.125 O
) O
2.3.5 O
sequential B
estimation I
our O
discussion O
of O
the O
maximum B
likelihood I
solution O
for O
the O
parameters O
of O
a O
gaus- O
sian O
distribution O
provides O
a O
convenient O
opportunity O
to O
give O
a O
more O
general O
discussion O
of O
the O
topic O
of O
sequential B
estimation I
for O
maximum B
likelihood I
. O
it O
turns O
out O
that O
the O
two O
types O
of O
graph O
can O
express O
different O
conditional B
independence I
properties O
, O
and O
it O
is O
worth O
exploring O
this O
issue O
in O
more O
detail O
. O
as O
it O
stands O
, O
the O
kernel B
density I
estimator I
( O
2.249 O
) O
will O
suffer O
from O
one O
of O
the O
same O
problems O
that O
the O
histogram O
method O
suffered O
from O
, O
namely O
the O
presence O
of O
artiﬁcial O
discontinuities O
, O
in O
this O
case O
at O
the O
boundaries O
of O
the O
cubes O
. O
for O
a O
ﬁnite O
data O
set O
, O
the O
posterior O
mean O
for O
µ O
always O
lies O
between O
the O
prior B
mean O
and O
the O
maximum B
likelihood I
estimate O
for O
µ O
corresponding O
to O
the O
relative B
frequencies O
of O
events O
given O
by O
( O
2.7 O
) O
. O
we O
have O
already O
seen O
in O
our O
discussion O
of O
the O
kullback-leibler O
divergence O
that O
the O
convexity O
of O
the O
logarithm O
function O
played O
a O
key O
role O
in O
developing O
the O
lower B
bound I
in O
the O
global O
variational O
approach O
. O
6.21 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
gaussian O
process O
regression B
model O
in O
which O
the O
kernel B
function I
is O
deﬁned O
in O
terms O
of O
a O
ﬁxed O
set O
of O
nonlinear O
basis O
functions O
. O
markov O
chain O
monte O
carlo O
: O
stochastic B
simulation O
for O
bayesian O
inference B
. O
we O
shall O
see O
that O
there O
is O
an O
efﬁcient O
algorithm O
ﬁnding O
such O
solutions O
known O
as O
iterative B
reweighted I
least I
squares I
, O
or O
irls O
. O
( O
4.85 O
) O
similarly O
, O
for O
the O
k-class O
problem O
, O
we O
substitute O
the O
class-conditional O
density B
ex- O
pression O
into O
( O
4.63 O
) O
to O
give O
ak O
( O
x O
) O
= O
λt O
k O
x O
+ O
ln O
g O
( O
λk O
) O
+ O
ln O
p O
( O
ck O
) O
( O
4.86 O
) O
and O
so O
again O
is O
a O
linear O
function O
of O
x O
. O
taking O
the O
limit O
of O
an O
inﬁnite O
sum O
, O
we O
can O
also O
consider O
kernels O
of O
the O
form O
( O
cid:6 O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
p O
( O
x|z O
) O
p O
( O
x O
( O
cid:4 O
) O
|z O
) O
p O
( O
z O
) O
dz O
( O
6.30 O
) O
where O
z O
is O
a O
continuous O
latent B
variable I
. O
we O
need O
to O
generalize O
the O
cauchy O
slightly O
to O
ensure O
that O
it O
nowhere O
has O
a O
smaller O
value O
than O
the O
gamma B
distribution I
. O
note O
that O
from O
the O
result O
of O
exercise O
12.20 O
, O
the O
parameter O
j1 O
can O
be O
replaced O
by O
the O
sample B
mean I
x O
. O
sequential O
updating O
of O
conditional B
probabilities O
on O
directed B
graphical O
structures O
. O
note O
that O
in O
these O
recursion O
relations O
the O
observations O
enter O
through O
conditional B
distributions O
of O
the O
form O
p O
( O
xn|zn O
) O
. O
the O
conditional B
independence I
assumption O
of O
this O
model O
is O
clearly O
a O
strong O
one O
that O
may O
lead O
to O
rather O
poor O
representations O
of O
the O
class-conditional O
densities O
. O
the O
output O
at O
each O
time O
step O
is O
determined O
by O
stochas- O
tically O
choosing O
one O
of O
the O
continuous O
latent O
chains O
, O
using O
the O
state O
of O
the O
discrete O
latent B
variable I
as O
a O
switch O
, O
and O
then O
emitting O
an O
observation O
from O
the O
corresponding O
conditional B
output O
distribution O
. O
) O
, O
advances O
in O
mean B
field O
methods O
, O
pp O
. O
for O
consistency O
, O
we O
shall O
refer O
to O
the O
normal- O
gamma B
distribution I
as O
the O
gaussian-gamma O
distribution O
, O
and O
similarly O
the O
normal- O
wishart O
is O
called O
the O
gaussian-wishart O
. O
to O
do O
this O
we O
shall O
use O
a O
simple O
iterative O
technique O
called O
iterated B
conditional I
modes I
, O
or O
icm O
( O
kittler O
and O
f¨oglein O
, O
1984 O
) O
, O
which O
is O
simply O
an O
application O
of O
coordinate-wise O
gradient O
ascent O
. O
the O
joint O
distribution O
of O
all O
7 O
variables O
362 O
8. O
graphical O
models O
figure O
8.2 O
example O
of O
a O
directed B
acyclic I
graph I
describing O
the O
joint O
distribution O
over O
variables O
x1 O
, O
. O
v O
, O
( O
12.73 O
) O
( O
12,74 O
) O
( O
12.76 O
) O
588 O
12. O
continuous O
latent O
variables O
substituting O
this O
expansion O
back O
into O
the O
eigenvector O
equation O
, O
we O
obtain O
1 O
n O
n O
l O
n=l O
n O
¢ O
( O
xn O
) O
¢ O
( O
xn O
) O
t O
l O
n O
aim¢ O
( O
xm O
) O
= O
ai O
l O
m=l O
n=l O
ain¢ O
( O
xn O
) O
, O
( O
12.77 O
) O
the O
key O
step O
is O
now O
to O
express O
this O
in O
terms O
of O
the O
kernel B
function I
k O
( O
xn O
, O
x O
m O
) O
= O
¢ O
( O
xn O
) O
t¢ O
( O
xm O
) O
, O
which O
we O
do O
by O
multiplying O
both O
sides O
by O
¢ O
( O
xz O
) O
t O
to O
give O
1 O
n O
m O
n O
lk O
( O
xi'xn O
) O
l O
n=l O
m=l O
aimk O
( O
xn O
, O
xm O
) O
= O
ai O
laink O
( O
xi'xn O
) O
, O
( O
12.78 O
) O
n O
n=l O
this O
can O
be O
written O
in O
matrix O
notation O
as O
where O
ai O
is O
an O
n-dimensional O
column O
vector O
with O
elements O
ani O
for O
n O
= O
1 O
, O
... O
, O
n. O
we O
can O
find O
solutions O
for O
ai O
by O
solving O
the O
following O
eigenvalue O
problem O
( O
12.80 O
) O
( O
12.79 O
) O
exercise O
12.26 O
in O
which O
we O
have O
removed O
a O
factor O
of O
k O
from O
both O
sides O
of O
( O
12.79 O
) O
. O
first O
we O
remove O
the O
current O
estimate O
( O
cid:4 O
) O
fn O
( O
θ O
) O
from O
q O
( O
θ O
) O
by O
division O
using O
( O
10.205 O
) O
\n O
( O
θ O
) O
, O
which O
has O
mean B
and O
inverse B
variance O
given O
by O
n O
( O
m O
− O
mn O
) O
\nv O
m\n O
= O
m O
+ O
v O
−1 O
−1 O
− O
v O
−1 O
\n O
) O
−1 O
= O
v O
n O
. O
neal O
( O
1996 O
) O
has O
shown O
that O
, O
for O
a O
broad O
class O
of O
prior B
distributions O
over O
w O
, O
the O
distribution O
of O
functions O
generated O
by O
a O
neural B
network I
will O
tend O
to O
a O
gaussian O
process O
in O
the O
limit O
m O
→ O
∞ O
. O
to O
generate O
a O
particular O
data O
set O
from O
a O
spe- O
ciﬁc O
model O
, O
we O
ﬁrst O
choose O
the O
values O
of O
the O
parameters O
from O
their O
prior B
distribution O
p O
( O
w O
) O
, O
and O
then O
for O
these O
parameter O
values O
we O
sample O
the O
data O
from O
p O
( O
d|w O
) O
. O
in O
which O
( O
for O
a O
given O
ge- O
ometrical O
configuration O
of O
the O
gas O
, O
woller O
, O
and O
oil O
phases O
) O
there O
are O
only O
two O
degrees B
of I
freedom I
of O
variability O
corresponding O
to O
the O
fraction O
of O
oil O
in O
the O
pipe O
and O
the O
frac O
( O
cid:173 O
) O
tion O
of O
water O
( O
the O
fraction O
of O
gas O
ihen O
being O
determined O
) O
. O
if O
the O
posterior O
distribution O
p O
( O
α O
, O
β|t O
) O
is O
sharply O
peaked O
around O
values O
( O
cid:1 O
) O
α O
and O
( O
cid:1 O
) O
β O
, O
then O
the O
predictive B
distribution I
is O
obtained O
simply O
by O
marginalizing O
over O
w O
in O
which O
α O
and O
β O
are O
ﬁxed O
to O
the O
values O
( O
cid:1 O
) O
α O
and O
( O
cid:1 O
) O
β O
, O
so O
that O
p O
( O
t|t O
) O
( O
cid:7 O
) O
p O
( O
t|t O
, O
( O
cid:1 O
) O
α O
, O
( O
cid:1 O
) O
β O
) O
= O
p O
( O
t|w O
, O
( O
cid:1 O
) O
β O
) O
p O
( O
w|t O
, O
( O
cid:1 O
) O
α O
, O
( O
cid:1 O
) O
β O
) O
dw O
. O
another O
possibility O
is O
the O
sigmoidal O
basis B
function I
of O
the O
form O
( O
cid:18 O
) O
( O
cid:17 O
) O
x O
− O
µj O
s O
φj O
( O
x O
) O
= O
σ O
where O
σ O
( O
a O
) O
is O
the O
logistic B
sigmoid I
function O
deﬁned O
by O
σ O
( O
a O
) O
= O
1 O
1 O
+ O
exp O
( O
−a O
) O
. O
3.5.1 O
evaluation O
of O
the O
evidence B
function I
the O
marginal B
likelihood I
function O
p O
( O
t|α O
, O
β O
) O
is O
obtained O
by O
integrating O
over O
the O
weight O
parameters O
w O
, O
so O
that O
( O
cid:6 O
) O
p O
( O
t|α O
, O
β O
) O
= O
p O
( O
t|w O
, O
β O
) O
p O
( O
w|α O
) O
dw O
. O
5.1. O
feed-forward O
network O
functions O
227 O
5.1. O
feed-forward O
network O
functions O
the O
linear O
models O
for B
regression I
and O
classiﬁcation B
discussed O
in O
chapters O
3 O
and O
4 O
, O
re- O
spectively O
, O
are O
based O
on O
linear O
combinations O
of O
ﬁxed O
nonlinear O
basis O
functions O
φj O
( O
x O
) O
and O
take O
the O
form O
( O
cid:23 O
) O
( O
cid:22 O
) O
m O
( O
cid:2 O
) O
j=1 O
wjφj O
( O
x O
) O
y O
( O
x O
, O
w O
) O
= O
f O
( O
5.1 O
) O
where O
f O
( O
· O
) O
is O
a O
nonlinear O
activation B
function I
in O
the O
case O
of O
classiﬁcation O
and O
is O
the O
identity O
in O
the O
case O
of O
regression B
. O
suppose O
we O
are O
given O
a O
training B
data O
set O
{ O
φn O
, O
tn O
} O
where O
n O
= O
1 O
, O
. O
3.14 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
this O
exercise O
, O
we O
explore O
in O
more O
detail O
the O
properties O
of O
the O
equivalent B
kernel I
deﬁned O
by O
( O
3.62 O
) O
, O
where O
sn O
is O
deﬁned O
by O
( O
3.54 O
) O
. O
as O
usual O
, O
we O
consider O
a O
conjugate B
hyperprior O
over O
α O
given O
by O
a O
gamma B
distribution I
p O
( O
α O
) O
= O
gam O
( O
α|a0 O
, O
b0 O
) O
( O
10.166 O
) O
governed O
by O
the O
constants O
a0 O
and O
b0 O
. O
2.5 O
nonparametric B
methods I
. O
the O
covariance B
matrix I
c O
depends O
on O
the O
parameters O
w O
, O
, O
giving O
a O
total O
parameter O
count O
of O
dm O
+ O
1. O
however O
, O
which O
has O
size O
d O
x O
m O
, O
and O
a O
2 O
we O
have O
seen O
that O
there O
is O
some O
redundancy O
in O
this O
parameterization O
associated O
with O
rotations O
of O
the O
coordinate O
system O
in O
the O
latent O
space O
. O
( O
cid:6 O
) O
( O
cid:6 O
) O
1 O
2 O
5.17 O
( O
( O
cid:12 O
) O
) O
consider O
a O
squared O
loss B
function I
of O
the O
form O
{ O
y O
( O
x O
, O
w O
) O
− O
t O
} O
2 O
e O
= O
p O
( O
x O
, O
t O
) O
dx O
dt O
( O
5.193 O
) O
where O
y O
( O
x O
, O
w O
) O
is O
a O
parametric O
function O
such O
as O
a O
neural B
network I
. O
gibbs O
( O
1997 O
) O
proposes O
a O
method O
for O
constructing O
a O
gaussian O
distribution O
that O
is O
conjectured O
to O
be O
a O
bound O
( O
although O
no O
rigorous O
proof O
is O
given O
) O
, O
which O
may O
be O
used O
to O
apply O
local B
variational O
methods O
to O
multiclass B
problems O
. O
( O
8.60 O
) O
this O
can O
be O
expressed O
by O
the O
factor B
graph I
shown O
in O
figure O
8.40. O
note O
that O
there O
are O
two O
factors O
fa O
( O
x1 O
, O
x2 O
) O
and O
fb O
( O
x1 O
, O
x2 O
) O
that O
are O
deﬁned O
over O
the O
same O
set O
of O
variables O
. O
show O
that O
the O
regularization B
term O
ω O
can O
be O
written O
as O
a O
sum O
over O
patterns O
of O
terms O
of O
the O
form O
( O
cid:2 O
) O
( O
cid:2 O
) O
where O
g O
is O
a O
differential B
operator O
deﬁned O
by O
ωn O
= O
1 O
2 O
k O
( O
gyk O
) O
2 O
g O
≡ O
∂ O
∂xi O
τi O
i O
by O
acting O
on O
the O
forward B
propagation I
equations O
. O
, O
xd O
) O
t O
and O
‘ O
const O
’ O
denotes O
terms O
independent B
of O
x. O
we O
see O
that O
this O
is O
a O
quadratic O
function O
of O
the O
components O
of O
x O
, O
and O
hence O
the O
joint O
distribution O
p O
( O
x O
) O
is O
a O
multivariate O
gaussian O
. O
if O
the O
convergence O
criterion O
is O
not O
satisﬁed O
, O
then O
let O
θold O
← O
θnew O
( O
9.34 O
) O
and O
return O
to O
step O
2. O
the O
em O
algorithm O
can O
also O
be O
used O
to O
ﬁnd O
map O
( O
maximum B
posterior I
) O
solutions O
for O
models O
in O
which O
a O
prior B
p O
( O
θ O
) O
is O
deﬁned O
over O
the O
parameters O
. O
( O
cid:2 O
) O
( O
wi O
− O
µj O
) O
σ2 O
j O
= O
∂e O
∂wi O
+ O
λ O
γj O
( O
wi O
) O
j O
∂ O
( O
cid:4 O
) O
e O
∂wi O
the O
derivatives O
of O
the O
total O
error B
function I
with O
respect O
to O
the O
weights O
are O
then O
given O
by O
. O
in O
the O
ﬁrst O
approach O
, O
we O
recognize O
that O
the O
function O
l O
( O
ξ O
) O
is O
deﬁned O
by O
an O
integration O
over O
w O
and O
so O
we O
can O
view O
w O
as O
a O
latent B
variable I
and O
invoke O
the O
em O
algorithm O
. O
gaussians O
, O
we O
ﬁrst O
chose O
one O
of O
the O
components O
at O
random O
with O
probability B
given O
by O
the O
mixing O
coefﬁcients O
πk O
and O
then O
generate O
a O
sample O
vector O
x O
from O
the O
correspond- O
ing O
gaussian O
component O
. O
em O
can O
then O
be O
used O
to O
maximize O
the O
corresponding O
likelihood B
function I
. O
an O
example O
binary O
image O
, O
together O
with O
a O
noise O
corrupted O
image O
obtained O
by O
ﬂipping O
the O
sign O
of O
the O
pixels O
with O
probability B
10 O
% O
, O
is O
shown O
in O
figure O
8.30. O
given O
the O
noisy O
image O
, O
our O
goal O
is O
to O
recover O
the O
original O
noise-free O
image O
. O
here O
we O
make O
use O
of O
the O
analogous O
law O
for O
the O
max O
operator O
( O
8.90 O
) O
which O
holds O
if O
a O
( O
cid:2 O
) O
0 O
( O
as O
will O
always O
be O
the O
case O
for O
the O
factors O
in O
a O
graphical B
model I
) O
. O
note O
that O
the O
result O
of O
averaging O
many O
solutions O
for O
the O
complex O
model O
with O
m O
= O
25 O
is O
a O
very O
good O
ﬁt O
to O
the O
regression B
function I
, O
which O
suggests O
that O
averaging O
may O
be O
a O
beneﬁcial O
procedure O
. O
10.8 O
( O
( O
cid:12 O
) O
) O
consider O
the O
variational B
posterior O
distribution O
for O
the O
precision O
of O
a O
univariate O
gaussian O
whose O
parameters O
are O
given O
by O
( O
10.29 O
) O
and O
( O
10.30 O
) O
. O
( O
1.50 O
) O
from O
( O
1.49 O
) O
and O
( O
1.50 O
) O
, O
it O
follows O
that O
the O
variance B
of O
x O
is O
given O
by O
var O
[ O
x O
] O
= O
e O
[ O
x2 O
] O
− O
e O
[ O
x O
] O
2 O
= O
σ2 O
( O
1.51 O
) O
( O
cid:12 O
) O
( O
cid:13 O
) O
and O
hence O
σ2 O
is O
referred O
to O
as O
the O
variance B
parameter O
. O
these O
can O
be O
addressed O
through O
a O
closely O
related O
algorithm O
called O
max-sum O
, O
which O
can O
be O
viewed O
as O
an O
application O
of O
dynamic B
programming I
in O
the O
context O
of O
graphical O
models O
( O
cormen O
et O
al. O
, O
2001 O
) O
. O
5.5. O
regularization B
in O
neural O
networks O
263 O
figure O
5.14 O
illustration O
of O
the O
synthetic O
warping O
of O
a O
handwritten B
digit I
. O
this O
example O
also O
provides O
a O
geometrical O
insight O
into O
the O
origin O
of O
sparsity B
in O
the O
svm O
. O
now O
let O
us O
consider O
in O
more O
detail O
how O
the O
concept O
of O
variational B
optimization O
can O
be O
applied O
to O
the O
inference B
problem O
. O
mt O
( O
10.104 O
) O
comparison O
with O
( O
9.63 O
) O
shows O
that O
in O
the O
case O
of O
this O
particularly O
simple O
model O
, O
the O
variational B
approach O
gives O
precisely O
the O
same O
expression O
as O
that O
obtained O
by O
maximizing O
the O
evidence B
function I
using O
em O
except O
that O
the O
point O
estimate O
for O
α O
is O
replaced O
by O
its O
expected O
value O
. O
, O
n O
, O
the O
likelihood B
function I
can O
be O
written O
p O
( O
t|w O
) O
= O
n O
{ O
1 O
− O
yn O
} O
1−tn O
ytn O
( O
4.89 O
) O
where O
t O
= O
( O
t1 O
, O
. O
recall O
that O
a O
general O
gaussian O
distribution O
has O
d O
( O
d O
+ O
1 O
) O
/2 O
independent B
parameters O
in O
its O
covariance B
matrix I
( O
plus O
another O
d O
parameters O
in O
its O
mean B
) O
. O
we O
can O
illustrate O
the O
evidence O
framework O
for O
setting O
hyperparameters O
using O
the O
sinusoidal O
synthetic O
data O
set O
from O
section O
1.1 O
, O
together O
with O
the O
gaussian O
basis O
func- O
tion O
model O
comprising O
9 O
basis O
functions O
, O
so O
that O
the O
total O
number O
of O
parameters O
in O
the O
model O
is O
given O
by O
m O
= O
10 O
including O
the O
bias B
. O
542 O
11. O
sampling B
methods I
figure O
11.10 O
schematic O
illustration O
of O
the O
use O
of O
an O
isotropic B
gaussian O
proposal B
distribution I
( O
blue O
circle O
) O
to O
sample O
from O
a O
correlated O
multivariate O
gaussian O
distribution O
( O
red O
ellipse O
) O
having O
very O
different O
stan- O
dard O
deviations O
in O
different O
directions O
, O
using O
the O
metropolis-hastings O
algorithm O
. O
thus O
in O
general O
to O
convert O
a O
directed B
graph O
into O
an O
undirected B
graph I
, O
we O
ﬁrst O
add O
additional O
undirected B
links O
between O
all O
pairs O
of O
parents O
for O
each O
node B
in O
the O
graph O
and O
392 O
8. O
graphical O
models O
section O
8.4 O
section O
8.2 O
then O
drop O
the O
arrows O
on O
the O
original O
links O
to O
give O
the O
moral O
graph O
. O
4.3.2 O
logistic B
regression I
we O
begin O
our O
treatment O
of O
generalized O
linear O
models O
by O
considering O
the O
problem O
of O
two-class O
classiﬁcation B
. O
thus O
, O
for O
a O
homogeneous B
markov O
chain O
with O
transition O
probabilities O
t O
( O
z O
( O
cid:4 O
) O
, O
z O
) O
, O
the O
distribution O
p O
( O
cid:1 O
) O
( O
z O
) O
is O
invariant O
if O
, O
z O
) O
p O
( O
cid:1 O
) O
( O
z O
( O
cid:4 O
) O
) O
. O
suppose O
we O
have O
a O
set O
of O
n O
observations O
of O
x O
such O
that O
ni O
of O
these O
observations O
fall O
in O
region O
i. O
using O
a O
lagrange O
multiplier O
to O
enforce O
the O
normalization O
constraint O
on O
the O
density B
, O
derive O
an O
expression O
for O
the O
maximum B
likelihood I
estimator O
for O
the O
{ O
hi O
} O
. O
finally O
, O
predictions O
are O
expressed O
as O
linear O
combinations O
of O
kernel O
functions O
that O
are O
centred O
on O
training B
data O
points O
and O
that O
are O
required O
to O
be O
positive B
deﬁnite I
. O
4.3. O
probabilistic O
discriminative O
models O
207 O
4.3.3 O
iterative B
reweighted I
least I
squares I
in O
the O
case O
of O
the O
linear B
regression I
models O
discussed O
in O
chapter O
3 O
, O
the O
maxi- O
mum O
likelihood O
solution O
, O
on O
the O
assumption O
of O
a O
gaussian O
noise O
model O
, O
leads O
to O
a O
closed-form O
solution O
. O
we O
ﬁrst O
remove O
this O
factor O
from O
the O
approximating O
distribution O
to O
give O
\b O
( O
x O
) O
= O
( O
cid:4 O
) O
fa1 O
( O
x1 O
) O
( O
cid:4 O
) O
fa2 O
( O
x2 O
) O
( O
cid:4 O
) O
fc2 O
( O
x2 O
) O
( O
cid:4 O
) O
fc4 O
( O
x4 O
) O
q O
x2 O
( O
cid:2 O
) O
( O
cid:2 O
) O
x3 O
x2 O
( O
cid:4 O
) O
fb2 O
( O
x2 O
) O
∝ O
( O
cid:4 O
) O
fb3 O
( O
x3 O
) O
∝ O
( O
10.228 O
) O
( O
10.229 O
) O
and O
we O
then O
multiply O
this O
by O
the O
exact O
factor O
fb O
( O
x2 O
, O
x3 O
) O
to O
give O
the O
result O
, O
as O
noted O
above O
, O
is O
that O
qnew O
( O
z O
) O
comprises O
the O
product O
of O
factors O
, O
one O
for O
each O
variable O
xi O
, O
in O
which O
each O
factor O
is O
given O
by O
the O
corresponding O
marginal B
of O
\b O
( O
x O
) O
fb O
( O
x2 O
, O
x3 O
) O
= O
( O
cid:4 O
) O
fa1 O
( O
x1 O
) O
( O
cid:4 O
) O
fa2 O
( O
x2 O
) O
( O
cid:4 O
) O
fc2 O
( O
x2 O
) O
( O
cid:4 O
) O
fc4 O
( O
x4 O
) O
fb O
( O
x2 O
, O
x3 O
) O
. O
sub O
( O
cid:173 O
) O
stituting O
for O
xn O
, O
setting O
the O
derivative B
with O
respect O
to O
znj O
to O
zero O
, O
and O
making O
use O
of O
the O
orthonormality O
conditions O
, O
we O
obtain O
where O
j O
= O
1 O
, O
... O
, O
m. O
similarly O
, O
setting O
the O
derivative B
of O
j O
with O
respect O
to O
bi O
to O
zero O
, O
and O
again O
making O
use O
of O
the O
orthonormality O
relations O
, O
gives O
( O
12.12 O
) O
( O
12.13 O
) O
where O
j O
= O
m O
+1 O
, O
... O
, O
d. O
if O
we O
substitute O
for O
zni O
and O
bi O
, O
and O
make O
use O
of O
the O
general O
expansion O
( O
12.9 O
) O
, O
we O
obtain O
b O
j O
= O
x O
uj O
-t O
x O
n O
- O
x O
n O
= O
l O
{ O
( O
x O
n O
- O
x O
) O
tud O
ui O
d O
i=m+l O
( O
12.14 O
) O
from O
which O
we O
see O
that O
the O
displacement O
vector O
from O
x O
n O
to O
xn O
lies O
in O
the O
space O
orthogonal O
to O
the O
principal B
subspace I
, O
because O
it O
is O
a O
linear O
combination O
of O
{ O
ud O
for O
i O
= O
m O
+ O
1 O
, O
... O
, O
d O
, O
as O
illustrated O
in O
figure O
12.2. O
this O
is O
to O
be O
expected O
because O
the O
projected O
points O
xn O
must O
lie O
within O
the O
principal B
subspace I
, O
but O
we O
can O
move O
them O
freely O
within O
that O
subspace O
, O
and O
so O
the O
minimum O
error O
is O
given O
by O
the O
orthogonal O
projection O
. O
plot O
( O
b O
) O
shows O
the O
result O
of O
the O
initial O
e O
step O
, O
in O
which O
each O
data O
point O
is O
depicted O
using O
a O
proportion O
of O
blue O
ink O
equal O
to O
the O
posterior B
probability I
of O
having O
been O
generated O
from O
the O
blue O
com- O
ponent O
, O
and O
a O
corresponding O
proportion O
of O
red O
ink O
given O
by O
the O
posterior B
probability I
of O
having O
been O
generated O
by O
the O
red O
component O
. O
it O
is O
straightforward O
to O
show O
that O
( O
cid:15 O
) O
( O
cid:16 O
) O
e O
[ O
µml O
] O
= O
µ O
e O
[ O
σ2 O
ml O
] O
= O
n O
− O
1 O
n O
σ2 O
( O
1.57 O
) O
( O
1.58 O
) O
so O
that O
on O
average O
the O
maximum B
likelihood I
estimate O
will O
obtain O
the O
correct O
mean B
but O
will O
underestimate O
the O
true O
variance B
by O
a O
factor O
( O
n O
− O
1 O
) O
/n O
. O
( O
8.29 O
) O
now O
suppose O
we O
condition O
on O
c O
, O
as O
indicated O
in O
figure O
8.20. O
the O
conditional B
distri- O
bution O
of O
a O
and O
b O
is O
then O
given O
by O
p O
( O
a O
, O
b|c O
) O
= O
p O
( O
a O
, O
b O
, O
c O
) O
p O
( O
c O
) O
= O
p O
( O
a O
) O
p O
( O
b O
) O
p O
( O
c|a O
, O
b O
) O
p O
( O
c O
) O
exercise O
8.10 O
which O
in O
general O
does O
not O
factorize O
into O
the O
product O
p O
( O
a O
) O
p O
( O
b O
) O
, O
and O
so O
a O
( O
cid:9 O
) O
⊥⊥ O
b O
| O
c. O
thus O
our O
third O
example O
has O
the O
opposite O
behaviour O
from O
the O
ﬁrst O
two O
. O
thus O
the O
model O
is O
speciﬁed O
in O
terms O
of O
a O
joint O
distribution O
( O
14.2 O
) O
and O
the O
corresponding O
density B
over O
the O
observed B
variable I
x O
is O
obtained O
by O
marginal- O
izing O
over O
the O
latent B
variable I
p O
( O
x O
, O
z O
) O
p O
( O
x O
) O
= O
p O
( O
x O
, O
z O
) O
. O
( O
5.20 O
) O
5.2. O
network O
training B
235 O
if O
we O
consider O
a O
training B
set I
of O
independent B
observations O
, O
then O
the O
error B
function I
, O
which O
is O
given O
by O
the O
negative O
log O
likelihood O
, O
is O
then O
a O
cross-entropy B
error I
function I
of O
the O
form O
{ O
tn O
ln O
yn O
+ O
( O
1 O
− O
tn O
) O
ln O
( O
1 O
− O
yn O
) O
} O
( O
5.21 O
) O
e O
( O
w O
) O
= O
− O
n O
( O
cid:2 O
) O
exercise O
5.4 O
n=1 O
where O
yn O
denotes O
y O
( O
xn O
, O
w O
) O
. O
3.4. O
bayesian O
model B
comparison I
in O
chapter O
1 O
, O
we O
highlighted O
the O
problem O
of O
over-ﬁtting B
as O
well O
as O
the O
use O
of O
cross- O
validation O
as O
a O
technique O
for O
setting O
the O
values O
of O
regularization B
parameters O
or O
for O
choosing O
between O
alternative O
models O
. O
we O
saw O
that O
such O
models O
have O
useful O
analytical O
and O
computational O
properties O
but O
that O
their O
practical O
applicability O
was O
limited O
by O
the O
curse B
of I
dimensionality I
. O
indeed O
, O
these O
two O
approaches O
can O
be O
combined O
and O
further O
extended B
to O
derive O
a O
very O
rich O
set O
of O
hierarchical B
models O
that O
can O
be O
adapted O
to O
a O
broad O
range O
of O
prac- O
tical O
applications O
. O
cancer O
normal O
1.5. O
decision B
theory I
( O
cid:15 O
) O
cancer O
normal O
( O
cid:16 O
) O
0 O
1 O
1000 O
0 O
41 O
1.5.2 O
minimizing O
the O
expected O
loss O
for O
many O
applications O
, O
our O
objective O
will O
be O
more O
complex O
than O
simply O
mini- O
mizing O
the O
number O
of O
misclassiﬁcations O
. O
14 O
combining B
models I
14.1 O
bayesian O
model B
averaging I
. O
2.3.1 O
conditional B
gaussian O
distributions O
an O
important O
property O
of O
the O
multivariate O
gaussian O
distribution O
is O
that O
if O
two O
sets O
of O
variables O
are O
jointly O
gaussian O
, O
then O
the O
conditional B
distribution O
of O
one O
set O
conditioned O
on O
the O
other O
is O
again O
gaussian O
. O
in O
the O
machine O
learning O
literature O
, O
the O
negative O
log O
of O
the O
likelihood B
function I
is O
called O
an O
error B
function I
. O
4.1.6 O
4.1.7 O
the O
perceptron B
algorithm O
. O
they O
can O
be O
used O
, O
for O
example O
, O
in O
real-time O
learning B
scenarios O
where O
a O
steady O
stream O
of O
data O
is O
arriving O
, O
and O
predictions O
must O
be O
made O
before O
all O
of O
the O
data O
is O
seen O
. O
satisﬁes O
the O
conditional B
independence I
properties O
p O
( O
xn|x1 O
, O
. O
we O
see O
from O
figure O
7.10 O
that O
, O
if O
there O
is O
a O
poor O
alignment O
between O
the O
direction O
of O
ϕ O
and O
that O
of O
the O
training B
data O
vector O
t O
, O
then O
the O
corresponding O
hyperparameter B
α O
will O
be O
driven O
to O
∞ O
, O
and O
the O
basis O
vector O
will O
be O
pruned O
from O
the O
model O
. O
( O
6.41 O
) O
n=1 O
we O
see O
that O
there O
is O
one O
basis B
function I
centred O
on O
every O
data O
point O
. O
inferring O
parameters O
and O
struc- O
ture O
of O
latent B
variable I
models O
by O
variational B
bayes O
. O
if O
we O
now O
specialize O
to O
the O
case O
of O
a O
model O
in O
which O
all O
of O
the O
conditional B
dis- O
tributions O
have O
a O
conjugate-exponential O
structure O
, O
then O
the O
variational B
update O
proce- O
dure O
can O
be O
cast O
in O
terms O
of O
a O
local B
message O
passing O
algorithm O
( O
winn O
and O
bishop O
, O
2005 O
) O
. O
write O
down O
the O
em O
algorithm O
for O
determining O
the O
parameters O
of O
this O
model O
through O
maximum B
likelihood I
. O
x2 O
θ3 O
θ2 O
b O
a O
e O
c O
d O
θ1 O
θ4 O
x1 O
664 O
14. O
combining B
models I
figure O
14.6 O
binary O
tree O
corresponding O
to O
the O
par- O
input O
space O
shown O
in O
fig- O
titioning O
of O
ure O
14.5. O
x1 O
> O
θ1 O
x2 O
( O
cid:1 O
) O
θ2 O
x2 O
> O
θ3 O
x1 O
( O
cid:1 O
) O
θ4 O
a O
b O
c O
d O
e O
divides O
the O
whole O
of O
the O
input O
space O
into O
two O
regions O
according O
to O
whether O
x1 O
( O
cid:1 O
) O
θ1 O
or O
x1 O
> O
θ1 O
where O
θ1 O
is O
a O
parameter O
of O
the O
model O
. O
11.8 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
determine O
expressions O
for O
the O
coefﬁcients O
ki O
in O
the O
envelope O
distribution O
( O
11.17 O
) O
for O
adaptive O
rejection B
sampling I
using O
the O
requirements O
of O
continuity O
and O
nor- O
malization O
. O
the O
intuition O
behind O
this O
result O
is O
that O
as O
σ2 O
is O
reduced O
, O
the O
hyperplane O
is O
increasingly O
dominated O
by O
nearby O
data O
points O
relative B
to O
more O
distant O
ones O
. O
, O
m. O
we O
see O
that O
the O
variance B
, O
as O
well O
as O
the O
mean B
, O
of O
the O
predictive B
distribution I
in O
( O
1.69 O
) O
is O
dependent O
on O
x. O
the O
ﬁrst O
term O
in O
( O
1.71 O
) O
represents O
the O
uncertainty O
in O
the O
predicted O
value O
of O
t O
due O
to O
the O
noise O
on O
the O
target O
variables O
and O
was O
expressed O
already O
−1 O
in O
the O
maximum B
likelihood I
predictive O
distribution O
( O
1.64 O
) O
through O
β O
ml O
. O
the O
curse B
of I
dimensionality I
. O
if O
we O
consider O
the O
limit O
n O
→ O
∞ O
, O
then O
the O
right-hand O
side O
of O
( O
2.228 O
) O
becomes O
e O
[ O
u O
( O
x O
) O
] O
, O
and O
so O
by O
comparing O
with O
( O
2.226 O
) O
we O
see O
that O
in O
this O
limit O
ηml O
will O
equal O
the O
true O
value O
η. O
in O
fact O
, O
this O
sufﬁciency O
property O
holds O
also O
for O
bayesian O
inference B
, O
although O
we O
shall O
defer O
discussion O
of O
this O
until O
chapter O
8 O
when O
we O
have O
equipped O
ourselves O
with O
the O
tools O
of O
graphical O
models O
and O
can O
thereby O
gain O
a O
deeper O
insight O
into O
these O
important O
concepts O
. O
the O
kl O
divergence O
can O
then O
be O
written O
in O
the O
form O
kl O
( O
p O
( O
cid:5 O
) O
q O
) O
= O
− O
p O
( O
z O
) O
ln O
qi O
( O
zi O
) O
dz O
+ O
const O
( O
10.16 O
) O
section O
10.7 O
exercise O
10.3 O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:31 O
) O
m O
( O
cid:2 O
) O
i=1 O
( O
cid:14 O
) O
i O
( O
cid:9 O
) O
=j O
where O
the O
constant O
term O
is O
simply O
the O
entropy B
of O
p O
( O
z O
) O
and O
so O
does O
not O
depend O
on O
q O
( O
z O
) O
. O
furthermore O
, O
the O
precisions O
are O
additive O
, O
so O
that O
the O
precision O
of O
the O
posterior O
is O
given O
by O
the O
precision O
of O
the O
prior B
plus O
one O
contribution O
of O
the O
data O
precision O
from O
each O
of O
the O
observed O
data O
points O
. O
given O
valid O
kernels O
k1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
and O
k2 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
, O
the O
following O
new O
kernels O
will O
also O
be O
valid O
: O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
ck1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
f O
( O
x O
) O
k1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
f O
( O
x O
( O
cid:4 O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
q O
( O
k1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
exp O
( O
k1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
k1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
+ O
k2 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
k1 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
k2 O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
k3 O
( O
φ O
( O
x O
) O
, O
φ O
( O
x O
( O
cid:4 O
) O
) O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
xtax O
( O
cid:4 O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
ka O
( O
xa O
, O
x O
( O
cid:4 O
) O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
ka O
( O
xa O
, O
x O
( O
cid:4 O
) O
( O
6.13 O
) O
( O
6.14 O
) O
( O
6.15 O
) O
( O
6.16 O
) O
( O
6.17 O
) O
( O
6.18 O
) O
( O
6.19 O
) O
( O
6.20 O
) O
( O
6.21 O
) O
( O
6.22 O
) O
where O
c O
> O
0 O
is O
a O
constant O
, O
f O
( O
· O
) O
is O
any O
function O
, O
q O
( O
· O
) O
is O
a O
polynomial O
with O
nonneg- O
m O
, O
k3 O
( O
· O
, O
· O
) O
is O
a O
valid O
kernel O
in O
ative O
coefﬁcients O
, O
φ O
( O
x O
) O
is O
a O
function O
from O
x O
to O
r O
m O
, O
a O
is O
a O
symmetric O
positive B
semideﬁnite I
matrix I
, O
xa O
and O
xb O
are O
variables O
( O
not O
r O
necessarily O
disjoint O
) O
with O
x O
= O
( O
xa O
, O
xb O
) O
, O
and O
ka O
and O
kb O
are O
valid O
kernel O
functions O
over O
their O
respective O
spaces O
. O
in O
a O
fully O
bayesian O
treatment O
, O
we O
need O
to O
evaluate O
marginals O
over O
θ O
weighted O
by O
the O
product O
of O
the O
prior B
p O
( O
θ O
) O
and O
the O
likelihood O
func- O
tion O
p O
( O
t|θ O
) O
. O
from O
the O
form O
of O
the O
equivalent B
kernel I
, O
we O
see O
that O
the O
predictive O
mean O
at O
nearby O
points O
will O
be O
highly O
correlated O
, O
whereas O
for O
more O
distant O
pairs O
of O
points O
the O
correlation O
will O
be O
smaller O
. O
other O
examples O
of O
kernel B
substitution I
include O
nearest-neighbour O
classiﬁers O
and O
the O
kernel O
fisher O
discriminant O
( O
mika O
et O
al. O
, O
1999 O
; O
roth O
and O
steinhage O
, O
2000 O
; O
baudat O
and O
anouar O
, O
2000 O
) O
. O
loss O
of O
ge O
'' O
'' O
rajity O
in O
assuming O
a O
zero O
mean B
. O
3.2 O
3.3 O
bayesian O
linear B
regression I
. O
remarkably O
, O
this O
greater O
sparsity B
is O
achieved O
with O
little O
or O
no O
reduction O
in O
generalization B
error O
compared O
with O
the O
corresponding O
svm O
. O
, O
n O
, O
and O
we O
set O
( O
cid:4 O
) O
f0 O
( O
θ O
) O
equal O
to O
the O
prior B
p O
( O
θ O
) O
. O
at O
this O
point O
, O
it O
is O
worth O
pausing O
to O
summarize O
the O
particular O
version O
of O
the O
sum- O
product O
algorithm O
obtained O
so O
far O
for O
evaluating O
the O
marginal B
p O
( O
x O
) O
. O
( O
8.18 O
) O
section O
2.3 O
exercise O
8.7 O
372 O
8. O
graphical O
models O
we O
can O
readily O
extend O
the O
linear-gaussian O
graphical B
model I
to O
the O
case O
in O
which O
the O
nodes O
of O
the O
graph O
represent O
multivariate O
gaussian O
variables O
. O
note O
that O
any O
variable O
node B
that O
has O
only O
two O
neighbours O
performs O
no O
computation O
but O
simply O
passes O
messages O
through O
un- O
changed O
. O
in O
each O
case O
, O
the O
resulting O
numerical O
quantities O
behave O
pre- O
cisely O
according O
to O
the O
rules O
of O
probability B
. O
to O
model O
this O
we O
can O
use O
exercises O
651 O
a O
linear B
dynamical I
system I
governed O
by O
( O
13.75 O
) O
and O
( O
13.76 O
) O
, O
with O
latent O
variables O
{ O
z1 O
, O
. O
the O
expectation B
of O
a O
function O
f O
( O
x O
, O
y O
) O
with O
respect O
to O
a O
random O
variable O
x O
is O
de- O
noted O
by O
ex O
[ O
f O
( O
x O
, O
y O
) O
] O
. O
we O
turn O
now O
to O
an O
exploration B
of O
some O
particular O
examples O
of O
probability B
distributions O
and O
their O
properties O
. O
( O
2.300 O
) O
2.59 O
( O
( O
cid:12 O
) O
) O
by O
changing O
variables O
using O
y O
= O
x/σ O
, O
show O
that O
the O
density B
( O
2.236 O
) O
will O
be O
correctly O
normalized O
, O
provided O
f O
( O
x O
) O
is O
correctly O
normalized O
. O
8.4.2 O
trees O
we O
have O
seen O
that O
exact O
inference O
on O
a O
graph O
comprising O
a O
chain O
of O
nodes O
can O
be O
performed O
efﬁciently O
in O
time O
that O
is O
linear O
in O
the O
number O
of O
nodes O
, O
using O
an O
algorithm O
exercise O
8.15 O
chapter O
9 O
8.4. O
inference B
in O
graphical O
models O
399 O
figure O
8.39 O
examples O
tree- O
structured O
graphs O
, O
showing O
( O
a O
) O
an O
undirected B
tree O
, O
( O
b O
) O
a O
directed B
tree O
, O
and O
( O
c O
) O
a O
directed B
polytree O
. O
n=1 O
w O
= O
0 O
= O
( O
7.8 O
) O
( O
7.9 O
) O
7.1. O
maximum B
margin I
classiﬁers O
329 O
eliminating O
w O
and O
b O
from O
l O
( O
w O
, O
b O
, O
a O
) O
using O
these O
conditions O
then O
gives O
the O
dual B
representation I
of O
the O
maximum B
margin I
problem O
in O
which O
we O
maximize O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
an O
− O
1 O
2 O
n O
( O
cid:2 O
) O
( O
cid:4 O
) O
l O
( O
a O
) O
= O
n O
( O
cid:2 O
) O
n=1 O
with O
respect O
to O
a O
subject O
to O
the O
constraints O
n=1 O
m=1 O
anamtntmk O
( O
xn O
, O
xm O
) O
( O
7.10 O
) O
an O
( O
cid:2 O
) O
0 O
, O
n O
= O
1 O
, O
. O
many O
algorithms O
make O
use O
of O
gradient O
information O
and O
therefore O
require O
that O
, O
after O
each O
update O
, O
the O
value O
of O
∇e O
( O
w O
) O
is O
evaluated O
at O
the O
new O
weight B
vector I
w O
( O
τ O
+1 O
) O
. O
, O
yn O
) O
t O
is O
given O
by O
an O
isotropic B
gaussian O
of O
the O
form O
p O
( O
t|y O
) O
= O
n O
( O
t|y O
, O
β O
−1in O
) O
( O
6.59 O
) O
where O
in O
denotes O
the O
n O
× O
n O
unit O
matrix O
. O
p O
( O
w O
, O
β O
) O
= O
n O
( O
w|m0 O
, O
β O
( O
3.112 O
) O
176 O
3. O
linear O
models O
for B
regression I
show O
that O
the O
corresponding O
posterior O
distribution O
takes O
the O
same O
functional B
form O
, O
so O
that O
p O
( O
w O
, O
β|t O
) O
= O
n O
( O
w|mn O
, O
β O
−1sn O
) O
gam O
( O
β|an O
, O
bn O
) O
( O
3.113 O
) O
and O
ﬁnd O
expressions O
for O
the O
posterior O
parameters O
mn O
, O
sn O
, O
an O
, O
and O
bn O
. O
7.2.3 O
rvm O
for O
classiﬁcation O
we O
can O
extend O
the O
relevance B
vector I
machine I
framework O
to O
classiﬁcation B
prob- O
lems O
by O
applying O
the O
ard O
prior B
over O
weights O
to O
a O
probabilistic O
linear O
classiﬁcation B
model O
of O
the O
kind O
studied O
in O
chapter O
4. O
to O
start O
with O
, O
we O
consider O
two-class O
prob- O
lems O
with O
a O
binary O
target O
variable O
t O
∈ O
{ O
0 O
, O
1 O
} O
. O
6 O
kernel O
methods O
in O
chapters O
3 O
and O
4 O
, O
we O
considered O
linear O
parametric O
models O
for B
regression I
and O
classiﬁcation B
in O
which O
the O
form O
of O
the O
mapping O
y O
( O
x O
, O
w O
) O
from O
input O
x O
to O
output O
y O
is O
governed O
by O
a O
vector O
w O
of O
adaptive O
parameters O
. O
any O
subsequent O
regression B
or O
classi- O
ﬁcation O
system O
that O
uses O
such O
features O
as O
inputs O
will O
necessarily O
also O
respect O
these O
invariances O
. O
this O
family O
of O
densities O
exhibits O
translation B
invariance I
because O
if O
we O
shift O
x O
by O
a O
constant O
to O
give O
( O
cid:1 O
) O
x O
= O
x O
+ O
c O
, O
where O
we O
have O
deﬁned O
( O
cid:1 O
) O
µ O
= O
µ O
+ O
c. O
thus O
the O
density B
takes O
the O
same O
form O
in O
the O
p O
( O
( O
cid:1 O
) O
x| O
( O
cid:1 O
) O
µ O
) O
= O
f O
( O
( O
cid:1 O
) O
x O
− O
( O
cid:1 O
) O
µ O
) O
( O
2.233 O
) O
then O
new O
variable O
as O
in O
the O
original O
one O
, O
and O
so O
the O
density B
is O
independent B
of O
the O
choice O
of O
origin O
. O
this O
is O
illustrated O
for O
the O
chain O
model O
in O
figure O
8.11. O
the O
corresponding O
model O
in O
which O
we O
tie O
the O
parame- O
ters O
governing O
the O
conditional B
distributions O
p O
( O
xi|xi−1 O
) O
, O
for O
i O
= O
2 O
, O
. O
setting O
the O
derivative B
with O
respect O
to O
θ0 O
equal O
to O
zero O
gives O
n O
( O
cid:2 O
) O
n=1 O
sin O
( O
θn O
− O
θ0 O
) O
= O
0. O
to O
solve O
for O
θ0 O
, O
we O
make O
use O
of O
the O
trigonometric O
identity O
sin O
( O
a O
− O
b O
) O
= O
cos O
b O
sin O
a O
− O
cos O
a O
sin O
b O
( O
cid:13 O
) O
( O
cid:12 O
) O
( O
cid:5 O
) O
( O
cid:5 O
) O
n O
sin O
θn O
n O
cos O
θn O
exercise O
2.53 O
from O
which O
we O
obtain O
0 O
= O
tan−1 O
θml O
( O
2.182 O
) O
( O
2.183 O
) O
( O
2.184 O
) O
( O
cid:4 O
) O
0 O
( O
m O
) O
= O
( O
2.186 O
) O
which O
we O
recognize O
as O
the O
result O
( O
2.169 O
) O
obtained O
earlier O
for O
the O
mean B
of O
the O
obser- O
vations O
viewed O
in O
a O
two-dimensional O
cartesian O
space O
. O
the O
principal O
disadvantage O
of O
the O
relevance B
vector I
machine I
is O
the O
relatively O
long O
training B
times O
compared O
with O
the O
svm O
. O
using O
the O
recursion O
relations O
( O
8.15 O
) O
and O
( O
8.16 O
) O
, O
we O
see O
that O
the O
mean B
and O
covariance B
of O
the O
joint O
distribution O
are O
given O
by O
( O
cid:22 O
) O
µ O
= O
( O
b1 O
, O
b2 O
+ O
w21b1 O
, O
b3 O
+ O
w32b2 O
+ O
w32w21b1 O
) O
t O
σ O
= O
v1 O
w21v1 O
v2 O
+ O
w2 O
w21v1 O
21v1 O
w32w21v1 O
w32 O
( O
v2 O
+ O
w2 O
21v1 O
) O
v3 O
+ O
w2 O
w32w21v1 O
w32 O
( O
v2 O
+ O
w2 O
21v1 O
) O
32 O
( O
v2 O
+ O
w2 O
21v1 O
) O
( O
cid:23 O
) O
( O
8.17 O
) O
. O
this O
can O
be O
seen O
by O
noting O
that O
the O
likelihood B
function I
is O
bounded O
above O
because O
0 O
( O
cid:1 O
) O
p O
( O
xn|µk O
) O
( O
cid:1 O
) O
1. O
there O
exist O
singularities B
at O
which O
the O
likelihood B
function I
goes O
to O
zero O
, O
but O
these O
will O
not O
be O
found O
by O
em O
provided O
it O
is O
not O
initialized O
to O
a O
pathological O
starting O
point O
, O
because O
the O
em O
algorithm O
always O
increases O
the O
value O
of O
the O
likelihood B
function I
, O
until O
a O
local B
maximum O
is O
found O
. O
now O
consider O
a O
fully B
connected I
graph O
in O
which O
each O
node B
has O
all O
lower O
num- O
bered O
nodes O
as O
parents O
. O
for O
many O
of O
the O
models O
discussed O
in O
this O
book O
, O
the O
complete-data O
likelihood O
is O
drawn O
from O
the O
exponential B
family I
. O
it O
should O
be O
emphasized O
that O
the O
factorization B
in O
( O
c O
) O
does O
not O
correspond O
to O
any O
conditional B
independence I
properties O
. O
if O
we O
now O
substitute O
the O
deﬁnition O
of O
δ O
given O
by O
( O
5.51 O
) O
into O
( O
5.55 O
) O
, O
and O
make O
use O
of O
( O
5.48 O
) O
and O
( O
5.49 O
) O
, O
we O
obtain O
the O
following O
backpropagation B
formula O
δj O
= O
h O
( O
cid:4 O
) O
( O
aj O
) O
wkjδk O
k O
( O
5.56 O
) O
which O
tells O
us O
that O
the O
value O
of O
δ O
for O
a O
particular O
hidden B
unit I
can O
be O
obtained O
by O
propagating O
the O
δ O
’ O
s O
backwards O
from O
units O
higher O
up O
in O
the O
network O
, O
as O
illustrated O
in O
figure O
5.7. O
note O
that O
the O
summation O
in O
( O
5.56 O
) O
is O
taken O
over O
the O
ﬁrst O
index O
on O
wkj O
( O
corresponding O
to O
backward O
propagation O
of O
information O
through O
the O
network O
) O
, O
whereas O
in O
the O
forward B
propagation I
equation O
( O
5.10 O
) O
it O
is O
taken O
over O
the O
second O
index O
. O
as O
in O
section O
5.5.4 O
, O
we O
shall O
consider O
a O
transformation O
governed O
by O
a O
single O
parameter O
ξ O
and O
described O
by O
the O
function O
s O
( O
x O
, O
ξ O
) O
, O
with O
s O
( O
x O
, O
0 O
) O
= O
x. O
we O
shall O
also O
consider O
a O
sum-of-squares B
error I
function O
. O
w O
( O
τ O
+1 O
) O
= O
w O
( O
τ O
) O
− O
η∇e O
( O
w O
( O
τ O
) O
) O
5.2.4 O
gradient B
descent I
optimization O
the O
simplest O
approach O
to O
using O
gradient O
information O
is O
to O
choose O
the O
weight O
update O
in O
( O
5.27 O
) O
to O
comprise O
a O
small O
step O
in O
the O
direction O
of O
the O
negative O
gradient O
, O
so O
that O
( O
5.41 O
) O
where O
the O
parameter O
η O
> O
0 O
is O
known O
as O
the O
learning O
rate O
. O
by O
setting O
the O
vector O
v O
in O
( O
5.39 O
) O
equal O
to O
each O
of O
the O
eigenvectors O
ui O
in O
turn O
, O
show O
that O
h O
is O
positive B
deﬁnite I
if O
, O
and O
only O
if O
, O
all O
of O
its O
eigenvalues O
are O
positive O
. O
e O
[ O
πk O
] O
= O
αk O
+ O
nk O
kα0 O
+ O
n O
480 O
10. O
approximate O
inference B
figure O
10.6 O
variational B
bayesian O
mixture O
of O
k O
= O
6 O
gaussians O
ap- O
plied O
to O
the O
old O
faithful O
data O
set O
, O
in O
which O
the O
ellipses O
denote O
the O
one O
standard-deviation O
density B
contours O
for O
each O
of O
the O
components O
, O
and O
the O
density B
of O
red O
ink O
inside O
each O
ellipse O
corresponds O
to O
the O
mean O
value O
of O
the O
mixing B
coefﬁcient I
for O
each O
com- O
ponent O
. O
( O
2.31 O
) O
( O
2.32 O
) O
section O
2.4 O
appendix O
e O
76 O
2. O
probability B
distributions O
( O
cid:5 O
) O
we O
can O
solve O
for O
the O
lagrange O
multiplier O
λ O
by O
substituting O
( O
2.32 O
) O
into O
the O
constraint O
k O
µk O
= O
1 O
to O
give O
λ O
= O
−n O
. O
he O
studied O
logic O
and O
theology O
at O
edinburgh O
univer- O
sity O
and O
was O
elected O
fellow O
of O
the O
royal O
society O
in O
1742. O
during O
the O
18th O
century O
, O
is- O
sues O
regarding O
probability B
arose O
in O
connection O
with O
gambling O
and O
with O
the O
new O
concept O
of O
insurance O
. O
the O
mean B
of O
the O
target O
distribution O
is O
now O
given O
by O
the O
function O
wtφ O
( O
x O
) O
, O
which O
contains O
m O
parameters O
. O
a O
more O
powerful O
approach O
, O
however O
, O
models O
the O
conditional B
probability I
distribution O
p O
( O
ck|x O
) O
in O
an O
inference B
stage O
, O
and O
then O
subse- O
quently O
uses O
this O
distribution O
to O
make O
optimal O
decisions O
. O
section O
7.2 O
7.1. O
maximum B
margin I
classiﬁers O
we O
begin O
our O
discussion O
of O
support B
vector I
machines O
by O
returning O
to O
the O
two-class O
classiﬁcation B
problem O
using O
linear O
models O
of O
the O
form O
y O
( O
x O
) O
= O
wtφ O
( O
x O
) O
+ O
b O
( O
7.1 O
) O
where O
φ O
( O
x O
) O
denotes O
a O
ﬁxed O
feature-space O
transformation O
, O
and O
we O
have O
made O
the O
bias B
parameter I
b O
explicit O
. O
p O
( O
t|w O
) O
p O
( O
w|α O
) O
dw O
p O
( O
t|α O
) O
= O
( O
7.114 O
) O
if O
we O
substitute O
for O
p O
( O
t|w O
( O
cid:1 O
) O
) O
and O
p O
( O
w O
( O
cid:1 O
) O
|α O
) O
and O
then O
set O
the O
derivative B
of O
the O
marginal B
likelihood I
with O
respect O
to O
αi O
equal O
to O
zero O
, O
we O
obtain O
exercise O
7.19 O
−1 O
2 O
( O
w O
( O
cid:1 O
) O
i O
) O
2 O
+ O
1 O
2αi O
− O
1 O
2 O
σii O
= O
0 O
. O
recall O
that O
we O
made O
an O
analogous O
interpretation O
for O
the O
dirichlet O
prior B
. O
( O
b.51 O
) O
gaussian-gamma O
this O
is O
the O
conjugate B
prior I
distribution O
for O
a O
univariate O
gaussian O
n O
( O
x|µ O
, O
λ O
−1 O
) O
in O
which O
the O
mean B
µ O
and O
the O
precision O
λ O
are O
both O
unknown O
and O
is O
also O
called O
the O
normal-gamma B
distribution I
. O
the O
prior B
is O
given O
by O
a O
beta B
distribution I
with O
parameters O
a O
= O
2 O
, O
b O
= O
2 O
, O
and O
the O
likelihood B
function I
, O
given O
by O
( O
2.9 O
) O
with O
n O
= O
m O
= O
1 O
, O
corresponds O
to O
a O
single O
observation O
of O
x O
= O
1 O
, O
so O
that O
the O
posterior O
is O
given O
by O
a O
beta B
distribution I
with O
parameters O
a O
= O
3 O
, O
b O
= O
2. O
distribution O
by O
multiplying O
by O
the O
likelihood B
function I
for O
the O
new O
observation O
and O
then O
normalizing O
to O
obtain O
the O
new O
, O
revised O
posterior O
distribution O
. O
multicategory O
support B
vector I
machines O
. O
this O
can O
be O
seen O
by O
noting O
that O
in O
figure O
13.19 O
, O
the O
variables O
z O
( O
1 O
) O
n O
are O
connected O
by O
a O
path O
which O
is O
head-to-head O
at O
node B
xn O
and O
hence O
they O
are O
not O
d-separated O
. O
we O
shall O
also O
use O
γ O
( O
znk O
) O
to O
denote O
the O
conditional B
probability I
of O
znk O
= O
1 O
, O
with O
a O
similar O
use O
of O
notation O
for O
ξ O
( O
zn−1 O
, O
j O
, O
znk O
) O
and O
for O
other O
probabilistic O
variables O
introduced O
later O
. O
( O
cid:1 O
) O
ξn O
( O
cid:2 O
) O
0 O
, O
where O
ξn O
> O
0 O
corresponds O
to O
a O
point O
for O
which O
tn O
> O
y O
( O
xn O
) O
+ O
 O
, O
and O
( O
cid:1 O
) O
ξn O
> O
0 O
the O
condition O
for O
a O
target O
point O
to O
lie O
inside O
the O
-tube B
is O
that O
yn O
− O
 O
( O
cid:1 O
) O
tn O
( O
cid:1 O
) O
yn+ O
, O
where O
yn O
= O
y O
( O
xn O
) O
. O
( O
4.63 O
) O
the O
normalized B
exponential I
is O
also O
known O
as O
the O
softmax B
function I
, O
as O
it O
represents O
a O
smoothed O
version O
of O
the O
‘ O
max O
’ O
function O
because O
, O
if O
ak O
( O
cid:12 O
) O
aj O
for O
all O
j O
( O
cid:9 O
) O
= O
k O
, O
then O
p O
( O
ck|x O
) O
( O
cid:7 O
) O
1 O
, O
and O
p O
( O
cj|x O
) O
( O
cid:7 O
) O
0. O
we O
now O
investigate O
the O
consequences O
of O
choosing O
speciﬁc O
forms O
for O
the O
class- O
conditional B
densities O
, O
looking O
ﬁrst O
at O
continuous O
input O
variables O
x O
and O
then O
dis- O
cussing O
brieﬂy O
the O
case O
of O
discrete O
inputs O
. O
in O
this O
case O
, O
the O
manifold B
comprises O
scveral O
distinct O
segments O
corresponding O
to O
different O
flow O
regimes O
. O
again O
, O
by O
comparison O
with O
( O
2.71 O
) O
, O
we O
see O
that O
the O
covariance B
of O
the O
marginal B
distribution O
of O
p O
( O
xa O
) O
is O
given O
by O
σa O
= O
( O
λaa O
− O
λabλ O
−1 O
bb O
λba O
) O
−1 O
. O
given O
a O
training B
set I
d O
, O
we O
then O
wish O
to O
evaluate O
the O
posterior O
distribution O
( O
3.66 O
) O
the O
prior B
allows O
us O
to O
express O
a O
preference O
for O
different O
models O
. O
we O
shall O
suppose O
that O
direct O
optimization O
of O
p O
( O
x|θ O
) O
is O
difﬁcult O
, O
but O
that O
opti- O
mization O
of O
the O
complete-data O
likelihood B
function I
p O
( O
x O
, O
z|θ O
) O
is O
signiﬁcantly O
easier O
. O
this O
leads O
to O
a O
practical O
framework O
for B
regression I
( O
and O
classiﬁcation B
) O
called O
gaussian O
processes O
, O
which O
will O
be O
discussed O
in O
detail O
in O
section O
6.4. O
we O
have O
seen O
that O
the O
effective O
kernel O
deﬁnes O
the O
weights O
by O
which O
the O
training B
set I
target O
values O
are O
combined O
in O
order O
to O
make O
a O
prediction O
at O
a O
new O
value O
of O
x O
, O
and O
it O
can O
be O
shown O
that O
these O
weights O
sum O
to O
one O
, O
in O
other O
words O
n O
( O
cid:2 O
) O
k O
( O
x O
, O
xn O
) O
= O
1 O
( O
3.64 O
) O
exercise O
3.14 O
n=1 O
by O
noting O
that O
the O
summation O
is O
equivalent O
to O
considering O
the O
predictive O
mean O
( O
cid:1 O
) O
y O
( O
x O
) O
for O
all O
values O
of O
x. O
this O
intuitively O
pleasing O
result O
can O
easily O
be O
proven O
informally O
for O
a O
set O
of O
target O
data O
in O
which O
tn O
= O
1 O
for O
all O
n. O
provided O
the O
basis O
functions O
are O
linearly O
independent O
, O
that O
there O
are O
more O
data O
points O
than O
basis O
functions O
, O
and O
that O
one O
of O
the O
basis O
functions O
is O
constant O
( O
corresponding O
to O
the O
bias B
parameter I
) O
, O
then O
it O
is O
clear O
that O
we O
can O
ﬁt O
the O
training B
data O
exactly O
and O
hence O
that O
the O
predictive O
mean O
will O
be O
simply O
( O
cid:1 O
) O
y O
( O
x O
) O
= O
1 O
, O
from O
which O
we O
obtain O
( O
3.64 O
) O
. O
13.8 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
for O
a O
hidden O
markov O
model O
having O
discrete O
observations O
governed O
by O
a O
multinomial B
distribution I
, O
show O
that O
the O
conditional B
distribution O
of O
the O
observations O
given O
the O
hidden O
variables O
is O
given O
by O
( O
13.22 O
) O
and O
the O
corresponding O
m O
step O
equa- O
tions O
are O
given O
by O
( O
13.23 O
) O
. O
we O
can O
interpret O
the O
distribution O
p O
( O
x O
) O
as O
a O
prior B
distribution O
over O
x. O
if O
the O
variable O
y O
is O
observed O
, O
then O
the O
conditional B
distribution O
p O
( O
x|y O
) O
represents O
the O
corresponding O
posterior O
distribution O
over O
x. O
having O
found O
the O
marginal B
and O
conditional B
distribu- O
tions O
, O
we O
effectively O
expressed O
the O
joint O
distribution O
p O
( O
z O
) O
= O
p O
( O
x O
) O
p O
( O
y|x O
) O
in O
the O
form O
p O
( O
x|y O
) O
p O
( O
y O
) O
. O
by O
deﬁnition O
, O
the O
marginal B
is O
obtained O
by O
sum- O
ming O
the O
joint O
distribution O
over O
all O
variables O
except O
x O
so O
that O
p O
( O
x O
) O
= O
p O
( O
x O
) O
( O
8.61 O
) O
( O
cid:2 O
) O
x\x O
where O
x O
\ O
x O
denotes O
the O
set O
of O
variables O
in O
x O
with O
variable O
x O
omitted O
. O
thus O
, O
provided O
we O
can O
normalize O
a O
distribution O
from O
the O
exponential B
family I
, O
we O
can O
always O
ﬁnd O
its O
moments O
by O
simple O
differentiation O
. O
thus O
the O
adaboost O
algorithm O
is O
seeking O
the O
best O
approx- O
imation O
to O
the O
log B
odds I
ratio O
, O
within O
the O
space O
of O
functions O
represented O
by O
the O
linear O
combination O
of O
base O
classiﬁers O
, O
subject O
to O
the O
constrained O
minimization O
resulting O
from O
the O
sequential O
optimization O
strategy O
. O
( O
10.147 O
) O
we O
ﬁrst O
note O
that O
the O
conditional B
distribution O
for O
t O
can O
be O
written O
as O
( O
cid:6 O
) O
p O
( O
t O
) O
= O
1 O
( O
cid:16 O
) O
1−t O
p O
( O
t|w O
) O
p O
( O
w O
) O
dw O
= O
( O
cid:15 O
) O
p O
( O
t|w O
) O
= O
σ O
( O
a O
) O
t O
{ O
1 O
− O
σ O
( O
a O
) O
} O
1−t O
1 O
− O
= O
1 O
n=1 O
( O
cid:16 O
) O
t O
( O
cid:15 O
) O
1 O
+ O
e−a O
1 O
+ O
e−a O
−a O
1 O
+ O
e−a O
= O
eatσ O
( O
−a O
) O
e O
= O
eat O
where O
a O
= O
wtφ O
. O
10.20 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
this O
exercise O
explores O
the O
variational B
bayes O
solution O
for O
the O
mixture O
of O
gaussians O
model O
when O
the O
size O
n O
of O
the O
data O
set O
is O
large O
and O
shows O
that O
it O
reduces O
( O
as O
we O
would O
expect O
) O
to O
the O
maximum B
likelihood I
solution O
based O
on O
em O
derived O
in O
chap- O
ter O
9. O
note O
that O
results O
from O
appendix O
b O
may O
be O
used O
to O
help O
answer O
this O
exercise O
. O
also O
show O
that O
a O
distribution O
expressed O
as O
an O
undirected B
tree O
can O
, O
by O
suitable O
normaliza- O
tion O
of O
the O
clique B
potentials O
, O
be O
written O
as O
a O
directed B
tree O
. O
1.3 O
model B
selection I
. O
thus O
the O
likelihood B
function I
is O
given O
by O
n O
( O
cid:14 O
) O
p O
( O
t|π O
, O
µ1 O
, O
µ2 O
, O
σ O
) O
= O
[ O
πn O
( O
xn|µ1 O
, O
σ O
) O
] O
tn O
[ O
( O
1 O
− O
π O
) O
n O
( O
xn|µ2 O
, O
σ O
) O
] O
1−tn O
( O
4.71 O
) O
n=1 O
where O
t O
= O
( O
t1 O
, O
. O
the O
resulting O
decision O
boundaries O
will O
be O
linear O
in O
the O
feature B
space I
φ O
, O
and O
these O
correspond O
to O
nonlinear O
decision O
boundaries O
in O
the O
original O
x O
space O
, O
as O
illustrated O
in O
figure O
4.12. O
classes O
that O
are O
linearly B
separable I
in O
the O
feature B
space I
φ O
( O
x O
) O
need O
not O
be O
linearly B
separable I
in O
the O
original O
observation O
space O
x. O
note O
that O
as O
in O
our O
discussion O
of O
linear O
models O
for B
regression I
, O
one O
of O
the O
4.3. O
probabilistic O
discriminative O
models O
205 O
basis O
functions O
is O
typically O
set O
to O
a O
constant O
, O
say O
φ0 O
( O
x O
) O
= O
1 O
, O
so O
that O
the O
correspond- O
ing O
parameter O
w0 O
plays O
the O
role O
of O
a O
bias B
. O
one O
advantage O
of O
approach O
( O
a O
) O
, O
however O
, O
is O
that O
it O
also O
allows O
the O
marginal B
density O
of O
data O
p O
( O
x O
) O
to O
be O
determined O
from O
( O
1.83 O
) O
. O
although O
this O
gives O
greater O
ﬂexibility O
in O
choosing O
the O
potential O
functions O
, O
because O
there O
is O
no O
normalization O
constraint O
, O
it O
does O
raise O
the O
question O
of O
how O
to O
motivate O
a O
choice O
of O
potential B
function I
for O
a O
particular O
application O
. O
we O
can O
formu O
( O
cid:173 O
) O
late O
probabilistic O
pca O
by O
first O
introducing O
an O
explicit O
latent B
variable I
z O
corresponding O
to O
the O
principal-component O
subspace O
. O
obviously O
, O
we O
must O
have O
σ2 O
q O
( O
cid:2 O
) O
σ2 O
532 O
11. O
sampling B
methods I
figure O
11.8 O
importance B
sampling I
addresses O
the O
prob- O
lem O
of O
evaluating O
the O
expectation B
of O
a O
func- O
tion O
f O
( O
z O
) O
with O
respect O
to O
a O
distribution O
p O
( O
z O
) O
from O
which O
it O
is O
difﬁcult O
to O
draw O
samples O
di- O
instead O
, O
samples O
{ O
z O
( O
l O
) O
} O
are O
drawn O
rectly O
. O
160 O
3. O
linear O
models O
for B
regression I
figure O
3.11 O
examples O
of O
equiva- O
lent O
kernels O
k O
( O
x O
, O
x O
( O
cid:1 O
) O
) O
for O
x O
= O
0 O
plotted O
as O
a O
function O
of O
x O
( O
cid:1 O
) O
, O
corre- O
sponding O
( O
left O
) O
to O
the O
polynomial O
ba- O
sis O
functions O
and O
( O
right O
) O
to O
the O
sig- O
moidal O
basis O
functions O
shown O
in O
fig- O
ure O
3.1. O
note O
that O
these O
are O
local- O
ized O
functions O
of O
x O
( O
cid:1 O
) O
even O
though O
the O
corresponding O
basis O
functions O
are O
nonlocal O
. O
the O
total O
number O
of O
independent B
parameters O
{ O
wij O
} O
and O
{ O
vi O
} O
in O
the O
covariance B
matrix I
is O
therefore O
d O
( O
d O
+ O
1 O
) O
/2 O
corresponding O
to O
a O
general O
symmetric O
covariance B
matrix I
. O
12.1 O
( O
* O
* O
) O
lib O
in O
this O
exercise O
, O
we O
use O
proof O
by O
induction O
to O
show O
that O
the O
linear O
projection O
onto O
an O
m O
-dimensional O
subspace O
that O
maximizes O
the O
variance B
of O
the O
pro O
( O
cid:173 O
) O
jected O
data O
is O
defined O
by O
the O
m O
eigenvectors O
of O
the O
data O
covariance O
matrix O
s O
, O
given O
by O
( O
12.3 O
) O
, O
corresponding O
to O
the O
m O
largest O
eigenvalues O
. O
clearly O
a O
fully B
connected I
graph O
will O
be O
a O
trivial O
i O
map O
for O
any O
distribution O
. O
, O
xn O
and O
l O
test B
set I
input O
vectors O
xn O
+1 O
, O
. O
for O
the O
case O
of O
the O
mixture B
( O
11.42 O
) O
, O
if O
each O
of O
the O
base O
transitions O
sat- O
isﬁes O
detailed O
balance O
, O
then O
the O
mixture B
transition O
t O
will O
also O
satisfy O
detailed O
bal- O
ance O
. O
here O
the O
noise O
precision B
parameter I
β O
is O
also O
determined O
through O
evidence O
maximization O
. O
70 O
2. O
probability B
distributions O
figure O
2.1 O
histogram O
plot O
of O
the O
binomial O
dis- O
tribution O
( O
2.9 O
) O
as O
a O
function O
of O
m O
for O
n O
= O
10 O
and O
µ O
= O
0.25 O
. O
( O
2.267 O
) O
( O
2.268 O
) O
( O
2.269 O
) O
exercises O
129 O
2.7 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
binomial O
random O
variable O
x O
given O
by O
( O
2.9 O
) O
, O
with O
prior B
distribution O
for O
µ O
given O
by O
the O
beta B
distribution I
( O
2.13 O
) O
, O
and O
suppose O
we O
have O
observed O
m O
occur- O
rences O
of O
x O
= O
1 O
and O
l O
occurrences O
of O
x O
= O
0. O
show O
that O
the O
posterior O
mean O
value O
of O
x O
lies O
between O
the O
prior B
mean O
and O
the O
maximum B
likelihood I
estimate O
for O
µ. O
to O
do O
this O
, O
show O
that O
the O
posterior O
mean O
can O
be O
written O
as O
λ O
times O
the O
prior B
mean O
plus O
( O
1 O
− O
λ O
) O
times O
the O
maximum B
likelihood I
estimate O
, O
where O
0 O
( O
cid:1 O
) O
λ O
( O
cid:1 O
) O
1. O
this O
illustrates O
the O
con- O
cept O
of O
the O
posterior O
distribution O
being O
a O
compromise O
between O
the O
prior B
distribution O
and O
the O
maximum B
likelihood I
solution O
. O
exercise O
9.26 O
exercises O
456 O
9. O
mixture B
models O
and O
em O
9.3 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
gaussian O
mixture B
model I
in O
which O
the O
marginal B
distribution O
p O
( O
z O
) O
for O
the O
latent B
variable I
is O
given O
by O
( O
9.10 O
) O
, O
and O
the O
conditional B
distribution O
p O
( O
x|z O
) O
for O
the O
observed B
variable I
is O
given O
by O
( O
9.11 O
) O
. O
d O
u O
p O
8.4. O
inference B
in O
graphical O
models O
figure O
8.35 O
a O
directed B
graph O
whose O
conditional B
independence I
properties O
can O
not O
be O
expressed O
using O
an O
undirected B
graph I
over O
the O
same O
three O
variables O
. O
each O
weight O
was O
implemented O
using O
a O
rotary O
variable O
resistor O
, O
also O
called O
a O
potentiometer O
, O
driven O
by O
an O
electric O
motor O
thereby O
allowing O
the O
value O
of O
the O
weight O
to O
be O
adjusted O
automatically O
by O
the O
learning B
algorithm O
. O
this O
corresponds O
to O
an O
example O
of O
automatic B
relevance I
determination I
, O
or O
ard O
, O
discussed O
in O
section O
7.2.2. O
specifically O
, O
we O
define O
an O
independent B
gaussian O
prior B
over O
each O
column O
of O
w O
, O
which O
represent O
the O
vectors O
defining O
the O
principal B
subspace I
. O
keeping O
only O
the O
factors O
that O
depend O
on O
µ O
, O
we O
see O
that O
this O
posterior O
distribution O
has O
the O
form O
p O
( O
µ|m O
, O
l O
, O
a O
, O
b O
) O
∝ O
µm+a−1 O
( O
1 O
− O
µ O
) O
l+b−1 O
( O
2.17 O
) O
72 O
2. O
probability B
distributions O
a O
= O
0.1 O
b O
= O
0.1 O
a O
= O
2 O
b O
= O
3 O
3 O
2 O
1 O
0 O
0 O
3 O
2 O
1 O
0 O
0 O
0.5 O
µ O
1 O
a O
= O
1 O
b O
= O
1 O
0 O
0.5 O
µ O
1 O
a O
= O
8 O
b O
= O
4 O
3 O
2 O
1 O
0 O
3 O
2 O
1 O
0.5 O
µ O
1 O
0 O
0 O
0.5 O
µ O
1 O
figure O
2.2 O
plots O
of O
the O
beta B
distribution I
beta O
( O
µ|a O
, O
b O
) O
given O
by O
( O
2.13 O
) O
as O
a O
function O
of O
µ O
for O
various O
values O
of O
the O
hyperparameters O
a O
and O
b. O
where O
l O
= O
n O
− O
m O
, O
and O
therefore O
corresponds O
to O
the O
number O
of O
‘ O
tails O
’ O
in O
the O
coin O
example O
. O
( O
2.230 O
) O
n=1 O
this O
again O
takes O
the O
same O
functional B
form O
as O
the O
prior B
( O
2.229 O
) O
, O
conﬁrming O
conjugacy O
. O
in O
order O
to O
learn O
the O
parameters O
of O
these O
potentials O
in O
situa- O
tions O
where O
not O
all O
of O
the O
variables O
are O
observed O
, O
we O
can O
employ O
the O
em O
algorithm O
, O
and O
it O
turns O
out O
that O
the O
local B
joint O
distributions O
of O
the O
cliques O
, O
conditioned O
on O
any O
observed O
data O
, O
is O
precisely O
what O
is O
needed O
in O
the O
e O
step O
. O
consider O
a O
gaussian O
mixture B
model I
in O
which O
the O
covariance B
matrices O
of O
the O
mixture B
components O
are O
given O
by O
i O
, O
where O
 O
is O
a O
variance B
parameter O
that O
is O
shared O
444 O
9. O
mixture B
models O
and O
em O
by O
all O
of O
the O
components O
, O
and O
i O
is O
the O
identity O
matrix O
, O
so O
that O
p O
( O
x|µk O
, O
σk O
) O
= O
1 O
( O
2π O
) O
1/2 O
exp O
− O
1 O
2 O
( O
cid:5 O
) O
x O
− O
µk O
( O
cid:5 O
) O
2 O
( O
cid:12 O
) O
( O
cid:13 O
) O
. O
in O
each O
column O
the O
lower O
plot O
shows O
the O
kernel B
function I
k O
( O
x O
, O
x O
( O
cid:1 O
) O
) O
deﬁned O
by O
( O
6.10 O
) O
plotted O
as O
a O
function O
of O
x O
for O
x O
( O
cid:1 O
) O
= O
0 O
, O
while O
the O
upper O
plot O
shows O
the O
corresponding O
basis O
functions O
given O
by O
polynomials O
( O
left O
column O
) O
, O
‘ O
gaussians O
’ O
( O
centre O
column O
) O
, O
and O
logistic O
sigmoids O
( O
right O
column O
) O
. O
( O
2.253 O
) O
( O
2.254 O
) O
( O
2.255 O
) O
we O
can O
now O
combine O
( O
2.253 O
) O
, O
( O
2.254 O
) O
, O
and O
( O
2.255 O
) O
using O
bayes O
’ O
theorem O
to O
obtain O
the O
posterior B
probability I
of O
class O
membership O
p O
( O
ck|x O
) O
= O
p O
( O
x|ck O
) O
p O
( O
ck O
) O
p O
( O
x O
) O
= O
kk O
k O
. O
other O
forms O
of O
prior B
over O
the O
parameters O
can O
be O
considered O
. O
the O
right O
plot O
shows O
the O
result O
of O
ﬁtting O
a O
mixture O
of O
two O
logistic B
regression I
models O
, O
which O
now O
gives O
much O
higher O
probability B
to O
the O
correct O
labels O
for O
many O
of O
the O
points O
in O
the O
blue O
class O
. O
together O
with O
the O
class O
prior B
p O
( O
c1 O
) O
, O
this O
gives O
a O
total O
of O
m O
( O
m O
+5 O
) O
/2+1 O
parameters O
, O
which O
grows O
quadratically O
with O
m O
, O
in O
contrast O
to O
the O
linear O
dependence O
on O
m O
of O
the O
number O
of O
parameters O
in O
logistic B
regression I
. O
in O
a O
general O
feed-forward O
network O
, O
each O
unit O
computes O
a O
weighted O
sum O
of O
its O
( O
cid:2 O
) O
inputs O
of O
the O
form O
aj O
= O
wjizi O
i O
( O
5.48 O
) O
5.3. O
error B
backpropagation I
243 O
where O
zi O
is O
the O
activation O
of O
a O
unit O
, O
or O
input O
, O
that O
sends O
a O
connection O
to O
unit O
j O
, O
and O
wji O
is O
the O
weight O
associated O
with O
that O
connection O
. O
( O
3.55 O
) O
maximization O
of O
this O
posterior O
distribution O
with O
respect O
to O
w O
is O
therefore O
equiva- O
lent O
to O
the O
minimization O
of O
the O
sum-of-squares B
error I
function O
with O
the O
addition O
of O
a O
quadratic O
regularization O
term O
, O
corresponding O
to O
( O
3.27 O
) O
with O
λ O
= O
α/β O
. O
( O
14.14 O
) O
this O
apparently O
dramatic O
result O
suggests O
that O
the O
average O
error B
of O
a O
model O
can O
be O
reduced O
by O
a O
factor O
of O
m O
simply O
by O
averaging O
m O
versions O
of O
the O
model O
. O
( O
10.54 O
) O
we O
observe O
that O
the O
right-hand O
side O
of O
this O
expression O
decomposes O
into O
a O
sum O
of O
terms O
involving O
only O
π O
together O
with O
terms O
only O
involving O
µ O
and O
λ O
, O
which O
implies O
that O
the O
variational B
posterior O
q O
( O
π O
, O
µ O
, O
λ O
) O
factorizes O
to O
give O
q O
( O
π O
) O
q O
( O
µ O
, O
λ O
) O
. O
support B
vector I
networks O
. O
( O
1986 O
) O
, O
involves O
gradient B
descent I
. O
in O
order O
to O
keep O
the O
rejection O
rate O
low O
, O
the O
scale O
ρ O
of O
the O
proposal B
distribution I
should O
be O
on O
the O
order O
of O
the O
smallest O
standard B
deviation I
σmin O
, O
which O
leads O
to O
random O
walk O
behaviour O
in O
which O
the O
number O
of O
steps O
sep- O
arating O
states O
that O
are O
approximately O
independent B
is O
of O
order O
( O
σmax/σmin O
) O
2 O
where O
σmax O
is O
the O
largest O
standard B
deviation I
. O
this O
work O
included O
a O
discus- O
sion B
of O
the O
inverse B
probability O
calculation O
( O
later O
termed O
bayes O
’ O
theorem O
by O
poincar´e O
) O
, O
which O
he O
used O
to O
solve O
problems O
in O
life O
expectancy O
, O
jurisprudence O
, O
planetary O
masses O
, O
triangulation O
, O
and O
error B
estimation O
. O
lle O
then O
maps O
the O
high-dimensional O
data O
points O
down O
to O
a O
lower O
dimensional O
space O
while O
preserv O
( O
cid:173 O
) O
if O
the O
local B
neighbourhood O
for O
a O
particular O
ing O
these O
neighbourhood O
coefficients O
. O
2.4. O
the O
exponential B
family I
the O
probability B
distributions O
that O
we O
have O
studied O
so O
far O
in O
this O
chapter O
( O
with O
the O
exception O
of O
the O
gaussian O
mixture B
) O
are O
speciﬁc O
examples O
of O
a O
broad O
class O
of O
distri- O
butions O
called O
the O
exponential B
family I
( O
duda O
and O
hart O
, O
1973 O
; O
bernardo O
and O
smith O
, O
1994 O
) O
. O
( O
cid:6 O
) O
∞ O
−∞ O
( O
1.105 O
) O
( O
1.106 O
) O
( O
1.107 O
) O
appendix O
e O
appendix O
d O
exercise O
1.34 O
exercise O
1.35 O
the O
constrained O
maximization O
can O
be O
performed O
using O
lagrange O
multipliers O
so O
that O
we O
maximize O
the O
following O
functional B
with O
respect O
to O
p O
( O
x O
) O
p O
( O
x O
) O
dx O
− O
1 O
p O
( O
x O
) O
ln O
p O
( O
x O
) O
dx O
+ O
λ1 O
− O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:6 O
) O
∞ O
( O
cid:15 O
) O
( O
cid:6 O
) O
∞ O
−∞ O
( O
cid:16 O
) O
( O
cid:16 O
) O
xp O
( O
x O
) O
dx O
− O
µ O
+ O
λ3 O
( O
x O
− O
µ O
) O
2p O
( O
x O
) O
dx O
− O
σ2 O
. O
for O
a O
given O
choice O
of O
model O
space O
f O
, O
and O
for O
given O
parameters O
 O
and O
δ O
, O
pac O
learning B
aims O
to O
provide O
bounds O
on O
the O
minimum O
size O
n O
of O
data O
set O
needed O
to O
meet O
this O
criterion O
. O
a O
further O
difﬁculty O
with O
the O
expression O
( O
13.11 O
) O
for O
the O
likelihood B
function I
is O
that O
, O
because O
it O
corresponds O
to O
a O
generalization B
of O
a O
mixture B
distribution I
, O
it O
represents O
a O
summation O
over O
the O
emission O
models O
for O
different O
settings O
of O
the O
latent O
variables O
. O
because O
all O
such O
paths O
are O
head-to-tail O
, O
it O
follows O
that O
the O
conditional B
independence I
property O
must O
hold O
. O
first O
show O
that O
the O
posterior O
distribution O
q O
( O
cid:1 O
) O
( O
λk O
) O
of O
the O
precisions O
becomes O
sharply O
peaked O
around O
the O
maximum B
likelihood I
solution O
. O
we O
can O
test O
to O
see O
if O
this O
relation O
does O
hold O
, O
for O
any O
choice O
of O
a O
and O
b O
by O
making O
use O
of O
the O
d-separation B
criterion O
. O
section O
12.2.2 O
576 O
12. O
continuous O
latent O
variables O
the O
rotational O
invariance B
in O
latent O
space O
represents O
a O
form O
of O
statistical O
noniden O
( O
cid:173 O
) O
tifiability O
, O
analogous O
to O
that O
encountered O
for O
mixture O
models O
in O
the O
case O
of O
discrete O
latent O
variables O
. O
because O
there O
are O
loops O
in O
the O
graph O
, O
this O
raises O
the O
problem O
of O
how O
to O
initiate O
the O
message B
passing I
algorithm O
. O
if O
we O
have O
a O
mixture B
model I
com- O
prising O
k O
components O
, O
then O
each O
parameter O
setting O
will O
be O
a O
member O
of O
a O
family O
of O
k O
! O
equivalent O
settings O
. O
( O
10.37 O
) O
n O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
n=1 O
k=1 O
n O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
n O
( O
cid:10 O
) O
n=1 O
k=1 O
similarly O
, O
from O
( O
9.11 O
) O
, O
we O
can O
write O
down O
the O
conditional B
distribution O
of O
the O
ob- O
served O
data O
vectors O
, O
given O
the O
latent O
variables O
and O
the O
component O
parameters O
p O
( O
x|z O
, O
µ O
, O
λ O
) O
= O
xn|µk O
, O
λ O
−1 O
k O
( O
10.38 O
) O
where O
µ O
= O
{ O
µk O
} O
and O
λ O
= O
{ O
λk O
} O
. O
( O
8.58 O
) O
thus O
we O
can O
obtain O
the O
joint O
distributions O
over O
all O
of O
the O
sets O
of O
variables O
in O
each O
of O
the O
potentials O
directly O
once O
we O
have O
completed O
the O
message B
passing I
required O
to O
obtain O
the O
marginals O
. O
by O
mak- O
ing O
use O
of O
the O
results O
( O
2.92 O
) O
and O
( O
2.93 O
) O
show O
that O
the O
marginal B
distribution O
p O
( O
x O
) O
is O
given O
( O
2.99 O
) O
. O
5.26 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
multilayer B
perceptron I
with O
arbitrary O
feed-forward O
topology O
, O
which O
is O
to O
be O
trained O
by O
minimizing O
the O
tangent B
propagation I
error O
function O
( O
5.127 O
) O
in O
which O
the O
regularizing O
function O
is O
given O
by O
( O
5.128 O
) O
. O
because O
there O
are O
n O
− O
1 O
summations O
and O
multiplications O
of O
this O
kind O
, O
the O
total O
cost O
of O
evaluating O
the O
marginal B
p O
( O
xn O
) O
is O
o O
( O
n O
k O
2 O
) O
. O
each O
basis B
function I
represents O
a O
speciﬁc O
fre- O
quency O
and O
has O
inﬁnite O
spatial O
extent O
. O
7.1.4 O
svms O
for B
regression I
. O
( O
4.7 O
) O
this O
result O
is O
illustrated O
in O
figure O
4.1. O
as O
with O
the O
linear B
regression I
models O
in O
chapter O
3 O
, O
it O
is O
sometimes O
convenient O
to O
use O
a O
more O
compact O
notation O
in O
which O
we O
introduce O
an O
additional O
dummy O
‘ O
input O
’ O
value O
x0 O
= O
1 O
and O
then O
deﬁne O
( O
cid:4 O
) O
w O
= O
( O
w0 O
, O
w O
) O
and O
( O
cid:4 O
) O
x O
= O
( O
x0 O
, O
x O
) O
so O
that O
y O
( O
x O
) O
= O
( O
cid:4 O
) O
wt O
( O
cid:4 O
) O
x. O
in O
this O
case O
, O
the O
decision O
surfaces O
are O
d-dimensional O
hyperplanes O
passing O
through O
the O
origin O
of O
the O
d O
+ O
1-dimensional O
expanded O
input O
space O
. O
note O
that O
the O
joint O
marginal B
distribution O
p O
( O
xi O
, O
xb O
) O
will O
typically O
not O
factorize O
under O
this O
model O
. O
∆wposterior O
∆wprior O
( O
3.71 O
) O
this O
approximation O
is O
illustrated O
in O
figure O
3.12. O
the O
ﬁrst O
term O
represents O
the O
ﬁt O
to O
the O
data O
given O
by O
the O
most O
probable O
parameter O
values O
, O
and O
for O
a O
ﬂat O
prior B
this O
would O
correspond O
to O
the O
log O
likelihood O
. O
logistic B
regression I
has O
also O
been O
treated O
by O
dybowski O
and O
roberts O
( O
2005 O
) O
from O
a O
bayesian O
perspective O
using O
monte O
carlo O
sampling O
techniques O
. O
we O
ﬁrst O
note O
that O
the O
objective O
function O
( O
cid:4 O
) O
l O
( O
a O
) O
−2 O
0 O
2 O
given O
by O
( O
7.10 O
) O
or O
( O
7.32 O
) O
is O
quadratic O
and O
so O
any O
local B
optimum O
will O
also O
be O
a O
global O
optimum O
provided O
the O
constraints O
deﬁne O
a O
convex O
region O
( O
which O
they O
do O
as O
a O
conse- O
quence O
of O
being O
linear O
) O
. O
some O
simpliﬁcations O
and O
combination O
of O
terms O
can O
be O
performed O
when O
these O
expressions O
are O
summed O
to O
give O
the O
lower B
bound I
. O
if O
we O
have O
a O
distribution O
p O
( O
x|λ O
) O
governed O
by O
a O
parameter O
λ O
, O
we O
might O
be O
tempted O
to O
propose O
a O
prior B
distribution O
p O
( O
λ O
) O
= O
const O
as O
a O
suitable O
prior B
. O
11.6 O
estimating O
the O
partition B
function I
. O
section O
1.5.4 O
the O
bayesian O
view O
of O
model B
comparison I
simply O
involves O
the O
use O
of O
probabilities O
to O
represent O
uncertainty O
in O
the O
choice O
of O
model O
, O
along O
with O
a O
consistent B
application O
of O
the O
sum O
and O
product O
rules O
of O
probability B
. O
the O
proposal O
distribu- O
tion O
is O
an O
isotropic B
gaussian O
distri- O
bution O
whose O
standard B
deviation I
is O
0.2. O
steps O
that O
are O
accepted O
are O
shown O
as O
green O
lines O
, O
and O
rejected O
steps O
are O
shown O
in O
red O
. O
the O
corresponding O
maximum B
likelihood I
solution O
for O
( O
j'2 O
is O
then O
given O
by O
( O
j'~l O
= O
d-m O
l O
ai O
d O
1 O
i=m+l O
( O
12.46 O
) O
so O
that O
( O
j'~l O
is O
the O
average O
variance B
associated O
with O
the O
discarded O
dimensions O
. O
it O
is O
instructive O
to O
relate O
the O
variational B
solution O
to O
that O
found O
using O
the O
evidence O
framework O
in O
section O
3.5. O
to O
do O
this O
consider O
the O
case O
a0 O
= O
b0 O
= O
0 O
, O
corresponding O
to O
the O
limit O
of O
an O
inﬁnitely O
broad O
prior B
over O
α. O
the O
mean B
of O
the O
variational B
posterior O
distribution O
q O
( O
α O
) O
is O
then O
given O
by O
e O
[ O
α O
] O
= O
an O
bn O
= O
m/2 O
e O
[ O
wtw O
] O
/2 O
= O
m O
n O
mn O
+ O
tr O
( O
sn O
) O
. O
as O
we O
increase O
the O
number O
of O
observed O
data O
points O
, O
the O
precision O
steadily O
increases O
, O
corresponding O
to O
a O
posterior O
distribution O
with O
steadily O
decreasing O
variance B
. O
ex O
, O
t O
[ O
i O
( O
f O
( O
x O
; O
d O
) O
( O
cid:9 O
) O
= O
t O
) O
] O
< O
 O
the O
bounds O
derived O
within O
the O
pac O
framework O
are O
often O
described O
as O
worst- O
7.2. O
relevance B
vector I
machines O
345 O
case O
, O
because O
they O
apply O
to O
any O
choice O
for O
the O
distribution O
p O
( O
x O
, O
t O
) O
, O
so O
long O
as O
both O
the O
training B
and O
the O
test O
examples O
are O
drawn O
( O
independently O
) O
from O
the O
same O
distribu- O
tion O
, O
and O
for O
any O
choice O
for O
the O
function O
f O
( O
x O
) O
so O
long O
as O
it O
belongs O
to O
f. O
in O
real-world O
applications O
of O
machine O
learning O
, O
we O
deal O
with O
distributions O
that O
have O
signiﬁcant O
reg- O
ularity O
, O
for O
example O
in O
which O
large O
regions O
of O
input O
space O
carry O
the O
same O
class O
label O
. O
in O
section O
12.1 O
, O
this O
result O
was O
proven O
for O
the O
case O
of O
m O
= O
1. O
now O
suppose O
the O
result O
holds O
for O
some O
general O
value O
of O
m O
and O
show O
that O
it O
consequently O
holds O
for O
dimensionality O
m O
+ O
1. O
to O
do O
this O
, O
first O
set O
the O
derivative B
of O
the O
variance B
of O
the O
projected O
data O
with O
respect O
to O
a O
vector O
um+1 O
defining O
the O
new O
direction O
in O
data O
space O
equal O
to O
zero O
. O
as O
we O
shall O
see O
, O
the O
over-ﬁtting B
associated O
with O
maximum B
likelihood I
can O
be O
avoided O
by O
marginalizing O
( O
summing O
or O
integrating O
) O
over O
the O
model O
parameters O
in- O
stead O
of O
making O
point O
estimates O
of O
their O
values O
. O
22 O
1. O
introduction O
tion O
of O
probability B
. O
the O
indirect O
approach O
to O
ﬁnding O
the O
parameters O
of O
a O
generalized B
linear I
model I
, O
by O
ﬁtting O
class-conditional O
densities O
and O
class O
priors O
separately O
and O
then O
applying O
204 O
4. O
linear O
models O
for O
classification O
x2 O
1 O
0 O
−1 O
−1 O
0 O
x1 O
1 O
1 O
φ2 O
0.5 O
0 O
0 O
0.5 O
φ1 O
1 O
figure O
4.12 O
illustration O
of O
the O
role O
of O
nonlinear O
basis O
functions O
in O
linear O
classiﬁcation O
models O
. O
this O
can O
be O
motivated O
from O
the O
perspective O
of O
information B
geometry I
( O
amari O
, O
1998 O
) O
, O
which O
considers O
the O
differential B
geometry O
of O
the O
space O
of O
model O
parameters O
. O
as O
we O
shall O
see O
shortly O
, O
the O
columns O
of O
w O
span O
a O
linear O
subspace O
within O
the O
data O
space O
that O
corresponds O
to O
the O
principal B
subspace I
. O
it O
is O
worth O
taking O
time O
to O
study O
this O
ﬁgure O
in O
detail O
as O
it O
illustrates O
several O
important O
aspects O
of O
bayesian O
inference B
. O
the O
model B
evidence I
p O
( O
d|m O
) O
then O
provides O
the O
score O
for O
each O
model O
. O
bottom O
row O
: O
synthetic O
digits O
sam- O
pled O
generatively O
from O
a O
left-to-right B
hid- O
den O
markov O
model O
that O
has O
been O
trained O
on O
a O
data O
set O
of O
45 O
handwritten O
digits O
. O
this O
is O
the O
simplest O
example O
of O
an O
inference B
problem O
for O
a O
graphical B
model I
. O
we O
have O
thereby O
accepted O
an O
increased O
computational O
cost O
during O
the O
training B
phase O
in O
order O
to O
have O
a O
model O
that O
is O
faster O
at O
making O
predictions O
. O
( O
6.69 O
) O
section O
3.5 O
for O
nonlinear O
optimization O
, O
we O
also O
need O
the O
gradient O
of O
the O
log O
likelihood O
func- O
tion O
with O
respect O
to O
the O
parameter O
vector O
θ. O
we O
shall O
assume O
that O
evaluation O
of O
the O
derivatives O
of O
cn O
is O
straightforward O
, O
as O
would O
be O
the O
case O
for O
the O
covariance B
func- O
tions O
considered O
in O
this O
chapter O
. O
statistical O
decision O
theory B
and O
bayesian O
analysis O
( O
second O
ed. O
) O
. O
regularization B
allows O
complex O
models O
to O
be O
trained O
on O
data O
sets O
of O
limited O
size O
without O
severe O
over-ﬁtting B
, O
essentially O
by O
limiting O
the O
effective O
model O
complexity O
. O
( O
c.31 O
) O
next O
we O
take O
the O
complex O
conjugate B
of O
( O
c.29 O
) O
and O
left O
multiply O
by O
ut O
i O
to O
give O
i O
au O
( O
cid:3 O
) O
ut O
i O
= O
λ O
( O
cid:3 O
) O
i O
ut O
i O
u O
( O
cid:3 O
) O
i O
. O
note O
that O
the O
gaussian O
noise O
assumption O
implies O
that O
the O
conditional B
distribution O
of O
t O
given O
x O
is O
unimodal O
, O
which O
may O
be O
inappropriate O
for O
some O
applications O
. O
after O
nor- O
malization O
the O
resulting O
values O
for O
q O
( O
m O
) O
can O
be O
used O
for O
model O
selection O
or O
model B
averaging I
in O
the O
usual O
way O
. O
clearly O
, O
for O
a O
graph O
comprising O
a O
variable O
root B
node I
connected O
directly O
to O
several O
factor O
leaf O
nodes O
, O
the O
algorithm O
trivially O
involves O
sending O
messages O
of O
the O
form O
( O
8.71 O
) O
directly O
from O
the O
leaves O
to O
the O
root O
. O
we O
see O
that O
, O
for O
a O
value O
of O
ln O
λ O
= O
−18 O
, O
the O
over-ﬁtting B
has O
been O
suppressed O
and O
we O
now O
obtain O
a O
much O
closer O
representation O
of O
the O
underlying O
function O
sin O
( O
2πx O
) O
. O
sion B
for O
the O
f O
→ O
z O
messages O
of O
the O
form O
we O
can O
eliminate O
µzn−1→fn O
( O
zn−1 O
) O
from O
( O
13.48 O
) O
using O
( O
13.47 O
) O
to O
give O
a O
recur- O
( O
cid:2 O
) O
µfn→zn O
( O
zn O
) O
= O
fn O
( O
zn−1 O
, O
zn O
) O
µfn−1→zn−1 O
( O
zn−1 O
) O
. O
then O
in O
the O
subsequent O
variational B
equivalent O
of O
the O
m O
step O
, O
we O
keep O
these O
responsibilities O
ﬁxed O
and O
use O
them O
to O
re-compute O
the O
variational B
distribution O
over O
the O
parameters O
using O
( O
10.57 O
) O
and O
( O
10.59 O
) O
. O
1 O
2 O
n O
( O
cid:2 O
) O
n=1 O
e O
( O
w O
) O
= O
1 O
2 O
{ O
tn O
− O
wtφ O
( O
xn O
) O
} O
2 O
( O
3.25 O
) O
( O
3.26 O
) O
( O
3.27 O
) O
then O
the O
total O
error B
function I
becomes O
n O
( O
cid:2 O
) O
n=1 O
1 O
2 O
{ O
tn O
− O
wtφ O
( O
xn O
) O
} O
2 O
+ O
λ O
2 O
wtw O
. O
( O
5.176 O
) O
we O
see O
that O
this O
takes O
the O
same O
form O
as O
the O
corresponding O
result O
( O
3.86 O
) O
for O
the O
linear B
regression I
model O
. O
to O
do O
this O
, O
we O
note O
that O
p O
( O
t O
= O
1|y O
) O
= O
σ O
( O
y O
) O
where O
y O
( O
x O
) O
is O
given O
by O
( O
7.1 O
) O
, O
and O
σ O
( O
y O
) O
is O
the O
logistic B
sigmoid I
function O
deﬁned O
by O
( O
4.59 O
) O
. O
now O
consider O
a O
probability B
density O
px O
( O
x O
) O
that O
corresponds O
to O
a O
density B
py O
( O
y O
) O
with O
respect O
to O
the O
new O
variable O
y O
, O
where O
the O
sufﬁces O
denote O
the O
fact O
that O
px O
( O
x O
) O
and O
py O
( O
y O
) O
are O
different O
densities O
. O
( O
b O
) O
first O
solve O
the O
inference B
problem O
of O
determining O
the O
posterior O
class O
probabilities O
p O
( O
ck|x O
) O
, O
and O
then O
subsequently O
use O
decision B
theory I
to O
assign O
each O
new O
x O
to O
one O
of O
the O
classes O
. O
note O
that O
again O
this O
is O
an O
improper B
prior I
because O
the O
integral O
of O
the O
distribution O
over O
0 O
( O
cid:1 O
) O
σ O
( O
cid:1 O
) O
∞ O
is O
divergent O
. O
the O
resulting O
numerical O
difﬁculties O
can O
be O
addressed O
using O
the O
technique O
of O
singular B
value I
decomposition I
, O
or O
svd O
( O
press O
et O
al. O
, O
1992 O
; O
bishop O
and O
nabney O
, O
2008 O
) O
. O
thus O
the O
ex- O
ponential O
error B
function I
will O
be O
much O
less O
robust O
to O
outliers B
or O
misclassiﬁed O
data O
points O
. O
to O
see O
that O
this O
is O
indeed O
conjugate B
, O
let O
us O
multiply O
the O
prior B
( O
2.229 O
) O
by O
the O
likelihood B
function I
( O
2.227 O
) O
to O
obtain O
the O
posterior O
distribution O
, O
up O
to O
a O
nor- O
malization O
coefﬁcient O
, O
in O
the O
form O
( O
cid:24 O
) O
( O
cid:22 O
) O
n O
( O
cid:2 O
) O
( O
cid:23 O
) O
( O
cid:25 O
) O
2.4. O
the O
exponential B
family I
117 O
( O
cid:5 O
) O
p O
( O
η|x O
, O
χ O
, O
ν O
) O
∝ O
g O
( O
η O
) O
ν+n O
exp O
ηt O
u O
( O
xn O
) O
+ O
νχ O
. O
for O
a O
ﬁnite O
integration O
time O
, O
the O
individual O
data O
points O
will O
be O
perturbed O
away O
from O
the O
manifold B
by O
the O
photon B
noise I
. O
indeed O
, O
we O
have O
chosen O
to O
use O
the O
same O
notation O
for O
the O
covariance B
matrix I
sn O
in O
both O
cases O
. O
some O
of O
the O
data O
was O
collected O
from O
census O
bureau O
employees O
and O
the O
rest O
was O
collected O
from O
high-school O
children O
, O
and O
care O
was O
taken O
to O
ensure O
that O
the O
test O
examples O
were O
written O
by O
different O
individuals O
to O
the O
training B
examples O
. O
in O
doing O
so O
it O
is O
useful O
to O
note O
that O
on O
the O
right-hand O
side O
we O
only O
need O
to O
retain O
those O
terms O
that O
have O
some O
functional B
dependence O
on O
z1 O
because O
all O
other O
terms O
can O
be O
absorbed O
into O
the O
normalization O
constant O
. O
we O
have O
introduced O
gaussian O
process O
regression B
for O
the O
case O
of O
a O
single O
tar- O
get O
variable O
. O
this O
is O
much O
like O
the O
classical B
problem O
of O
tomographic O
re- O
construction O
, O
used O
in O
medical O
imaging O
for O
example O
, O
in O
which O
a O
two-dimensional O
dis- O
figure O
a.3 O
cross O
section O
of O
the O
pipe O
showing O
the O
arrangement O
of O
the O
six O
beam O
lines O
, O
each O
of O
which O
comprises O
a O
single O
dual- O
energy O
gamma O
densitometer O
. O
for O
histograms O
, O
section O
1.4 O
122 O
2. O
probability B
distributions O
this O
neighbourhood O
property O
was O
deﬁned O
by O
the O
bins O
, O
and O
there O
is O
a O
natural O
‘ O
smooth- O
ing O
’ O
parameter O
describing O
the O
spatial O
extent O
of O
the O
local B
region O
, O
in O
this O
case O
the O
bin O
width O
. O
these O
notions O
are O
incorporated O
into O
convolutional O
neural O
networks O
through O
three O
mechanisms O
: O
( O
i O
) O
local O
receptive O
ﬁelds O
, O
( O
ii O
) O
weight B
sharing I
, O
and O
( O
iii O
) O
subsampling B
. O
the O
model O
described O
here O
involves O
a O
prior B
only O
over O
the O
matrix O
w. O
a O
fully O
bayesian O
treatment O
of O
pca O
, O
including O
priors O
over O
1- O
'' O
, O
a O
2 O
, O
and O
n O
, O
and O
solved O
us O
( O
cid:173 O
) O
ing O
variational O
methods O
, O
is O
described O
in O
bishop O
( O
1999b O
) O
. O
each O
such O
gaussian O
has O
an O
independent B
variance O
governed O
by O
a O
precision O
hyperparameter O
o O
: O
i O
so O
that O
( O
12.60 O
) O
where O
wi O
is O
the O
i O
th O
column O
of O
w. O
the O
resulting O
model O
can O
be O
represented O
using O
the O
directed B
graph O
shown O
in O
figure O
12.13. O
the O
values O
for O
o O
: O
i O
will O
be O
found O
iteratively O
by O
maximizing O
the O
marginallikeli O
( O
cid:173 O
) O
hood O
function O
in O
which O
w O
has O
been O
integrated O
out O
. O
we O
can O
also O
consider O
joint O
probability B
distributions O
over O
a O
combination O
of O
discrete O
and O
continuous O
variables O
. O
an O
important O
limitation O
of O
this O
approach O
is O
that O
the O
chosen O
density B
might O
be O
a O
poor O
model O
of O
the O
distribution O
that O
generates O
the O
data O
, O
which O
can O
result O
in O
poor O
predictive O
performance O
. O
figure O
3.7 O
shows O
the O
results O
of O
bayesian O
learning B
in O
this O
model O
as O
the O
size O
of O
the O
data O
set O
is O
increased O
and O
demonstrates O
the O
sequential O
nature O
of O
bayesian O
learning B
in O
which O
the O
current O
posterior O
distribution O
forms O
the O
prior B
when O
a O
new O
data O
point O
is O
observed O
. O
hierarchical B
dirichlet O
processes O
. O
( O
2002 O
) O
, O
and O
for O
extensions O
to O
modelling O
conditional B
densities O
for B
regression I
problems O
, O
see O
bishop O
and O
nabney O
( O
1996 O
) O
. O
note O
that O
the O
{ O
zn O
} O
, O
as O
well O
as O
the O
missing B
data I
values O
that O
are O
components O
of O
the O
vectors O
{ O
x O
n O
} O
, O
are O
now O
latent O
variables O
. O
because O
the O
distribution O
q O
( O
w O
) O
depends O
on O
q O
( O
α O
) O
only O
through O
the O
expectation B
e O
[ O
α O
] O
, O
we O
see O
that O
the O
two O
approaches O
will O
give O
identical O
results O
for O
the O
case O
of O
an O
inﬁnitely O
broad O
prior B
. O
how- O
ever O
, O
the O
error B
function I
typically O
has O
a O
highly O
nonlinear O
dependence O
on O
the O
weights O
and O
bias B
parameters O
, O
and O
so O
there O
will O
be O
many O
points O
in O
weight O
space O
at O
which O
the O
gradient O
vanishes O
( O
or O
is O
numerically O
very O
small O
) O
. O
we O
then O
calculate O
the O
conjugate B
function O
and O
then O
transform O
back O
to O
the O
original O
variables O
. O
the O
ﬁrst O
term O
, O
called O
the O
squared O
bias B
, O
represents O
the O
extent O
to O
which O
the O
average O
prediction O
over O
all O
data O
sets O
differs O
from O
the O
desired O
regression B
function I
. O
( O
1.125 O
) O
1.6 O
( O
( O
cid:1 O
) O
) O
show O
that O
if O
two O
variables O
x O
and O
y O
are O
independent B
, O
then O
their O
covariance B
is O
zero O
. O
wtw O
( O
10.177 O
) O
( O
10.178 O
) O
( O
10.179 O
) O
10.7. O
expectation B
propagation I
505 O
maximizing O
the O
lower B
bound I
( O
cid:4 O
) O
l O
( O
q O
, O
ξ O
) O
. O
similarly O
, O
for O
the O
combination O
of O
the O
logistic B
sigmoid I
activation O
function O
and O
the O
cross-entropy B
error I
function I
( O
4.90 O
) O
, O
and O
for O
the O
softmax O
activation O
function O
with O
the O
multiclass B
cross-entropy O
error B
function I
( O
4.108 O
) O
, O
we O
again O
obtain O
this O
same O
simple O
form O
. O
we O
have O
seen O
that O
the O
bayesian O
framework O
avoids O
the O
problem O
of O
over-ﬁtting B
and O
allows O
models O
to O
be O
compared O
on O
the O
basis O
of O
the O
training B
data O
alone O
. O
the O
ﬁrst O
part O
of O
the O
digit O
, O
which O
starts O
at O
the O
top O
left O
, O
has O
a O
sweeping O
arc B
down O
to O
the O
cusp O
or O
loop O
at O
the O
bottom O
left O
, O
followed O
by O
a O
second O
more- O
or-less O
straight O
sweep O
ending O
at O
the O
bottom O
right O
. O
neural O
networks O
use O
basis O
functions O
that O
follow O
the O
same O
form O
as O
( O
5.1 O
) O
, O
so O
that O
each O
basis B
function I
is O
itself O
a O
nonlinear O
function O
of O
a O
linear O
combination O
of O
the O
inputs O
, O
where O
the O
coefﬁcients O
in O
the O
linear O
combination O
are O
adaptive O
parameters O
. O
methods O
based O
on O
changing O
w O
using O
the O
gradi- O
ent O
of O
the O
error B
function I
can O
not O
then O
be O
applied O
, O
because O
the O
gradient O
is O
zero O
almost O
everywhere O
. O
( O
4.84 O
) O
note O
that O
we O
are O
allowing O
each O
class O
to O
have O
its O
own O
parameter O
vector O
λk O
but O
we O
are O
assuming O
that O
the O
classes O
share O
the O
same O
scale B
parameter I
s. O
for O
the O
two-class O
problem O
, O
we O
substitute O
this O
expression O
for O
the O
class-conditional O
densities O
into O
( O
4.58 O
) O
and O
we O
see O
that O
the O
posterior O
class O
probability B
is O
again O
given O
by O
a O
logistic B
sigmoid I
acting O
on O
a O
linear O
function O
a O
( O
x O
) O
which O
is O
given O
by O
a O
( O
x O
) O
= O
( O
λ1 O
− O
λ2 O
) O
tx O
+ O
ln O
g O
( O
λ1 O
) O
− O
ln O
g O
( O
λ2 O
) O
+ O
ln O
p O
( O
c1 O
) O
− O
ln O
p O
( O
c2 O
) O
. O
various O
schemes O
have O
been O
proposed O
for O
speeding O
up O
the O
k-means O
algorithm O
, O
some O
of O
which O
are O
based O
on O
precomputing O
a O
data O
structure O
such O
as O
a O
tree B
such O
that O
nearby O
points O
are O
in O
the O
same O
subtree O
( O
ramasubramanian O
and O
paliwal O
, O
1990 O
; O
moore O
, O
2000 O
) O
. O
use O
these O
results O
, O
together O
with O
the O
identity O
( O
5.88 O
) O
, O
to O
ﬁnd O
sequential O
update O
expressions O
analogous O
to O
( O
5.89 O
) O
for O
ﬁnding O
the O
inverse B
of O
the O
hessian O
by O
incrementally O
including O
both O
extra O
patterns O
and O
extra O
outputs O
. O
( O
5.146 O
) O
exercise O
5.32 O
the O
derivatives O
of O
the O
regularized O
error O
function O
with O
respect O
to O
the O
{ O
ηj O
} O
then O
take O
the O
form O
272 O
5. O
neural O
networks O
figure O
5.18 O
the O
left O
ﬁgure O
shows O
a O
two-link O
robot B
arm I
, O
in O
which O
the O
cartesian O
coordinates O
( O
x1 O
, O
x2 O
) O
of O
the O
end O
ef- O
fector O
are O
determined O
uniquely O
by O
the O
two O
joint O
angles O
θ1 O
and O
θ2 O
and O
the O
( O
ﬁxed O
) O
lengths O
l1 O
and O
l2 O
of O
the O
arms O
. O
the O
outgoing O
message O
µα O
( O
xn O
) O
in O
( O
8.55 O
) O
is O
obtained O
by O
multiplying O
the O
incoming O
message O
µα O
( O
xn−1 O
) O
by O
the O
local B
potential O
involving O
the O
node B
variable O
and O
the O
outgoing O
variable O
and O
then O
summing O
over O
the O
node B
variable O
. O
a O
single O
evaluation O
of O
the O
error B
function I
( O
for O
a O
given O
input O
pattern O
) O
would O
require O
o O
( O
w O
) O
operations O
, O
for O
sufﬁciently O
large O
w O
. O
a O
mathematical O
theory B
of O
communication O
. O
( O
cid:2 O
) O
( O
cid:2 O
) O
i O
i O
vthv O
= O
c2 O
i O
λi O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
∂2e O
∂w2 O
exercise O
5.10 O
exercise O
5.11 O
and O
so O
h O
will O
be O
positive B
deﬁnite I
if O
, O
and O
only O
if O
, O
all O
of O
its O
eigenvalues O
are O
positive O
. O
we O
shall O
suppose O
that O
the O
image O
is O
obtained O
by O
taking O
an O
unknown O
noise-free O
image O
, O
described O
by O
binary O
pixel O
values O
xi O
∈ O
{ O
−1 O
, O
+1 O
} O
and O
randomly O
ﬂipping O
the O
sign O
of O
pixels O
with O
some O
small O
probability B
. O
because O
only O
one O
variable O
is O
changed O
, O
this O
is O
a O
simple O
local B
computation O
that O
can O
be O
performed O
efﬁciently O
. O
for O
the O
remainder O
of O
this O
chapter O
, O
we O
shall O
consider O
a O
particular O
form O
of O
gaus- O
sian O
prior B
in O
order O
to O
simplify O
the O
treatment O
. O
now O
consider O
the O
evaluation O
of O
the O
derivative B
of O
en O
with O
respect O
to O
a O
weight O
wji O
. O
similarly O
, O
if O
the O
latent B
variable I
is O
evolving O
slowly O
relative B
to O
the O
observation O
noise O
level O
, O
we O
ﬁnd O
that O
the O
posterior O
mean O
for O
zn O
is O
obtained O
by O
averaging O
all O
of O
the O
measurements O
obtained O
up O
to O
that O
time O
. O
the O
probability B
of O
observing O
the O
value O
xi O
is O
then O
p O
( O
xi O
) O
∆ O
. O
( O
cid:2 O
) O
( O
cid:2 O
) O
zj O
= O
h O
( O
aj O
) O
, O
( O
5.203 O
) O
with O
the O
operator O
g O
, O
show O
that O
ωn O
can O
be O
evaluated O
by O
forward B
propagation I
using O
the O
following O
equations O
: O
wjizi O
aj O
= O
i O
αj O
= O
h O
( O
cid:4 O
) O
( O
aj O
) O
βj O
, O
βj O
= O
wjiαi O
. O
! O
efficiently O
o O
, O
ing O
the O
em O
algorithm_ O
the O
resolting O
gtm O
model O
his O
a O
lwo-dimensional O
nonlinear O
manifold B
10 O
tile O
dala O
sel O
. O
( O
13.81 O
) O
( O
13.82 O
) O
( O
13.83 O
) O
the O
parameters O
of O
the O
model O
, O
denoted O
by O
θ O
= O
{ O
a O
, O
γ O
, O
c O
, O
σ O
, O
µ0 O
, O
v0 O
} O
, O
can O
be O
determined O
using O
maximum B
likelihood I
through O
the O
em O
algorithm O
. O
( O
4.99 O
) O
( O
4.100 O
) O
we O
see O
that O
the O
update O
formula O
( O
4.99 O
) O
takes O
the O
form O
of O
a O
set O
of O
normal B
equations I
for O
a O
weighted O
least-squares O
problem O
. O
assuming O
that O
the O
data O
points O
are O
drawn O
independently O
from O
this O
model O
, O
show O
that O
the O
maximum-likelihood O
solution O
for O
the O
prior B
probabilities O
is O
given O
by O
where O
nk O
is O
the O
number O
of O
data O
points O
assigned O
to O
class O
ck O
. O
we O
shall O
assume O
that O
the O
original O
graph O
is O
an O
undirected B
tree O
or O
a O
directed B
tree O
or O
polytree B
, O
so O
that O
the O
corresponding O
factor B
graph I
has O
a O
tree B
structure O
. O
the O
result O
of O
the O
ﬁrst O
summation O
over O
zn O
can O
be O
stored O
and O
used O
once O
the O
value O
of O
xn O
+1 O
is O
observed O
in O
order O
to O
run O
the O
α B
recursion I
forward O
to O
the O
next O
step O
in O
order O
to O
predict O
the O
subsequent O
value O
xn O
+2 O
. O
this O
is O
illustrated O
by O
considering O
the O
sinusoidal B
data I
set O
from O
chapter O
1. O
here O
we O
generate O
100 O
data O
sets O
, O
each O
containing O
n O
= O
25 O
data O
points O
, O
independently O
from O
the O
sinusoidal O
curve O
h O
( O
x O
) O
= O
sin O
( O
2πx O
) O
. O
thus O
we O
write O
down O
the O
complete-data O
log O
likelihood O
and O
take O
its O
expectation B
with O
respect O
to O
the O
posterior O
distribution O
of O
the O
latent O
distribution O
evaluated O
using O
'old O
' O
parameter O
values O
. O
we O
now O
observe O
data O
point O
xn O
, O
and O
we O
obtain O
our O
revised O
estimate O
µ O
( O
n O
) O
ml O
by O
moving O
the O
old O
estimate O
a O
small O
amount O
, O
proportional O
to O
1/n O
, O
in O
the O
direction O
of O
the O
‘ O
error B
signal O
’ O
( O
xn O
− O
µ O
( O
n−1 O
) O
) O
. O
extending O
the O
model O
to O
allow O
more O
general O
gaussian O
latent O
distributions O
does O
not O
change O
this O
conclusion O
because O
, O
as O
we O
have O
seen O
, O
such O
a O
model O
is O
equivalent O
to O
the O
zero-mean O
isotropic B
gaussian O
latent B
variable I
model O
. O
( O
4.160 O
) O
show O
that O
the O
maximum B
likelihood I
solution O
for O
the O
mean B
of O
the O
gaussian O
distribution O
for O
class O
ck O
is O
given O
by O
tnkφn O
( O
4.161 O
) O
n O
( O
cid:2 O
) O
n=1 O
µk O
= O
1 O
nk O
222 O
4. O
linear O
models O
for O
classification O
which O
represents O
the O
mean B
of O
those O
feature O
vectors O
assigned O
to O
class O
ck O
. O
on O
the O
right O
is O
the O
corresponding O
posterior B
probability I
p O
( O
c1|x O
) O
, O
which O
is O
given O
by O
a O
logistic B
sigmoid I
of O
a O
linear O
function O
of O
x. O
the O
surface O
in O
the O
right-hand O
plot O
is O
coloured O
using O
a O
proportion O
of O
red O
ink O
given O
by O
p O
( O
c1|x O
) O
and O
a O
proportion O
of O
blue O
ink O
given O
by O
p O
( O
c2|x O
) O
= O
1 O
− O
p O
( O
c1|x O
) O
. O
many O
error B
functions O
of O
practical O
interest O
, O
for O
instance O
those O
deﬁned O
by O
maxi- O
mum O
likelihood O
for O
a O
set O
of O
i.i.d O
. O
6.23 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
gaussian O
process O
regression B
model O
in O
which O
the O
target O
variable O
t O
has O
dimensionality O
d. O
write O
down O
the O
conditional B
distribution O
of O
tn O
+1 O
for O
a O
test O
input O
vector O
xn O
+1 O
, O
given O
a O
training B
set I
of O
input O
vectors O
x1 O
, O
. O
from O
the O
product B
rule I
, O
we O
see O
that O
p O
( O
y O
|x O
) O
= O
p O
( O
y O
) O
, O
and O
so O
the O
conditional B
distribution O
of O
y O
given O
x O
is O
indeed O
independent B
of O
the O
value O
of O
x. O
for O
instance O
, O
in O
our O
boxes O
of O
fruit O
example O
, O
if O
each O
box O
contained O
the O
same O
fraction O
of O
apples O
and O
oranges O
, O
then O
p O
( O
f|b O
) O
= O
p O
( O
f O
) O
, O
so O
that O
the O
probability B
of O
selecting O
, O
say O
, O
an O
apple O
is O
independent B
of O
which O
box O
is O
chosen O
. O
n O
( O
d O
, O
m O
) O
= O
( O
1.137 O
) O
to O
do O
this O
, O
ﬁrst O
show O
that O
the O
result O
is O
true O
for O
m O
= O
2 O
, O
and O
any O
value O
of O
d O
( O
cid:2 O
) O
1 O
, O
by O
comparison O
with O
the O
result O
of O
exercise O
1.14. O
then O
make O
use O
of O
( O
1.135 O
) O
, O
together O
with O
( O
1.136 O
) O
, O
to O
show O
that O
, O
if O
the O
result O
holds O
at O
order O
m O
− O
1 O
, O
then O
it O
will O
also O
hold O
at O
order O
m O
1.16 O
( O
( O
cid:1 O
) O
( O
cid:1 O
) O
( O
cid:1 O
) O
) O
in O
exercise O
1.15 O
, O
we O
proved O
the O
result O
( O
1.135 O
) O
for O
the O
number O
of O
independent B
parameters O
in O
the O
m O
th O
order O
term O
of O
a O
d-dimensional O
polynomial O
. O
similarly O
show O
that O
the O
factor O
qτ O
( O
τ O
) O
is O
a O
gamma B
distribution I
of O
the O
form O
gam O
( O
τ|an O
, O
bn O
) O
with O
parameters O
given O
by O
( O
10.29 O
) O
and O
( O
10.30 O
) O
. O
using O
d-separation B
, O
we O
note O
that O
there O
is O
a O
unique O
path O
from O
any O
xi O
to O
any O
other O
xj O
( O
cid:9 O
) O
=i O
and O
that O
this O
path O
is O
tail-to-tail O
with O
respect O
to O
the O
observed O
node O
µ. O
every O
such O
path O
is O
blocked O
and O
so O
the O
observations O
d O
= O
{ O
x1 O
, O
. O
the O
selection O
of O
which O
pairs O
of O
cliques O
to O
connect O
in O
this O
way O
is O
important O
and O
is O
done O
so O
as O
to O
give O
a O
maximal B
spanning I
tree I
deﬁned O
as O
follows O
. O
5.8 O
( O
( O
cid:12 O
) O
) O
we O
saw O
in O
( O
4.88 O
) O
that O
the O
derivative B
of O
the O
logistic B
sigmoid I
activation O
function O
can O
be O
expressed O
in O
terms O
of O
the O
function O
value O
itself O
. O
( O
13.56 O
) O
628 O
13. O
sequential B
data I
from O
the O
product B
rule I
, O
we O
then O
have O
n O
( O
cid:14 O
) O
m=1 O
cm O
( O
cid:22 O
) O
n O
( O
cid:14 O
) O
p O
( O
x1 O
, O
. O
by O
contrast O
, O
the O
polynomial O
regression O
model O
described O
by O
figure O
8.5 O
is O
not O
generative O
because O
there O
is O
no O
probability B
distribution O
associated O
with O
the O
input O
variable O
x O
, O
and O
so O
it O
is O
not O
possible O
to O
generate O
synthetic O
data O
points O
from O
this O
model O
. O
the O
use O
of O
probability B
to O
represent O
uncertainty O
, O
however O
, O
is O
not O
an O
ad-hoc O
choice O
, O
but O
is O
inevitable O
if O
we O
are O
to O
respect O
common O
sense O
while O
making O
rational O
coherent O
inferences O
. O
show O
that O
maximizing O
this O
lower B
bound I
with O
respect O
to O
the O
mixing O
coefﬁcients O
, O
using O
a O
lagrange O
multiplier O
to O
enforce O
the O
constraint O
that O
the O
mixing O
coefﬁcients O
sum O
to O
one O
, O
leads O
to O
the O
re-estimation O
result O
( O
10.83 O
) O
. O
from O
the O
product B
rule I
of I
probability I
we O
have O
p O
( O
x O
, O
ck O
) O
= O
p O
( O
ck|x O
) O
p O
( O
x O
) O
. O
unfortunately O
, O
there O
are O
some O
signiﬁcant O
shortcomings O
with O
linear O
models O
, O
which O
will O
cause O
us O
to O
turn O
in O
later O
chapters O
to O
more O
complex O
models O
such O
as O
support B
vector I
machines O
and O
neural O
networks O
. O
if O
we O
consider O
the O
likelihood B
function I
( O
3.10 O
) O
, O
then O
the O
conjugate B
prior I
for O
w O
and O
β O
is O
given O
by O
−1s0 O
) O
gam O
( O
β|a0 O
, O
b0 O
) O
. O
various O
approximation O
schemes O
have O
been O
used O
to O
evaluate O
the O
hessian O
matrix O
for O
a O
neural B
network I
. O
12.23 O
( O
* O
) O
iii O
! O
i O
draw O
a O
directed B
probabilistic O
graphical B
model I
representing O
a O
discrete O
mixture O
of O
probabilistic O
pca O
models O
in O
which O
each O
pca O
model O
has O
its O
own O
values O
of O
w O
, O
jl O
, O
and O
0- O
• O
now O
draw O
a O
modified O
graph O
in O
which O
these O
parameter O
values O
are O
shared O
between O
the O
components O
of O
the O
mixture B
. O
because O
the O
change O
to O
one O
variable O
is O
a O
function O
only O
of O
the O
other O
, O
any O
region O
in O
phase B
space I
will O
be O
sheared O
without O
change O
of O
volume O
. O
b. O
probability B
distributions O
693 O
von O
mises O
the O
von O
mises O
distribution O
, O
also O
known O
as O
the O
circular B
normal I
or O
the O
circular O
gaus- O
sian O
, O
is O
a O
univariate O
gaussian-like O
periodic O
distribution O
for O
a O
variable O
θ O
∈ O
[ O
0 O
, O
2π O
) O
. O
this O
follows O
from O
the O
shape O
of O
the O
error B
surface O
and O
the O
widely O
differing O
exercise O
5.25 O
decay O
. O
2.1.1 O
the O
beta B
distribution I
. O
( O
5.78 O
) O
note O
that O
it O
is O
sometimes O
convenient O
to O
consider O
all O
of O
the O
weight O
and O
bias B
parameters O
as O
elements O
wi O
of O
a O
single O
vector O
, O
denoted O
w O
, O
in O
which O
case O
the O
second O
derivatives O
form O
the O
elements O
hij O
of O
the O
hessian O
matrix O
h O
, O
where O
i O
, O
j O
∈ O
{ O
1 O
, O
. O
furthermore O
, O
there O
is O
no O
over-ﬁtting B
if O
we O
choose O
a O
large O
number O
k O
of O
components O
in O
the O
mixture B
, O
as O
we O
saw O
in O
fig- O
ure O
10.6. O
finally O
, O
the O
variational B
treatment O
opens O
up O
the O
possibility O
of O
determining O
the O
optimal O
number O
of O
components O
in O
the O
mixture B
without O
resorting O
to O
techniques O
such O
as O
cross O
validation O
. O
14 O
combining B
models I
in O
earlier O
chapters O
, O
we O
have O
explored O
a O
range O
of O
different O
models O
for O
solving O
classiﬁ- O
cation O
and O
regression B
problems O
. O
then O
we O
initialize O
all O
of O
the O
clique B
potentials O
of O
the O
moral O
graph O
to O
1. O
we O
then O
take O
each O
conditional B
distribution O
factor O
in O
the O
original O
directed B
graph O
and O
multiply O
it O
into O
one O
of O
the O
clique B
potentials O
. O
if O
x O
has O
dimensionality O
m O
and O
y O
has O
dimensionality O
d O
, O
then O
the O
matrix O
a O
has O
size O
d O
× O
m. O
first O
we O
ﬁnd O
an O
expression O
for O
the O
joint O
distribution O
over O
x O
and O
y. O
to O
do O
this O
, O
we O
deﬁne O
z O
= O
( O
cid:15 O
) O
( O
cid:16 O
) O
x O
y O
( O
2.99 O
) O
( O
2.100 O
) O
( O
2.101 O
) O
and O
then O
consider O
the O
log O
of O
the O
joint O
distribution O
ln O
p O
( O
z O
) O
= O
ln O
p O
( O
x O
) O
+ O
ln O
p O
( O
y|x O
) O
= O
−1 O
2 O
−1 O
2 O
( O
x O
− O
µ O
) O
tλ O
( O
x O
− O
µ O
) O
( O
y O
− O
ax O
− O
b O
) O
tl O
( O
y O
− O
ax O
− O
b O
) O
+ O
const O
( O
2.102 O
) O
where O
‘ O
const O
’ O
denotes O
terms O
independent B
of O
x O
and O
y. O
as O
before O
, O
we O
see O
that O
this O
is O
a O
quadratic O
function O
of O
the O
components O
of O
z O
, O
and O
hence O
p O
( O
z O
) O
is O
gaussian O
distribution O
. O
this O
factorized O
form O
of O
variational B
inference I
corresponds O
to O
an O
ap- O
proximation B
framework O
developed O
in O
physics O
called O
mean B
ﬁeld I
theory I
( O
parisi O
, O
1988 O
) O
. O
12 O
continuous O
latent O
variables O
12.1 O
principal B
component I
analysis I
. O
10.9 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
making O
use O
of O
the O
standard O
result O
e O
[ O
τ O
] O
= O
an O
/bn O
for O
the O
mean B
of O
a O
gamma B
distribution I
, O
together O
with O
( O
10.26 O
) O
, O
( O
10.27 O
) O
, O
( O
10.29 O
) O
, O
and O
( O
10.30 O
) O
, O
derive O
the O
result O
( O
10.33 O
) O
for O
the O
reciprocal O
of O
the O
expected O
precision O
in O
the O
factorized O
variational O
treat- O
ment O
of O
a O
univariate O
gaussian O
. O
in O
writing O
down O
( O
5.55 O
) O
, O
we O
are O
making O
use O
of O
the O
fact O
that O
variations O
in O
aj O
give O
rise O
to O
variations O
in O
the O
error B
func- O
tion O
only O
through O
variations O
in O
the O
variables O
ak O
. O
learning B
in O
graphical O
models O
. O
if O
we O
know O
the O
posterior O
probabilities O
, O
we O
can O
trivially O
revise O
the O
minimum B
risk I
decision O
criterion O
by O
modifying O
( O
1.81 O
) O
appropriately O
. O
in O
the O
m O
step O
, O
we O
then O
use O
this O
variational B
posterior O
to O
compute O
a O
new O
value O
for O
ξ O
given O
by O
( O
10.163 O
) O
. O
one O
of O
the O
most O
powerful O
properties O
of O
hidden O
markov O
models O
is O
their O
ability O
to O
exhibit O
some O
degree O
of O
invariance B
to O
local B
warping O
( O
compression O
and O
stretching O
) O
of O
the O
time O
axis O
. O
in O
fact O
, O
as O
we O
shall O
see O
, O
the O
issue O
of O
bias B
in O
maximum B
likelihood I
lies O
at O
the O
root O
of O
the O
over-ﬁtting B
problem O
that O
we O
encountered O
earlier O
in O
the O
context O
of O
polynomial B
curve I
ﬁtting I
. O
this O
transformation O
cor- O
( O
2 O
) O
responds O
to O
the O
second O
layer O
of O
the O
network O
, O
and O
again O
the O
w O
k0 O
are O
bias B
parameters O
. O
note O
that O
for O
two O
classes O
, O
we O
can O
either O
employ O
the O
formalism O
discussed O
here O
, O
based O
on O
two O
discriminant O
functions O
y1 O
( O
x O
) O
and O
y2 O
( O
x O
) O
, O
or O
else O
use O
the O
simpler O
but O
equivalent O
formulation O
described O
in O
section O
4.1.1 O
based O
on O
a O
single O
discriminant B
function I
y O
( O
x O
) O
. O
5.33 O
( O
( O
cid:12 O
) O
) O
write O
down O
a O
pair O
of O
equations O
that O
express O
the O
cartesian O
coordinates O
( O
x1 O
, O
x2 O
) O
for O
the O
robot B
arm I
shown O
in O
figure O
5.18 O
in O
terms O
of O
the O
joint O
angles O
θ1 O
and O
θ2 O
and O
the O
lengths O
l1 O
and O
l2 O
of O
the O
links O
. O
( O
10.172 O
) O
next O
we O
assume O
that O
the O
variational B
distribution O
factorizes O
between O
parameters O
and O
hyperparameters O
so O
that O
q O
( O
w O
, O
α O
) O
= O
q O
( O
w O
) O
q O
( O
α O
) O
. O
if O
we O
now O
observe O
c O
, O
as O
in O
figure O
8.18 O
, O
then O
this O
observation O
‘ O
blocks O
’ O
the O
path O
from O
a O
to O
b O
and O
so O
we O
obtain O
the O
conditional B
independence I
property O
a O
⊥⊥ O
b O
| O
c. O
finally O
, O
we O
consider O
the O
third O
of O
our O
3-node O
examples O
, O
shown O
by O
the O
graph O
in O
figure O
8.19. O
as O
we O
shall O
see O
, O
this O
has O
a O
more O
subtle O
behaviour O
than O
the O
two O
previous O
graphs O
. O
comparison O
with O
the O
results O
shown O
in O
figure O
7.4 O
for O
the O
corresponding O
support B
vector I
machine I
shows O
that O
the O
rvm O
gives O
a O
much O
sparser O
model O
. O
5.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
by O
considering O
the O
local B
taylor O
expansion O
( O
5.32 O
) O
of O
an O
error B
function I
about O
a O
stationary B
point O
w O
( O
cid:1 O
) O
, O
show O
that O
the O
necessary O
and O
sufﬁcient O
condition O
for O
the O
stationary B
point O
to O
be O
a O
local B
minimum I
of O
the O
error B
function I
is O
that O
the O
hessian O
matrix O
h O
, O
deﬁned O
by O
( O
5.30 O
) O
with O
( O
cid:1 O
) O
w O
= O
w O
( O
cid:1 O
) O
, O
be O
positive B
deﬁnite I
. O
( O
1.59 O
) O
n O
( O
cid:2 O
) O
n=1 O
28 O
1. O
introduction O
figure O
1.15 O
illustration O
of O
how O
bias B
arises O
in O
using O
max- O
imum O
likelihood O
to O
determine O
the O
variance B
of O
a O
gaussian O
. O
this O
is O
straightforward O
because O
the O
log O
of O
the O
envelope O
distribution O
is O
a O
succession O
exercise O
11.9 O
figure O
11.6 O
in O
the O
case O
of O
distributions O
that O
are O
log O
concave O
, O
an O
envelope O
function O
for O
use O
in O
rejection B
sampling I
can O
be O
constructed O
using O
the O
tangent O
lines O
computed O
at O
a O
set O
of O
grid O
points O
. O
these O
are O
examples O
of O
memory-based B
methods I
that O
involve O
storing O
the O
entire O
training B
set I
in O
order O
to O
make O
predictions O
for O
future O
data O
points O
. O
the O
likelihood B
function I
is O
obtained O
from O
the O
joint O
distribution O
( O
13.10 O
) O
by O
marginalizing O
over O
the O
latent O
variables O
( O
cid:2 O
) O
p O
( O
x|θ O
) O
= O
p O
( O
x O
, O
z|θ O
) O
. O
outliers B
can O
arise O
in O
practical O
applications O
either O
because O
the O
process O
that O
generates O
the O
data O
corresponds O
to O
a O
distribution O
having O
a O
heavy O
tail O
or O
simply O
through O
mislabelled O
data O
. O
appendix O
e O
appendix O
e. O
lagrange O
multipliers O
lagrange O
multipliers O
, O
also O
sometimes O
called O
undetermined O
multipliers O
, O
are O
used O
to O
ﬁnd O
the O
stationary B
points O
of O
a O
function O
of O
several O
variables O
subject O
to O
one O
or O
more O
constraints O
. O
this O
leads O
to O
a O
probabilislic O
fonnulation O
of O
the O
well-known O
technique O
of O
principal B
component I
analysis I
( O
pea O
) O
, O
as O
well O
as O
10 O
a O
related O
model O
called O
factor B
analysis I
. O
however O
, O
training B
the O
network O
now O
involves O
a O
nonlinear O
optimization O
problem O
, O
since O
the O
error B
function I
( O
12.91 O
) O
is O
no O
longer O
a O
quadratic O
function O
of O
the O
network O
parameters O
. O
the O
relevance B
vector I
machine I
or O
rvm O
( O
tipping O
, O
2001 O
) O
is O
a O
bayesian O
sparse O
ker- O
nel O
technique O
for B
regression I
and O
classiﬁcation B
that O
shares O
many O
of O
the O
characteristics O
of O
the O
svm O
whilst O
avoiding O
its O
principal O
limitations O
. O
5.5.5 O
training B
with O
transformed O
data O
we O
have O
seen O
that O
one O
way O
to O
encourage O
invariance B
of O
a O
model O
to O
a O
set O
of O
trans- O
formations O
is O
to O
expand O
the O
training B
set I
using O
transformed O
versions O
of O
the O
original O
input O
patterns O
. O
( O
9.11 O
) O
exercise O
9.3 O
the O
joint O
distribution O
is O
given O
by O
p O
( O
z O
) O
p O
( O
x|z O
) O
, O
and O
the O
marginal B
distribution O
of O
x O
is O
then O
obtained O
by O
summing O
the O
joint O
distribution O
over O
all O
possible O
states O
of O
z O
to O
give O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
p O
( O
x O
) O
= O
p O
( O
z O
) O
p O
( O
x|z O
) O
= O
πkn O
( O
x|µk O
, O
σk O
) O
( O
9.12 O
) O
z O
k=1 O
( O
cid:5 O
) O
where O
we O
have O
made O
use O
of O
( O
9.10 O
) O
and O
( O
9.11 O
) O
. O
we O
consider O
a O
generative B
model I
in O
which O
there O
are O
two O
latent O
variables O
corresponding O
to O
the O
unobserved O
speech O
signal O
amplitudes O
, O
and O
there O
are O
two O
observed O
variables O
given O
by O
the O
signal O
values O
at O
the O
microphones O
. O
although O
we O
have O
considered O
the O
marginal B
probability I
p O
( O
x O
) O
, O
the O
same O
consid- O
erations O
apply O
for O
the O
predictive O
density O
p O
( O
x|x O
) O
or O
for O
conditional O
distributions O
such O
as O
p O
( O
t|x O
, O
x O
, O
t O
) O
. O
464 O
10. O
approximate O
inference B
1 O
0.8 O
0.6 O
0.4 O
0.2 O
0 O
−2 O
−1 O
0 O
1 O
2 O
3 O
4 O
40 O
30 O
20 O
10 O
0 O
−2 O
−1 O
0 O
1 O
2 O
3 O
4 O
figure O
10.1 O
illustration O
of O
the O
variational B
approximation O
for O
the O
example O
considered O
earlier O
in O
figure O
4.14. O
the O
left-hand O
plot O
shows O
the O
original O
distribution O
( O
yellow O
) O
along O
with O
the O
laplace O
( O
red O
) O
and O
variational B
( O
green O
) O
approx- O
imations O
, O
and O
the O
right-hand O
plot O
shows O
the O
negative O
logarithms O
of O
the O
corresponding O
curves O
. O
12.9 O
( O
* O
) O
verify O
that O
maximizing O
the O
log O
likelihood O
( O
12.43 O
) O
for O
the O
probabilistic O
pca O
model O
with O
respect O
to O
the O
parameter O
jl O
gives O
the O
result O
jlml O
= O
x O
where O
x O
is O
the O
mean B
of O
the O
data O
vectors O
. O
5.4.4 O
finite O
differences O
as O
in O
the O
case O
of O
the O
ﬁrst O
derivatives O
of O
the O
error B
function I
, O
we O
can O
ﬁnd O
the O
second O
derivatives O
by O
using O
ﬁnite B
differences I
, O
with O
accuracy O
limited O
by O
numerical O
precision O
. O
later O
we O
shall O
give O
a O
general O
treatment O
of O
em O
, O
and O
we O
shall O
also O
show O
how O
em O
can O
be O
generalized B
to O
obtain O
the O
variational B
inference I
framework O
. O
the O
second O
term O
is O
the O
variance B
of O
the O
distribution O
of O
t O
, O
averaged O
over O
x. O
it O
represents O
the O
intrinsic O
variability O
of O
the O
target O
data O
and O
can O
be O
regarded O
as O
noise O
. O
similarly O
, O
undirected B
graphs O
, O
given O
by O
( O
8.39 O
) O
, O
are O
a O
special O
case O
in O
which O
the O
factors O
are O
po- O
tential O
functions O
over O
the O
maximal O
cliques O
( O
the O
normalizing O
coefﬁcient O
1/z O
can O
be O
viewed O
as O
a O
factor O
deﬁned O
over O
the O
empty O
set O
of O
variables O
) O
. O
498 O
10. O
approximate O
inference B
although O
the O
bound O
σ O
( O
a O
) O
( O
cid:2 O
) O
f O
( O
a O
, O
ξ O
) O
on O
the O
logistic B
sigmoid I
can O
be O
optimized O
exactly O
, O
the O
required O
choice O
for O
ξ O
depends O
on O
the O
value O
of O
a O
, O
so O
that O
the O
bound O
is O
exact O
for O
one O
value O
of O
a O
only O
. O
a O
training B
algorithm O
for O
optimal O
margin B
classi- O
ﬁers O
. O
to O
do O
this O
, O
show O
that O
the O
integral O
of O
the O
probability B
density O
over O
a O
thin O
shell O
of O
radius O
r O
and O
thickness O
 O
, O
where O
 O
( O
cid:12 O
) O
1 O
, O
is O
given O
by O
p O
( O
r O
) O
 O
where O
( O
cid:16 O
) O
p O
( O
r O
) O
= O
sdrd−1 O
p O
( O
r O
) O
has O
a O
single O
stationary B
point O
located O
, O
for O
large O
d O
, O
at O
( O
cid:1 O
) O
r O
( O
cid:8 O
) O
√ O
p O
( O
( O
cid:1 O
) O
r O
+ O
 O
) O
where O
 O
( O
cid:12 O
) O
( O
cid:1 O
) O
r O
, O
show O
that O
for O
large O
d O
, O
p O
( O
( O
cid:1 O
) O
r O
+ O
 O
) O
= O
p O
( O
( O
cid:1 O
) O
r O
) O
exp O
which O
shows O
that O
( O
cid:1 O
) O
r O
is O
a O
maximum O
of O
the O
radial O
probability O
density B
and O
also O
that O
p O
( O
r O
) O
decays O
exponentially O
away O
from O
its O
maximum O
at O
( O
cid:1 O
) O
r O
with O
length O
scale O
σ. O
we O
have O
already O
seen O
that O
σ O
( O
cid:12 O
) O
( O
cid:1 O
) O
r O
for O
large O
d O
, O
and O
so O
we O
see O
that O
most O
of O
the O
probability B
density O
p O
( O
x O
) O
is O
larger O
at O
the O
origin O
than O
at O
the O
radius O
( O
cid:1 O
) O
r O
by O
a O
factor O
of O
exp O
( O
d/2 O
) O
. O
if O
we O
model O
the O
h O
( O
x O
) O
using O
a O
parametric O
function O
y O
( O
x O
, O
w O
) O
governed O
by O
a O
pa- O
rameter O
vector O
w O
, O
then O
from O
a O
bayesian O
perspective O
the O
uncertainty O
in O
our O
model O
is O
expressed O
through O
a O
posterior O
distribution O
over O
w. O
a O
frequentist B
treatment O
, O
however O
, O
involves O
making O
a O
point O
estimate O
of O
w O
based O
on O
the O
data O
set O
d O
, O
and O
tries O
instead O
to O
interpret O
the O
uncertainty O
of O
this O
estimate O
through O
the O
following O
thought O
experi- O
ment O
. O
t O
tn O
y O
( O
xn O
, O
w O
) O
xn O
x O
exercise O
1.1 O
function O
y O
( O
x O
, O
w O
) O
were O
to O
pass O
exactly O
through O
each O
training B
data O
point O
. O
if O
we O
con- O
sider O
a O
proper O
prior B
and O
then O
take O
a O
suitable O
limit O
in O
order O
to O
obtain O
an O
improper B
prior I
( O
for O
example O
, O
a O
gaussian O
prior B
in O
which O
we O
take O
the O
limit O
of O
inﬁnite O
variance B
) O
then O
the O
evidence O
will O
go O
to O
zero O
, O
as O
can O
be O
seen O
from O
( O
3.70 O
) O
and O
figure O
3.12. O
it O
may O
, O
however O
, O
be O
possible O
to O
consider O
the O
evidence O
ratio O
between O
two O
models O
ﬁrst O
and O
then O
take O
a O
limit O
to O
obtain O
a O
meaningful O
answer O
. O
suppose O
we O
wish O
to O
evaluate O
an O
integral O
of O
the O
form O
( O
cid:6 O
) O
i O
= O
σ O
( O
a O
) O
p O
( O
a O
) O
da O
( O
10.145 O
) O
where O
σ O
( O
a O
) O
is O
the O
logistic B
sigmoid I
, O
and O
p O
( O
a O
) O
is O
a O
gaussian O
probability B
density O
. O
in O
the O
variational B
e O
step O
, O
we O
evaluate O
the O
expected O
sufﬁcient B
statistics I
e O
[ O
u O
( O
xn O
, O
zn O
) O
] O
using O
the O
current O
posterior O
distribution O
q O
( O
zn O
) O
over O
the O
latent O
variables O
and O
use O
this O
to O
compute O
a O
revised O
posterior O
distribution O
q O
( O
η O
) O
over O
the O
parameters O
. O
10.39 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
mean B
and O
variance B
of O
qnew O
( O
θ O
) O
for O
ep O
applied O
to O
the O
clutter B
problem I
are O
given O
by O
( O
10.217 O
) O
and O
( O
10.218 O
) O
. O
4.9 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
generative O
classiﬁcation O
model O
for O
k O
classes O
deﬁned O
by O
prior B
class O
probabilities O
p O
( O
ck O
) O
= O
πk O
and O
general O
class-conditional O
densities O
p O
( O
φ|ck O
) O
where O
φ O
is O
the O
input O
feature O
vector O
. O
now O
suppose O
that O
the O
variables O
x1 O
and O
x2 O
were O
independent B
, O
corresponding O
to O
the O
graphical B
model I
shown O
in O
figure O
8.9 O
( O
b O
) O
. O
the O
effective B
number I
of I
parameters I
that O
are O
determined O
by O
the O
data O
is O
γ O
, O
with O
the O
remaining O
m O
−γ O
parameters O
set O
to O
small O
values O
by O
the O
prior B
. O
10.32 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
the O
variational B
treatment O
of O
logistic B
regression I
with O
sequen- O
tial O
learning B
in O
which O
data O
points O
are O
arriving O
one O
at O
a O
time O
and O
each O
must O
be O
pro- O
cessed O
and O
discarded O
before O
the O
next O
data O
point O
arrives O
. O
stochastic B
re- O
laxation O
, O
gibbs O
distributions O
, O
and O
the O
bayesian O
restoration O
of O
images O
. O
an O
example O
of O
this O
approach O
, O
in O
which O
the O
variational B
distribution O
is O
a O
gaussian O
and O
we O
have O
optimized O
with O
respect O
to O
its O
mean B
and O
variance B
, O
is O
shown O
in O
figure O
10.1 O
. O
( O
4.15 O
) O
( O
4.16 O
) O
( O
4.17 O
) O
exercise O
4.2 O
section O
2.3.7 O
an O
interesting O
property O
of O
least-squares O
solutions O
with O
multiple O
target O
variables O
is O
that O
if O
every O
target B
vector I
in O
the O
training B
set I
satisﬁes O
some O
linear O
constraint O
attn O
+ O
b O
= O
0 O
( O
4.18 O
) O
for O
some O
constants O
a O
and O
b O
, O
then O
the O
model O
prediction O
for O
any O
value O
of O
x O
will O
satisfy O
the O
same O
constraint O
so O
that O
aty O
( O
x O
) O
+ O
b O
= O
0 O
. O
359 O
360 O
8. O
graphical O
models O
3. O
complex O
computations O
, O
required O
to O
perform O
inference B
and O
learning B
in O
sophis- O
ticated O
models O
, O
can O
be O
expressed O
in O
terms O
of O
graphical O
manipulations O
, O
in O
which O
underlying O
mathematical O
expressions O
are O
carried O
along O
implicitly O
. O
for O
the O
purposes O
of O
image O
restoration O
, O
we O
wish O
to O
ﬁnd O
an O
image O
x O
having O
a O
high O
probability B
( O
ideally O
the O
maximum O
probability O
) O
. O
( O
3.40 O
) O
( O
bias B
) O
2 O
variance B
we O
see O
that O
the O
expected O
squared O
difference O
between O
y O
( O
x O
; O
d O
) O
and O
the O
regression B
function I
h O
( O
x O
) O
can O
be O
expressed O
as O
the O
sum O
of O
two O
terms O
. O
the O
blue O
ellipse O
around O
each O
data O
point O
shows O
one O
standard B
deviation I
contour O
for O
the O
corresponding O
kernel O
. O
next O
we O
take O
the O
expectation B
of O
the O
complete-data O
log O
likelihood O
with O
respect O
to O
the O
posterior O
distribution O
of O
the O
latent O
variables O
to O
give O
i=1 O
( O
cid:24 O
) O
ez O
[ O
ln O
p O
( O
x O
, O
z|µ O
, O
π O
) O
] O
= O
γ O
( O
znk O
) O
ln O
πk O
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
n=1 O
k=1 O
+ O
[ O
xni O
ln O
µki O
+ O
( O
1 O
− O
xni O
) O
ln O
( O
1 O
− O
µki O
) O
] O
( O
9.55 O
) O
( O
cid:25 O
) O
( O
cid:25 O
) O
d O
( O
cid:2 O
) O
d O
( O
cid:2 O
) O
i=1 O
where O
γ O
( O
znk O
) O
= O
e O
[ O
znk O
] O
is O
the O
posterior B
probability I
, O
or O
responsibility B
, O
of O
component O
k O
given O
data O
point O
xn O
. O
( O
1.119 O
) O
kl O
( O
p O
( O
cid:6 O
) O
q O
) O
( O
cid:8 O
) O
n O
( O
cid:2 O
) O
n=1 O
the O
second O
term O
on O
the O
right-hand O
side O
of O
( O
1.119 O
) O
is O
independent B
of O
θ O
, O
and O
the O
ﬁrst O
term O
is O
the O
negative O
log O
likelihood O
function O
for O
θ O
under O
the O
distribution O
q O
( O
x|θ O
) O
eval- O
uated O
using O
the O
training B
set I
. O
similarly O
, O
show O
that O
the O
responsibilities O
become O
equal O
to O
the O
corresponding O
maximum B
likelihood I
values O
for O
large O
n O
, O
by O
making O
use O
of O
the O
following O
asymptotic O
result O
for O
the O
digamma B
function I
for O
large O
x O
ψ O
( O
x O
) O
= O
ln O
x O
+ O
o O
( O
1/x O
) O
. O
next O
we O
write O
down O
the O
likelihood B
function I
. O
514 O
10. O
approximate O
inference B
posterior O
mean B
100 O
r O
o O
r O
r O
e O
10−5 O
laplace O
vb O
104 O
ep O
106 O
flops O
evidence O
vb O
10−200 O
r O
o O
r O
r O
e O
10−202 O
laplace O
10−204 O
104 O
106 O
flops O
ep O
figure O
10.17 O
comparison O
of O
expectation B
propagation I
, O
variational B
inference I
, O
and O
the O
laplace O
approximation O
on O
the O
clutter B
problem I
. O
taking O
the O
negative O
logarithm O
then O
leads O
to O
a O
regularization B
function O
of O
the O
form O
( O
cid:2 O
) O
( O
cid:22 O
) O
m O
( O
cid:2 O
) O
( O
cid:23 O
) O
ω O
( O
w O
) O
= O
− O
ln O
πjn O
( O
wi|µj O
, O
σ2 O
j O
) O
. O
we O
can O
determine O
the O
form O
of O
this O
distribution O
by O
transforming O
from O
cartesian O
coordinates O
( O
x1 O
, O
x2 O
) O
to O
polar O
coordinates O
( O
r O
, O
θ O
) O
so O
that O
x2 O
= O
r O
sin O
θ. O
we O
also O
map O
the O
mean B
µ O
into O
polar O
coordinates O
by O
writing O
x1 O
= O
r O
cos O
θ O
, O
µ1 O
= O
r0 O
cos O
θ0 O
, O
µ2 O
= O
r0 O
sin O
θ0 O
. O
the O
variational B
framework O
for O
gaussian O
process O
classiﬁcation B
can O
also O
be O
extended B
to O
multiclass B
( O
k O
> O
2 O
) O
problems O
by O
using O
a O
gaussian O
approximation O
to O
the O
softmax B
function I
( O
gibbs O
, O
1997 O
) O
. O
again O
we O
make O
use O
of O
the O
sum O
and O
product O
rules O
together O
with O
the O
conditional B
independence I
properties O
( O
13.29 O
) O
and O
( O
13.31 O
) O
giving O
p O
( O
xn O
+1|x O
) O
= O
= O
= O
= O
= O
= O
zn+1 O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
zn+1 O
zn+1 O
zn+1 O
zn+1 O
1 O
p O
( O
x O
) O
p O
( O
xn O
+1 O
, O
zn O
+1|x O
) O
p O
( O
xn O
+1|zn O
+1 O
) O
p O
( O
zn O
+1|x O
) O
p O
( O
xn O
+1|zn O
+1 O
) O
p O
( O
xn O
+1|zn O
+1 O
) O
p O
( O
zn O
+1 O
, O
zn|x O
) O
p O
( O
zn O
+1|zn O
) O
p O
( O
zn|x O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
zn O
zn O
zn O
p O
( O
xn O
+1|zn O
+1 O
) O
( O
cid:2 O
) O
p O
( O
zn O
+1|zn O
) O
p O
( O
zn O
, O
x O
) O
p O
( O
x O
) O
( O
cid:2 O
) O
p O
( O
xn O
+1|zn O
+1 O
) O
p O
( O
zn O
+1|zn O
) O
α O
( O
zn O
) O
( O
13.44 O
) O
zn+1 O
zn O
which O
can O
be O
evaluated O
by O
ﬁrst O
running O
a O
forward O
α O
recursion O
and O
then O
computing O
the O
ﬁnal O
summations O
over O
zn O
and O
zn O
+1 O
. O
the O
centre O
plot O
shows O
the O
result O
of O
standardizing B
the O
individual O
variables O
to O
zero O
mean B
and O
unit O
variance B
. O
we O
have O
seen O
that O
a O
directed B
graph O
of O
linear-gaussian O
units O
is O
equivalent O
to O
a O
joint O
gaussian O
distribution O
over O
all O
of O
the O
variables O
. O
substitute O
these O
into O
( O
10.70 O
) O
and O
hence O
obtain O
the O
lower B
bound I
as O
a O
function O
of O
the O
parameters O
of O
the O
varia- O
tional O
distribution O
. O
a O
more O
complete O
analysis O
( O
faul O
and O
tipping O
, O
2002 O
) O
, O
based O
on O
the O
second O
derivatives O
of O
the O
marginal B
likelihood I
, O
conﬁrms O
these O
solutions O
are O
indeed O
the O
unique O
maxima O
of O
λ O
( O
αi O
) O
. O
1.2. O
probability B
theory O
31 O
in O
the O
curve B
ﬁtting I
problem O
, O
we O
are O
given O
the O
training B
data O
x O
and O
t O
, O
along O
with O
a O
new O
test O
point O
x O
, O
and O
our O
goal O
is O
to O
predict O
the O
value O
of O
t. O
we O
therefore O
wish O
to O
evaluate O
the O
predictive B
distribution I
p O
( O
t|x O
, O
x O
, O
t O
) O
. O
a O
more O
efﬁcient O
version O
of O
numerical O
differentiation O
can O
be O
found O
by O
applying O
central B
differences I
to O
the O
ﬁrst O
derivatives O
of O
the O
error B
function I
, O
which O
are O
themselves O
calculated O
using O
backpropagation B
. O
for O
a O
d-dimensional O
variable O
x O
, O
student O
’ O
s O
t-distribution O
corresponds O
to O
marginal- O
izing O
the O
precision B
matrix I
of O
a O
multivariate O
gaussian O
with O
respect O
to O
a O
conjugate B
wishart O
prior B
and O
takes O
the O
form O
( O
cid:29 O
) O
( O
cid:30 O
) O
−ν/2−d/2 O
st O
( O
x|µ O
, O
λ O
, O
ν O
) O
= O
γ O
( O
ν/2 O
+ O
d/2 O
) O
γ O
( O
ν/2 O
) O
|λ|1/2 O
( O
νπ O
) O
d/2 O
1 O
+ O
∆2 O
ν O
e O
[ O
x O
] O
= O
µ O
for O
ν O
> O
1 O
cov O
[ O
x O
] O
= O
mode O
[ O
x O
] O
= O
µ O
−1 O
λ O
ν O
ν O
− O
2 O
for O
ν O
> O
2 O
where O
∆2 O
is O
the O
squared O
mahalanobis O
distance O
deﬁned O
by O
∆2 O
= O
( O
x O
− O
µ O
) O
tλ O
( O
x O
− O
µ O
) O
. O
5.3. O
error B
backpropagation I
our O
goal O
in O
this O
section O
is O
to O
ﬁnd O
an O
efﬁcient O
technique O
for O
evaluating O
the O
gradient O
of O
an O
error B
function I
e O
( O
w O
) O
for O
a O
feed-forward O
neural B
network I
. O
if O
the O
desired O
output O
consists O
of O
one O
or O
more O
continuous O
variables O
, O
then O
the O
task O
is O
called O
regression B
. O
f O
( O
cid:19 O
) O
now O
consider O
the O
log O
likelihood O
function O
for O
this O
model O
, O
which O
, O
as O
a O
function O
of O
η O
, O
is O
given O
by O
( O
cid:20 O
) O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
ln O
p O
( O
t|η O
, O
s O
) O
= O
ln O
p O
( O
tn|η O
, O
s O
) O
= O
n=1 O
n=1 O
ln O
g O
( O
ηn O
) O
+ O
ηntn O
s O
+ O
const O
( O
4.121 O
) O
where O
we O
are O
assuming O
that O
all O
observations O
share O
a O
common O
scale B
parameter I
( O
which O
corresponds O
to O
the O
noise O
variance B
for O
a O
gaussian O
distribution O
for O
instance O
) O
and O
so O
s O
is O
independent B
of O
n. O
the O
derivative B
of O
the O
log O
likelihood O
with O
respect O
to O
the O
model O
parameters O
w O
is O
then O
given O
by O
( O
cid:13 O
) O
∇w O
ln O
p O
( O
t|η O
, O
s O
) O
= O
= O
d O
dηn O
ln O
g O
( O
ηn O
) O
+ O
tn O
s O
dηn O
dyn O
dyn O
dan O
∇an O
{ O
tn O
− O
yn O
} O
ψ O
( O
cid:4 O
) O
( O
yn O
) O
f O
( O
cid:4 O
) O
( O
an O
) O
φn O
1 O
s O
( O
4.122 O
) O
( O
cid:12 O
) O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n=1 O
where O
an O
= O
wtφn O
, O
and O
we O
have O
used O
yn O
= O
f O
( O
an O
) O
together O
with O
the O
result O
( O
4.119 O
) O
for O
e O
[ O
t|η O
] O
. O
we O
have O
seen O
in O
chapter O
1 O
that O
probability B
theory O
can O
be O
expressed O
in O
terms O
of O
two O
simple O
equations O
corresponding O
to O
the O
sum B
rule I
and O
the O
product B
rule I
. O
in O
particular O
, O
the O
functional B
form O
of O
the O
factors O
q O
( O
z O
) O
and O
q O
( O
π O
, O
µ O
, O
λ O
) O
will O
be O
determined O
automatically O
by O
optimization O
of O
the O
variational B
distribution O
. O
0.5 O
p O
( O
z O
) O
0.25 O
0 O
−5 O
0 O
z O
5 O
of O
linear O
functions O
, O
and O
hence O
the O
envelope O
distribution O
itself O
comprises O
a O
piecewise O
exponential B
distribution I
of O
the O
form O
q O
( O
z O
) O
= O
kiλi O
exp O
{ O
−λi O
( O
z O
− O
zi−1 O
) O
} O
zi−1 O
< O
z O
( O
cid:1 O
) O
zi O
. O
we O
see O
that O
this O
can O
be O
viewed O
as O
a O
prior B
distribution O
, O
which O
is O
combined O
using O
bayes O
’ O
theorem O
with O
the O
likelihood B
function I
associated O
with O
data O
point O
xn O
to O
arrive O
at O
the O
posterior O
distribution O
after O
observing O
n O
data O
points O
. O
−1 O
2 O
26 O
1. O
introduction O
figure O
1.14 O
illustration O
of O
the O
likelihood B
function I
for O
a O
gaussian O
distribution O
, O
shown O
by O
the O
red O
curve O
. O
note O
that O
the O
addition O
of O
a O
regularization B
term O
ensures O
that O
the O
matrix O
is O
non- O
singular O
, O
even O
in O
the O
presence O
of O
degeneracies O
. O
14.5 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
committee B
in O
which O
we O
allow O
unequal O
weighting O
of O
the O
constituent O
models O
, O
so O
that O
ycom O
( O
x O
) O
= O
m O
( O
cid:2 O
) O
m O
( O
cid:2 O
) O
m=1 O
αmym O
( O
x O
) O
. O
if O
we O
choose O
m O
= O
d O
- O
1 O
then O
, O
if O
all O
ai O
values O
are O
finite O
, O
the O
model O
represents O
a O
full-covariance O
gaussian O
, O
while O
if O
all O
the O
ai O
go O
to O
infinity O
the O
model O
is O
equivalent O
to O
an O
isotropic B
gaussian O
, O
and O
so O
the O
model O
can O
encompass O
all O
pennissible O
values O
for O
the O
effective O
dimensionality O
of O
the O
principal B
subspace I
. O
provided O
the O
transformation O
is O
continuous O
( O
such O
as O
translation O
or O
rotation O
, O
but O
not O
mirror O
reﬂection O
for O
instance O
) O
, O
then O
the O
transformed O
pattern O
will O
sweep O
out O
a O
manifold B
m O
within O
the O
d-dimensional O
input O
space O
. O
here O
we O
turn O
to O
a O
discussion O
of O
decision B
theory I
that O
, O
when O
combined O
with O
probability B
theory O
, O
allows O
us O
to O
make O
optimal O
decisions O
in O
situations O
involving O
uncertainty O
such O
as O
those O
encountered O
in O
pattern O
recognition O
. O
statistical O
field O
theory B
. O
one O
consequence O
of O
this O
result O
is O
that O
the O
binomial B
distribution I
( O
2.9 O
) O
, O
which O
is O
a O
distribution O
over O
m O
deﬁned O
by O
the O
sum O
of O
n O
observations O
of O
the O
random O
binary O
variable O
x O
, O
will O
tend O
to O
a O
gaussian O
as O
n O
→ O
∞ O
( O
see O
figure O
2.1 O
for O
the O
case O
of O
n O
= O
10 O
) O
. O
we O
shall O
, O
212 O
4. O
linear O
models O
for O
classification O
however O
, O
ﬁnd O
another O
use O
for O
the O
probit O
model O
when O
we O
discuss O
bayesian O
treatments O
of O
logistic B
regression I
in O
section O
4.5. O
one O
issue O
that O
can O
occur O
in O
practical O
applications O
is O
that O
of O
outliers B
, O
which O
can O
arise O
for O
instance O
through O
errors O
in O
measuring O
the O
input O
vector O
x O
or O
through O
misla- O
belling O
of O
the O
target O
value O
t. O
because O
such O
points O
can O
lie O
a O
long O
way O
to O
the O
wrong O
side O
of O
the O
ideal O
decision B
boundary I
, O
they O
can O
seriously O
distort O
the O
classiﬁer O
. O
although O
such O
distributions O
are O
relatively O
simple O
, O
they O
form O
useful O
building O
blocks O
for O
constructing O
more O
complex O
probability B
section O
2.4 O
8.1. O
bayesian O
networks O
367 O
figure O
8.9 O
( O
a O
) O
this O
fully-connected O
graph O
describes O
a O
general O
distribu- O
tion O
over O
two O
k-state O
discrete O
variables O
having O
a O
total O
of O
k O
2 O
− O
1 O
parameters O
. O
because O
the O
kullback-leibler O
divergence O
satisﬁes O
kl O
( O
q O
( O
cid:3 O
) O
p O
) O
( O
cid:2 O
) O
0 O
, O
we O
see O
that O
the O
quan- O
tity O
l O
( O
q O
, O
θ O
) O
is O
a O
lower B
bound I
on O
the O
log O
likelihood O
function O
ln O
p O
( O
x|θ O
) O
. O
td-gammon O
, O
a O
self-teaching O
backgammon B
program O
, O
achieves O
master-level O
play O
. O
by O
application O
of O
the O
product B
rule I
of I
probability I
( O
1.11 O
) O
, O
we O
can O
write O
the O
joint O
distribution O
in O
the O
form O
p O
( O
a O
, O
b O
, O
c O
) O
= O
p O
( O
c|a O
, O
b O
) O
p O
( O
a O
, O
b O
) O
. O
in O
that O
example O
, O
bayes O
’ O
theorem O
was O
used O
to O
convert O
a O
prior B
probability O
into O
a O
posterior B
probability I
by O
incorporating O
the O
evidence O
provided O
by O
the O
observed O
data O
. O
the O
evaluation O
of O
the O
probability B
maximum O
can O
be O
written O
as O
max O
x O
p O
( O
x O
) O
= O
max O
x1 O
( O
cid:29 O
) O
1 O
z O
··· O
max O
xn O
( O
cid:29 O
) O
= O
1 O
z O
max O
x1 O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
[ O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
··· O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
] O
··· O
max O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
. O
( O
1.93 O
) O
x O
this O
important O
quantity O
is O
called O
the O
entropy B
of O
the O
random O
variable O
x. O
note O
that O
limp→0 O
p O
ln O
p O
= O
0 O
and O
so O
we O
shall O
take O
p O
( O
x O
) O
ln O
p O
( O
x O
) O
= O
0 O
whenever O
we O
encounter O
a O
value O
for O
x O
such O
that O
p O
( O
x O
) O
= O
0. O
so O
far O
we O
have O
given O
a O
rather O
heuristic O
motivation O
for O
the O
deﬁnition O
of O
informa- O
50 O
1. O
introduction O
tion O
( O
1.92 O
) O
and O
the O
corresponding O
entropy B
( O
1.93 O
) O
. O
66 O
1. O
introduction O
1.40 O
( O
( O
cid:1 O
) O
) O
by O
applying O
jensen O
’ O
s O
inequality O
( O
1.115 O
) O
with O
f O
( O
x O
) O
= O
ln O
x O
, O
show O
that O
the O
arith- O
metic O
mean B
of O
a O
set O
of O
real O
numbers O
is O
never O
less O
than O
their O
geometrical O
mean B
. O
we O
can O
the O
introduce O
the O
concept O
of O
a O
functional B
derivative O
, O
which O
ex- O
presses O
how O
the O
value O
of O
the O
functional B
changes O
in O
response O
to O
inﬁnitesimal O
changes O
to O
the O
input O
function O
( O
feynman O
et O
al. O
, O
1964 O
) O
. O
in O
one O
dimension O
, O
the O
wishart O
reduces O
to O
the O
gamma B
distribution I
gam O
( O
λ|a O
, O
b O
) O
given O
by O
( O
b.26 O
) O
with O
parameters O
a O
= O
ν/2 O
and O
b O
= O
1/2w O
. O
it O
can O
also O
be O
applied O
to O
error B
functions O
other O
that O
just O
the O
simple O
sum-of-squares O
, O
and O
to O
the O
eval- O
242 O
5. O
neural O
networks O
uation O
of O
other O
derivatives O
such O
as O
the O
jacobian O
and O
hessian O
matrices O
, O
as O
we O
shall O
see O
later O
in O
this O
chapter O
. O
not O
surprisingly O
, O
this O
leads O
to O
a O
much O
simpler O
solution O
to O
the O
maximum B
likelihood I
problem O
, O
as O
we O
now O
show O
. O
by O
contrast O
, O
variables O
such O
as O
µ O
that O
are O
outside O
the O
plate B
are O
ﬁxed O
in O
number O
independently O
of O
the O
size O
of O
the O
data O
set O
, O
and O
so O
are O
regarded O
as O
parameters O
. O
in O
fact O
, O
bayes O
only O
formulated O
his O
theory B
for O
the O
case O
of O
a O
uni- O
form O
prior B
, O
and O
it O
was O
pierre-simon O
laplace O
who O
inde- O
pendently O
rediscovered O
the O
theory B
in O
general O
form O
and O
who O
demonstrated O
its O
broad O
applicability O
. O
derive O
the O
corresponding O
result O
for O
the O
‘ O
tanh O
’ O
activation B
function I
deﬁned O
by O
( O
5.59 O
) O
. O
figure O
2.1 O
shows O
a O
plot O
of O
the O
binomial B
distribution I
for O
n O
= O
10 O
and O
µ O
= O
0.25. O
the O
mean B
and O
variance B
of O
the O
binomial B
distribution I
can O
be O
found O
by O
using O
the O
result O
of O
exercise O
1.10 O
, O
which O
shows O
that O
for O
independent O
events O
the O
mean B
of O
the O
sum O
is O
the O
sum O
of O
the O
means O
, O
and O
the O
variance B
of O
the O
sum O
is O
the O
sum O
of O
the O
variances O
. O
288 O
5. O
neural O
networks O
components O
of O
the O
weight B
vector I
parallel O
to O
the O
eigenvectors O
of O
the O
hessian O
satisfy O
( O
τ O
) O
j O
( O
cid:7 O
) O
w O
( O
cid:1 O
) O
| O
( O
cid:13 O
) O
|w O
( O
cid:1 O
) O
j O
when O
ηj O
( O
cid:12 O
) O
( O
ρτ O
) O
−1 O
j| O
when O
ηj O
( O
cid:13 O
) O
( O
ρτ O
) O
−1 O
. O
mt O
( O
9.63 O
) O
exercise O
9.21 O
an O
analogous O
result O
holds O
for O
β. O
note O
that O
this O
re-estimation O
equation O
takes O
a O
slightly O
different O
form O
from O
the O
corresponding O
result O
( O
3.92 O
) O
derived O
by O
direct O
evaluation O
of O
the O
evidence B
function I
. O
as O
in O
our O
discussion O
of O
em O
, O
we O
can O
decompose O
the O
log O
marginal O
probability B
using O
ln O
p O
( O
x O
) O
= O
l O
( O
q O
) O
+ O
kl O
( O
q O
( O
cid:5 O
) O
p O
) O
( O
cid:6 O
) O
( O
cid:12 O
) O
where O
we O
have O
deﬁned O
l O
( O
q O
) O
= O
( O
cid:13 O
) O
( O
cid:13 O
) O
dz O
dz O
. O
( O
5.24 O
) O
236 O
5. O
neural O
networks O
figure O
5.5 O
geometrical O
view O
of O
the O
error B
function I
e O
( O
w O
) O
as O
a O
surface O
sitting O
over O
weight O
space O
. O
this O
sum O
provides O
an O
example O
of O
a O
sufﬁcient O
statistic O
for O
the O
data O
under O
this O
distribution O
, O
and O
we O
shall O
study O
the O
impor- O
tant O
role O
of O
sufﬁcient B
statistics I
in O
some O
detail O
. O
in O
particular O
, O
suppose O
that O
we O
designate O
a O
particular O
variable O
node B
as O
the O
‘ O
root O
’ O
of O
the O
graph O
. O
for O
a O
d-dimensional O
vector O
x O
, O
the O
multivariate O
gaussian O
distribution O
takes O
the O
form O
−1 O
2 O
( O
2.43 O
) O
where O
µ O
is O
a O
d-dimensional O
mean B
vector O
, O
σ O
is O
a O
d O
× O
d O
covariance B
matrix I
, O
and O
|σ| O
denotes O
the O
determinant O
of O
σ O
. O
similarly O
, O
the O
covariance B
of O
the O
blue O
gaussian O
is O
set O
equal O
to O
the O
covariance B
of O
the O
blue O
ink O
. O
note O
that O
in O
all O
cases O
the O
partition B
function I
is O
given O
by O
z O
= O
1. O
the O
process O
of O
converting O
a O
directed B
graph O
into O
an O
undirected B
graph I
plays O
an O
important O
role O
in O
exact O
inference O
techniques O
such O
as O
the O
junction B
tree I
algorithm I
. O
so O
, O
for O
instance O
, O
if O
q O
( O
z O
) O
is O
a O
gaussian O
n O
( O
z|µ O
, O
σ O
) O
then O
we O
minimize O
the O
kullback-leibler O
divergence O
by O
setting O
the O
mean B
µ O
of O
q O
( O
z O
) O
equal O
to O
the O
mean B
of O
the O
distribution O
p O
( O
z O
) O
and O
the O
covariance B
σ O
equal O
to O
the O
covariance B
of O
p O
( O
z O
) O
. O
( O
3.37 O
) O
recall O
that O
the O
second O
term O
, O
which O
is O
independent B
of O
y O
( O
x O
) O
, O
arises O
from O
the O
intrinsic O
noise O
on O
the O
data O
and O
represents O
the O
minimum O
achievable O
value O
of O
the O
expected O
loss O
. O
1 O
z2 O
−1 O
−1 O
z1 O
1 O
528 O
11. O
sampling B
methods I
y1 O
= O
z1 O
y2 O
= O
z2 O
( O
cid:16 O
) O
1/2 O
( O
cid:16 O
) O
1/2 O
r2 O
( O
cid:15 O
) O
−2 O
ln O
z1 O
( O
cid:15 O
) O
−2 O
ln O
z2 O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
∂ O
( O
z1 O
, O
z2 O
) O
( O
cid:30 O
) O
( O
cid:29 O
) O
∂ O
( O
y1 O
, O
y2 O
) O
r2 O
exercise O
11.4 O
exercise O
11.5 O
( O
11.10 O
) O
( O
11.11 O
) O
( O
11.12 O
) O
where O
r2 O
= O
z2 O
1 O
+ O
z2 O
2. O
then O
the O
joint O
distribution O
of O
y1 O
and O
y2 O
is O
given O
by O
p O
( O
y1 O
, O
y2 O
) O
= O
p O
( O
z1 O
, O
z2 O
) O
( O
cid:29 O
) O
= O
1√ O
2π O
exp O
( O
−y2 O
1/2 O
) O
( O
cid:30 O
) O
exp O
( O
−y2 O
2/2 O
) O
1√ O
2π O
and O
so O
y1 O
and O
y2 O
are O
independent B
and O
each O
has O
a O
gaussian O
distribution O
with O
zero O
mean B
and O
unit O
variance B
. O
11.1.3 O
adaptive B
rejection I
sampling I
in O
many O
instances O
where O
we O
might O
wish O
to O
apply O
rejection B
sampling I
, O
it O
proves O
difﬁcult O
to O
determine O
a O
suitable O
analytic O
form O
for O
the O
envelope O
distribution O
q O
( O
z O
) O
. O
the O
general O
form O
of O
the O
inference B
algorithms O
will O
, O
however O
, O
be O
the O
same O
as O
for O
the O
hidden O
markov O
model O
. O
however O
, O
we O
are O
now O
able O
to O
work O
with O
the O
joint O
distribution O
p O
( O
x O
, O
z O
) O
432 O
9. O
mixture B
models O
and O
em O
instead O
of O
the O
marginal B
distribution O
p O
( O
x O
) O
, O
and O
this O
will O
lead O
to O
signiﬁcant O
simpliﬁca- O
tions O
, O
most O
notably O
through O
the O
introduction O
of O
the O
expectation-maximization O
( O
em O
) O
algorithm O
. O
one O
way O
to O
achieve O
this O
is O
through O
the O
use O
of O
local O
receptive O
ﬁelds O
and O
shared O
weights O
, O
as O
discussed O
in O
the O
context O
of O
convolutional O
neural O
networks O
in O
section O
5.5.6. O
approach O
1 O
is O
often O
relatively O
easy O
to O
implement O
and O
can O
be O
used O
to O
encourage O
com- O
plex O
invariances O
such O
as O
those O
illustrated O
in O
figure O
5.14. O
for O
sequential O
training B
algorithms O
, O
this O
can O
be O
done O
by O
transforming O
each O
input O
pattern O
before O
it O
is O
presented O
to O
the O
model O
so O
that O
, O
if O
the O
patterns O
are O
being O
recycled O
, O
a O
different O
transformation O
( O
drawn O
from O
an O
appropriate O
distribution O
) O
is O
added O
each O
time O
. O
( O
b O
) O
the O
same O
graph O
drawn O
using O
the O
plate B
notation O
. O
it O
is O
worth O
spending O
a O
moment O
to O
understand O
further O
the O
unusual O
behaviour O
of O
the O
graph O
of O
figure O
8.20. O
consider O
a O
particular O
instance O
of O
such O
a O
graph O
corresponding O
to O
a O
problem O
with O
three O
binary O
random O
variables O
relating O
to O
the O
fuel B
system I
on O
a O
car O
, O
as O
shown O
in O
figure O
8.21. O
the O
variables O
are O
called O
b O
, O
representing O
the O
state O
of O
a O
battery O
that O
is O
either O
charged O
( O
b O
= O
1 O
) O
or O
ﬂat O
( O
b O
= O
0 O
) O
, O
f O
representing O
the O
state O
of O
the O
fuel O
tank O
that O
is O
either O
full O
of O
fuel O
( O
f O
= O
1 O
) O
or O
empty O
( O
f O
= O
0 O
) O
, O
and O
g O
, O
which O
is O
the O
state O
of O
an O
electric O
fuel O
gauge O
and O
which O
indicates O
either O
full O
( O
g O
= O
1 O
) O
or O
empty O
b O
f O
b O
f O
b O
f O
8.2. O
conditional B
independence I
377 O
g O
g O
g O
figure O
8.21 O
an O
example O
of O
a O
3-node O
graph O
used O
to O
illustrate O
the O
phenomenon O
of O
‘ O
explaining B
away I
’ O
. O
this O
corresponds O
to O
a O
simple O
two-node O
graph O
in O
which O
the O
node B
representing O
µ O
is O
the O
parent O
of O
the O
node B
representing O
x. O
the O
mean B
of O
the O
distribution O
over O
µ O
is O
a O
parameter O
controlling O
a O
prior B
, O
and O
so O
it O
can O
be O
viewed O
as O
a O
hyperparameter B
. O
x O
such O
superpositions O
, O
formed O
by O
taking O
linear O
combinations O
of O
more O
basic O
dis- O
tributions O
such O
as O
gaussians O
, O
can O
be O
formulated O
as O
probabilistic O
models O
known O
as O
mixture B
distributions O
( O
mclachlan O
and O
basford O
, O
1988 O
; O
mclachlan O
and O
peel O
, O
2000 O
) O
. O
similarly O
, O
the O
variance B
is O
denoted O
var O
[ O
f O
( O
x O
) O
] O
, O
and O
for O
vector O
variables O
the O
covariance B
is O
written O
cov O
[ O
x O
, O
y O
] O
. O
in O
this O
case O
, O
the O
random O
variable O
z O
corresponds O
to O
the O
derivative B
of O
the O
log O
likelihood O
function O
and O
is O
given O
by O
( O
x O
− O
µml O
) O
/σ2 O
, O
and O
its O
expectation B
that O
deﬁnes O
the O
regression B
function I
is O
a O
straight O
line O
given O
by O
( O
µ O
− O
µml O
) O
/σ2 O
. O
, O
n O
, O
derive O
the O
e O
and O
m O
step O
equations O
of O
the O
em O
algorithm O
for O
optimizing O
the O
mixing O
coefﬁcients O
πk O
and O
the O
component O
parameters O
µkij O
of O
this O
distribution O
by O
maximum B
likelihood I
. O
we O
can O
also O
express O
these O
results O
in O
terms O
of O
the O
corresponding O
partitioned B
covariance O
matrix O
. O
exercise O
8.14 O
section O
8.4 O
8.3.4 O
relation O
to O
directed O
graphs O
we O
have O
introduced O
two O
graphical O
frameworks O
for O
representing O
probability B
dis- O
tributions O
, O
corresponding O
to O
directed B
and O
undirected B
graphs O
, O
and O
it O
is O
instructive O
to O
discuss O
the O
relation O
between O
these O
. O
( O
1.84 O
) O
section O
8.2 O
section O
8.2.2 O
section O
1.1 O
appendix O
d O
this O
is O
an O
example O
of O
conditional B
independence I
property O
, O
because O
the O
indepen- O
dence O
holds O
when O
the O
distribution O
is O
conditioned O
on O
the O
class O
ck O
. O
chapter O
10 O
more O
generally O
, O
introducing O
transition O
or O
emission O
models O
that O
depart O
from O
the O
linear-gaussian O
( O
or O
other O
exponential B
family I
) O
model O
leads O
to O
an O
intractable O
infer- O
ence O
problem O
. O
t O
1 O
0 O
−1 O
0 O
x O
1 O
1.3. O
model B
selection I
in O
our O
example O
of O
polynomial B
curve I
ﬁtting I
using O
least O
squares O
, O
we O
saw O
that O
there O
was O
an O
optimal O
order O
of O
polynomial O
that O
gave O
the O
best O
generalization B
. O
as O
we O
saw O
in O
figure O
10.2 O
, O
the O
factorized O
assumption O
causes O
the O
variance B
of O
the O
posterior O
distribution O
to O
be O
under-estimated O
for O
certain O
directions O
in O
parameter O
space O
. O
journal O
of O
ma- O
chine O
learning B
research O
1 O
, O
211–244 O
. O
essential O
wavelets B
for O
statisti- O
cal O
applications O
and O
data O
analysis O
. O
( O
13.40 O
) O
α O
( O
znk O
) O
β O
( O
znk O
) O
n=1 O
n=1 O
however O
, O
the O
quantity O
p O
( O
x O
) O
represents O
the O
likelihood B
function I
whose O
value O
we O
typ- O
ically O
wish O
to O
monitor O
during O
the O
em O
optimization O
, O
and O
so O
it O
is O
useful O
to O
be O
able O
to O
evaluate O
it O
. O
for O
the O
moment O
, O
however O
, O
it O
is O
instructive O
to O
continue O
with O
the O
current O
approach O
and O
to O
consider O
how O
in O
practice O
we O
can O
apply O
it O
to O
data O
sets O
of O
limited O
size O
where O
we O
section O
3.4 O
10 O
1. O
introduction O
t O
1 O
0 O
−1 O
ln O
λ O
= O
−18 O
t O
1 O
0 O
−1 O
ln O
λ O
= O
0 O
0 O
x O
1 O
0 O
x O
1 O
figure O
1.7 O
plots O
of O
m O
= O
9 O
polynomials O
ﬁtted O
to O
the O
data O
set O
shown O
in O
figure O
1.2 O
using O
the O
regularized O
error O
function O
( O
1.4 O
) O
for O
two O
values O
of O
the O
regularization B
parameter O
λ O
corresponding O
to O
ln O
λ O
= O
−18 O
and O
ln O
λ O
= O
0. O
the O
case O
of O
no O
regularizer O
, O
i.e. O
, O
λ O
= O
0 O
, O
corresponding O
to O
ln O
λ O
= O
−∞ O
, O
is O
shown O
at O
the O
bottom O
right O
of O
figure O
1.4. O
may O
wish O
to O
use O
relatively O
complex O
and O
ﬂexible O
models O
. O
the O
probability B
of O
this O
occurring O
is O
given O
by O
p O
( O
mistake O
) O
= O
p O
( O
x O
∈ O
r1 O
, O
c2 O
) O
+ O
p O
( O
x O
∈ O
r2 O
, O
c1 O
) O
p O
( O
x O
, O
c2 O
) O
dx O
+ O
p O
( O
x O
, O
c1 O
) O
dx O
. O
( O
1988 O
) O
also O
used O
the O
diagonal B
approximation I
to O
the O
hessian O
, O
but O
they O
retained O
all O
terms O
in O
the O
evaluation O
of O
∂2en/∂a2 O
j O
and O
so O
obtained O
exact O
expres- O
sions O
for O
the O
diagonal B
terms O
. O
here O
f O
is O
the O
fisher O
information O
matrix O
, O
given O
by O
( O
cid:8 O
) O
( O
cid:9 O
) O
( O
6.32 O
) O
( O
6.33 O
) O
f O
= O
ex O
g O
( O
θ O
, O
x O
) O
g O
( O
θ O
, O
x O
) O
t O
( O
6.34 O
) O
where O
the O
expectation B
is O
with O
respect O
to O
x O
under O
the O
distribution O
p O
( O
x|θ O
) O
. O
in O
figure O
1.11 O
, O
we O
show O
a O
simple O
example O
involving O
a O
joint O
distribution O
over O
two O
variables O
to O
illustrate O
the O
concept O
of O
marginal B
and O
conditional B
distributions O
. O
2.4 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
mean B
of O
the O
binomial B
distribution I
is O
given O
by O
( O
2.11 O
) O
. O
however O
, O
it O
now O
treats O
the O
variables O
as O
if O
they O
were O
independent B
and O
hence O
can O
no O
longer O
express O
any O
correlations O
between O
them O
. O
this O
is O
most O
easily O
done O
using O
the O
1-of-k O
coding O
scheme O
in O
which O
the O
target B
vector I
tn O
for O
a O
feature O
vector O
φn O
belonging O
to O
class O
ck O
is O
a O
binary O
vector O
with O
all O
elements O
zero O
except O
for O
element O
k O
, O
which O
equals O
one O
. O
we O
see O
that O
after O
convergence O
, O
there O
are O
only O
two O
components O
for O
which O
the O
expected O
values O
of O
the O
mixing O
coefﬁcients O
are O
numerically O
distinguishable O
from O
their O
prior B
values O
. O
maximum B
likelihood I
methods O
can O
also O
be O
cast O
into O
a O
sequential O
framework O
. O
log2 O
1 O
16 O
1 O
64 O
log2 O
log2 O
log2 O
we O
see O
that O
the O
nonuniform O
distribution O
has O
a O
smaller O
entropy B
than O
the O
uniform O
one O
, O
and O
we O
shall O
gain O
some O
insight O
into O
this O
shortly O
when O
we O
discuss O
the O
interpretation O
of O
entropy B
in O
terms O
of O
disorder O
. O
now O
consider O
the O
corresponding O
results O
for O
the O
linear B
regression I
model O
. O
furthermore O
, O
we O
shall O
assume O
that O
the O
covariance B
of O
this O
gaus- O
sian O
is O
small O
so O
that O
the O
network O
function O
is O
approximately O
linear O
with O
respect O
to O
the O
parameters O
over O
the O
region O
of O
parameter O
space O
for O
which O
the O
posterior B
probability I
is O
signiﬁcantly O
nonzero O
. O
it O
has O
the O
property O
that O
the O
conditional B
distribution O
of O
xi O
, O
conditioned O
on O
all O
the O
remaining O
variables O
in O
the O
graph O
, O
is O
dependent O
only O
on O
the O
variables O
in O
the O
markov O
blanket O
. O
similarly O
, O
in O
the O
calculus B
of I
variations I
we O
seek O
a O
function O
y O
( O
x O
) O
that O
maximizes O
( O
or O
minimizes O
) O
a O
functional B
f O
[ O
y O
] O
. O
finally O
, O
we O
consider O
the O
standard O
multiclass O
classiﬁcation B
problem O
in O
which O
each O
input O
is O
assigned O
to O
one O
of O
k O
mutually O
exclusive O
classes O
. O
to O
do O
this O
, O
consider O
the O
following O
result O
, O
which O
is O
obtained O
by O
transforming O
from O
cartesian O
to O
polar O
coordinates O
( O
cid:6 O
) O
∞ O
d O
( O
cid:14 O
) O
−∞ O
i=1 O
( O
cid:6 O
) O
∞ O
0 O
−x2 O
e O
i O
dxi O
= O
sd O
−r2 O
e O
rd−1 O
dr. O
( O
1.142 O
) O
using O
the O
deﬁnition O
( O
1.141 O
) O
of O
the O
gamma B
function I
, O
together O
with O
( O
1.126 O
) O
, O
evaluate O
both O
sides O
of O
this O
equation O
, O
and O
hence O
show O
that O
sd O
= O
2πd/2 O
γ O
( O
d/2 O
) O
. O
the O
quantity O
p O
( O
d|w O
) O
on O
the O
right-hand O
side O
of O
bayes O
’ O
theorem O
is O
evaluated O
for O
the O
observed O
data O
set O
d O
and O
can O
be O
viewed O
as O
a O
function O
of O
the O
parameter O
vector O
w O
, O
in O
which O
case O
it O
is O
called O
the O
likelihood B
function I
. O
the O
oil B
ﬂow I
data I
set O
is O
generated O
using O
realistic O
known O
values O
for O
the O
absorption O
properties O
of O
oil O
, O
water O
, O
and O
gas O
at O
the O
two O
gamma O
energies O
used O
, O
and O
with O
a O
speciﬁc O
choice O
of O
integration O
time O
( O
10 O
seconds O
) O
chosen O
as O
characteristic O
of O
a O
typical O
practical O
setup O
. O
each O
data O
point O
is O
labelled O
according O
to O
which O
of O
the O
three O
geomet- O
rical O
classes O
it O
belongs O
to O
, O
and O
our O
goal O
is O
to O
use O
this O
data O
as O
a O
training B
set I
in O
order O
to O
be O
able O
to O
classify O
a O
new O
observation O
( O
x6 O
, O
x7 O
) O
, O
such O
as O
the O
one O
denoted O
by O
the O
cross O
in O
figure O
1.19. O
we O
observe O
that O
the O
cross O
is O
surrounded O
by O
numerous O
red O
points O
, O
and O
so O
we O
might O
suppose O
that O
it O
belongs O
to O
the O
red O
class O
. O
10.17 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
verify O
the O
results O
( O
10.73 O
) O
– O
( O
10.77 O
) O
for O
the O
remaining O
terms O
in O
the O
lower B
bound I
for O
the O
variational B
gaussian O
mixture B
model I
given O
by O
( O
10.70 O
) O
. O
10.1.4 O
model B
comparison I
. O
together O
, O
these O
will O
motivate O
and O
illustrate O
the O
key O
concepts O
of O
d-separation B
. O
for O
instance O
, O
a O
feature B
map I
might O
consist O
of O
100 O
units O
arranged O
in O
a O
10 O
× O
10 O
grid O
, O
with O
each O
unit O
taking O
inputs O
from O
a O
5×5 O
pixel O
patch O
of O
the O
image O
. O
the O
quantity O
zn O
, O
which O
corresponds O
to O
the O
nth O
element O
of O
z O
, O
can O
then O
be O
given O
a O
simple O
interpretation O
as O
an O
effective O
target O
value O
in O
this O
space O
obtained O
by O
making O
a O
local B
linear O
approximation O
to O
the O
logistic B
sigmoid I
function O
around O
the O
current O
operating O
point O
w O
( O
old O
) O
an O
( O
w O
) O
( O
cid:7 O
) O
an O
( O
w O
( O
old O
) O
) O
+ O
dan O
dyn O
nw O
( O
old O
) O
− O
( O
yn O
− O
tn O
) O
yn O
( O
1 O
− O
yn O
) O
w O
( O
old O
) O
( O
tn O
− O
yn O
) O
= O
zn O
. O
7.2.2 O
analysis O
of O
sparsity B
. O
the O
total O
by O
a O
number O
of O
network O
outputs O
is O
given O
by O
( O
k O
+ O
2 O
) O
l O
, O
as O
compared O
with O
the O
usual O
k O
outputs O
for O
a O
network O
, O
which O
simply O
predicts O
the O
conditional B
means O
of O
the O
target O
variables O
. O
this O
can O
be O
done O
by O
view- O
ing O
the O
potential B
function I
as O
expressing O
which O
conﬁgurations O
of O
the O
local B
variables O
are O
preferred O
to O
others O
. O
10.26 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
extend O
the O
variational B
treatment O
of O
bayesian O
linear B
regression I
to O
include O
a O
gamma O
hyperprior O
gam O
( O
β|c0 O
, O
d0 O
) O
over O
β O
and O
solve O
variationally O
, O
by O
assuming O
a O
factorized O
variational O
distribution O
of O
the O
form O
q O
( O
w O
) O
q O
( O
α O
) O
q O
( O
β O
) O
. O
5.2.2 O
local B
quadratic O
approximation O
insight O
into O
the O
optimization O
problem O
, O
and O
into O
the O
various O
techniques O
for O
solv- O
ing O
it O
, O
can O
be O
obtained O
by O
considering O
a O
local B
quadratic O
approximation O
to O
the O
error B
function I
. O
this O
leads O
to O
a O
lower B
bound I
on O
f O
( O
x O
) O
, O
which O
is O
a O
linear O
function O
of O
x2 O
whose O
conjugate B
function O
is O
given O
by O
( O
cid:18 O
) O
( O
cid:20 O
) O
( O
cid:19 O
) O
( O
cid:17 O
) O
√ O
−x/2 O
( O
ex/2 O
+ O
e O
e O
−x/2 O
) O
. O
here O
we O
have O
introduced O
the O
two O
quantities O
si O
= O
ϕt O
qi O
= O
ϕt O
i O
c O
i O
c O
−1−i O
ϕi O
−1−i O
t. O
( O
7.98 O
) O
( O
7.99 O
) O
here O
si O
is O
called O
the O
sparsity B
and O
qi O
is O
known O
as O
the O
quality O
of O
ϕi O
, O
and O
as O
we O
shall O
see O
, O
a O
large O
value O
of O
si O
relative B
to O
the O
value O
of O
qi O
means O
that O
the O
basis B
function I
ϕi O
exercise O
7.15 O
352 O
7. O
sparse O
kernel O
machines O
of O
the O
figure O
7.11 O
plots O
log O
likelihood O
λ O
( O
αi O
) O
versus O
marginal B
ln O
αi O
showing O
on O
the O
left O
, O
the O
single O
maximum O
at O
a O
ﬁnite O
αi O
for O
q2 O
i O
= O
4 O
and O
si O
= O
1 O
( O
so O
that O
q2 O
i O
> O
si O
) O
and O
on O
the O
right O
, O
the O
maximum O
at O
αi O
= O
∞ O
for O
q2 O
i O
= O
1 O
and O
si O
= O
2 O
( O
so O
that O
q2 O
i O
< O
si O
) O
. O
vapnik O
: O
the O
nature O
of O
statistical B
learning I
theory I
, O
second O
edition O
. O
here O
we O
see O
that O
the O
effect O
of O
a O
small O
number O
of O
exercise O
2.47 O
exercise O
12.24 O
104 O
2. O
probability B
distributions O
0.5 O
0.4 O
0.3 O
0.2 O
0.1 O
0 O
−5 O
0.5 O
0.4 O
0.3 O
0.2 O
0.1 O
0 O
−5 O
0 O
5 O
10 O
( O
a O
) O
0 O
5 O
10 O
( O
b O
) O
figure O
2.16 O
illustration O
of O
the O
robustness B
of O
student O
’ O
s O
t-distribution O
compared O
to O
a O
gaussian O
. O
this O
process O
is O
repeated O
n O
times O
to O
generate O
a O
data O
set O
of O
n O
independent B
samples O
. O
decomposition B
methods I
( O
osuna O
et O
al. O
, O
1996 O
) O
also O
solve O
a O
series O
of O
smaller O
quadratic O
programming O
problems O
but O
are O
designed O
so O
that O
each O
of O
these O
is O
of O
a O
ﬁxed O
size O
, O
and O
so O
the O
technique O
can O
be O
applied O
to O
arbitrarily O
large O
data O
sets O
. O
if O
we O
evaluate O
the O
differential B
entropy I
of O
the O
gaussian O
, O
we O
obtain O
( O
cid:26 O
) O
h O
[ O
x O
] O
= O
1 O
2 O
( O
cid:27 O
) O
1 O
+ O
ln O
( O
2πσ2 O
) O
. O
begin O
by O
using O
the O
division O
formula O
( O
10.205 O
) O
to O
derive O
the O
expressions O
( O
10.214 O
) O
and O
( O
10.215 O
) O
by O
completing B
the I
square I
inside O
the O
exponential O
to O
identify O
the O
mean B
and O
variance B
. O
the O
joint O
distribution O
deﬁned O
by O
a O
graph O
is O
given O
by O
the O
product O
, O
over O
all O
of O
the O
nodes O
of O
the O
graph O
, O
of O
a O
conditional B
distribution O
for O
each O
node B
conditioned O
on O
the O
variables O
corresponding O
to O
the O
parents O
of O
that O
node B
in O
the O
graph O
. O
n O
h O
( O
x O
− O
xn O
) O
= O
1 O
for O
any O
value O
of O
x. O
the O
effect O
of O
such O
normalization O
is O
shown O
in O
figure O
6.2. O
normal- O
ization O
is O
sometimes O
used O
in O
practice O
as O
it O
avoids O
having O
regions O
of O
input O
space O
where O
all O
of O
the O
basis O
functions O
take O
small O
values O
, O
which O
would O
necessarily O
lead O
to O
predic- O
tions O
in O
such O
regions O
that O
are O
either O
small O
or O
controlled O
purely O
by O
the O
bias B
parameter I
. O
the O
algorithm O
used O
to O
determine O
the O
parameters O
w O
of O
the O
perceptron B
can O
most O
easily O
be O
motivated O
by O
error B
function I
minimization O
. O
variational B
principal O
bishop O
, O
c. O
m. O
components O
. O
techniques O
such O
as O
this O
are O
known O
in O
the O
statistics O
literature O
as O
shrinkage B
methods O
because O
they O
reduce O
the O
value O
of O
the O
coefﬁcients O
. O
although O
the O
gaussian O
distribution O
( O
2.43 O
) O
is O
widely O
used O
as O
a O
density B
model O
, O
it O
suffers O
from O
some O
signiﬁcant O
limitations O
. O
8.4.8 O
learning B
the O
graph O
structure O
in O
our O
discussion O
of O
inference B
in O
graphical O
models O
, O
we O
have O
assumed O
that O
the O
structure O
of O
the O
graph O
is O
known O
and O
ﬁxed O
. O
3. O
the O
inverse B
of O
the O
hessian O
has O
been O
used O
to O
identify O
the O
least O
signiﬁcant O
weights O
in O
a O
network O
as O
part O
of O
network O
‘ O
pruning O
’ O
algorithms O
( O
le O
cun O
et O
al. O
, O
1990 O
) O
. O
( O
14.6 O
) O
h=1 O
this O
is O
an O
example O
of O
bayesian O
model B
averaging I
. O
these O
feature O
values O
can O
conveniently O
be O
grouped O
together O
to O
form O
a O
vector O
y. O
similarly O
, O
the O
weight O
vectors O
{ O
wk O
} O
can O
be O
considered O
to O
be O
the O
columns O
of O
a O
matrix O
w O
, O
so O
that O
> O
1 O
linear O
‘ O
features O
’ O
yk O
= O
wt O
( O
cid:4 O
) O
( O
4.39 O
) O
note O
that O
again O
we O
are O
not O
including O
any O
bias B
parameters O
in O
the O
deﬁnition O
of O
y. O
the O
generalization B
of O
the O
within-class B
covariance I
matrix O
to O
the O
case O
of O
k O
classes O
follows O
from O
( O
4.28 O
) O
to O
give O
y O
= O
wtx O
. O
for O
example O
, O
by O
using O
appropriate O
reinforcement B
learning I
techniques O
a O
neural B
network I
can O
learn O
to O
play O
the O
game O
of O
backgammon B
to O
a O
high O
standard O
( O
tesauro O
, O
1994 O
) O
. O
a O
plot O
of O
the O
cost B
function I
j O
given O
by O
( O
9.1 O
) O
for O
the O
old O
faithful O
example O
is O
shown O
in O
figure O
9.2. O
note O
that O
we O
have O
deliberately O
chosen O
poor O
initial O
values O
for O
the O
cluster O
centres O
so O
that O
the O
algorithm O
takes O
several O
steps O
before O
convergence O
. O
we O
ﬁrst O
obtained O
by O
sampling O
from O
the O
conditional B
distri- O
p O
( O
z1|z O
( O
τ O
) O
1 O
by O
a O
new O
value O
z O
( O
τ O
+1 O
) O
1 O
( O
τ O
) O
1 O
, O
z O
and O
z O
( O
τ O
) O
2 O
, O
z O
( O
τ O
) O
3 O
) O
. O
5.3. O
error B
backpropagation I
245 O
for O
batch O
methods O
, O
the O
derivative B
of O
the O
total O
error B
e O
can O
then O
be O
obtained O
by O
repeating O
the O
above O
steps O
for O
each O
pattern O
in O
the O
training B
set I
and O
then O
summing O
over O
all O
patterns O
: O
∂e O
∂wji O
= O
∂en O
∂wji O
. O
p O
( O
x O
) O
p O
( O
x O
) O
δx O
x O
because O
probabilities O
are O
nonnegative O
, O
and O
because O
the O
value O
of O
x O
must O
lie O
some- O
where O
on O
the O
real O
axis O
, O
the O
probability B
density O
p O
( O
x O
) O
must O
satisfy O
the O
two O
conditions O
( O
cid:6 O
) O
∞ O
p O
( O
x O
) O
( O
cid:2 O
) O
0 O
p O
( O
x O
) O
dx O
= O
1 O
. O
before O
doing O
this O
, O
we O
can O
simplify O
these O
expressions O
by O
considering O
broad O
, O
noninformative B
priors O
in O
which O
µ0 O
= O
a0 O
= O
b0 O
= O
λ0 O
= O
0. O
although O
these O
parameter O
settings O
correspond O
to O
improper B
priors O
, O
we O
see O
that O
the O
posterior O
distribution O
is O
still O
well O
deﬁned O
. O
finally O
, O
it O
is O
worth O
noting O
that O
there O
exists O
a O
closely O
related O
linear O
dimensionality O
reduction O
technique O
called O
canonical B
correlation I
analysis I
, O
or O
cca O
( O
hotelling O
, O
1936 O
; O
bach O
and O
jordan O
, O
2002 O
) O
. O
here O
we O
turn O
to O
a O
family O
of O
approximation O
techniques O
called O
variational B
inference I
or O
vari- O
ational O
bayes O
, O
which O
use O
more O
global O
criteria O
and O
which O
have O
been O
widely O
applied O
. O
we O
can O
also O
use O
maximum B
likelihood I
to O
determine O
the O
precision B
parameter I
β O
of O
the O
gaussian O
conditional B
distribution O
. O
this O
is O
expressed O
through O
the O
famous O
equation O
s O
= O
k O
ln O
w O
in O
which O
w O
represents O
the O
number O
of O
possible O
microstates O
in O
a O
macrostate B
, O
and O
k O
( O
cid:3 O
) O
1.38 O
× O
10−23 O
( O
in O
units O
of O
joules O
per O
kelvin O
) O
is O
known O
as O
boltzmann O
’ O
s O
constant O
. O
statistical B
learning I
theory I
. O
however O
, O
the O
mixture B
model I
also O
assigns O
signiﬁcant O
probability B
mass O
to O
regions O
where O
there O
is O
no O
data O
because O
its O
predictive B
distribution I
is O
bimodal O
for O
all O
values O
of O
x. O
this O
problem O
can O
be O
resolved O
by O
extending O
the O
model O
to O
allow O
the O
mixture B
coefﬁcients O
themselves O
to O
be O
functions O
of O
x O
, O
leading O
to O
models O
such O
as O
the O
mixture O
density O
networks O
discussed O
in O
section O
5.6 O
, O
and O
hierarchical B
mixture I
of I
experts I
discussed O
in O
section O
14.5.3 O
. O
( O
d.8 O
) O
( O
d.9 O
) O
( O
d.10 O
) O
this O
second B
order I
differential O
equation O
can O
be O
solved O
for O
y O
( O
x O
) O
by O
making O
use O
of O
the O
boundary O
conditions O
on O
y O
( O
x O
) O
. O
( O
1.86 O
) O
a O
common O
choice O
of O
loss B
function I
in O
regression B
problems O
is O
the O
squared O
loss O
given O
by O
l O
( O
t O
, O
y O
( O
x O
) O
) O
= O
{ O
y O
( O
x O
) O
− O
t O
} O
2. O
in O
this O
case O
, O
the O
expected O
loss O
can O
be O
written O
e O
[ O
l O
] O
= O
{ O
y O
( O
x O
) O
− O
t O
} O
2p O
( O
x O
, O
t O
) O
dx O
dt O
. O
if O
we O
introduce O
a O
prior B
p O
( O
θi|mi O
) O
over O
the O
parameters O
, O
then O
we O
are O
interested O
in O
computing O
the O
model O
evi- O
dence O
p O
( O
d|mi O
) O
for O
the O
various O
models O
. O
although O
such O
theorems O
are O
reassuring O
, O
the O
key O
problem O
is O
how O
to O
ﬁnd O
suitable O
parameter O
values O
given O
a O
set O
of O
training B
data O
, O
and O
in O
later O
sections O
of O
this O
chapter O
we O
figure O
5.3 O
illustration O
of O
the O
ca- O
pability O
of O
a O
multilayer B
perceptron I
to O
approximate O
four O
different O
func- O
tions O
comprising O
( O
a O
) O
f O
( O
x O
) O
= O
x2 O
, O
( O
b O
) O
f O
( O
x O
) O
= O
sin O
( O
x O
) O
, O
( O
c O
) O
, O
f O
( O
x O
) O
= O
|x| O
, O
and O
( O
d O
) O
f O
( O
x O
) O
= O
h O
( O
x O
) O
where O
h O
( O
x O
) O
is O
the O
heaviside O
step O
function O
. O
this O
will O
happen O
if O
, O
and O
only O
if O
, O
ln O
p O
( O
a O
, O
b|x O
, O
c O
) O
= O
ln O
p O
( O
a|x O
, O
c O
) O
+ O
ln O
p O
( O
b|x O
, O
c O
) O
, O
that O
is O
, O
if O
the O
conditional B
inde- O
pendence O
relation O
a O
⊥⊥ O
b O
| O
x O
, O
c O
( O
10.86 O
) O
486 O
10. O
approximate O
inference B
is O
satisﬁed O
. O
( O
b O
) O
and O
( O
c O
) O
two O
factor O
graphs O
each O
of O
which O
corresponds O
to O
the O
undirected B
graph I
in O
( O
a O
) O
. O
however O
, O
we O
see O
from O
figure O
5.21 O
( O
c O
) O
that O
the O
model O
is O
able O
to O
produce O
a O
conditional B
density O
that O
is O
unimodal O
for O
some O
values O
of O
x O
and O
trimodal O
for O
other O
values O
by O
modulating O
the O
amplitudes O
of O
the O
mixing O
components O
πk O
( O
x O
) O
. O
we O
have O
seen O
that O
the O
procedure O
for O
determining O
the O
conditional B
independence I
properties O
is O
different O
between O
directed B
and O
undirected B
graphs O
. O
armed O
with O
the O
knowledge O
that O
the O
optimal O
solution O
is O
the O
conditional B
expectation I
, O
we O
can O
expand O
the O
square O
term O
as O
follows O
{ O
y O
( O
x O
) O
− O
t O
} O
2 O
= O
{ O
y O
( O
x O
) O
− O
e O
[ O
t|x O
] O
+ O
e O
[ O
t|x O
] O
− O
t O
} O
2 O
= O
{ O
y O
( O
x O
) O
− O
e O
[ O
t|x O
] O
} O
2 O
+ O
2 O
{ O
y O
( O
x O
) O
− O
e O
[ O
t|x O
] O
} O
{ O
e O
[ O
t|x O
] O
− O
t O
} O
+ O
{ O
e O
[ O
t|x O
] O
− O
t O
} O
2 O
where O
, O
to O
keep O
the O
notation O
uncluttered O
, O
we O
use O
e O
[ O
t|x O
] O
to O
denote O
et O
[ O
t|x O
] O
. O
, O
tn O
explicitly O
as O
in O
figure O
8.3. O
we O
therefore O
introduce O
a O
graphical O
notation O
that O
allows O
such O
multiple O
nodes O
to O
be O
expressed O
more O
compactly O
, O
in O
which O
we O
draw O
a O
single O
representative O
node B
tn O
and O
then O
surround O
this O
with O
a O
box O
, O
called O
a O
plate B
, O
labelled O
with O
n O
indicating O
that O
there O
are O
n O
nodes O
of O
this O
kind O
. O
( O
10.58 O
) O
finally O
, O
the O
variational B
posterior O
distribution O
q O
( O
cid:1 O
) O
( O
µk O
, O
λk O
) O
does O
not O
factorize O
into O
the O
product O
of O
the O
marginals O
, O
but O
we O
can O
always O
use O
the O
product B
rule I
to O
write O
it O
in O
the O
form O
q O
( O
cid:1 O
) O
( O
µk O
, O
λk O
) O
= O
q O
( O
cid:1 O
) O
( O
µk|λk O
) O
q O
( O
cid:1 O
) O
( O
λk O
) O
. O
we O
shall O
see O
shortly O
how O
to O
arrive O
at O
more O
sensible O
conclusions O
through O
the O
introduction O
of O
a O
prior B
distribution O
over O
µ. O
we O
can O
also O
work O
out O
the O
distribution O
of O
the O
number O
m O
of O
observations O
of O
x O
= O
1 O
, O
given O
that O
the O
data O
set O
has O
size O
n. O
this O
is O
called O
the O
binomial B
distribution I
, O
and O
from O
( O
2.5 O
) O
we O
see O
that O
it O
is O
proportional O
to O
µm O
( O
1 O
− O
µ O
) O
n−m O
. O
the O
gamma B
distribution I
is O
the O
conjugate B
prior I
for O
the O
precision O
( O
inverse B
variance O
) O
of O
a O
univariate O
gaussian O
. O
we O
then O
use O
these O
probabilities O
in O
the O
maximization B
step I
, O
or O
m O
step O
, O
to O
re-estimate O
the O
means O
, O
covariances O
, O
and O
mix- O
ing O
coefﬁcients O
using O
the O
results O
( O
9.17 O
) O
, O
( O
9.19 O
) O
, O
and O
( O
9.22 O
) O
. O
we O
see O
that O
when O
∆ O
is O
very O
small O
( O
top O
ﬁgure O
) O
, O
the O
resulting O
density B
model O
is O
very O
spiky O
, O
with O
a O
lot O
of O
structure O
that O
is O
not O
present O
in O
the O
underlying O
distribution O
that O
generated O
the O
data O
set O
. O
note O
that O
we O
did O
not O
constrain O
the O
distribution O
to O
be O
nonnegative O
when O
we O
maximized O
the O
entropy B
. O
we O
now O
turn O
to O
the O
case O
of O
regression B
problems O
, O
such O
as O
the O
curve B
ﬁtting I
example O
discussed O
earlier O
. O
now O
suppose O
we O
wish O
to O
ﬁnd O
the O
marginals O
for O
every O
variable O
node B
in O
the O
graph O
. O
a O
hard-assignment O
version O
of O
the O
gaussian O
mixture B
model I
with O
general O
covariance B
matrices O
, O
known O
as O
the O
elliptical O
k-means O
algorithm O
, O
has O
been O
considered O
by O
sung O
and O
poggio O
( O
1994 O
) O
. O
in O
particular O
, O
the O
result O
( O
8.63 O
) O
shows O
that O
the O
local B
marginal O
at O
the O
node B
zn O
is O
given O
by O
the O
product O
of O
the O
incoming O
messages O
. O
often O
these O
were O
wired O
up O
at O
random O
to O
demonstrate O
the O
ability O
of O
the O
perceptron B
to O
learn O
without O
the O
need O
for O
precise O
wiring O
, O
in O
contrast O
to O
a O
modern O
digital O
computer O
. O
p O
( O
x O
) O
points O
so O
that O
µj O
= O
xn O
for O
some O
value O
of O
n. O
this O
data O
point O
will O
then O
contribute O
a O
term O
in O
the O
likelihood B
function I
of O
the O
form O
x O
1 O
1 O
σj O
. O
an O
example O
of O
a O
functional B
is O
the O
length O
of O
a O
curve O
drawn O
in O
a O
two-dimensional O
plane O
in O
which O
the O
path O
of O
the O
curve O
is O
deﬁned O
in O
terms O
of O
a O
function O
. O
4.15 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
hessian O
matrix O
h O
for O
the O
logistic B
regression I
model O
, O
given O
by O
( O
4.97 O
) O
, O
is O
positive B
deﬁnite I
. O
without O
loss O
of O
generality O
, O
we O
can O
take O
xa O
to O
form O
the O
ﬁrst O
m O
components O
of O
x O
, O
with O
xb O
comprising O
the O
remaining O
d O
− O
m O
components O
, O
so O
that O
we O
also O
deﬁne O
corresponding O
partitions O
of O
the O
mean B
vector O
µ O
given O
by O
and O
of O
the O
covariance B
matrix I
σ O
given O
by O
note O
that O
the O
symmetry O
σt O
= O
σ O
of O
the O
covariance B
matrix I
implies O
that O
σaa O
and O
σbb O
are O
symmetric O
, O
while O
σba O
= O
σt O
ab O
. O
in O
many O
situations O
, O
it O
will O
be O
convenient O
to O
work O
with O
the O
inverse B
of O
the O
covari- O
ance O
matrix O
λ O
≡ O
σ O
−1 O
( O
2.68 O
) O
which O
is O
known O
as O
the O
precision B
matrix I
. O
links O
between O
factor B
analysis I
and O
pca O
were O
investigated O
by O
lilwley O
( O
1953 O
) O
and O
anderson O
( O
1963 O
) O
who O
showed O
that O
at O
stationary B
points O
of O
the O
likelihood B
function I
. O
arrows O
denote O
the O
direc- O
tion O
of O
information O
ﬂow O
through O
the O
network O
during O
forward B
propagation I
. O
for O
instance O
, O
the O
multilayer B
perceptron I
architecture O
is O
sometimes O
called O
a O
backpropagation B
network O
. O
a O
perfect B
map I
is O
therefore O
both O
an O
i O
map O
and O
a O
d O
map O
. O
to O
do O
this O
, O
we O
ﬁrst O
generate O
a O
value O
for O
z O
, O
which O
we O
denote O
( O
cid:1 O
) O
z O
, O
from O
the O
marginal B
distribution O
p O
( O
z O
) O
and O
then O
generate O
a O
value O
for O
x O
from O
the O
conditional B
distribution O
p O
( O
x| O
( O
cid:1 O
) O
z O
) O
. O
this O
can O
be O
interpreted O
as O
an O
inﬁnite O
mixture O
of O
gaussians O
( O
gaussian O
mixtures O
will O
be O
discussed O
in O
detail O
in O
section O
2.3.9. O
the O
result O
is O
a O
distribution O
that O
in O
gen- O
eral O
has O
longer O
‘ O
tails O
’ O
than O
a O
gaussian O
, O
as O
was O
seen O
in O
figure O
2.15. O
this O
gives O
the O
t- O
distribution O
an O
important O
property O
called O
robustness B
, O
which O
means O
that O
it O
is O
much O
less O
sensitive O
than O
the O
gaussian O
to O
the O
presence O
of O
a O
few O
data O
points O
which O
are O
outliers B
. O
we O
partition O
the O
latent O
variables O
into O
three O
disjoint O
groups O
a O
, O
b O
, O
c O
and O
then O
let O
us O
suppose O
that O
we O
are O
assuming O
a O
factorization B
between O
c O
and O
the O
remaining O
latent O
variables O
, O
so O
that O
q O
( O
a O
, O
b O
, O
c O
) O
= O
q O
( O
a O
, O
b O
) O
q O
( O
c O
) O
. O
the O
plot O
on O
the O
right O
shows O
the O
predictive O
density O
for O
a O
single O
linear B
regression I
model O
ﬁtted O
to O
the O
same O
data O
set O
using O
maximum B
likelihood I
. O
first O
of O
all O
, O
recall O
from O
( O
10.17 O
) O
that O
if O
we O
minimize O
the O
kullback-leibler O
diver- O
gence O
kl O
( O
p O
( O
cid:5 O
) O
q O
) O
with O
respect O
to O
a O
factorized B
distribution I
q O
, O
then O
the O
optimal O
solution O
for O
each O
factor O
is O
simply O
the O
corresponding O
marginal B
of O
p. O
section O
8.4.4 O
now O
consider O
the O
factor B
graph I
shown O
on O
the O
left O
in O
figure O
10.18 O
, O
which O
was O
introduced O
earlier O
in O
the O
context O
of O
the O
sum-product B
algorithm I
. O
for O
instance O
, O
we O
might O
seek O
a O
bound O
on O
a O
conditional B
distribution O
p O
( O
y|x O
) O
, O
which O
is O
itself O
just O
one O
factor O
in O
a O
much O
larger O
probabilistic O
model O
speciﬁed O
by O
a O
directed B
graph O
. O
completing B
the I
square I
in O
the O
usual O
way O
, O
we O
obtain O
( O
cid:27 O
) O
( O
10.174 O
) O
( O
10.175 O
) O
( O
10.176 O
) O
where O
we O
have O
deﬁned O
−1 O
n O
µn O
= O
σ O
q O
( O
w O
) O
= O
n O
( O
w|µn O
, O
σn O
) O
n O
( O
cid:2 O
) O
n=1 O
( O
tn O
− O
1/2 O
) O
φn O
n O
( O
cid:2 O
) O
−1 O
n O
= O
e O
[ O
α O
] O
i O
+ O
2 O
σ O
λ O
( O
ξn O
) O
φnφt O
n. O
similarly O
, O
the O
optimal O
solution O
for O
the O
factor O
q O
( O
α O
) O
is O
obtained O
from O
n=1 O
ln O
q O
( O
α O
) O
= O
ew O
[ O
ln O
p O
( O
w|α O
) O
] O
+ O
ln O
p O
( O
α O
) O
+ O
const O
. O
the O
exponential B
family I
of O
distributions O
over O
x O
, O
given O
parameters O
η O
, O
is O
deﬁned O
to O
be O
the O
set O
of O
distributions O
of O
the O
form O
p O
( O
x|η O
) O
= O
h O
( O
x O
) O
g O
( O
η O
) O
exp O
ηtu O
( O
x O
) O
( O
2.194 O
) O
where O
x O
may O
be O
scalar O
or O
vector O
, O
and O
may O
be O
discrete O
or O
continuous O
. O
again O
, O
the O
evaluation O
of O
the O
acceptance B
criterion I
does O
not O
require O
knowledge O
of O
the O
normal- O
izing O
constant O
zp O
in O
the O
probability B
distribution O
p O
( O
z O
) O
= O
( O
cid:4 O
) O
p O
( O
z O
) O
/zp O
. O
, O
xm O
that O
are O
neighbours O
of O
that O
fac- O
tor O
node B
, O
using O
( O
8.93 O
) O
. O
however O
, O
in O
practice O
it O
is O
found O
that O
the O
particular O
tree B
structure O
that O
is O
learned O
is O
very O
sensitive O
to O
the O
details O
of O
the O
data O
set O
, O
so O
that O
a O
small O
change O
to O
the O
training B
data O
can O
result O
in O
a O
very O
different O
set O
of O
splits O
( O
hastie O
et O
al. O
, O
2001 O
) O
. O
9.27 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
derive O
m-step O
formulae O
for O
updating O
the O
covariance B
matrices O
and O
mixing O
coefﬁcients O
in O
a O
gaussian O
mixture B
model I
when O
the O
responsibilities O
are O
updated O
in- O
crementally O
, O
analogous O
to O
the O
result O
( O
9.78 O
) O
for O
updating O
the O
means O
. O
' O
, O
h.re O
a O
conlnlon O
`` O
.riance.•nd O
the O
mean B
, O
are O
con'trained O
to O
lie O
on O
a O
'mooih O
tw-o-diitlcn O
, O
iooal O
n1anifold O
. O
as O
an O
example O
, O
suppose O
that O
the O
probability B
density O
within O
each O
class O
is O
chosen O
to O
be O
gaussian O
. O
then O
figure O
8.29 O
a O
four-node O
undirected B
graph I
showing O
a O
clique B
( O
outlined O
in O
green O
) O
and O
a O
maximal B
clique I
( O
outlined O
in O
blue O
) O
. O
( O
8.44 O
) O
now O
let O
us O
convert O
this O
to O
an O
undirected B
graph I
representation O
, O
as O
shown O
in O
fig- O
ure O
8.32. O
in O
the O
undirected B
graph I
, O
the O
maximal O
cliques O
are O
simply O
the O
pairs O
of O
neigh- O
bouring O
nodes O
, O
and O
so O
from O
( O
8.39 O
) O
we O
wish O
to O
write O
the O
joint O
distribution O
in O
the O
form O
p O
( O
x O
) O
= O
1 O
z O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
ψ2,3 O
( O
x2 O
, O
x3 O
) O
··· O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
. O
, O
k. O
once O
we O
know O
z2 O
we O
can O
draw O
a O
sample O
for O
x2 O
and O
also O
sample O
the O
next O
latent B
variable I
z3 O
and O
so O
on O
. O
k O
= O
1 O
x7 O
2 O
1 O
k O
= O
3 O
x7 O
2 O
1 O
x7 O
2 O
1 O
k O
= O
3 O
1 O
0 O
0 O
1 O
x6 O
2 O
0 O
0 O
1 O
x6 O
2 O
0 O
0 O
1 O
x6 O
2 O
figure O
2.28 O
plot O
of O
200 O
data O
points O
from O
the O
oil O
data O
set O
showing O
values O
of O
x6 O
plotted O
against O
x7 O
, O
where O
the O
red O
, O
green O
, O
and O
blue O
points O
correspond O
to O
the O
‘ O
laminar O
’ O
, O
‘ O
annular O
’ O
, O
and O
‘ O
homogeneous B
’ O
classes O
, O
respectively O
. O
we O
can O
view O
this O
network O
as O
two O
successive O
functional B
mappings O
f O
] O
and O
f O
2 O
, O
as O
indicated O
in O
figure O
12.19. O
the O
first O
mapping O
f O
] O
projects O
the O
original O
d O
( O
cid:173 O
) O
dimensional O
data O
onto O
an O
ai-dimensional O
subspace O
s O
defined O
by O
the O
activations O
of O
the O
units O
in O
the O
second O
hidden O
layer O
. O
this O
suggests O
a O
different O
approach O
to O
solving O
the O
optimization O
problem O
for O
the O
rvm O
, O
in O
which O
we O
make O
explicit O
all O
of O
the O
dependence O
of O
the O
marginal B
likelihood I
( O
7.85 O
) O
on O
a O
particular O
αi O
and O
then O
determine O
its O
stationary B
points O
explicitly O
( O
faul O
and O
tipping O
, O
2002 O
; O
tipping O
and O
faul O
, O
2003 O
) O
. O
if O
the O
number O
of O
observed O
data O
points O
n O
= O
0 O
, O
then O
( O
2.141 O
) O
reduces O
to O
the O
prior B
mean O
as O
expected O
. O
on O
estimating O
regression B
. O
the O
number O
of O
steps O
needed O
to O
obtain O
an O
independent B
sample O
from O
the O
distribution O
is O
o O
( O
( O
l/l O
) O
2 O
) O
. O
the O
posterior B
probability I
, O
given O
both O
the O
x-ray O
and O
blood O
data O
, O
is O
then O
given O
by O
p O
( O
ck|xi O
, O
xb O
) O
∝ O
p O
( O
xi O
, O
xb|ck O
) O
p O
( O
ck O
) O
∝ O
p O
( O
xi|ck O
) O
p O
( O
xb|ck O
) O
p O
( O
ck O
) O
∝ O
p O
( O
ck|xi O
) O
p O
( O
ck|xb O
) O
p O
( O
ck O
) O
( O
1.85 O
) O
thus O
we O
need O
the O
class O
prior B
probabilities O
p O
( O
ck O
) O
, O
which O
we O
can O
easily O
estimate O
from O
the O
fractions O
of O
data O
points O
in O
each O
class O
, O
and O
then O
we O
need O
to O
normalize O
the O
resulting O
posterior O
probabilities O
so O
they O
sum O
to O
one O
. O
furthermore O
, O
the O
splits O
in O
a O
decision B
tree I
are O
hard O
, O
so O
that O
each O
region O
of O
input O
space O
is O
associated O
with O
one O
, O
and O
only O
one O
, O
leaf O
node B
model O
. O
174 O
3. O
linear O
models O
for B
regression I
3.2 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
matrix O
φ O
( O
φtφ O
) O
−1φt O
( O
3.103 O
) O
takes O
any O
vector O
v O
and O
projects O
it O
onto O
the O
space O
spanned O
by O
the O
columns O
of O
φ. O
use O
this O
result O
to O
show O
that O
the O
least-squares O
solution O
( O
3.15 O
) O
corresponds O
to O
an O
orthogonal O
projection O
of O
the O
vector O
t O
onto O
the O
manifold B
s O
as O
shown O
in O
figure O
3.2 O
. O
this O
requires O
little O
additional O
computation O
compared O
with O
em O
, O
and O
it O
resolves O
the O
principal O
difﬁculties O
of O
maxi- O
mum O
likelihood O
while O
also O
allowing O
the O
number O
of O
components O
in O
the O
mixture B
to O
be O
inferred O
automatically O
from O
the O
data O
. O
this O
should O
be O
com- O
pared O
with O
the O
case O
of O
a O
single O
gaus- O
sian O
shown O
in O
figure O
1.14 O
for O
which O
no O
singularities B
arise O
. O
( O
6.62 O
) O
this O
result O
reﬂects O
the O
fact O
that O
the O
two O
gaussian O
sources O
of O
randomness O
, O
namely O
that O
associated O
with O
y O
( O
x O
) O
and O
that O
associated O
with O
 O
, O
are O
independent B
and O
so O
their O
covariances O
simply O
add O
. O
, O
xm O
) O
, O
however O
, O
would O
require O
2m O
parameters O
representing O
the O
probability B
p O
( O
y O
= O
1 O
) O
for O
each O
of O
the O
2m O
possible O
settings O
of O
the O
parent O
variables O
. O
we O
now O
make O
a O
further O
distinction O
between O
latent O
variables O
, O
denoted O
z O
, O
and O
parameters O
, O
denoted O
θ O
, O
where O
parameters O
are O
intensive O
( O
ﬁxed O
in O
num- O
ber O
independent B
of O
the O
size O
of O
the O
data O
set O
) O
, O
whereas O
latent O
variables O
are O
extensive O
( O
scale O
in O
number O
with O
the O
size O
of O
the O
data O
set O
) O
. O
prediction O
with O
gaussian O
processes O
: O
from O
linear B
regression I
to O
linear O
pre- O
diction O
and O
beyond O
. O
again O
, O
our O
calculations O
will O
be O
greatly O
simpliﬁed O
if O
we O
choose O
a O
conjugate B
form O
for O
the O
prior B
distribution O
. O
2.3 O
2.4 O
sequential B
estimation I
. O
one O
widely O
used O
kernel B
function I
for O
gaussian O
process O
regression B
is O
given O
by O
the O
exponential O
of O
a O
quadratic O
form O
, O
with O
the O
addition O
of O
constant O
and O
linear O
terms O
to O
give O
k O
( O
xn O
, O
xm O
) O
= O
θ0 O
exp O
+ O
θ2 O
+ O
θ3xt O
nxm O
. O
consider O
a O
parametric O
generative B
model I
p O
( O
x|θ O
) O
where O
θ O
denotes O
the O
vector O
of O
parameters O
. O
the O
quadratic O
form O
, O
and O
hence O
the O
gaussian O
density B
, O
will O
be O
constant O
on O
surfaces O
if O
all O
of O
the O
eigenvalues O
λi O
are O
positive O
, O
then O
these O
for O
which O
( O
2.51 O
) O
is O
constant O
. O
thus O
both O
the O
e O
step O
and O
the O
m O
step O
take O
ﬁxed O
time O
that O
is O
independent B
of O
the O
total O
number O
of O
data O
points O
. O
its O
validity O
can O
be O
seen O
informally O
, O
however O
, O
by O
dividing O
each O
real O
variable O
into O
intervals O
of O
width O
∆ O
and O
considering O
the O
discrete O
probability B
dis- O
tribution O
over O
these O
intervals O
. O
jacob O
’ O
s O
most O
sig- O
niﬁcant O
contributions O
to O
mathematics O
appeared O
in O
the O
artofconjecture O
published O
in O
1713 O
, O
eight O
years O
after O
his O
death O
, O
which O
deals O
with O
topics O
in O
probability B
the- O
ory O
including O
what O
has O
become O
known O
as O
the O
bernoulli O
distribution O
. O
5.25 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
quadratic O
error O
function O
of O
the O
form O
( O
w O
− O
w O
( O
cid:1 O
) O
) O
th O
( O
w O
− O
w O
( O
cid:1 O
) O
) O
e O
= O
e0 O
+ O
1 O
2 O
( O
5.195 O
) O
where O
w O
( O
cid:1 O
) O
represents O
the O
minimum O
, O
and O
the O
hessian O
matrix O
h O
is O
positive B
deﬁnite I
and O
constant O
. O
the O
predictive B
distribution I
p O
( O
x O
) O
is O
governed O
by O
the O
parameters O
jl O
, O
w O
, O
and O
0- O
2 O
• O
however O
, O
there O
is O
redundancy O
in O
this O
parameterization O
corresponding O
to O
rotations O
of O
the O
latent O
space O
coordinates O
. O
1.5.1 O
minimizing O
the O
misclassiﬁcation O
rate O
1.5.2 O
minimizing O
the O
expected O
loss O
1.5.3 O
the O
reject B
option I
. O
weston O
and O
watkins O
( O
1999 O
) O
deﬁne O
a O
single O
objective O
function O
for O
training B
all O
k O
svms O
simultaneously O
, O
based O
on O
maximizing O
the O
margin B
from O
each O
to O
remaining O
classes O
. O
first O
of O
all O
, O
we O
note O
that O
the O
mean B
of O
the O
posterior O
distribution O
given O
by O
( O
2.141 O
) O
is O
a O
compromise O
between O
the O
prior B
mean O
µ0 O
and O
the O
maximum B
likelihood I
solution O
µml O
. O
these O
distributions O
are O
examples O
of O
the O
exponential B
family I
, O
and O
we O
shall O
see O
that O
the O
interpretation O
of O
a O
conjugate B
prior I
in O
terms O
of O
effective O
ﬁctitious O
data O
points O
is O
a O
general O
one O
for O
the O
exponential B
family I
of O
distributions O
. O
the O
noninformative B
prior I
is O
obtained O
as O
the O
special O
case O
a0 O
= O
b0 O
= O
0. O
again O
, O
if O
we O
examine O
the O
results O
( O
2.150 O
) O
and O
( O
2.151 O
) O
for O
the O
posterior O
distribution O
of O
λ O
, O
we O
see O
that O
for O
a0 O
= O
b0 O
= O
0 O
, O
the O
posterior O
depends O
only O
on O
terms O
arising O
from O
the O
data O
and O
not O
from O
the O
prior B
. O
these O
can O
all O
be O
combined O
with O
the O
squared O
loss B
function I
for O
the O
purpose O
of O
making O
predictions O
. O
note O
that O
there O
are O
k O
terms O
in O
the O
summation O
, O
and O
the O
right-hand O
side O
has O
to O
be O
evaluated O
for O
each O
of O
the O
k O
values O
of O
zn O
so O
each O
step O
of O
the O
α B
recursion I
has O
computational O
cost O
that O
scaled O
like O
o O
( O
k O
2 O
) O
. O
the O
probability B
that O
x O
lies O
in O
the O
interval O
( O
−∞ O
, O
z O
) O
is O
given O
by O
the O
cumulative B
distribution I
function I
deﬁned O
by O
( O
cid:6 O
) O
z O
p O
( O
z O
) O
= O
p O
( O
x O
) O
dx O
−∞ O
( O
1.28 O
) O
which O
satisﬁes O
p O
( O
cid:2 O
) O
( O
x O
) O
= O
p O
( O
x O
) O
, O
as O
shown O
in O
figure O
1.12. O
if O
we O
have O
several O
continuous O
variables O
x1 O
, O
. O
this O
leads O
to O
a O
formalism O
that O
is O
analytically O
equivalent O
to O
the O
backpropagation B
procedure O
of O
bishop O
( O
1992 O
) O
, O
as O
described O
in O
section O
5.4.5 O
, O
though O
with O
some O
loss O
of O
efﬁciency O
due O
to O
redundant O
calculations O
. O
the O
origin O
of O
the O
sparsity B
can O
be O
seen O
from O
figure O
3.4 O
, O
which O
shows O
that O
the O
minimum O
of O
the O
error B
function I
, O
subject O
to O
the O
constraint O
( O
3.30 O
) O
. O
many O
linear O
parametric O
models O
can O
be O
re-cast O
into O
an O
equivalent O
‘ O
dual O
represen- O
tation O
’ O
in O
which O
the O
predictions O
are O
also O
based O
on O
linear O
combinations O
of O
a O
kernel B
function I
evaluated O
at O
the O
training B
data O
points O
. O
the O
corresponding O
value O
of O
the O
entropy B
is O
then O
h O
= O
ln O
m. O
this O
result O
can O
also O
be O
derived O
from O
jensen O
’ O
s O
inequality O
( O
to O
be O
discussed O
shortly O
) O
. O
both O
of O
these O
formulae O
express O
the O
variance B
( O
the O
inverse B
precision O
) O
as O
an O
average O
of O
the O
squared O
differences O
between O
the O
targets O
and O
the O
model O
predictions O
. O
in O
order O
to O
perform O
this O
variational B
m O
step O
, O
we O
need O
the O
expectations O
e O
[ O
znk O
] O
= O
rnk O
representing O
the O
responsibilities O
. O
however O
, O
the O
problem O
of O
determining O
the O
optimal O
model O
complexity O
is O
then O
shifted O
from O
one O
of O
ﬁnding O
the O
appropriate O
number O
of O
basis O
functions O
to O
one O
of O
determining O
a O
suitable O
value O
of O
the O
regularization B
coefﬁcient O
λ. O
we O
shall O
return O
to O
the O
issue O
of O
model O
complexity O
later O
in O
this O
chapter O
. O
calculate O
the O
number O
of O
distinct O
directed B
trees O
that O
can O
be O
constructed O
from O
a O
given O
undirected B
tree O
. O
to O
do O
this O
, O
we O
partition O
the O
covariance B
matrix I
as O
follows O
( O
cid:15 O
) O
( O
cid:16 O
) O
cn O
+1 O
= O
( O
6.65 O
) O
where O
cn O
is O
the O
n O
× O
n O
covariance B
matrix I
with O
elements O
given O
by O
( O
6.62 O
) O
for O
n O
, O
m O
= O
1 O
, O
. O
for O
any O
member O
of O
the O
exponential B
family I
( O
2.194 O
) O
, O
there O
exists O
a O
conjugate B
prior I
that O
can O
be O
written O
in O
the O
form O
p O
( O
η|χ O
, O
ν O
) O
= O
f O
( O
χ O
, O
ν O
) O
g O
( O
η O
) O
ν O
exp O
νηtχ O
( O
cid:26 O
) O
( O
cid:27 O
) O
( O
2.229 O
) O
where O
f O
( O
χ O
, O
ν O
) O
is O
a O
normalization O
coefﬁcient O
, O
and O
g O
( O
η O
) O
is O
the O
same O
function O
as O
ap- O
pears O
in O
( O
2.194 O
) O
. O
this O
result O
therefore O
represents O
a O
procedure O
for O
evaluating O
the O
inverse B
of O
the O
hessian O
using O
a O
single O
pass O
through O
the O
data O
set O
. O
make O
use O
of O
this O
, O
along O
with O
the O
sum O
and O
product O
rules O
of O
probability B
, O
to O
write O
down O
the O
normalized O
form O
for O
the O
distribution O
over O
z O
, O
and O
show O
that O
it O
equals O
p O
( O
z O
) O
. O
note O
that O
the O
problem O
will O
arise O
even O
if O
the O
number O
of O
data O
points O
is O
large O
compared O
with O
the O
number O
of O
parameters O
in O
the O
model O
, O
so O
long O
as O
the O
training B
data O
set O
is O
linearly B
separable I
. O
we O
shall O
therefore O
begin O
in O
this O
section O
with O
a O
‘ O
con- O
ventional O
’ O
derivation O
of O
the O
forward-backward O
equations O
, O
making O
use O
of O
the O
sum O
and O
product O
rules O
of O
probability B
, O
and O
exploiting O
conditional B
independence I
properties O
which O
we O
shall O
obtain O
from O
the O
corresponding O
graphical B
model I
using O
d-separation B
. O
as O
we O
shall O
see O
, O
the O
important O
contribution O
of O
the O
backpropagation B
technique O
is O
in O
pro- O
viding O
a O
computationally O
efﬁcient O
method O
for O
evaluating O
such O
derivatives O
. O
in O
the O
case O
of O
two O
vectors O
of O
random O
variables O
x O
and O
y O
, O
the O
covariance B
is O
a O
matrix O
( O
cid:8 O
) O
{ O
x O
− O
e O
[ O
x O
] O
} O
{ O
yt O
− O
e O
[ O
yt O
] O
} O
( O
cid:9 O
) O
cov O
[ O
x O
, O
y O
] O
= O
ex O
, O
y O
= O
ex O
, O
y O
[ O
xyt O
] O
− O
e O
[ O
x O
] O
e O
[ O
yt O
] O
. O
however O
, O
by O
adjusting O
the O
com- O
ponents O
of O
the O
weight B
vector I
w O
, O
we O
can O
select O
a O
projection O
that O
maximizes O
the O
class O
separation O
. O
3. O
backpropagate O
the O
δ O
’ O
s O
using O
( O
5.56 O
) O
to O
obtain O
δj O
for O
each O
hidden B
unit I
in O
the O
network O
. O
( O
cid:6 O
) O
∞ O
( O
cid:15 O
) O
( O
cid:6 O
) O
∞ O
−∞ O
+λ2 O
−∞ O
using O
the O
calculus B
of I
variations I
, O
we O
set O
the O
derivative B
of O
this O
functional B
to O
zero O
giving O
p O
( O
x O
) O
= O
exp O
. O
the O
conversion O
of O
a O
directed B
graph O
to O
a O
factor B
graph I
is O
illustrated O
in O
figure O
8.42. O
we O
have O
already O
noted O
the O
importance O
of O
tree-structured O
graphs O
for O
performing O
efﬁcient O
inference B
. O
in O
section O
6.4 O
, O
we O
will O
develop O
a O
dual- O
ity O
between O
probabilistic O
linear O
models O
for B
regression I
and O
the O
technique O
of O
gaussian O
processes O
. O
2. O
a O
regularization B
term O
is O
added O
to O
the O
error B
function I
that O
penalizes O
changes O
in O
the O
model O
output O
when O
the O
input O
is O
transformed O
. O
for O
example O
, O
in O
a O
mixture O
of O
gaussians O
, O
the O
joint O
distribution O
of O
obser- O
vations O
xn O
and O
corresponding O
hidden O
variables O
zn O
is O
a O
member O
of O
the O
exponential B
family I
, O
whereas O
the O
marginal B
distribution O
of O
xn O
is O
a O
mixture O
of O
gaussians O
and O
hence O
is O
not O
. O
naively O
, O
we O
could O
consider O
explicitly O
all O
of O
the O
exponentially O
many O
paths O
through O
the O
lattice O
, O
evaluate O
the O
probability B
for O
each O
, O
and O
then O
select O
the O
path O
having O
the O
highest O
proba- O
bility O
. O
provided O
p O
( O
x O
, O
z|θ O
) O
is O
a O
continuous O
function O
of O
θ O
exercises O
455 O
then O
, O
by O
continuity O
, O
any O
local B
maximum O
of O
l O
( O
q O
, O
θ O
) O
will O
also O
be O
a O
local B
maximum O
of O
ln O
p O
( O
x|θ O
) O
. O
show O
that O
this O
can O
be O
interpreted O
as O
a O
‘ O
soft B
’ O
( O
probabilistic O
) O
form O
of O
the O
logical O
or O
function O
( O
i.e. O
, O
the O
function O
that O
gives O
y O
= O
1 O
whenever O
at O
least O
one O
of O
the O
xi O
= O
1 O
) O
. O
although O
chunking B
reduces O
the O
size O
of O
the O
matrix O
in O
the O
quadratic O
function O
from O
the O
number O
of O
data O
points O
squared O
to O
approximately O
the O
number O
of O
nonzero O
lagrange O
multipliers O
squared O
, O
even O
this O
may O
be O
too O
big O
to O
ﬁt O
in O
memory O
for O
large-scale O
appli- O
cations O
. O
exact O
calculation O
of O
the O
hes- O
sian O
matrix O
for O
the O
multilayer B
perceptron I
. O
( O
14.17 O
) O
( O
c O
) O
update O
the O
data O
weighting O
coefﬁcients O
w O
( O
m+1 O
) O
n O
= O
w O
( O
m O
) O
n O
exp O
{ O
αmi O
( O
ym O
( O
xn O
) O
( O
cid:9 O
) O
= O
tn O
) O
} O
( O
14.18 O
) O
3. O
make O
predictions O
using O
the O
ﬁnal O
model O
, O
which O
is O
given O
by O
14.3. O
boosting B
659 O
( O
cid:22 O
) O
m O
( O
cid:2 O
) O
( O
cid:23 O
) O
ym O
( O
x O
) O
= O
sign O
αmym O
( O
x O
) O
. O
from O
now O
on O
, O
we O
shall O
switch O
to O
the O
use O
of O
natural O
logarithms O
in O
deﬁning O
en- O
tropy O
, O
as O
this O
will O
provide O
a O
more O
convenient O
link B
with O
ideas O
elsewhere O
in O
this O
book O
. O
as O
an O
illustration O
we O
consider O
the O
case O
of O
a O
single O
input O
variable O
x O
in O
which O
f O
( O
x O
, O
t O
) O
is O
given O
by O
a O
zero-mean O
isotropic B
gaussian O
over O
the O
variable O
z O
= O
( O
x O
, O
t O
) O
with O
variance B
σ2 O
. O
one O
way O
to O
exploit O
the O
gem O
approach O
would O
be O
to O
use O
one O
of O
the O
nonlinear O
optimization O
strategies O
, O
such O
as O
the O
conjugate B
gradients O
algorithm O
, O
during O
the O
m O
step O
. O
n=1 O
k=1 O
we O
now O
take O
the O
gradient O
of O
the O
error B
function I
with O
respect O
to O
one O
of O
the O
param- O
eter O
vectors O
wj O
. O
however O
, O
from O
a O
bayesian O
perspective O
it O
makes O
little O
sense O
to O
limit O
the O
number O
of O
parameters O
in O
the O
network O
according O
to O
the O
size O
of O
the O
training B
set I
. O
the O
dimensionality O
m O
of O
the O
feature B
space I
, O
however O
, O
can O
be O
much O
larger O
than O
d O
( O
even O
infinite O
) O
, O
and O
thus O
we O
can O
find O
a O
number O
of O
nonlinear O
principal O
components O
that O
can O
exceed O
d. O
note O
, O
however O
, O
that O
the O
number O
of O
nonzero O
eigenvalues O
can O
not O
exceed O
the O
number O
n O
of O
data O
points O
, O
because O
( O
even O
if O
m O
> O
n O
) O
the O
covariance B
matrix I
in O
feature B
space I
has O
rank O
at O
most O
equal O
to O
n. O
this O
is O
reflected O
in O
the O
fact O
that O
kernel O
pca O
involves O
the O
eigenvector O
expansion O
of O
the O
n O
x O
n O
matrix O
k. O
12.3. O
kernel O
pca O
589 O
so O
far O
we O
have O
assumed O
that O
the O
projected O
data O
set O
given O
by O
¢ O
( O
xn O
) O
has O
zero O
mean B
, O
which O
in O
general O
will O
not O
be O
the O
case O
. O
it O
follows O
that O
the O
error B
function I
is O
a O
concave B
function I
of O
w O
and O
hence O
has O
a O
unique O
minimum O
. O
5.5.4 O
tangent B
propagation I
we O
can O
use O
regularization B
to O
encourage O
models O
to O
be O
invariant O
to O
transformations O
of O
the O
input O
through O
the O
technique O
of O
tangent B
propagation I
( O
simard O
et O
al. O
, O
1992 O
) O
. O
the O
graphical O
framework O
can O
be O
extended B
in O
a O
consistent B
way O
to O
graphs O
that O
include O
both O
directed B
and O
undirected B
links O
. O
as O
we O
shall O
see O
, O
for O
models O
which O
are O
based O
on O
a O
ﬁxed O
nonlinear O
feature B
space I
mapping O
φ O
( O
x O
) O
, O
the O
kernel B
function I
is O
given O
by O
the O
relation O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
φ O
( O
x O
) O
tφ O
( O
x O
( O
cid:4 O
) O
) O
. O
conversely O
on O
the O
bottom O
row O
, O
for O
which O
λ O
is O
small O
, O
there O
is O
large O
variance O
( O
shown O
by O
the O
high O
variability O
between O
the O
red O
curves O
in O
the O
left O
plot O
) O
but O
low O
bias B
( O
shown O
by O
the O
good O
ﬁt O
between O
the O
average O
model O
ﬁt O
and O
the O
original O
sinusoidal O
function O
) O
. O
then O
even O
if O
( O
cid:1 O
) O
α O
( O
z1 O
) O
is O
gaussian O
, O
the O
quantity O
( O
cid:1 O
) O
α O
( O
z2 O
) O
will O
be O
a O
mixture O
of O
k O
gaussians O
, O
( O
cid:1 O
) O
α O
( O
z3 O
) O
will O
be O
a O
mixture O
of O
k O
2 O
sages O
is O
preserved O
and O
we O
will O
obtain O
an O
efﬁcient O
inference B
algorithm O
. O
13.3.3 O
extensions O
of O
lds O
as O
with O
the O
hidden O
markov O
model O
, O
there O
is O
considerable O
interest O
in O
extending O
the O
basic O
linear B
dynamical I
system I
in O
order O
to O
increase O
its O
capabilities O
. O
we O
can O
take O
this O
a O
stage O
further O
to O
provide O
a O
deeper O
test O
of O
the O
correctness O
of O
both O
the O
mathematical O
derivation O
of O
the O
update O
equations O
and O
of O
their O
software O
im- O
plementation O
by O
using O
ﬁnite B
differences I
to O
check O
that O
each O
update O
does O
indeed O
give O
a O
( O
constrained O
) O
maximum O
of O
the O
bound O
( O
svens´en O
and O
bishop O
, O
2004 O
) O
. O
if O
y O
has O
a O
gaussian O
distribution O
with O
zero O
mean B
and O
unit O
variance B
, O
then O
σy O
+ O
µ O
will O
have O
a O
gaussian O
distribution O
with O
mean B
µ O
and O
variance B
σ2 O
. O
because O
the O
perceptron B
function O
y O
( O
x O
, O
w O
) O
is O
unchanged O
if O
we O
multiply O
w O
by O
a O
constant O
, O
we O
can O
set O
the O
learning B
rate I
parameter I
η O
equal O
to O
1 O
without O
of O
generality O
. O
because O
the O
model O
is O
represented O
by O
a O
tree-structured O
directed B
graph O
, O
inference B
problems O
can O
be O
solved O
efﬁciently O
using O
the O
sum-product B
algorithm I
. O
furthermore O
, O
the O
network O
must O
also O
exhibit O
invariance B
to O
more O
subtle O
transformations O
such O
as O
elastic O
deformations O
of O
the O
kind O
illustrated O
in O
figure O
5.14. O
one O
simple O
approach O
would O
be O
to O
treat O
the O
image O
as O
the O
input O
to O
a O
fully B
connected I
network O
, O
such O
as O
the O
kind O
shown O
in O
figure O
5.1. O
given O
a O
sufﬁciently O
large O
training O
set O
, O
such O
a O
network O
could O
in O
principle O
yield O
a O
good O
solution O
to O
this O
problem O
and O
would O
learn O
the O
appropriate O
invariances O
by O
example O
. O
, O
tn O
) O
t. O
we O
now O
deﬁne O
the O
gram O
matrix O
k O
= O
φφt O
, O
which O
is O
an O
n O
× O
n O
symmetric O
matrix O
with O
elements O
knm O
= O
φ O
( O
xn O
) O
tφ O
( O
xm O
) O
= O
k O
( O
xn O
, O
xm O
) O
( O
6.6 O
) O
where O
we O
have O
introduced O
the O
kernel B
function I
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
deﬁned O
by O
( O
6.1 O
) O
. O
it O
expresses O
how O
probable O
the O
observed O
data O
set O
is O
for O
different O
settings O
of O
the O
parameter O
vector O
w. O
note O
that O
the O
likelihood O
is O
not O
a O
probability B
distribution O
over O
w O
, O
and O
its O
integral O
with O
respect O
to O
w O
does O
not O
( O
necessarily O
) O
equal O
one O
. O
, O
n. O
similarly O
, O
show O
that O
the O
second-order O
markov O
model O
described O
by O
the O
joint O
distribution O
( O
13.4 O
) O
satisﬁes O
the O
conditional B
independence I
property O
p O
( O
xn|x1 O
, O
. O
we O
can O
alternatively O
use O
the O
graph O
to O
ﬁlter O
distributions O
according O
to O
whether O
they O
respect O
all O
of O
the O
conditional B
independencies O
implied O
by O
the O
d-separation B
properties O
of O
the O
graph O
. O
x O
( O
10.129 O
) O
10.5. O
local B
variational O
methods O
495 O
now O
, O
instead O
of O
ﬁxing O
λ O
and O
varying O
x O
, O
we O
can O
consider O
a O
particular O
x O
and O
then O
adjust O
λ O
until O
the O
tangent O
plane O
is O
tangent O
at O
that O
particular O
x. O
because O
the O
y O
value O
of O
the O
tangent O
line O
at O
a O
particular O
x O
is O
maximized O
when O
that O
value O
coincides O
with O
its O
contact O
point O
, O
we O
have O
{ O
λx O
− O
g O
( O
λ O
) O
} O
. O
it O
should O
be O
stressed O
at O
this O
point O
that O
, O
for O
instance O
, O
λaa O
is O
not O
simply O
given O
by O
the O
inverse B
of O
σaa O
. O
consider O
for O
ex- O
ample O
the O
graph O
shown O
in O
figure O
8.14 O
, O
which O
has O
a O
link B
missing O
between O
variables O
x1 O
and O
x3 O
. O
here O
we O
consider O
the O
use O
of O
maximum B
likelihood I
to O
determine O
the O
parameters O
{ O
wk O
} O
of O
this O
model O
directly O
. O
because O
every O
variable O
node B
will O
have O
received O
messages O
from O
all O
of O
its O
neighbours O
, O
we O
can O
readily O
calculate O
the O
marginal B
distribution O
for O
every O
variable O
in O
the O
graph O
. O
2.5.2 O
nearest-neighbour B
methods I
. O
note O
that O
in O
( O
2.149 O
) O
there O
is O
no O
need O
to O
keep O
track O
of O
the O
normalization O
constants O
in O
the O
prior B
and O
the O
likelihood B
function I
because O
, O
if O
required O
, O
the O
correct O
coefﬁcient O
can O
be O
found O
at O
the O
end O
using O
the O
normalized O
form O
( O
2.146 O
) O
for O
the O
gamma B
distribution I
. O
nevertheless O
, O
it O
provides O
a O
good O
exercise O
in O
the O
use O
of O
variational B
methods O
and O
will O
also O
lay O
the O
foundation O
for O
variational O
treatment O
of O
bayesian O
logistic B
regression I
in O
section O
10.6. O
recall O
that O
the O
likelihood B
function I
for O
w O
, O
and O
the O
prior B
over O
w O
, O
are O
given O
by O
n O
( O
cid:14 O
) O
n O
( O
tn|wtφn O
, O
β O
−1 O
) O
( O
10.87 O
) O
p O
( O
t|w O
) O
= O
p O
( O
w|α O
) O
= O
n O
( O
w|0 O
, O
α O
n=1 O
−1i O
) O
( O
10.88 O
) O
where O
φn O
= O
φ O
( O
xn O
) O
. O
expectation B
propagation I
is O
based O
on O
an O
approximation O
to O
the O
posterior O
distribu- O
tion O
which O
is O
also O
given O
by O
a O
product O
of O
factors O
q O
( O
θ O
) O
= O
in O
which O
each O
factor O
( O
cid:4 O
) O
fi O
( O
θ O
) O
in O
the O
approximation O
corresponds O
to O
one O
of O
the O
factors O
obtain O
a O
practical O
algorithm O
, O
we O
need O
to O
constrain O
the O
factors O
( O
cid:4 O
) O
fi O
( O
θ O
) O
in O
some O
way O
, O
fi O
( O
θ O
) O
in O
the O
true O
posterior O
( O
10.189 O
) O
, O
and O
the O
factor O
1/z O
is O
the O
normalizing O
constant O
needed O
to O
ensure O
that O
the O
left-hand O
side O
of O
( O
10.191 O
) O
integrates O
to O
unity O
. O
more O
generally O
, O
mixture B
models O
can O
comprise O
linear O
com- O
binations O
of O
other O
distributions O
. O
as O
a O
result O
, O
the O
maximum B
likelihood I
solution O
for O
the O
parameters O
no O
longer O
has O
a O
closed-form O
analytical O
solution O
. O
vall O
> O
ll O
$ O
ewrrise O
/2 O
, O
/7 O
subspace O
to O
minimize O
! O
he O
squared O
reoonslructioo O
error B
in O
'oihich O
the O
proje O
< O
: O
tion O
, O
are O
c. O
, O
n O
. O
looking O
again O
at O
figure O
1.5 O
, O
we O
see O
that O
the O
generalization B
error O
is O
roughly O
constant O
between O
m O
= O
3 O
and O
m O
= O
8 O
, O
and O
it O
would O
be O
difﬁcult O
to O
choose O
between O
these O
models O
on O
the O
basis O
of O
this O
plot O
alone O
. O
this O
target O
value O
approximates O
the O
reciprocal O
of O
the O
prior B
probability O
for O
class O
c1 O
. O
the O
product O
of O
the O
factors O
will O
therefore O
also O
be O
from O
the O
exponential B
family I
and O
so O
can O
( O
cid:14 O
) O
( O
cid:4 O
) O
fi O
( O
θ O
) O
1 O
z O
10.7. O
expectation B
propagation I
be O
described O
by O
a O
ﬁnite O
set O
of O
sufﬁcient B
statistics I
. O
p O
( O
z O
) O
f O
( O
z O
) O
z O
variables O
, O
we O
wish O
to O
evaluate O
the O
expectation B
( O
cid:6 O
) O
e O
[ O
f O
] O
= O
f O
( O
z O
) O
p O
( O
z O
) O
dz O
( O
11.1 O
) O
where O
the O
integral O
is O
replaced O
by O
summation O
in O
the O
case O
of O
discrete O
variables O
. O
note O
that O
the O
kernel B
function I
can O
3.4. O
bayesian O
model B
comparison I
161 O
chapter O
6 O
be O
negative O
as O
well O
as O
positive O
, O
so O
although O
it O
satisﬁes O
a O
summation O
constraint O
, O
the O
corresponding O
predictions O
are O
not O
necessarily O
convex O
combinations O
of O
the O
training B
set I
target O
variables O
. O
gaussian O
processes O
for O
machine O
learning B
. O
members O
of O
the O
exponential B
family I
have O
many O
important O
properties O
in O
com- O
mon O
, O
and O
it O
is O
illuminating O
to O
discuss O
these O
properties O
in O
some O
generality O
. O
the O
remaining O
data O
points O
are O
called O
support O
vectors O
, O
and O
because O
they O
satisfy O
tny O
( O
xn O
) O
= O
1 O
, O
they O
correspond O
to O
points O
that O
lie O
on O
the O
maximum B
margin I
hyperplanes O
in O
feature B
space I
, O
as O
illustrated O
in O
figure O
7.1. O
this O
property O
is O
central O
to O
the O
practical O
applicability O
of O
support B
vector I
machines O
. O
thus O
the O
maximum B
likelihood I
estimate O
for O
π O
is O
simply O
the O
fraction O
of O
points O
in O
class O
c1 O
as O
expected O
. O
although O
the O
bias-variance O
decomposition O
may O
provide O
some O
interesting O
in- O
sights O
into O
the O
model O
complexity O
issue O
from O
a O
frequentist B
perspective O
, O
it O
is O
of O
lim- O
ited O
practical O
value O
, O
because O
the O
bias-variance O
decomposition O
is O
based O
on O
averages O
with O
respect O
to O
ensembles O
of O
data O
sets O
, O
whereas O
in O
practice O
we O
have O
only O
the O
single O
observed O
data O
set O
. O
µxi O
( O
9.45 O
) O
( O
9.46 O
) O
( O
9.47 O
) O
( O
9.48 O
) O
( O
9.49 O
) O
( O
9.50 O
) O
exercise O
9.12 O
the O
mean B
and O
covariance B
of O
this O
mixture B
distribution I
are O
given O
by O
d O
( O
cid:14 O
) O
i=1 O
p O
( O
x|µk O
) O
= O
k O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
k=1 O
πkµk O
( O
cid:26 O
) O
e O
[ O
x O
] O
= O
( O
cid:27 O
) O
− O
e O
[ O
x O
] O
e O
[ O
x O
] O
t O
cov O
[ O
x O
] O
= O
σk O
+ O
µkµt O
k O
πk O
k=1 O
where O
σk O
= O
diag O
{ O
µki O
( O
1 O
− O
µki O
) O
} O
. O
if O
we O
further O
assume O
that O
the O
prior B
is O
ﬂat O
with O
width O
∆wprior O
so O
that O
p O
( O
w O
) O
= O
1/∆wprior O
, O
then O
we O
have O
( O
cid:6 O
) O
p O
( O
d O
) O
= O
p O
( O
d|w O
) O
p O
( O
w O
) O
dw O
( O
cid:7 O
) O
p O
( O
d|wmap O
) O
∆wposterior O
∆wprior O
( O
3.70 O
) O
3.4. O
bayesian O
model B
comparison I
163 O
figure O
3.12 O
we O
can O
obtain O
a O
rough O
approximation O
to O
the O
model B
evidence I
if O
we O
assume O
that O
the O
posterior O
distribution O
over O
parame- O
ters O
is O
sharply O
peaked O
around O
its O
mode O
wmap O
. O
for O
re- O
gression O
we O
use O
linear O
outputs O
and O
a O
sum-of-squares B
error I
, O
for O
( O
multiple O
independent B
) O
binary O
classiﬁcations O
we O
use O
logistic B
sigmoid I
outputs O
and O
a O
cross-entropy O
error O
func- O
tion O
, O
and O
for O
multiclass O
classiﬁcation B
we O
use O
softmax O
outputs O
with O
the O
corresponding O
multiclass B
cross-entropy O
error B
function I
. O
, O
xn−1 O
to O
the O
node B
xn O
passes O
through O
the O
node B
zn O
, O
which O
is O
observed O
. O
recall O
that O
for O
directed O
graphs O
, O
the O
joint O
distribution O
was O
automatically O
normalized O
as O
a O
consequence O
of O
the O
normalization O
of O
each O
of O
the O
conditional B
distributions O
in O
the O
factorization B
. O
suppose O
we O
wish O
to O
minimize O
an O
error B
function I
e O
with O
respect O
to O
the O
parameter O
w O
in O
figure O
5.8. O
the O
derivative B
of O
the O
error B
function I
is O
given O
by O
( O
cid:2 O
) O
k O
, O
j O
∂e O
∂w O
= O
∂e O
∂yk O
∂yk O
∂zj O
∂zj O
∂w O
( O
5.71 O
) O
in O
which O
the O
jacobian O
matrix O
for O
the O
red O
module O
in O
figure O
5.8 O
appears O
in O
the O
middle O
term O
. O
with O
no O
observed O
data O
points O
, O
we O
have O
the O
prior B
variance O
, O
whereas O
if O
the O
number O
of O
data O
points O
n O
→ O
∞ O
, O
the O
variance B
σ2 O
n O
goes O
to O
zero O
and O
the O
posterior O
distribution O
becomes O
inﬁnitely O
peaked O
around O
the O
maximum B
likelihood I
solution O
. O
using O
a O
= O
wtφ O
, O
and O
multiplying O
by O
the O
prior B
distribution O
, O
we O
obtain O
the O
following O
bound O
on O
the O
joint O
distribution O
of O
t O
and O
w O
p O
( O
t O
, O
w O
) O
= O
p O
( O
t|w O
) O
p O
( O
w O
) O
( O
cid:2 O
) O
h O
( O
w O
, O
ξ O
) O
p O
( O
w O
) O
where O
ξ O
denotes O
the O
set O
{ O
ξn O
} O
of O
variational B
parameters O
, O
and O
n O
( O
cid:14 O
) O
− O
λ O
( O
ξn O
) O
( O
[ O
wtφn O
] O
2 O
− O
ξ2 O
n O
) O
wtφntn O
− O
( O
wtφn O
+ O
ξn O
) O
/2 O
h O
( O
w O
, O
ξ O
) O
= O
σ O
( O
ξn O
) O
exp O
( O
10.152 O
) O
( O
10.153 O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
n=1 O
. O
10.38 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
in O
this O
exercise O
and O
the O
next O
, O
we O
shall O
verify O
the O
results O
( O
10.214 O
) O
– O
( O
10.224 O
) O
for O
the O
expectation B
propagation I
algorithm O
applied O
to O
the O
clutter B
problem I
. O
wallace O
: O
statistical O
and O
inductive O
inference B
by O
minimum O
massage O
length O
. O
halting O
training B
before O
260 O
5. O
neural O
networks O
αw O
1 O
= O
1 O
, O
αb O
1 O
= O
1 O
, O
αw O
2 O
= O
1 O
, O
αb O
2 O
= O
1 O
−0.5 O
0 O
0.5 O
1 O
αw O
1 O
= O
1000 O
, O
αb O
1 O
= O
100 O
, O
αw O
2 O
= O
1 O
, O
αb O
2 O
= O
1 O
−0.5 O
0 O
0.5 O
1 O
4 O
2 O
0 O
−2 O
−4 O
−6 O
−1 O
5 O
0 O
−5 O
−10 O
−1 O
40 O
20 O
0 O
−20 O
−40 O
−60 O
−1 O
5 O
0 O
−5 O
−10 O
−1 O
αw O
1 O
= O
1 O
, O
αb O
1 O
= O
1 O
, O
αw O
2 O
= O
10 O
, O
αb O
2 O
= O
1 O
−0.5 O
0 O
0.5 O
1 O
αw O
1 O
= O
1000 O
, O
αb O
1 O
= O
1000 O
, O
αw O
2 O
= O
1 O
, O
αb O
2 O
= O
1 O
−0.5 O
0 O
0.5 O
1 O
figure O
5.11 O
illustration O
of O
the O
effect O
of O
the O
hyperparameters O
governing O
the O
prior B
distribution O
over O
weights O
and O
biases O
in O
a O
two-layer O
network O
having O
a O
single O
input O
, O
a O
single O
linear O
output O
, O
and O
12 O
hidden O
units O
having O
‘ O
tanh O
’ O
activation O
functions O
. O
atl O
( O
y O
− O
b O
) O
+ O
λµ O
( O
2.111 O
) O
( O
2.112 O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
the O
evaluation O
of O
this O
conditional B
can O
be O
seen O
as O
an O
example O
of O
bayes O
’ O
theorem O
. O
thus O
the O
objective O
function O
( O
7.21 O
) O
can O
be O
written O
( O
up O
to O
an O
overall O
multiplicative O
constant O
) O
in O
the O
form O
n O
( O
cid:2 O
) O
esv O
( O
yntn O
) O
+ O
λ O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
2 O
( O
7.44 O
) O
n=1 O
where O
λ O
= O
( O
2c O
) O
−1 O
, O
and O
esv O
( O
· O
) O
is O
the O
hinge B
error I
function I
deﬁned O
by O
esv O
( O
yntn O
) O
= O
[ O
1 O
− O
yntn O
] O
+ O
( O
7.45 O
) O
where O
[ O
· O
] O
+ O
denotes O
the O
positive O
part O
. O
the O
resulting O
regression B
function I
, O
given O
by O
the O
condi- O
tional O
mean B
, O
is O
shown O
by O
the O
red O
line O
, O
along O
with O
the O
two- O
standard-deviation O
region O
for O
the O
conditional B
distribution O
p O
( O
t|x O
) O
shown O
by O
the O
red O
shading O
. O
following O
the O
discussion O
of O
canonical O
link O
functions O
in O
section O
4.3.6 O
, O
we O
consider O
a O
network O
having O
a O
single O
output O
whose O
activation B
function I
is O
a O
logistic B
sigmoid I
y O
= O
σ O
( O
a O
) O
≡ O
1 O
( O
5.19 O
) O
so O
that O
0 O
( O
cid:1 O
) O
y O
( O
x O
, O
w O
) O
( O
cid:1 O
) O
1. O
we O
can O
interpret O
y O
( O
x O
, O
w O
) O
as O
the O
conditional B
probability I
p O
( O
c1|x O
) O
, O
with O
p O
( O
c2|x O
) O
given O
by O
1 O
− O
y O
( O
x O
, O
w O
) O
. O
furthermore O
, O
the O
change O
in O
weight B
vector I
may O
have O
caused O
some O
previously O
correctly O
classiﬁed O
patterns O
to O
become O
misclassiﬁed O
. O
the O
corresponding O
sum-of-squares B
error I
function O
has O
the O
property O
= O
yk O
− O
tk O
∂e O
∂ak O
( O
5.18 O
) O
which O
we O
shall O
make O
use O
of O
when O
discussing O
error B
backpropagation I
in O
section O
5.3. O
now O
consider O
the O
case O
of O
binary O
classiﬁcation O
in O
which O
we O
have O
a O
single O
target O
variable O
t O
such O
that O
t O
= O
1 O
denotes O
class O
c1 O
and O
t O
= O
0 O
denotes O
class O
c2 O
. O
be- O
cause O
we O
know O
the O
class O
priors O
and O
the O
class-conditional O
densities O
, O
it O
is O
straightfor- O
ward O
to O
evaluate O
and O
plot O
the O
true O
posterior O
probabilities O
as O
well O
as O
the O
minimum O
misclassiﬁcation-rate O
decision B
boundary I
, O
as O
shown O
in O
figure O
a.7 O
. O
also O
shown O
are O
the O
misclassiﬁcation O
error B
in O
black O
and O
the O
squared O
error B
in O
green O
. O
the O
reduction O
in O
variance B
is O
greater O
if O
the O
variance B
in O
the O
posterior O
mean O
is O
greater O
. O
4.3.2 O
logistic B
regression I
. O
4.8 O
( O
( O
cid:12 O
) O
) O
using O
( O
4.57 O
) O
and O
( O
4.58 O
) O
, O
derive O
the O
result O
( O
4.65 O
) O
for O
the O
posterior O
class O
probability B
in O
the O
two-class O
generative B
model I
with O
gaussian O
densities O
, O
and O
verify O
the O
results O
( O
4.66 O
) O
and O
( O
4.67 O
) O
for O
the O
parameters O
w O
and O
w0 O
. O
it O
is O
possible O
to O
make O
use O
of O
kl O
( O
p O
( O
cid:5 O
) O
q O
) O
to O
deﬁne O
a O
useful O
inference B
procedure O
, O
but O
this O
requires O
a O
rather O
different O
approach O
to O
the O
one O
discussed O
here O
, O
and O
will O
be O
considered O
in O
detail O
when O
we O
discuss O
expectation B
propagation I
. O
the O
mechanism O
by O
which O
appropriate O
inputs O
are O
preferred O
is O
discussed O
in O
section O
7.2.2. O
consider O
a O
gaussian O
process O
with O
a O
two-dimensional O
input O
space O
x O
= O
( O
x1 O
, O
x2 O
) O
, O
having O
a O
kernel B
function I
of O
the O
form O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
θ0 O
exp O
−1 O
2 O
ηi O
( O
xi O
− O
x O
( O
cid:4 O
) O
i O
) O
2 O
. O
additionally O
, O
it O
typically O
leads O
to O
much O
sparser O
models O
resulting O
in O
correspondingly O
faster O
performance O
on O
test O
data O
whilst O
maintaining O
comparable O
generalization B
error O
. O
( O
a O
) O
histogram O
distribution O
of O
30 O
data O
points O
drawn O
from O
a O
gaussian O
distribution O
, O
together O
with O
the O
maximum B
likelihood I
ﬁt O
ob- O
tained O
from O
a O
t-distribution O
( O
red O
curve O
) O
and O
a O
gaussian O
( O
green O
curve O
, O
largely O
hidden O
by O
the O
red O
curve O
) O
. O
the O
normalization O
condition O
for O
the O
coefficients O
ai O
is O
obtained O
by O
requiring O
that O
the O
eigenvectors O
in O
feature B
space I
be O
normalized O
. O
we O
can O
rewrite O
this O
in O
terms O
of O
the O
corresponding O
partitioning O
of O
the O
covariance B
matrix I
given O
by O
( O
2.67 O
) O
, O
as O
we O
did O
for O
the O
conditional B
distribution O
. O
( O
11.18 O
) O
l=1 O
an O
obvious O
problem O
with O
this O
approach O
is O
that O
the O
number O
of O
terms O
in O
the O
summation O
grows O
exponentially O
with O
the O
dimensionality O
of O
z. O
furthermore O
, O
as O
we O
have O
already O
noted O
, O
the O
kinds O
of O
probability B
distributions O
of O
interest O
will O
often O
have O
much O
of O
their O
mass O
conﬁned O
to O
relatively O
small O
regions O
of O
z O
space O
and O
so O
uniform B
sampling I
will O
be O
very O
inefﬁcient O
because O
in O
high-dimensional O
problems O
, O
only O
a O
very O
small O
proportion O
of O
the O
samples O
will O
make O
a O
signiﬁcant O
contribution O
to O
the O
sum O
. O
x1 O
x2 O
8.4. O
inference B
in O
graphical O
models O
409 O
x3 O
fa O
fb O
fc O
x4 O
graph O
whose O
unnormalized O
joint O
distribution O
is O
given O
by O
( O
cid:4 O
) O
p O
( O
x O
) O
= O
fa O
( O
x1 O
, O
x2 O
) O
fb O
( O
x2 O
, O
x3 O
) O
fc O
( O
x2 O
, O
x4 O
) O
. O
provided O
this O
is O
so O
, O
we O
can O
show O
that O
bayesian O
model B
comparison I
will O
on O
average O
favour O
the O
correct O
model O
. O
9.1. O
k-means O
clustering B
427 O
figure O
9.2 O
plot O
of O
the O
cost B
function I
j O
given O
by O
( O
9.1 O
) O
after O
each O
e O
step O
( O
blue O
points O
) O
and O
m O
step O
( O
red O
points O
) O
of O
the O
k- O
means O
algorithm O
for O
the O
example O
shown O
in O
figure O
9.1. O
the O
algo- O
rithm O
has O
converged O
after O
the O
third O
m O
step O
, O
and O
the O
ﬁnal O
em O
cycle O
pro- O
duces O
no O
changes O
in O
either O
the O
as- O
signments O
or O
the O
prototype O
vectors O
. O
n=1 O
( O
2.143 O
) O
it O
is O
worth O
spending O
a O
moment O
studying O
the O
form O
of O
the O
posterior O
mean O
and O
variance B
. O
another O
generalization B
of O
the O
network O
architecture O
is O
to O
include O
skip-layer O
con- O
nections O
, O
each O
of O
which O
is O
associated O
with O
a O
corresponding O
adaptive O
parameter O
. O
it O
has O
elements O
iij O
that O
equal O
1 O
if O
i O
= O
j O
and O
0 O
if O
i O
( O
cid:2 O
) O
= O
j. O
functional B
is O
discussed O
in O
appendix O
d. O
a O
functional B
is O
denoted O
f O
[ O
y O
] O
where O
y O
( O
x O
) O
is O
some O
function O
. O
two O
other O
common O
tasks O
are O
to O
ﬁnd O
a O
setting O
of O
the O
variables O
that O
has O
the O
largest O
prob- O
ability O
and O
to O
ﬁnd O
the O
value O
of O
that O
probability B
. O
note O
that O
this O
is O
independent B
of O
the O
covariance B
matrix I
σ. O
show O
that O
the O
maximum B
likelihood I
solution O
for O
σ O
is O
given O
by O
n O
( O
cid:2 O
) O
( O
cid:10 O
) O
n=1 O
σ O
= O
1 O
n O
( O
cid:11 O
) O
( O
cid:10 O
) O
( O
cid:11 O
) O
t O
tn O
− O
wt O
mlφ O
( O
xn O
) O
tn O
− O
wt O
mlφ O
( O
xn O
) O
. O
if O
we O
pick O
out O
all O
terms O
that O
are O
second B
order I
in O
xa O
, O
we O
have O
−1 O
2 O
xt O
a O
λaaxa O
( O
2.72 O
) O
from O
which O
we O
can O
immediately O
conclude O
that O
the O
covariance B
( O
inverse B
precision O
) O
of O
p O
( O
xa|xb O
) O
is O
given O
by O
σa|b O
= O
λ O
−1 O
aa O
. O
recall O
that O
the O
goal O
of O
the O
leapfrog O
integration O
in O
hybrid O
monte O
carlo O
is O
to O
move O
a O
substantial O
distance O
through O
phase B
space I
to O
a O
new O
state O
that O
is O
relatively O
independent B
of O
the O
initial O
state O
and O
still O
achieve O
a O
high O
probability B
of O
acceptance O
. O
( O
10.151 O
) O
note O
that O
because O
this O
bound O
is O
applied O
to O
each O
of O
the O
terms O
in O
the O
likelihood B
function I
separately O
, O
there O
is O
a O
variational B
parameter O
ξn O
corresponding O
to O
each O
training B
set I
observation O
( O
φn O
, O
tn O
) O
. O
the O
behaviour O
of O
the O
network O
in O
this O
case O
is O
sometimes O
explained O
qualitatively O
in O
terms O
of O
the O
effective O
number O
of O
degrees O
of O
freedom O
in O
the O
network O
, O
in O
which O
this O
number O
starts O
out O
small O
and O
then O
to O
grows O
during O
the O
training B
process O
, O
corresponding O
to O
a O
steady O
increase O
in O
the O
effective O
complexity O
of O
the O
model O
. O
the O
transformed O
< O
p O
remains O
diagonal B
, O
and O
hence O
factor B
analysis I
is O
covariant O
under O
component-wise O
re-scaling O
of O
the O
data O
variables O
; O
( O
ii O
) O
a O
is O
orthogonal O
and O
< O
p O
is O
pro O
( O
cid:173 O
) O
21. O
this O
corresponds O
to O
probabilistic O
pca O
. O
fundamentals O
of O
speech B
recognition I
. O
we O
have O
also O
specialized O
to O
the O
case O
of O
isotropic B
co- O
variances O
for O
the O
components O
, O
although O
the O
mixture B
density I
network I
can O
readily O
be O
extended B
to O
allow O
for O
general O
covariance B
matrices O
by O
representing O
the O
covariances O
using O
a O
cholesky O
factorization B
( O
williams O
, O
1996 O
) O
. O
cos O
θn O
( O
cid:23 O
) O
( O
cid:22 O
) O
1 O
n O
n=1 O
( O
cid:23 O
) O
( O
cid:22 O
) O
1 O
n O
n=1 O
110 O
2. O
probability B
distributions O
100 O
80 O
60 O
40 O
1 O
on O
the O
left O
figure O
2.21 O
plots O
of O
the O
‘ O
old O
faith- O
ful O
’ O
data O
in O
which O
the O
blue O
curves O
show O
contours O
of O
constant O
proba- O
bility O
density B
. O
show O
that O
the O
predictive B
distribution I
is O
identical O
to O
the O
result O
( O
3.58 O
) O
obtained O
in O
section O
3.3.2 O
for O
the O
bayesian O
linear B
regression I
model O
. O
we O
see O
that O
( O
2.17 O
) O
has O
the O
same O
functional B
dependence O
on O
µ O
as O
the O
prior B
distribution O
, O
reﬂecting O
the O
conjugacy O
properties O
of O
the O
prior B
with O
respect O
to O
the O
like- O
lihood O
function O
. O
the O
problem O
with O
an O
exponentially O
large O
number O
of O
cells O
is O
that O
we O
would O
need O
an O
exponentially O
large O
quantity O
of O
training B
data O
in O
order O
to O
ensure O
that O
the O
cells O
are O
not O
empty O
. O
, O
xn O
} O
called O
a O
training B
set I
is O
used O
to O
tune O
the O
parameters O
of O
an O
adaptive O
model O
. O
i O
the O
jacobian O
matrix O
can O
be O
evaluated O
using O
a O
backpropagation B
procedure O
that O
is O
similar O
to O
the O
one O
derived O
earlier O
for O
evaluating O
the O
derivatives O
of O
an O
error B
function I
with O
respect O
to O
the O
weights O
. O
this O
proba- O
section O
6.4 O
exercises O
appendix O
e O
exercises O
599 O
bilistic O
foundation O
also O
makes O
it O
very O
straightforward O
to O
define O
generalizations O
of O
gtm O
( O
bishop O
et O
al. O
, O
1998a O
) O
such O
as O
a O
bayesian O
treatment O
, O
dealing O
with O
missing O
val- O
ues O
, O
a O
principled O
extension O
to O
discrete O
variables O
, O
the O
use O
of O
gaussian O
processes O
to O
define O
the O
manifold B
, O
or O
a O
hierarchical B
gtm O
model O
( O
tino O
and O
nabney O
, O
2002 O
) O
. O
, O
tn O
) O
t O
, O
is O
given O
by O
n O
( O
cid:2 O
) O
n=1 O
ln O
p O
( O
t|x O
, O
w O
, O
σ2 O
) O
= O
− O
1 O
2σ2 O
|y O
( O
xn O
, O
w O
) O
− O
tn|q O
− O
n O
q O
ln O
( O
2σ2 O
) O
+ O
const O
( O
2.295 O
) O
where O
‘ O
const O
’ O
denotes O
terms O
independent B
of O
both O
w O
and O
σ2 O
. O
exercises O
8.1 O
( O
( O
cid:12 O
) O
) O
www O
by O
marginalizing O
out O
the O
variables O
in O
order O
, O
show O
that O
the O
representation O
( O
8.5 O
) O
for O
the O
joint O
distribution O
of O
a O
directed B
graph O
is O
correctly O
normalized O
, O
provided O
each O
of O
the O
conditional B
distributions O
is O
normalized O
. O
again O
, O
we O
shall O
consider O
an O
error B
function I
that O
consists O
of O
a O
sum O
of O
terms O
, O
one O
for O
each O
pattern O
in O
the O
data O
set O
, O
so O
that O
e O
= O
n O
en O
. O
exercises O
457 O
9.11 O
( O
( O
cid:12 O
) O
) O
in O
section O
9.3.2 O
, O
we O
obtained O
a O
relationship O
between O
k O
means O
and O
em O
for O
gaussian O
mixtures O
by O
considering O
a O
mixture B
model I
in O
which O
all O
components O
have O
covariance B
i O
. O
, O
n. O
6.17 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
the O
sum-of-squares B
error I
function O
( O
6.39 O
) O
for O
data O
having O
noisy O
inputs O
, O
where O
ν O
( O
ξ O
) O
is O
the O
distribution O
of O
the O
noise O
. O
t O
d O
( O
12.15 O
) O
x O
n O
ui O
- O
x O
ui O
n=l O
i=m+l O
i=m+l O
there O
remains O
the O
task O
of O
minimizing O
j O
with O
respect O
to O
the O
{ O
ui O
} O
, O
which O
must O
be O
a O
constrained O
minimization O
otherwise O
we O
will O
obtain O
the O
vacuous O
result O
ui O
= O
o. O
the O
constraints O
arise O
from O
the O
orthonormality O
conditions O
and O
, O
as O
we O
shall O
see O
, O
the O
solution O
will O
be O
expressed O
in O
terms O
of O
the O
eigenvector O
expansion O
of O
the O
covariance B
matrix I
. O
consider O
a O
correlated O
gaussian O
in O
two O
variables O
, O
as O
illustrated O
in O
figure O
11.11 O
, O
having O
con- O
ditional O
distributions O
of O
width O
l O
and O
marginal B
distributions O
of O
width O
l. O
the O
typical O
step O
size O
is O
governed O
by O
the O
conditional B
distributions O
and O
will O
be O
of O
order O
l. O
because O
the O
state O
evolves O
according O
to O
a O
random O
walk O
, O
the O
number O
of O
steps O
needed O
to O
obtain O
independent B
samples O
from O
the O
distribution O
will O
be O
of O
order O
( O
l/l O
) O
2. O
of O
course O
if O
the O
gaussian O
distribution O
were O
uncorrelated O
, O
then O
the O
gibbs O
sampling O
procedure O
would O
be O
optimally O
efﬁcient O
. O
is O
not O
a O
set O
of O
independent B
samples O
from O
p O
( O
z O
) O
because O
successive O
samples O
are O
highly O
correlated O
. O
two O
inputs O
x O
and O
x O
( O
cid:4 O
) O
will O
give O
a O
large O
value O
for O
the O
kernel B
function I
, O
and O
hence O
appear O
similar O
, O
if O
they O
have O
signiﬁcant O
probability B
under O
a O
range O
of O
different O
components O
. O
a O
popular O
choice O
is O
148 O
3. O
linear O
models O
for B
regression I
the O
squared O
loss B
function I
, O
for O
which O
the O
optimal O
prediction O
is O
given O
by O
the O
conditional B
expectation I
, O
which O
we O
denote O
by O
h O
( O
x O
) O
and O
which O
is O
given O
by O
h O
( O
x O
) O
= O
e O
[ O
t|x O
] O
= O
tp O
( O
t|x O
) O
dt O
. O
in O
particular O
, O
evaluation O
of O
the O
posterior O
distribution O
would O
require O
normalization O
of O
the O
product O
of O
a O
prior B
distribution O
and O
a O
likelihood B
function I
that O
itself O
comprises O
a O
product O
of O
logistic B
sigmoid I
functions O
, O
one O
for O
every O
data O
point O
. O
420 O
8. O
graphical O
models O
figure O
8.54 O
example O
of O
a O
graphical B
model I
used O
to O
explore O
the O
con- O
ditional O
independence O
properties O
of O
the O
head-to-head B
path I
a–c–b O
when O
a O
descendant O
of O
c O
, O
namely O
the O
node B
d O
, O
is O
observed O
. O
if O
the O
probability B
of O
a O
real-valued O
variable O
x O
falling O
in O
the O
interval O
( O
x O
, O
x O
+ O
δx O
) O
is O
given O
by O
p O
( O
x O
) O
δx O
for O
δx O
→ O
0 O
, O
then O
p O
( O
x O
) O
is O
called O
the O
probability B
density O
over O
x. O
this O
is O
illustrated O
in O
figure O
1.12. O
the O
probability B
that O
x O
will O
lie O
in O
an O
interval O
( O
a O
, O
b O
) O
is O
then O
given O
by O
( O
cid:6 O
) O
b O
p O
( O
x O
∈ O
( O
a O
, O
b O
) O
) O
= O
p O
( O
x O
) O
dx O
. O
the O
rules O
of O
probability B
sum O
rule O
p O
( O
x O
) O
= O
( O
cid:2 O
) O
y O
p O
( O
x O
, O
y O
) O
product B
rule I
p O
( O
x O
, O
y O
) O
= O
p O
( O
y O
|x O
) O
p O
( O
x O
) O
. O
in O
general O
, O
the O
network O
mapping O
rep- O
resented O
by O
a O
trained O
neural B
network I
will O
be O
nonlinear O
, O
and O
so O
the O
elements O
of O
the O
jacobian O
matrix O
will O
not O
be O
constants O
but O
will O
depend O
on O
the O
particular O
input O
vector O
used O
. O
figure O
11.15 O
a O
probability B
distribution O
over O
two O
variables O
z1 O
and O
z2 O
that O
is O
uniform O
over O
the O
shaded O
regions O
and O
that O
is O
zero O
everywhere O
else O
. O
section O
8.1.4 O
88 O
2. O
probability B
distributions O
2.3.2 O
marginal B
gaussian O
distributions O
we O
have O
seen O
that O
if O
a O
joint O
distribution O
p O
( O
xa O
, O
xb O
) O
is O
gaussian O
, O
then O
the O
condi- O
tional O
distribution O
p O
( O
xa|xb O
) O
will O
again O
be O
gaussian O
. O
one O
way O
to O
restrict O
the O
family O
of O
approximating O
distributions O
is O
to O
use O
a O
paramet- O
ric O
distribution O
q O
( O
z|ω O
) O
governed O
by O
a O
set O
of O
parameters O
ω. O
the O
lower B
bound I
l O
( O
q O
) O
then O
becomes O
a O
function O
of O
ω O
, O
and O
we O
can O
exploit O
standard O
nonlinear O
optimization O
techniques O
to O
determine O
the O
optimal O
values O
for O
the O
parameters O
. O
thus O
the O
number O
of O
degrees B
of I
freedom I
in O
the O
covariance B
matrix I
c O
is O
given O
by O
dm O
+ O
1 O
- O
m O
( O
m O
- O
1 O
) O
/2 O
. O
further O
suppose O
that O
, O
conditioned O
on O
the O
class O
ck O
, O
the O
m O
components O
of O
φ O
are O
independent B
, O
so O
that O
the O
class-conditional O
density B
factorizes O
with O
respect O
to O
the O
feature O
vector O
components O
. O
a O
particular O
instance O
of O
the O
monte O
carlo O
em O
algorithm O
, O
called O
stochastic B
em O
, O
arises O
if O
we O
consider O
a O
ﬁnite O
mixture O
model O
, O
and O
draw O
just O
one O
sample O
at O
each O
e O
step O
. O
, O
n O
and O
the O
kernel B
function I
k O
( O
x O
, O
xn O
) O
is O
given O
by O
and O
we O
have O
deﬁned O
g O
( O
x O
) O
= O
f O
( O
x O
, O
t O
) O
dt O
. O
note O
that O
low O
probability B
events O
x O
correspond O
to O
high O
information O
content O
. O
although O
such O
graphs O
can O
represent O
a O
broader O
class O
of O
distributions O
than O
either O
directed B
or O
undirected B
alone O
, O
there O
remain O
distributions O
for O
which O
even O
a O
chain B
graph I
can O
not O
provide O
a O
perfect B
map I
. O
note O
that O
the O
variables O
mk O
are O
subject O
to O
the O
constraint O
( O
cid:16 O
) O
k O
( O
cid:2 O
) O
k=1 O
mk O
= O
n. O
( O
2.35 O
) O
( O
2.36 O
) O
2.2.1 O
the O
dirichlet O
distribution O
we O
now O
introduce O
a O
family O
of O
prior B
distributions O
for O
the O
parameters O
{ O
µk O
} O
of O
the O
multinomial B
distribution I
( O
2.34 O
) O
. O
recall O
that O
in O
the O
boxes O
of O
fruit O
example O
, O
the O
observation O
of O
the O
identity O
of O
the O
fruit O
provided O
relevant O
information O
that O
altered O
the O
probability B
that O
the O
chosen O
box O
was O
the O
red O
one O
. O
show O
that O
the O
message B
passing I
algorithm O
discussed O
in O
section O
8.4.1 O
can O
be O
used O
to O
solve O
this O
efﬁciently O
, O
and O
discuss O
which O
messages O
are O
modiﬁed O
and O
in O
what O
way O
. O
2.20 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
a O
positive B
deﬁnite I
matrix I
σ O
can O
be O
deﬁned O
as O
one O
for O
which O
the O
quadratic O
form O
( O
2.285 O
) O
is O
positive O
for O
any O
real O
value O
of O
the O
vector O
a. O
show O
that O
a O
necessary O
and O
sufﬁcient O
condition O
for O
σ O
to O
be O
positive B
deﬁnite I
is O
that O
all O
of O
the O
eigenvalues O
λi O
of O
σ O
, O
deﬁned O
by O
( O
2.45 O
) O
, O
are O
positive O
. O
ieee O
transactions O
on O
information B
theory I
45 O
, O
399–431 O
. O
we O
therefore O
apply O
the O
local B
variational O
bound O
to O
each O
of O
the O
logistic B
sigmoid I
factors O
as O
before O
. O
in O
simple O
linear B
regression I
, O
we O
section O
3.1.4 O
340 O
7. O
sparse O
kernel O
machines O
figure O
7.6 O
plot O
of O
an O
-insensitive B
error I
function I
( O
in O
red O
) O
in O
which O
the O
error B
increases O
lin- O
early O
with O
distance O
beyond O
the O
insen- O
sitive O
region O
. O
now O
suppose O
that O
both O
the O
mean B
and O
the O
precision O
are O
unknown O
. O
9.2. O
mixtures O
of O
gaussians O
in O
section O
2.3.9 O
we O
motivated O
the O
gaussian O
mixture B
model I
as O
a O
simple O
linear O
super- O
position O
of O
gaussian O
components O
, O
aimed O
at O
providing O
a O
richer O
class O
of O
density B
mod- O
els O
than O
the O
single O
gaussian O
. O
these O
additional O
factorizations O
are O
a O
consequence O
of O
the O
interaction O
between O
the O
assumed O
factorization O
and O
the O
conditional B
independence I
properties O
of O
the O
true O
distribution O
, O
as O
characterized O
by O
the O
directed B
graph O
in O
figure O
10.5. O
we O
shall O
refer O
to O
these O
additional O
factorizations O
as O
induced O
factorizations O
be- O
cause O
they O
arise O
from O
an O
interaction O
between O
the O
factorization B
assumed O
in O
the O
varia- O
tional O
posterior O
distribution O
and O
the O
conditional B
independence I
properties O
of O
the O
true O
joint O
distribution O
. O
3.1.1 O
maximum B
likelihood I
and O
least O
squares O
. O
however O
, O
the O
overall O
probability B
of O
accepting O
a O
sample O
from O
the O
posterior O
decreases O
rapidly O
as O
the O
number O
of O
observed O
variables O
increases O
and O
as O
the O
number O
of O
states O
that O
those O
variables O
can O
take O
increases O
, O
and O
so O
this O
approach O
is O
rarely O
used O
in O
practice O
. O
the O
conditional B
distribution O
at O
each O
node B
is O
given O
by O
a O
set O
of O
nonnegative O
pa- O
rameters O
subject O
to O
the O
usual O
normalization O
constraint O
. O
error B
bounds O
for O
convolu- O
tional O
codes O
and O
an O
asymptotically O
optimum O
de- O
coding O
algorithm O
. O
in O
the O
limit O
, O
the O
hyperplane O
becomes O
independent B
of O
data O
points O
that O
are O
not O
support O
vectors O
. O
the O
process O
of O
moralization B
adds O
the O
fewest O
extra O
links O
and O
so O
retains O
the O
maximum O
number O
of O
independence O
properties O
. O
we O
turn O
now O
to O
a O
discussion O
of O
super- O
vised O
learning B
, O
starting O
with O
regression B
. O
show O
that O
the O
coefﬁcients O
w O
= O
{ O
wi O
} O
that O
minimize O
this O
error B
function I
are O
given O
by O
the O
solution O
to O
the O
following O
set O
of O
linear O
equations O
m O
( O
cid:2 O
) O
where O
n O
( O
cid:2 O
) O
n=1 O
aij O
= O
aijwj O
= O
ti O
j=0 O
( O
xn O
) O
i+j O
, O
ti O
= O
( O
1.122 O
) O
( O
xn O
) O
itn O
. O
an O
intro- O
duction O
to O
computational B
learning I
theory I
. O
by O
considering O
an O
additional O
data O
point O
( O
xn O
+1 O
, O
tn O
+1 O
) O
, O
and O
by O
completing B
the I
square I
in O
the O
exponential O
, O
show O
that O
the O
resulting O
posterior O
distribution O
is O
again O
given O
by O
( O
3.49 O
) O
but O
with O
sn O
replaced O
by O
sn O
+1 O
and O
mn O
replaced O
by O
mn O
+1 O
. O
the O
ﬁrst O
of O
the O
three O
examples O
is O
shown O
in O
figure O
8.15 O
, O
and O
the O
joint O
distribution O
corresponding O
to O
this O
graph O
is O
easily O
written O
down O
using O
the O
general O
result O
( O
8.5 O
) O
to O
give O
( O
8.23 O
) O
if O
none O
of O
the O
variables O
are O
observed O
, O
then O
we O
can O
investigate O
whether O
a O
and O
b O
are O
independent B
by O
marginalizing O
both O
sides O
of O
( O
8.23 O
) O
with O
respect O
to O
c O
to O
give O
p O
( O
a O
, O
b O
, O
c O
) O
= O
p O
( O
a|c O
) O
p O
( O
b|c O
) O
p O
( O
c O
) O
. O
the O
robustness B
of O
the O
t-distribution O
is O
illustrated O
in O
figure O
2.16 O
, O
which O
compares O
the O
maximum B
likelihood I
solutions O
for O
a O
gaussian O
and O
a O
t-distribution O
. O
one O
way O
to O
combine O
them O
is O
to O
use O
a O
generative B
model I
to O
deﬁne O
a O
kernel O
, O
and O
then O
use O
this O
kernel O
in O
a O
discriminative O
approach O
. O
13.26 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
special O
case O
of O
the O
linear B
dynamical I
system I
of O
section O
13.3 O
that O
is O
equivalent O
to O
probabilistic O
pca O
, O
so O
that O
the O
transition O
matrix O
a O
= O
0 O
, O
the O
covariance B
γ O
= O
i O
, O
and O
the O
noise O
covariance B
σ O
= O
σ2i O
. O
similarly O
, O
the O
posterior O
covariance O
is O
given O
from O
( O
12.42 O
) O
by O
0-2m- O
1 O
and O
is O
independent B
of O
x. O
if O
we O
take O
the O
limit O
0- O
2 O
-- O
-- O
t O
0 O
, O
then O
the O
posterior O
mean O
reduces O
to O
( O
12.50 O
) O
exercise O
12.11 O
exercise O
12.12 O
section O
2.3 O
which O
represents O
an O
orthogonal O
projection O
of O
the O
data O
point O
onto O
the O
latent O
space O
, O
and O
so O
we O
recover O
the O
standard O
pca O
model O
. O
making O
use O
of O
( O
2.76 O
) O
, O
we O
then O
have O
λaa O
− O
λabλ O
−1 O
bb O
λba O
( O
2.91 O
) O
thus O
we O
obtain O
the O
intuitively O
satisfying O
result O
that O
the O
marginal B
distribution O
p O
( O
xa O
) O
has O
mean B
and O
covariance B
given O
by O
e O
[ O
xa O
] O
= O
µa O
cov O
[ O
xa O
] O
= O
σaa O
. O
however O
, O
this O
too O
runs O
into O
the O
problem O
of O
ambiguous O
regions O
, O
as O
illustrated O
in O
the O
right-hand O
diagram O
of O
figure O
4.2. O
we O
can O
avoid O
these O
difﬁculties O
by O
considering O
a O
single O
k-class O
discriminant O
comprising O
k O
linear O
functions O
of O
the O
form O
yk O
( O
x O
) O
= O
wt O
( O
4.9 O
) O
and O
then O
assigning O
a O
point O
x O
to O
class O
ck O
if O
yk O
( O
x O
) O
> O
yj O
( O
x O
) O
for O
all O
j O
( O
cid:9 O
) O
= O
k. O
the O
decision B
boundary I
between O
class O
ck O
and O
class O
cj O
is O
therefore O
given O
by O
yk O
( O
x O
) O
= O
yj O
( O
x O
) O
and O
hence O
corresponds O
to O
a O
( O
d O
− O
1 O
) O
-dimensional O
hyperplane O
deﬁned O
by O
k O
x O
+ O
wk0 O
( O
wk O
− O
wj O
) O
tx O
+ O
( O
wk0 O
− O
wj0 O
) O
= O
0 O
. O
so O
far O
, O
we O
have O
worked O
with O
completely O
general O
joint O
distributions O
, O
so O
that O
the O
decompositions O
, O
and O
their O
representations O
as O
fully B
connected I
graphs O
, O
will O
be O
applica- O
ble O
to O
any O
choice O
of O
distribution O
. O
10.24 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
we O
have O
seen O
in O
section O
10.2 O
that O
the O
singularities B
arising O
in O
the O
max- O
imum O
likelihood O
treatment O
of O
gaussian O
mixture B
models O
do O
not O
arise O
in O
a O
bayesian O
treatment O
. O
use O
this O
result O
to O
show O
that O
the O
number O
of O
independent B
param- O
eters O
n O
( O
d O
, O
m O
) O
, O
which O
appear O
at O
order O
m O
, O
satisﬁes O
the O
following O
recursion O
relation O
next O
use O
proof O
by O
induction O
to O
show O
that O
the O
following O
result O
holds O
d O
( O
cid:2 O
) O
i=1 O
n O
( O
d O
, O
m O
) O
= O
n O
( O
i O
, O
m O
− O
1 O
) O
. O
( O
10.25 O
) O
µ|µn O
, O
λ O
−1 O
n O
with O
exercise O
10.7 O
mean B
and O
precision O
given O
by O
µn O
= O
λ0µ0 O
+ O
n O
x O
λ0 O
+ O
n O
λn O
= O
( O
λ0 O
+ O
n O
) O
e O
[ O
τ O
] O
. O
( O
1.61 O
) O
as O
we O
did O
in O
the O
case O
of O
the O
simple O
gaussian O
distribution O
earlier O
, O
it O
is O
convenient O
to O
maximize O
the O
logarithm O
of O
the O
likelihood B
function I
. O
it O
is O
straightforward O
to O
extend O
this O
to O
the O
problem O
of O
ﬁnding O
the O
mode O
of O
the O
posterior O
distribution O
over O
θ O
( O
the O
map O
estimate O
) O
when O
a O
prior B
distribution O
p O
( O
θ O
) O
has O
been O
deﬁned O
, O
simply O
by O
adding O
ln O
p O
( O
θ O
) O
to O
the O
function O
q O
( O
θ O
, O
θold O
) O
before O
performing O
the O
m O
step O
. O
variational B
inference I
for O
bayesian O
mixtures O
of O
factor O
ana- O
lyzers O
. O
( O
10.112 O
) O
figure O
10.9 O
shows O
a O
plot O
of O
the O
lower B
bound I
l O
( O
q O
) O
versus O
the O
degree O
of O
a O
polynomial O
model O
for O
a O
synthetic O
data O
set O
generated O
from O
a O
degree O
three O
polynomial O
. O
taking O
the O
negative O
logarithm O
of O
( O
1.66 O
) O
and O
combining O
with O
( O
1.62 O
) O
and O
( O
1.65 O
) O
, O
we O
ﬁnd O
that O
the O
maximum O
of O
the O
posterior O
is O
given O
by O
the O
minimum O
of O
thus O
we O
see O
that O
maximizing O
the O
posterior O
distribution O
is O
equivalent O
to O
minimizing O
the O
regularized O
sum-of-squares O
error B
function I
encountered O
earlier O
in O
the O
form O
( O
1.4 O
) O
, O
with O
a O
regularization B
parameter O
given O
by O
λ O
= O
α/β O
. O
this O
is O
illustrated O
in O
figure O
8.47. O
it O
is O
important O
to O
note O
that O
a O
factor O
node O
can O
send O
a O
message O
to O
a O
variable O
node B
once O
it O
has O
received O
incoming O
messages O
from O
all O
other O
neighbouring O
variable O
nodes O
. O
3.5 O
3.5.1 O
evaluation O
of O
the O
evidence B
function I
. O
experiments O
with O
a O
new O
boosting B
algorithm O
. O
pac-bayesian O
stochastic B
model O
selection O
. O
it O
should O
be O
emphasized O
that O
there O
will O
generally O
be O
multiple O
local B
maxima O
of O
the O
log O
likelihood O
function O
, O
and O
that O
em O
is O
not O
guaranteed O
to O
ﬁnd O
the O
largest O
of O
these O
maxima O
. O
because O
the O
kl O
divergence O
is O
nonnegative O
, O
this O
causes O
the O
log O
likelihood O
ln O
p O
( O
x|θ O
) O
to O
increase O
by O
at O
least O
as O
much O
as O
the O
lower B
bound I
does O
. O
probability B
, O
frequency O
and O
reasonable O
expectation B
. O
( O
6.28 O
) O
this O
is O
clearly O
a O
valid O
kernel B
function I
because O
we O
can O
interpret O
it O
as O
an O
inner O
product O
in O
the O
one-dimensional O
feature B
space I
deﬁned O
by O
the O
mapping O
p O
( O
x O
) O
. O
4.4. O
the O
laplace O
approximation O
in O
section O
4.5 O
we O
shall O
discuss O
the O
bayesian O
treatment O
of O
logistic B
regression I
. O
gtm O
: O
a O
principled O
alternative O
to O
the O
self-organizing B
map I
. O
the O
only O
change O
is O
to O
the O
m-step O
equation O
for O
w O
, O
which O
is O
modified O
to O
give O
( O
12.62 O
) O
( O
12.63 O
) O
where O
a O
= O
diag O
( O
ai O
) O
' O
the O
value O
of O
i- O
'' O
is O
given O
by O
the O
sample B
mean I
, O
as O
before O
. O
it O
is O
therefore O
again O
convenient O
to O
choose O
a O
conjugate B
gaussian O
prior B
of O
the O
form O
( O
4.140 O
) O
. O
these O
offer O
a O
complementary O
alternative O
to O
sampling B
methods I
and O
have O
allowed O
bayesian O
techniques O
to O
be O
used O
in O
large-scale O
applications O
( O
blei O
et O
al. O
, O
2003 O
) O
. O
in O
the O
context O
of O
neural O
networks O
, O
this O
approach O
is O
known O
as O
weight B
decay I
. O
from O
( O
9.10 O
) O
and O
( O
9.11 O
) O
, O
this O
likelihood B
function I
takes O
the O
form O
p O
( O
x O
, O
z|µ O
, O
σ O
, O
π O
) O
= O
k O
n O
( O
xn|µk O
, O
σk O
) O
znk O
πznk O
( O
9.35 O
) O
where O
znk O
denotes O
the O
kth O
component O
of O
zn O
. O
section O
6.4 O
exercise O
3.12 O
exercise O
3.13 O
3.3. O
bayesian O
linear B
regression I
159 O
figure O
3.10 O
the O
equivalent O
ker- O
nel O
k O
( O
x O
, O
x O
( O
cid:1 O
) O
) O
for O
the O
gaussian O
basis O
functions O
in O
figure O
3.1 O
, O
shown O
as O
a O
plot O
of O
x O
versus O
x O
( O
cid:1 O
) O
, O
together O
with O
three O
slices O
through O
this O
matrix O
cor- O
responding O
to O
three O
different O
values O
of O
x. O
the O
data O
set O
used O
to O
generate O
this O
kernel O
comprised O
200 O
values O
of O
x O
equally O
spaced O
over O
the O
interval O
( O
−1 O
, O
1 O
) O
. O
( O
14.27 O
) O
( O
cid:2 O
) O
t O
if O
we O
perform O
a O
variational B
minimization O
with O
respect O
to O
all O
possible O
functions O
y O
( O
x O
) O
, O
we O
obtain O
( O
cid:12 O
) O
( O
cid:13 O
) O
y O
( O
x O
) O
= O
ln O
1 O
2 O
p O
( O
t O
= O
1|x O
) O
p O
( O
t O
= O
−1|x O
) O
( O
14.28 O
) O
662 O
14. O
combining B
models I
figure O
14.3 O
plot O
of O
the O
exponential O
( O
green O
) O
and O
rescaled O
cross-entropy O
( O
red O
) O
error B
functions O
along O
with O
the O
hinge O
er- O
ror O
( O
blue O
) O
used O
in O
support B
vector I
machines O
, O
and O
the O
misclassiﬁcation O
for O
large O
error B
( O
black O
) O
. O
at O
this O
point O
, O
we O
shall O
ﬁnd O
it O
convenient O
to O
deﬁne O
three O
statistics O
of O
the O
observed O
data O
set O
evaluated O
with O
respect O
to O
the O
responsibilities O
, O
given O
by O
n O
( O
cid:2 O
) O
n=1 O
1 O
nk O
1 O
nk O
nk O
= O
xk O
= O
sk O
= O
rnkxn O
rnk O
( O
xn O
− O
xk O
) O
( O
xn O
− O
xk O
) O
t. O
( O
10.51 O
) O
( O
10.52 O
) O
( O
10.53 O
) O
rnk O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n=1 O
note O
that O
these O
are O
analogous O
to O
quantities O
evaluated O
in O
the O
maximum B
likelihood I
em O
algorithm O
for O
the O
gaussian O
mixture B
model I
. O
the O
covariance B
matrices O
can O
conveniently O
be O
ini- O
tialized O
to O
the O
sample O
covariances O
of O
the O
clusters O
found O
by O
the O
k-means O
algorithm O
, O
and O
the O
mixing O
coefﬁcients O
can O
be O
set O
to O
the O
fractions O
of O
data O
points O
assigned O
to O
the O
respective O
clusters O
. O
there O
has O
been O
much O
controversy O
and O
debate O
associated O
with O
the O
relative B
mer- O
its O
of O
the O
frequentist B
and O
bayesian O
paradigms O
, O
which O
have O
not O
been O
helped O
by O
the O
fact O
that O
there O
is O
no O
unique O
frequentist B
, O
or O
even O
bayesian O
, O
viewpoint O
. O
consider O
an O
arbitrary O
directed B
acyclic I
graph I
over O
d O
variables O
in O
which O
node B
i O
represents O
a O
single O
continuous O
random O
variable O
xi O
having O
a O
gaussian O
distribution O
. O
the O
calculus B
of I
variations I
can O
be O
used O
, O
for O
instance O
, O
to O
show O
that O
the O
shortest O
path O
between O
two O
points O
is O
a O
straight O
line O
or O
that O
the O
maximum O
entropy O
distribution O
is O
a O
gaussian O
. O
, O
xd O
) O
represented O
by O
a O
directed B
graph O
having O
d O
nodes O
, O
and O
consider O
the O
conditional B
distribution O
of O
a O
particular O
node B
with O
variables O
xi O
conditioned O
on O
all O
of O
the O
remaining O
variables O
xj O
( O
cid:9 O
) O
=i O
. O
the O
perceptron B
: O
a O
model O
for O
brain O
functioning O
. O
recall O
that O
both O
σ2 O
a O
and O
b O
are O
functions O
of O
x. O
figure O
5.23 O
shows O
an O
example O
of O
this O
framework O
applied O
to O
the O
synthetic O
classi- O
ﬁcation O
data O
set O
described O
in O
appendix O
a. O
exercises O
5.1 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
two-layer O
network O
function O
of O
the O
form O
( O
5.7 O
) O
in O
which O
the O
hidden- O
unit O
nonlinear O
activation O
functions O
g O
( O
· O
) O
are O
given O
by O
logistic B
sigmoid I
functions O
of O
the O
form O
σ O
( O
a O
) O
= O
{ O
1 O
+ O
exp O
( O
−a O
) O
} O
−1 O
. O
minka O
( O
2005 O
) O
has O
shown O
that O
a O
broad O
range O
of O
message B
passing I
algorithms O
can O
be O
derived O
from O
a O
com- O
mon O
framework O
involving O
minimization O
of O
members O
of O
the O
alpha O
family O
of O
diver- O
gences O
, O
given O
by O
( O
10.19 O
) O
. O
we O
ﬁrst O
of O
all O
use O
the O
gaussian O
mixture B
distribution I
to O
motivate O
the O
em O
algorithm O
in O
a O
fairly O
informal O
way O
, O
and O
then O
we O
give O
a O
more O
careful O
treatment O
based O
on O
the O
latent B
variable I
viewpoint O
. O
( O
3.105 O
) O
( O
3.106 O
) O
now O
suppose O
that O
gaussian O
noise O
i O
with O
zero O
mean B
and O
variance B
σ2 O
is O
added O
in- O
dependently O
to O
each O
of O
the O
input O
variables O
xi O
. O
however O
, O
a O
4 O
1. O
introduction O
figure O
1.2 O
plot O
of O
a O
training B
data O
set O
of O
n O
= O
10 O
points O
, O
shown O
as O
blue O
circles O
, O
each O
comprising O
an O
observation O
of O
the O
input O
variable O
x O
along O
with O
the O
corresponding O
target O
variable O
t. O
the O
green O
curve O
shows O
the O
function O
sin O
( O
2πx O
) O
used O
to O
gener- O
ate O
the O
data O
. O
10.7 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
problem O
of O
inferring O
the O
mean B
and O
precision O
of O
a O
univariate O
gaus- O
sian O
using O
a O
factorized O
variational O
approximation O
, O
as O
considered O
in O
section O
10.1.3. O
show O
that O
the O
factor O
qµ O
( O
µ O
) O
is O
a O
gaussian O
of O
the O
form O
n O
( O
µ|µn O
, O
λ O
−1 O
n O
) O
with O
mean B
and O
precision O
given O
by O
( O
10.26 O
) O
and O
( O
10.27 O
) O
, O
respectively O
. O
a O
vertical O
slice O
through O
one O
of O
these O
plots O
at O
a O
particular O
value O
of O
x O
represents O
the O
corresponding O
conditional B
distribution O
p O
( O
t|x O
) O
, O
which O
we O
see O
is O
bimodal O
. O
we O
can O
also O
derive O
an O
on-line O
stochastic O
algorithm O
( O
macqueen O
, O
1967 O
) O
by O
applying O
the O
robbins-monro O
procedure O
to O
the O
problem O
of O
ﬁnding O
the O
roots O
of O
the O
regression B
function I
given O
by O
the O
derivatives O
of O
j O
in O
( O
9.1 O
) O
with O
respect O
to O
µk O
. O
suppose O
that O
the O
result O
of O
running O
the O
variational B
inference I
algorithm O
is O
an O
approximate O
posterior O
distribu- O
tion O
q O
that O
is O
localized O
in O
the O
neighbourhood O
of O
one O
of O
the O
modes O
. O
here O
we O
consider O
a O
particularly O
important O
class O
of O
such O
data O
sets O
, O
namely O
those O
that O
describe O
sequential B
data I
. O
here O
we O
apply O
this O
technique O
of O
kernel B
substitution I
to O
principal B
component I
analysis I
, O
thereby O
obtaining O
a O
nonlinear O
generalization B
called O
kernel O
pea O
( O
scholkopf O
et O
al. O
, O
1998 O
) O
. O
outliers B
is O
much O
less O
signiﬁcant O
for O
the O
t-distribution O
than O
for O
the O
gaussian O
. O
5.2.1 O
5.2.2 O
local B
quadratic O
approximation O
. O
this O
particular O
choice O
of O
regularizer O
is O
known O
in O
the O
machine O
learning O
literature O
as O
weight B
decay I
because O
in O
sequential B
learning I
algorithms O
, O
it O
encourages O
weight O
values O
to O
decay O
towards O
zero O
, O
unless O
supported O
by O
the O
data O
. O
in O
spite O
of O
the O
nonlinearity O
, O
the O
optimization O
for O
lle O
does O
not O
exhibit O
local B
minima O
. O
if O
sufﬁciently O
large O
numbers O
of O
training B
patterns O
are O
available O
, O
then O
an O
adaptive O
model O
such O
as O
a O
neural B
network I
can O
learn O
the O
invariance B
, O
at O
least O
approximately O
. O
and O
xb O
both O
lie O
inside O
the O
same O
decision O
re- O
gion O
rk O
, O
then O
any O
point O
bx O
that O
lies O
on O
the O
line O
connecting O
these O
two O
points O
must O
also O
lie O
in O
rk O
, O
and O
hence O
the O
decision B
region I
must O
be O
singly O
connected O
and O
convex O
. O
this O
pre-processing O
stage O
is O
sometimes O
also O
called O
feature B
extraction I
. O
in O
some O
applications O
, O
it O
will O
be O
appropriate O
to O
avoid O
making O
decisions O
on O
the O
difﬁcult O
cases O
in O
anticipation O
of O
a O
lower O
error O
rate O
on O
those O
examples O
for O
which O
a O
classiﬁcation B
de- O
cision O
is O
made O
. O
thus O
, O
a O
signiﬁcant O
integration O
error B
in O
any O
one O
of O
the O
variables O
could O
lead O
to O
a O
high O
prob- O
ability O
of O
rejection O
. O
in O
figure O
10.6 O
, O
the O
prior B
over O
the O
mixing O
coefﬁcients O
is O
a O
dirichlet O
of O
the O
form O
( O
10.39 O
) O
. O
for O
each O
model O
we O
deﬁne O
a O
likelihood B
function I
p O
( O
d|θi O
, O
mi O
) O
. O
for O
example O
, O
we O
have O
already O
seen O
that O
for O
a O
single O
real O
variable O
, O
the O
distribution O
that O
maximizes O
the O
entropy B
is O
the O
gaussian O
. O
this O
result O
can O
also O
be O
derived O
directly O
by O
making O
use O
of O
the O
results O
for O
the O
marginal B
of O
a O
gaussian O
distribution O
given O
in O
section O
2.3.2. O
the O
integral O
over O
a O
represents O
the O
convolution O
of O
a O
gaussian O
with O
a O
logistic O
sig- O
moid O
, O
and O
can O
not O
be O
evaluated O
analytically O
. O
( O
7.35 O
) O
if O
an O
< O
c O
, O
then O
( O
7.31 O
) O
implies O
that O
µn O
> O
0 O
, O
which O
from O
( O
7.28 O
) O
requires O
ξn O
= O
0 O
and O
hence O
such O
points O
lie O
on O
the O
margin B
. O
thus O
the O
model O
predicts O
a O
noise O
variance B
orthogonal O
to O
the O
principal B
subspace I
, O
which O
, O
from O
( O
12.46 O
) O
, O
is O
just O
the O
average O
of O
the O
discarded O
eigenvalues O
. O
in O
other O
words O
, O
to O
compute O
the O
outgoing O
message O
from O
a O
factor O
node O
, O
we O
take O
the O
product O
of O
all O
the O
incoming O
messages O
from O
other O
factor O
nodes O
, O
multiply O
by O
the O
local B
factor O
, O
and O
then O
marginalize O
. O
the O
ﬁrst O
term O
in O
( O
3.59 O
) O
represents O
the O
noise O
on O
the O
data O
whereas O
the O
second O
term O
reﬂects O
the O
uncertainty O
associated O
with O
the O
parameters O
w. O
because O
the O
noise O
process O
and O
the O
distribution O
of O
w O
are O
independent B
gaussians O
, O
their O
variances O
are O
additive O
. O
, O
xn−1 O
) O
, O
which O
incorporates O
all O
the O
data O
up O
to O
step O
n O
− O
1. O
the O
diffusion O
arising O
from O
the O
nonzero O
variance B
of O
the O
transition B
probability I
p O
( O
zn|zn−1 O
) O
gives O
the O
distribution O
p O
( O
zn|x1 O
, O
. O
the O
resulting O
sequential O
sparse O
bayesian O
learning B
algorithm O
is O
described O
below O
. O
the O
geomet- O
rical O
interpretation O
of O
the O
sum-of-squares B
error I
function O
is O
illustrated O
in O
figure O
1.3. O
we O
can O
solve O
the O
curve B
ﬁtting I
problem O
by O
choosing O
the O
value O
of O
w O
for O
which O
e O
( O
w O
) O
is O
as O
small O
as O
possible O
. O
more O
generally O
, O
we O
can O
consider O
a O
partitioning O
of O
the O
components O
of O
x O
into O
three O
groups O
xa O
, O
xb O
, O
and O
xc O
, O
with O
a O
corresponding O
par- O
titioning O
of O
the O
mean B
vector O
µ O
and O
of O
the O
covariance B
matrix I
σ O
in O
the O
form O
( O
cid:22 O
) O
( O
cid:23 O
) O
µa O
µb O
µc O
( O
cid:23 O
) O
( O
cid:22 O
) O
σaa O
σab O
σac O
σba O
σbb O
σbc O
σca O
σcb O
σcc O
µ O
= O
, O
σ O
= O
. O
the O
quantity O
( O
yn O
− O
tn O
) O
is O
then O
a O
random O
variable O
with O
zero O
mean B
. O
( O
cid:4 O
) O
α O
= O
γ O
( O
α1 O
) O
··· O
γ O
( O
αk O
) O
k=1 O
ψ O
( O
a O
) O
≡ O
d O
da O
ln O
γ O
( O
a O
) O
where O
and O
here O
is O
known O
as O
the O
digamma B
function I
( O
abramowitz O
and O
stegun O
, O
1965 O
) O
. O
note O
that O
there O
is O
one O
such O
target B
vector I
t O
for O
each O
digit O
image O
x. O
the O
result O
of O
running O
the O
machine O
learning O
algorithm O
can O
be O
expressed O
as O
a O
function O
y O
( O
x O
) O
which O
takes O
a O
new O
digit O
image O
x O
as O
input O
and O
that O
generates O
an O
output O
vector O
y O
, O
encoded O
in O
the O
same O
way O
as O
the O
target O
vectors O
. O
( O
10.184 O
) O
as O
a O
function O
of O
η O
, O
the O
kullback-leibler O
divergence O
then O
becomes O
kl O
( O
p O
( O
cid:5 O
) O
q O
) O
= O
− O
ln O
g O
( O
η O
) O
− O
ηt O
ep O
( O
z O
) O
[ O
u O
( O
z O
) O
] O
+ O
const O
( O
10.185 O
) O
where O
the O
constant O
terms O
are O
independent B
of O
the O
natural B
parameters I
η. O
we O
can O
mini- O
mize O
kl O
( O
p O
( O
cid:5 O
) O
q O
) O
within O
this O
family O
of O
distributions O
by O
setting O
the O
gradient O
with O
respect O
to O
η O
to O
zero O
, O
giving O
( O
10.186 O
) O
however O
, O
we O
have O
already O
seen O
in O
( O
2.226 O
) O
that O
the O
negative O
gradient O
of O
ln O
g O
( O
η O
) O
is O
given O
by O
the O
expectation B
of O
u O
( O
z O
) O
under O
the O
distribution O
q O
( O
z O
) O
. O
thus O
to O
evaluate O
the O
message O
sent O
by O
a O
variable O
node B
to O
an O
adjacent O
factor O
node O
along O
the O
connecting O
link B
, O
we O
simply O
take O
the O
product O
of O
the O
incoming O
messages O
along O
all O
of O
the O
other O
links O
. O
in O
contrast O
to O
the O
regression B
model O
, O
we O
can O
no O
longer O
integrate O
analytically O
over O
the O
parameter O
vector O
w. O
here O
we O
follow O
tipping O
( O
2001 O
) O
and O
use O
the O
laplace O
ap- O
proximation B
, O
which O
was O
applied O
to O
the O
closely O
related O
problem O
of O
bayesian O
logistic B
regression I
in O
section O
4.5.1. O
we O
begin O
by O
initializing O
the O
hyperparameter B
vector O
α. O
for O
this O
given O
value O
of O
α O
, O
we O
then O
build O
a O
gaussian O
approximation O
to O
the O
posterior O
distribution O
and O
thereby O
obtain O
an O
approximation O
to O
the O
marginal B
likelihood I
. O
similarly O
, O
in O
section O
9.3.4 O
we O
used O
the O
em O
algorithm O
to O
maximize O
the O
same O
marginal B
likelihood I
, O
giving O
the O
re-estimation O
equations O
( O
9.67 O
) O
and O
( O
9.68 O
) O
. O
a O
sim- O
ple O
model O
( O
for O
example O
, O
based O
on O
a O
ﬁrst B
order I
polynomial O
) O
has O
little O
variability O
and O
so O
will O
generate O
data O
sets O
that O
are O
fairly O
similar O
to O
each O
other O
. O
in O
this O
case O
, O
the O
parameters O
αk O
can O
be O
interpreted O
as O
effective O
numbers O
of O
observations O
of O
the O
corresponding O
values O
of O
the O
k-dimensional O
binary O
observation O
vector O
x. O
as O
with O
the O
beta B
distribution I
, O
the O
dirichlet O
has O
ﬁnite O
density O
everywhere O
provided O
αk O
( O
cid:2 O
) O
1 O
for O
all O
k. O
688 O
b. O
probability B
distributions O
gamma O
the O
gamma O
is O
a O
probability B
distribution O
over O
a O
positive O
random O
variable O
τ O
> O
0 O
governed O
by O
parameters O
a O
and O
b O
that O
are O
subject O
to O
the O
constraints O
a O
> O
0 O
and O
b O
> O
0 O
to O
ensure O
that O
the O
distribution O
can O
be O
normalized O
. O
to O
see O
this O
, O
recall O
that O
in O
probabilistic O
pca O
( O
and O
in O
factor B
analysis I
) O
the O
latent-space O
distribution O
is O
given O
by O
a O
zero-mean O
isotropic B
gaussian O
. O
using O
this O
to O
eliminate O
λ O
and O
rearranging O
we O
obtain O
πk O
= O
nk O
n O
( O
9.22 O
) O
so O
that O
the O
mixing B
coefﬁcient I
for O
the O
kth O
component O
is O
given O
by O
the O
average O
respon- O
sibility O
which O
that O
component O
takes O
for O
explaining O
the O
data O
points O
. O
13.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
suppose O
we O
wish O
to O
train O
a O
hidden O
markov O
model O
by O
maximum B
likelihood I
using O
data O
that O
comprises O
r O
independent B
sequences O
of O
observations O
, O
which O
we O
de- O
note O
by O
x O
( O
r O
) O
where O
r O
= O
1 O
, O
. O
8.2. O
conditional B
independence I
an O
important O
concept O
for O
probability O
distributions O
over O
multiple O
variables O
is O
that O
of O
conditional B
independence I
( O
dawid O
, O
1980 O
) O
. O
in O
practice O
, O
producing O
synthetic O
observations O
from O
a O
generative B
model I
can O
prove O
informative O
in O
understanding O
the O
form O
of O
the O
probability B
distribution O
represented O
by O
that O
model O
. O
in O
the O
case O
of O
left-to-right B
models O
, O
this O
is O
particularly O
important O
because O
, O
in O
a O
given O
observation O
sequence O
, O
a O
given O
state O
transition O
corresponding O
to O
a O
nondiagonal O
element O
of O
a O
will O
seen O
at O
most O
once O
. O
as O
in O
the O
weighted O
least-squares O
problem O
, O
the O
elements O
of O
the O
diagonal B
weighting O
matrix O
r O
can O
be O
interpreted O
as O
variances O
because O
the O
mean B
and O
variance B
of O
t O
in O
the O
logistic B
regression I
model O
are O
given O
by O
e O
[ O
t O
] O
= O
σ O
( O
x O
) O
= O
y O
var O
[ O
t O
] O
= O
e O
[ O
t2 O
] O
− O
e O
[ O
t O
] O
2 O
= O
σ O
( O
x O
) O
− O
σ O
( O
x O
) O
2 O
= O
y O
( O
1 O
− O
y O
) O
( O
4.101 O
) O
( O
4.102 O
) O
where O
we O
have O
used O
the O
property O
t2 O
= O
t O
for O
t O
∈ O
{ O
0 O
, O
1 O
} O
. O
the O
graphical B
model I
that O
describes O
this O
problem O
is O
shown O
in O
figure O
8.7 O
, O
and O
the O
corresponding O
joint O
distribution O
of O
all O
of O
the O
random O
variables O
in O
this O
model O
, O
conditioned O
on O
the O
deterministic O
parameters O
, O
is O
then O
given O
by O
p O
( O
( O
cid:1 O
) O
t O
, O
t O
, O
w| O
( O
cid:1 O
) O
x O
, O
x O
, O
α O
, O
σ2 O
) O
= O
( O
cid:31 O
) O
n O
( O
cid:14 O
) O
p O
( O
tn|xn O
, O
w O
, O
σ2 O
) O
p O
( O
w|α O
) O
p O
( O
( O
cid:1 O
) O
t| O
( O
cid:1 O
) O
x O
, O
w O
, O
σ2 O
) O
. O
thus O
p O
( O
c1 O
) O
repre- O
sents O
the O
probability B
that O
a O
person O
has O
cancer O
, O
before O
we O
take O
the O
x-ray O
measurement O
. O
if O
the O
sets O
of O
variables O
are O
independent B
, O
then O
their O
joint O
distribution O
will O
factorize O
into O
the O
product O
of O
their O
marginals O
p O
( O
x O
, O
y O
) O
= O
p O
( O
x O
) O
p O
( O
y O
) O
. O
the O
fundamental O
problem O
that O
we O
therefore O
wish O
to O
address O
in O
this O
chapter O
involves O
ﬁnding O
the O
expectation B
of O
some O
function O
f O
( O
z O
) O
with O
respect O
to O
a O
probability B
distribution O
p O
( O
z O
) O
. O
if O
we O
treat O
the O
data O
as O
i.i.d. B
, O
then O
the O
only O
information O
we O
can O
glean O
from O
the O
data O
is O
the O
relative B
frequency O
of O
rainy O
days O
. O
we O
ﬁrst O
deﬁne O
to O
denoted O
hidden O
units O
, O
and O
indices O
k O
and O
k O
( O
cid:4 O
) O
( O
cid:4 O
) O
δk O
= O
∂en O
∂ak O
, O
mkk O
( O
cid:1 O
) O
≡ O
∂2en O
∂ak∂ak O
( O
cid:1 O
) O
( O
5.92 O
) O
where O
en O
is O
the O
contribution O
to O
the O
error B
from O
data O
point O
n. O
the O
hessian O
matrix O
for O
this O
network O
can O
then O
be O
considered O
in O
three O
separate O
blocks O
as O
follows O
. O
we O
shall O
see O
an O
example O
of O
the O
use O
of O
local B
variational O
bounds O
in O
sections O
10.6.1. O
for O
the O
moment O
, O
however O
, O
it O
is O
instructive O
to O
consider O
in O
general O
terms O
how O
these O
bounds O
can O
be O
used O
. O
we O
can O
therefore O
make O
use O
of O
the O
em O
algorithm O
to O
find O
maximum B
likelihood I
estimates O
of O
the O
model O
parameters O
. O
we O
shall O
shortly O
consider O
situations O
in O
which O
a O
( O
x O
) O
is O
a O
linear O
function O
of O
x O
, O
in O
which O
case O
the O
posterior B
probability I
is O
governed O
by O
a O
generalized B
linear I
model I
. O
we O
see O
from O
the O
digits O
example O
in O
figure O
13.11 O
that O
hidden O
markov O
models O
can O
be O
quite O
poor O
generative O
models O
for O
the O
data O
, O
because O
many O
of O
the O
synthetic O
digits O
look O
quite O
unrepresentative O
of O
the O
training B
data O
. O
sequential B
learning I
. O
from O
our O
discussion O
of O
polynomial B
curve I
ﬁtting I
in O
chapter O
1 O
, O
we O
see O
that O
an O
alternative O
approach O
is O
to O
choose O
a O
relatively O
large O
value O
for O
m O
and O
then O
to O
control O
complexity O
by O
the O
addition O
of O
a O
regularization B
term O
to O
the O
error B
function I
. O
for O
anything O
other O
than O
very O
small O
data O
sets O
, O
the O
dominant O
computational O
cost O
of O
the O
variational B
algorithm O
for O
gaussian O
mixtures O
arises O
from O
the O
evaluation O
of O
the O
responsibilities O
, O
together O
with O
the O
evaluation O
and O
inversion O
of O
the O
weighted O
data O
covariance B
matrices O
. O
this O
provides O
the O
basis O
for O
the O
( O
approximate O
) O
invariance B
of O
5.5. O
regularization B
in O
neural O
networks O
269 O
the O
network O
outputs O
to O
translations O
and O
distortions O
of O
the O
input O
image O
. O
5.5.6 O
convolutional B
networks O
. O
clearly O
, O
the O
set O
{ O
yn O
} O
has O
zero O
mean B
, O
and O
its O
covariance B
is O
given O
by O
the O
identity O
matrix O
because O
( O
12.24 O
) O
n O
1~ O
l O
l O
-1/2ut O
( O
xn O
- O
x O
) O
( O
xn O
- O
x O
) O
tul-1/2 O
l O
~1/2utsul O
-1/2 O
= O
l O
-1/2ll-1/2 O
= O
i. O
n=l O
( O
12.25 O
) O
appendix O
a O
appendix O
a O
this O
operation O
is O
known O
as O
whitening B
or O
sphereing B
the O
data O
and O
is O
illustrated O
for O
the O
old O
faithful O
data O
set O
in O
figure O
12.6. O
it O
is O
interesting O
to O
compare O
pca O
with O
the O
fisher O
linear B
discriminant I
which O
was O
discussed O
in O
section O
4.1.4. O
both O
methods O
can O
be O
viewed O
as O
techniques O
for O
linear O
dimensionality O
reduction O
. O
efﬁcient O
training B
of O
feed- O
forward O
neural O
networks O
. O
if O
there O
are O
multiple O
solutions O
all O
of O
which O
classify O
the O
training B
data O
set O
exactly O
, O
then O
we O
should O
try O
to O
ﬁnd O
the O
one O
that O
will O
give O
the O
smallest O
generalization B
error O
. O
this O
prior B
knowledge O
can O
be O
captured O
using O
the O
markov O
388 O
8. O
graphical O
models O
figure O
8.30 O
illustration O
of O
image B
de-noising I
using O
a O
markov O
random O
ﬁeld O
. O
5.4.6 O
fast B
multiplication I
by O
the O
hessian O
for O
many O
applications O
of O
the O
hessian O
, O
the O
quantity O
of O
interest O
is O
not O
the O
hessian O
matrix O
h O
itself O
but O
the O
product O
of O
h O
with O
some O
vector O
v. O
we O
have O
seen O
that O
the O
evaluation O
of O
the O
hessian O
takes O
o O
( O
w O
2 O
) O
operations O
, O
and O
it O
also O
requires O
storage O
that O
is O
o O
( O
w O
2 O
) O
. O
due O
to O
the O
complex O
relationships O
between O
the O
object O
position O
or O
orientation O
and O
the O
pixel O
intensities O
, O
this O
manifold B
will O
be O
highly O
nonlinear O
. O
from O
( O
5.48 O
) O
, O
the O
diagonal B
elements O
of O
the O
hessian O
, O
for O
pattern O
n O
, O
can O
be O
written O
( O
cid:5 O
) O
∂2en O
∂w2 O
ji O
= O
∂2en O
∂a2 O
j O
z2 O
i O
. O
the O
log O
likelihood O
function O
for O
a O
gaussian O
process O
regression B
model O
is O
easily O
evaluated O
using O
the O
standard O
form O
for O
a O
multivariate O
gaussian O
distribution O
, O
giving O
ln O
p O
( O
t|θ O
) O
= O
−1 O
2 O
ln|cn| O
− O
1 O
2 O
ttc O
n O
t O
− O
n O
−1 O
2 O
ln O
( O
2π O
) O
. O
we O
can O
evaluate O
this O
distribution O
by O
the O
usual O
procedure O
of O
completing B
the I
square I
in O
the O
exponential O
, O
and O
then O
ﬁnding O
the O
normalization O
coefﬁcient O
using O
the O
standard O
result O
for O
a O
normalized O
gaussian O
. O
this O
could O
be O
called O
the O
max-product O
algorithm O
and O
is O
identical O
to O
the O
sum-product B
algorithm I
except O
that O
summations O
are O
replaced O
by O
maximizations O
. O
−1i O
with O
α O
→ O
0 O
, O
the O
mean B
mn O
if O
we O
consider O
an O
inﬁnitely O
broad O
prior B
s0 O
= O
α O
of O
the O
posterior O
distribution O
reduces O
to O
the O
maximum B
likelihood I
value O
wml O
given O
by O
( O
3.15 O
) O
. O
the O
probability B
that O
x O
will O
take O
the O
value O
xi O
and O
y O
will O
take O
the O
value O
yj O
is O
written O
p O
( O
x O
= O
xi O
, O
y O
= O
yj O
) O
and O
is O
called O
the O
joint O
probability B
of O
x O
= O
xi O
and O
y O
= O
yj O
. O
there O
are O
many O
variants O
of O
this O
model O
in O
which O
parameters O
such O
as O
the O
w O
matrix O
or O
the O
noise O
variances O
are O
tied O
across O
components O
in O
the O
mixture B
, O
or O
in O
which O
the O
isotropic B
noise O
distributions O
are O
replaced O
by O
diagonal B
ones O
, O
giving O
rise O
to O
a O
mixture O
of O
factor O
analysers O
( O
ghahramani O
and O
hinton O
, O
1996a O
; O
ghahramani O
and O
beal O
, O
2000 O
) O
. O
we O
can O
evaluate O
the O
derivative B
of O
a O
( O
cid:1 O
) O
entiating O
the O
relation O
( O
6.84 O
) O
with O
respect O
to O
θj O
to O
give O
n O
) O
, O
and O
again O
we O
have O
used O
the O
result O
( O
c.22 O
) O
together O
with O
the O
n O
with O
respect O
to O
θj O
by O
differ- O
∂a O
( O
cid:1 O
) O
n O
∂θj O
= O
∂cn O
∂θj O
( O
tn O
− O
σn O
) O
− O
cn O
wn O
∂a O
( O
cid:1 O
) O
n O
∂θj O
. O
the O
required O
6.4. O
gaussian O
processes O
315 O
predictive B
distribution I
is O
given O
by O
( O
cid:6 O
) O
p O
( O
tn O
+1 O
= O
1|tn O
) O
= O
p O
( O
tn O
+1 O
= O
1|an O
+1 O
) O
p O
( O
an O
+1|tn O
) O
dan O
+1 O
( O
6.76 O
) O
where O
p O
( O
tn O
+1 O
= O
1|an O
+1 O
) O
= O
σ O
( O
an O
+1 O
) O
. O
simply O
applying O
the O
above O
procedure O
separately O
for O
each O
node B
will O
have O
computational O
cost O
that O
is O
o O
( O
n O
2m O
2 O
) O
. O
because O
a O
potential B
function I
is O
an O
arbitrary O
, O
nonnegative O
function O
over O
a O
maximal B
clique I
, O
we O
can O
multiply O
it O
by O
any O
nonnegative O
functions O
of O
subsets O
of O
the O
clique B
, O
or O
8.3. O
markov O
random O
fields O
389 O
figure O
8.31 O
an O
undirected B
graphical O
model O
representing O
a O
markov O
random O
ﬁeld O
for O
image O
de-noising O
, O
in O
which O
xi O
is O
a O
binary O
variable O
denoting O
the O
state O
of O
pixel O
i O
in O
the O
unknown O
noise-free O
image O
, O
and O
yi O
denotes O
the O
corresponding O
value O
of O
pixel O
i O
in O
the O
observed O
noisy O
image O
. O
this O
leads O
to O
the O
re-estimation O
equation O
n O
( O
cid:2 O
) O
πk O
= O
1 O
n O
rnk O
n=1 O
( O
10.83 O
) O
and O
this O
maximization O
is O
interleaved O
with O
the O
variational B
updates O
for O
the O
q O
distribution O
over O
the O
remaining O
parameters O
. O
7.1.4 O
svms O
for B
regression I
we O
now O
extend O
support B
vector I
machines O
to O
regression B
problems O
while O
at O
the O
same O
time O
preserving O
the O
property O
of O
sparseness O
. O
( O
10.241 O
) O
finally O
, O
by O
making O
use O
of O
( O
10.80 O
) O
, O
show O
that O
for O
large O
n O
, O
the O
predictive B
distribution I
becomes O
a O
mixture O
of O
gaussians O
. O
however O
, O
the O
error B
measured O
with O
respect O
to O
independent B
data O
, O
generally O
called O
a O
validation B
set I
, O
often O
shows O
a O
decrease O
at O
ﬁrst O
, O
followed O
by O
an O
in- O
crease O
as O
the O
network O
starts O
to O
over-ﬁt O
. O
the O
kernel O
formulation O
also O
makes O
clear O
the O
role O
of O
the O
constraint O
that O
the O
kernel B
function I
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
be O
positive B
deﬁnite I
, O
because O
this O
ensures O
that O
the O
lagrangian O
function O
( O
cid:4 O
) O
l O
( O
a O
) O
is O
bounded O
below O
, O
giving O
rise O
to O
a O
well- O
deﬁned O
optimization O
problem O
. O
although O
the O
application O
of O
svms O
to O
multiclass B
classiﬁcation O
problems O
remains O
an O
open O
issue O
, O
in O
practice O
the O
one-versus-the-rest O
approach O
is O
the O
most O
widely O
used O
in O
spite O
of O
its O
ad-hoc O
formula- O
tion O
and O
its O
practical O
limitations O
. O
essentially O
, O
the O
simpler O
model O
can O
not O
ﬁt O
the O
data O
well O
, O
whereas O
the O
more O
complex O
model O
spreads O
its O
predictive O
probability O
over O
too O
broad O
a O
range O
of O
data O
sets O
and O
so O
assigns O
relatively O
small O
probability B
to O
any O
one O
of O
them O
. O
a. O
data O
sets O
679 O
stratiﬁed O
annular O
oil O
water O
gas O
mix O
homogeneous B
ﬂow I
conﬁguration O
and O
is O
illustrated O
in O
figure O
a.2 O
. O
the O
decision O
process O
can O
be O
softened O
by O
moving O
to O
a O
probabilistic O
framework O
for O
combining O
models O
, O
as O
discussed O
in O
section O
14.5. O
for O
example O
, O
if O
we O
have O
a O
set O
of O
k O
models O
for O
a O
conditional B
distribution O
p O
( O
t|x O
, O
k O
) O
where O
x O
is O
the O
input O
variable O
, O
t O
is O
the O
target O
variable O
, O
and O
k O
= O
1 O
, O
. O
the O
model O
with O
the O
optimal O
predictive O
capability O
is O
the O
one O
that O
leads O
to O
the O
best O
balance O
between O
bias B
and O
variance B
. O
probability B
theory O
: O
the O
logic O
of O
science O
. O
multiplying O
both O
sides O
of O
( O
4.29 O
) O
by O
s O
w O
we O
then O
obtain O
( O
4.30 O
) O
note O
that O
if O
the O
within-class B
covariance I
is O
isotropic B
, O
so O
that O
sw O
is O
proportional O
to O
the O
unit O
matrix O
, O
we O
ﬁnd O
that O
w O
is O
proportional O
to O
the O
difference O
of O
the O
class O
means O
, O
as O
discussed O
above O
. O
in- O
troduction O
to O
the O
theory B
of O
neural O
computation O
. O
the O
idea O
is O
ﬁrst O
to O
initialize O
the O
variables O
{ O
xi O
} O
, O
which O
we O
do O
by O
simply O
setting O
xi O
= O
yi O
for O
all O
i. O
then O
we O
take O
one O
node B
xj O
at O
a O
time O
and O
we O
evaluate O
the O
total O
energy O
for O
the O
two O
possible O
states O
xj O
= O
+1 O
and O
xj O
= O
−1 O
, O
keeping O
all O
other O
node B
variables O
ﬁxed O
, O
and O
set O
xj O
to O
whichever O
state O
has O
the O
lower O
energy O
. O
error B
rates O
for O
classifying O
the O
digits O
range O
from O
12 O
% O
for O
a O
simple O
linear O
classi- O
ﬁer O
, O
through O
0.56 O
% O
for O
a O
carefully O
designed O
support B
vector I
machine I
, O
to O
0.4 O
% O
for O
a O
convolutional B
neural I
network I
( O
lecun O
et O
al. O
, O
1998 O
) O
. O
we O
have O
therefore O
found O
an O
equivalent O
formulation O
of O
the O
gaussian O
mixture B
in- O
volving O
an O
explicit O
latent B
variable I
. O
, O
k. O
if O
we O
assume O
that O
the O
class O
labels O
are O
independent B
, O
given O
the O
input O
vector O
, O
then O
the O
conditional B
distribution O
of O
the O
targets O
is O
p O
( O
t|x O
, O
w O
) O
= O
yk O
( O
x O
, O
w O
) O
tk O
[ O
1 O
− O
yk O
( O
x O
, O
w O
) O
] O
1−tk O
. O
one O
simple O
way O
to O
do O
this O
is O
to O
assume O
that O
, O
for O
each O
class O
separately O
, O
the O
distributions O
of O
inputs O
for O
the O
x-ray O
images O
, O
denoted O
by O
xi O
, O
and O
the O
blood O
data O
, O
denoted O
by O
xb O
, O
are O
46 O
1. O
introduction O
independent B
, O
so O
that O
p O
( O
xi O
, O
xb|ck O
) O
= O
p O
( O
xi|ck O
) O
p O
( O
xb|ck O
) O
. O
the O
left-hand O
plot O
shows O
the O
error B
in O
the O
predicted O
posterior O
mean O
versus O
the O
number O
of O
ﬂoating O
point O
operations O
, O
and O
the O
right-hand O
plot O
shows O
the O
corresponding O
results O
for O
the O
model B
evidence I
. O
speciﬁcally O
, O
we O
consider O
a O
zero-mean O
isotropic B
gaussian O
governed O
by O
a O
single O
precision B
parameter I
α O
so O
that O
p O
( O
w|α O
) O
= O
n O
( O
w|0 O
, O
α O
−1i O
) O
( O
3.52 O
) O
and O
the O
corresponding O
posterior O
distribution O
over O
w O
is O
then O
given O
by O
( O
3.49 O
) O
with O
mn O
= O
βsn O
φtt O
−1 O
n O
= O
αi O
+ O
βφtφ O
. O
thus O
we O
can O
interpret O
the O
parameter O
a0 O
in O
the O
prior B
in O
terms O
of O
2a0 O
‘ O
effective O
’ O
prior B
observations O
. O
if O
a O
message O
is O
sent O
from O
a O
factor O
node O
f O
to O
a O
variable O
node B
x O
, O
a O
maximization O
is O
performed O
over O
all O
other O
variable O
nodes O
x1 O
, O
. O
( O
cid:5 O
) O
the O
conjugate B
prior I
for O
the O
parameters O
of O
a O
bernoulli O
distribution O
is O
given O
by O
the O
beta B
distribution I
, O
and O
we O
have O
seen O
that O
a O
beta O
prior O
is O
equivalent O
to O
introducing O
448 O
9. O
mixture B
models O
and O
em O
figure O
9.10 O
illustration O
of O
the O
bernoulli O
mixture B
model I
in O
which O
the O
top O
row O
shows O
examples O
from O
the O
digits O
data O
set O
after O
converting O
the O
pixel O
values O
from O
grey O
scale O
to O
binary O
using O
a O
threshold O
of O
0.5. O
on O
the O
bottom O
row O
the O
ﬁrst O
three O
images O
show O
the O
parameters O
µki O
for O
each O
of O
the O
three O
components O
in O
the O
mixture B
model I
. O
derive O
the O
variational B
upper O
bound O
( O
10.137 O
) O
directly O
by O
making O
a O
second B
order I
taylor O
expansion O
of O
the O
log O
logistic O
function O
around O
a O
point O
x O
= O
ξ O
. O
( O
8.39 O
) O
( O
cid:14 O
) O
( O
cid:14 O
) O
c O
1 O
z O
( O
cid:2 O
) O
here O
the O
quantity O
z O
, O
sometimes O
called O
the O
partition B
function I
, O
is O
a O
normalization O
con- O
stant O
and O
is O
given O
by O
z O
= O
ψc O
( O
xc O
) O
( O
8.40 O
) O
x O
c O
which O
ensures O
that O
the O
distribution O
p O
( O
x O
) O
given O
by O
( O
8.39 O
) O
is O
correctly O
normalized O
. O
thus O
, O
we O
have O
a O
probability B
density O
of O
the O
form O
( O
cid:14 O
) O
p O
( O
w O
) O
= O
p O
( O
wi O
) O
where O
i O
πjn O
( O
wi|µj O
, O
σ2 O
j O
) O
m O
( O
cid:2 O
) O
j=1 O
p O
( O
wi O
) O
= O
( O
5.136 O
) O
( O
5.137 O
) O
and O
πj O
are O
the O
mixing O
coefﬁcients O
. O
again O
, O
we O
can O
make O
use O
of O
the O
eigenvector O
expansion O
of O
the O
covariance B
matrix I
given O
by O
( O
2.45 O
) O
, O
together O
with O
the O
completeness O
of O
the O
set O
of O
eigenvectors O
, O
to O
write O
d O
( O
cid:2 O
) O
j=1 O
z O
= O
yjuj O
( O
2.60 O
) O
( O
cid:6 O
) O
( O
cid:12 O
) O
d O
( O
cid:2 O
) O
−1 O
2 O
d O
( O
cid:2 O
) O
( O
cid:13 O
) O
( O
cid:6 O
) O
exp O
ztς O
−1z O
zzt O
dz O
1 O
|σ|1/2 O
uiut O
j O
exp O
i=1 O
j=1 O
k=1 O
( O
cid:24 O
) O
− O
d O
( O
cid:2 O
) O
( O
cid:25 O
) O
y2 O
k O
2λk O
yiyj O
dy O
where O
yj O
= O
ut O
j O
z O
, O
which O
gives O
1 O
( O
2π O
) O
d/2 O
1 O
|σ|1/2 O
1 O
( O
2π O
) O
d/2 O
d O
( O
cid:2 O
) O
= O
= O
uiut O
i O
λi O
= O
σ O
( O
2.61 O
) O
i=1 O
where O
we O
have O
made O
use O
of O
the O
eigenvector O
equation O
( O
2.45 O
) O
, O
together O
with O
the O
fact O
that O
the O
integral O
on O
the O
right-hand O
side O
of O
the O
middle O
line O
vanishes O
by O
symmetry O
unless O
i O
= O
j O
, O
and O
in O
the O
ﬁnal O
line O
we O
have O
made O
use O
of O
the O
results O
( O
1.50 O
) O
and O
( O
2.55 O
) O
, O
together O
with O
( O
2.48 O
) O
. O
if O
either O
end O
point O
does O
not O
, O
then O
the O
region O
is O
extended B
in O
that O
direction O
by O
increments O
of O
value O
w O
until O
the O
end O
point O
lies O
outside O
the O
region O
. O
2.3.8 O
periodic O
variables O
although O
gaussian O
distributions O
are O
of O
great O
practical O
signiﬁcance O
, O
both O
in O
their O
own O
right O
and O
as O
building O
blocks O
for O
more O
complex O
probabilistic O
models O
, O
there O
are O
situations O
in O
which O
they O
are O
inappropriate O
as O
density B
models O
for O
continuous O
vari- O
ables O
. O
similarly O
, O
the O
mean B
is O
given O
by O
σa O
( O
λaa O
− O
λabλ O
−1 O
bb O
λba O
) O
µa O
= O
µa O
where O
we O
have O
used O
( O
2.88 O
) O
. O
we O
can O
in O
principle O
extend O
the O
above O
procedure O
, O
at O
least O
in O
the O
case O
of O
nodes O
representing O
discrete O
variables O
, O
to O
give O
the O
following O
logic B
sampling I
approach O
( O
henrion O
, O
1988 O
) O
, O
which O
can O
be O
seen O
as O
a O
special O
case O
of O
impor- O
tance O
sampling O
discussed O
in O
section O
11.1.4. O
at O
each O
step O
, O
when O
a O
sampled O
value O
is O
obtained O
for O
a O
variable O
zi O
whose O
value O
is O
observed O
, O
the O
sampled O
value O
is O
compared O
to O
the O
observed O
value O
, O
and O
if O
they O
agree O
then O
the O
sample O
value O
is O
retained O
and O
the O
al- O
gorithm O
proceeds O
to O
the O
next O
variable O
in O
turn O
. O
typically O
there O
is O
a O
sequence O
of O
states O
and O
actions O
in O
which O
the O
learning B
algorithm O
is O
interacting O
with O
its O
environment O
. O
section O
12.2.3 O
• O
probabilistic O
pca O
forms O
the O
basis O
for O
a O
bayesian O
treatment O
of O
pca O
in O
which O
the O
dimensionality O
of O
the O
principal B
subspace I
can O
be O
found O
automatically O
from O
the O
data O
. O
( O
10.102 O
) O
( O
10.103 O
) O
the O
evaluation O
of O
the O
variational B
posterior O
distribution O
begins O
by O
initializing O
the O
pa- O
rameters O
of O
one O
of O
the O
distributions O
q O
( O
w O
) O
or O
q O
( O
α O
) O
, O
and O
then O
alternately O
re-estimates O
these O
factors O
in O
turn O
until O
a O
suitable O
convergence O
criterion O
is O
satisﬁed O
( O
usually O
speci- O
ﬁed O
in O
terms O
of O
the O
lower B
bound I
to O
be O
discussed O
shortly O
) O
. O
techniques O
for O
learning O
the O
hyperparameters O
are O
based O
on O
the O
evaluation O
of O
the O
likelihood B
function I
p O
( O
t|θ O
) O
where O
θ O
denotes O
the O
hyperparameters O
of O
the O
gaussian O
pro- O
cess O
model O
. O
( O
d.4 O
) O
because O
this O
must O
hold O
for O
an O
arbitrary O
choice O
of O
η O
( O
x O
) O
, O
it O
follows O
that O
the O
functional B
derivative O
must O
vanish O
. O
we O
can O
illustrate O
bayesian O
learning B
in O
a O
linear O
basis O
function O
model O
, O
as O
well O
as O
the O
sequential O
update O
of O
a O
posterior O
distribution O
, O
using O
a O
simple O
example O
involving O
straight-line O
ﬁtting O
. O
( O
14.7 O
) O
m O
( O
cid:2 O
) O
m=1 O
ycom O
( O
x O
) O
= O
1 O
m O
this O
procedure O
is O
known O
as O
bootstrap B
aggregation O
or O
bagging B
( O
breiman O
, O
1996 O
) O
. O
this O
concept O
can O
be O
extended B
to O
one O
( O
cid:173 O
) O
dimensional O
nonlinear O
surfaces O
in O
the O
form O
of O
principal O
curves O
( O
hastie O
and O
stuetzle O
, O
1989 O
) O
. O
14.5.2 O
mixtures O
of O
logistic O
models O
because O
the O
logistic B
regression I
model O
deﬁnes O
a O
conditional B
distribution O
for O
the O
target O
variable O
, O
given O
the O
input O
vector O
, O
it O
is O
straightforward O
to O
use O
it O
as O
the O
component O
distribution O
in O
a O
mixture B
model I
, O
thereby O
giving O
rise O
to O
a O
richer O
family O
of O
conditional B
distributions O
compared O
to O
a O
single O
logistic B
regression I
model O
. O
if O
there O
are O
nodes O
in O
a O
directed B
graph O
that O
have O
more O
than O
one O
parent O
, O
but O
there O
is O
still O
only O
one O
path O
( O
ignoring O
the O
direction O
of O
the O
arrows O
) O
between O
any O
two O
nodes O
, O
then O
the O
graph O
is O
a O
called O
a O
polytree B
, O
as O
illustrated O
in O
figure O
8.39 O
( O
c O
) O
. O
finally O
, O
there O
are O
undirected B
links O
connecting O
each O
factor O
node O
to O
all O
of O
the O
variables O
nodes O
on O
which O
that O
factor O
depends O
. O
one O
advantage O
of O
the O
bayesian O
viewpoint O
is O
that O
the O
inclusion O
of O
prior B
knowl- O
edge B
arises O
naturally O
. O
, O
y O
( O
xn O
) O
in O
a O
consistent B
manner O
. O
now O
suppose O
that O
the O
mixing O
coefﬁcients O
in O
both O
levels O
of O
such O
a O
hierar- O
chical O
model O
are O
arbitrary O
functions O
of O
x. O
again O
, O
show O
that O
this O
hierarchical B
model O
is O
again O
equivalent O
to O
a O
single-level O
model O
with O
x-dependent O
mixing O
coefﬁcients O
. O
in O
the O
case O
of O
probability B
distributions O
speciﬁed O
using O
graphical O
models O
, O
the O
conditional B
distribu- O
tions O
for O
individual O
nodes O
depend O
only O
on O
the O
variables O
in O
the O
corresponding O
markov O
blankets O
, O
as O
illustrated O
in O
figure O
11.12. O
for O
directed O
graphs O
, O
a O
wide O
choice O
of O
condi- O
tional O
distributions O
for O
the O
individual O
nodes O
conditioned O
on O
their O
parents O
will O
lead O
to O
conditional B
distributions O
for O
gibbs O
sampling O
that O
are O
log O
concave O
. O
( O
2.95 O
) O
( O
2.96 O
) O
( O
2.97 O
) O
( O
2.98 O
) O
we O
illustrate O
the O
idea O
of O
conditional B
and O
marginal B
distributions O
associated O
with O
a O
multivariate O
gaussian O
using O
an O
example O
involving O
two O
variables O
in O
figure O
2.9 O
. O
( O
independent B
identically I
distributed I
) O
data O
introduced O
in O
sec- O
tion O
1.2.4. O
consider O
the O
problem O
of O
ﬁnding O
the O
posterior O
distribution O
for O
the O
mean B
of O
a O
univariate O
gaussian O
distribution O
. O
if O
the O
starting O
point O
is O
a O
directed B
graph O
, O
it O
is O
ﬁrst O
converted O
to O
an O
undirected B
graph I
by O
moraliza- O
tion O
, O
whereas O
if O
starting O
from O
an O
undirected B
graph I
this O
step O
is O
not O
required O
. O
however O
, O
we O
need O
only O
retain O
that O
particular O
path O
that O
so O
far O
has O
the O
highest O
probability B
. O
4.3.4 O
multiclass B
logistic O
regression B
. O
in O
the O
variational B
framework O
, O
we O
seek O
to O
maximize O
a O
lower B
bound I
on O
the O
marginal B
likelihood I
. O
the O
extension O
to O
a O
general O
tree-structured O
factor B
graph I
should O
now O
be O
clear O
. O
of O
course O
this O
is O
a O
somewhat O
restrictive O
assumption O
, O
and O
in O
section O
5.6 O
we O
shall O
see O
how O
to O
extend O
this O
approach O
to O
allow O
for O
more O
general O
conditional B
distributions O
. O
3.19 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
show O
that O
the O
integration O
over O
w O
in O
the O
bayesian O
linear B
regression I
model O
gives O
the O
result O
( O
3.85 O
) O
. O
( O
10.207 O
) O
( O
10.208 O
) O
i O
a O
special O
case O
of O
ep O
, O
known O
as O
assumed B
density I
ﬁltering I
( O
adf O
) O
or O
moment B
matching I
( O
maybeck O
, O
1982 O
; O
lauritzen O
, O
1992 O
; O
boyen O
and O
koller O
, O
1998 O
; O
opper O
and O
winther O
, O
1999 O
) O
, O
is O
obtained O
by O
initializing O
all O
of O
the O
approximating O
factors O
except O
the O
ﬁrst O
to O
unity O
and O
then O
making O
one O
pass O
through O
the O
factors O
updating O
each O
of O
them O
once O
. O
precise O
form O
of O
the O
factorization B
. O
let O
us O
consider O
the O
problem O
of O
evaluating O
the O
mean B
of O
a O
set O
of O
observations O
d O
= O
{ O
θ1 O
, O
. O
more O
generally O
, O
inference B
can O
be O
performed O
efﬁciently O
using O
local B
message O
passing O
on O
a O
broader O
class O
of O
graphs O
called O
trees O
. O
this O
is O
the O
basis O
for O
the O
convolutional B
neural I
network I
( O
le O
cun O
et O
al. O
, O
1989 O
; O
lecun O
et O
al. O
, O
1998 O
) O
, O
which O
has O
been O
widely O
applied O
to O
image O
data O
. O
this O
leads O
us O
to O
consider O
markov O
models O
in O
which O
we O
assume O
that O
future O
predictions O
are O
inde- O
13.1. O
markov O
models O
607 O
figure O
13.2 O
the O
simplest O
approach O
to O
modelling O
a O
sequence O
of O
ob- O
servations O
is O
to O
treat O
them O
as O
independent B
, O
correspond- O
ing O
to O
a O
graph O
without O
links O
. O
when O
node B
c O
is O
unobserved O
, O
it O
‘ O
blocks O
’ O
the O
path O
, O
and O
the O
variables O
a O
and O
b O
are O
independent B
. O
the O
eigen- O
vectors O
therefore O
deﬁne O
a O
new O
set O
of O
shifted O
and O
rotated O
coordinates O
with O
respect O
to O
which O
the O
joint O
probability B
distribution O
factorizes O
into O
a O
product O
of O
independent B
distributions O
. O
the O
data O
set O
is O
divided O
into O
training B
, O
validation O
, O
and O
test O
sets O
, O
each O
of O
which O
comprises O
1 O
, O
000 O
independent B
data O
points O
. O
in O
particular O
, O
state-of-the-art O
algorithms O
for O
decoding O
certain O
kinds O
of O
error-correcting O
codes O
are O
equivalent O
to O
loopy B
belief I
propagation I
( O
gallager O
, O
1963 O
; O
berrou O
et O
al. O
, O
1993 O
; O
mceliece O
et O
al. O
, O
1998 O
; O
mackay O
and O
neal O
, O
1999 O
; O
frey O
, O
1998 O
) O
. O
the O
average O
value O
of O
some O
function O
f O
( O
x O
) O
under O
a O
probability B
distribution O
p O
( O
x O
) O
is O
called O
the O
expectation B
of O
f O
( O
x O
) O
and O
will O
be O
denoted O
by O
e O
[ O
f O
] O
. O
) O
, O
learning B
in O
graphical O
models O
, O
pp O
. O
before O
considering O
a O
formal O
solution O
, O
let O
us O
try O
to O
obtain O
some O
intuition O
about O
the O
result O
by O
considering O
the O
case O
of O
a O
two-dimensional O
data O
space O
d O
= O
2 O
and O
a O
one O
( O
cid:173 O
) O
dimensional O
principal B
subspace I
m O
= O
1. O
we O
have O
to O
choose O
a O
direction O
u2 O
so O
as O
to O
12.1. O
principal B
component I
analysis I
565 O
minimize O
j O
= O
uisu2 O
' O
subject O
to O
the O
normalization O
constraint O
ui O
u2 O
= O
1. O
using O
a O
lagrange O
multiplier O
a2 O
to O
enforce O
the O
constraint O
, O
we O
consider O
the O
minimization O
of O
( O
12.16 O
) O
setting O
the O
derivative B
with O
respect O
to O
u2 O
to O
zero O
, O
we O
obtain O
su2 O
= O
a2u2 O
so O
that O
u2 O
is O
an O
eigenvector O
of O
s O
with O
eigenvalue O
a2 O
. O
on O
the O
theory B
of O
brownian O
motion O
. O
5.6 O
( O
( O
cid:12 O
) O
) O
www O
show O
the O
derivative B
of O
the O
error B
function I
( O
5.21 O
) O
with O
respect O
to O
the O
activation O
ak O
for O
an O
output O
unit O
having O
a O
logistic B
sigmoid I
activation O
function O
satisﬁes O
( O
5.18 O
) O
. O
the O
tech- O
nique O
of O
chunking B
( O
vapnik O
, O
1982 O
) O
exploits O
the O
fact O
that O
the O
value O
of O
the O
lagrangian O
is O
unchanged O
if O
we O
remove O
the O
rows O
and O
columns O
of O
the O
kernel O
matrix O
corresponding O
to O
lagrange O
multipliers O
that O
have O
value O
zero O
. O
the O
analysis O
is O
straightforward O
and O
makes O
use O
of O
the O
usual O
rules O
of O
differential B
calculus O
, O
together O
with O
the O
result O
r O
{ O
w O
} O
= O
v. O
( O
5.97 O
) O
the O
technique O
is O
best O
illustrated O
with O
a O
simple O
example O
, O
and O
again O
we O
choose O
a O
two-layer O
network O
of O
the O
form O
shown O
in O
figure O
5.1 O
, O
with O
linear O
output O
units O
and O
a O
sum-of-squares B
error I
function O
. O
by O
comparison O
, O
if O
we O
had O
run O
the O
sum-product B
algorithm I
separately O
for O
each O
node B
, O
the O
amount O
of O
computation O
would O
grow O
quadratically O
with O
the O
size O
of O
the O
graph O
. O
to O
address O
this O
, O
we O
can O
take O
a O
frequentist B
view O
of O
bayesian O
learning B
and O
show O
that O
, O
on O
average O
, O
such O
a O
property O
does O
indeed O
hold O
. O
the O
cartesian O
coordinates O
of O
the O
observations O
are O
given O
by O
xn O
= O
( O
cos O
θn O
, O
sin O
θn O
) O
, O
and O
we O
can O
write O
the O
carte- O
sian O
coordinates O
of O
the O
sample B
mean I
in O
the O
form O
x O
= O
( O
r O
cos O
θ O
, O
r O
sin O
θ O
) O
. O
such O
models O
can O
be O
viewed O
as O
mixture B
distributions O
in O
which O
the O
component O
densities O
, O
as O
well O
as O
the O
mixing O
coefﬁcients O
, O
are O
conditioned O
on O
the O
input O
variables O
and O
are O
known O
as O
mixtures O
of O
experts O
. O
for O
instance O
, O
in O
controlling O
the O
simple O
robot B
arm I
shown O
in O
figure O
5.18 O
, O
we O
need O
to O
pick O
one O
of O
the O
two O
possible O
joint O
angle O
settings O
in O
order O
to O
achieve O
the O
desired O
end-effector O
location O
, O
whereas O
the O
average O
of O
the O
two O
solutions O
is O
not O
itself O
a O
solution O
. O
here O
η O
are O
called O
the O
natural B
parameters I
of O
the O
distribution O
, O
and O
u O
( O
x O
) O
is O
some O
function O
of O
x. O
the O
function O
g O
( O
η O
) O
can O
be O
interpreted O
as O
the O
coefﬁcient O
that O
ensures O
that O
the O
distribu- O
tion O
is O
normalized O
and O
therefore O
satisﬁes O
( O
cid:6 O
) O
g O
( O
η O
) O
h O
( O
x O
) O
exp O
ηtu O
( O
x O
) O
dx O
= O
1 O
( O
2.195 O
) O
( O
cid:27 O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
where O
the O
integration O
is O
replaced O
by O
summation O
if O
x O
is O
a O
discrete O
variable O
. O
to O
ensure O
this O
, O
we O
add O
extra O
links O
between O
all O
pairs O
of O
parents O
of O
the O
node B
x4 O
. O
as O
a O
result O
of O
the O
central B
limit I
theorem I
, O
the O
posterior O
distribution O
for O
a O
model O
is O
expected O
to O
become O
increasingly O
better O
approximated O
by O
a O
gaussian O
as O
the O
number O
of O
observed O
data O
points O
is O
increased O
, O
and O
so O
we O
would O
expect O
the O
laplace O
approximation O
to O
be O
most O
useful O
in O
situations O
where O
the O
number O
of O
data O
points O
is O
relatively O
large O
. O
note O
that O
the O
factor B
analysis I
model O
, O
in O
common O
with O
probabilistic O
pca O
. O
we O
shall O
see O
another O
example O
of O
this O
approach O
when O
we O
discuss O
hybrid O
monte O
carlo O
in O
section O
11.5. O
the O
goal O
is O
to O
sample O
uniformly O
from O
the O
area O
under O
the O
distribution O
˜p O
( O
z O
) O
u O
z O
( O
τ O
) O
11.4. O
slice B
sampling I
547 O
˜p O
( O
z O
) O
zmin O
u O
zmax O
z O
( O
a O
) O
z O
( O
τ O
) O
( O
b O
) O
z O
( O
a O
) O
for O
a O
given O
value O
z O
( O
τ O
) O
, O
a O
value O
of O
u O
is O
chosen O
uniformly O
in O
figure O
11.13 O
illustration O
of O
slice B
sampling I
. O
4.3.5 O
probit B
regression I
we O
have O
seen O
that O
, O
for O
a O
broad O
range O
of O
class-conditional O
distributions O
, O
described O
by O
the O
exponential B
family I
, O
the O
resulting O
posterior O
class O
probabilities O
are O
given O
by O
a O
logistic O
( O
or O
softmax O
) O
transformation O
acting O
on O
a O
linear O
function O
of O
the O
feature O
vari- O
ables O
. O
for O
the O
case O
of O
a O
single O
real-valued O
variable O
x O
, O
the O
gaussian O
distribution O
is O
de- O
( O
cid:12 O
) O
( O
cid:13 O
) O
ﬁned O
by O
n O
( O
cid:10 O
) O
( O
cid:11 O
) O
x|µ O
, O
σ2 O
= O
1 O
( O
2πσ2 O
) O
1/2 O
exp O
− O
1 O
2σ2 O
( O
x O
− O
µ O
) O
2 O
( O
1.46 O
) O
which O
is O
governed O
by O
two O
parameters O
: O
µ O
, O
called O
the O
mean B
, O
and O
σ2 O
, O
called O
the O
vari- O
ance O
. O
taking O
the O
expectation B
with O
respect O
to O
the O
posterior O
distribution O
of O
w O
then O
gives O
e O
[ O
ln O
p O
( O
t O
, O
w|α O
, O
β O
) O
] O
= O
m O
2 O
− O
β O
2 O
( O
cid:15 O
) O
( O
cid:16 O
) O
+ O
n O
2 O
ln O
β O
2π O
( O
cid:8 O
) O
( O
cid:18 O
) O
( O
cid:8 O
) O
( O
tn O
− O
wtφn O
) O
2 O
− O
α O
2 O
e O
wtw O
( O
cid:9 O
) O
( O
cid:9 O
) O
( O
cid:17 O
) O
n O
( O
cid:2 O
) O
ln O
α O
2π O
e O
n=1 O
. O
10.31 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
ﬁnding O
the O
second O
derivative O
with O
respect O
to O
x O
, O
show O
that O
the O
function O
f O
( O
x O
) O
= O
− O
ln O
( O
ex/2 O
+ O
e O
−x/2 O
) O
is O
a O
concave B
function I
of O
x. O
now O
consider O
the O
second O
derivatives O
with O
respect O
to O
the O
variable O
x2 O
and O
hence O
show O
that O
it O
is O
a O
convex B
function I
of O
x2 O
. O
0.3 O
0.2 O
0.1 O
0 O
0 O
1 O
2 O
3 O
4 O
5 O
m O
6 O
7 O
8 O
9 O
10 O
which O
is O
also O
known O
as O
the O
sample B
mean I
. O
from O
8.4. O
inference B
in O
graphical O
models O
413 O
the O
results O
( O
8.66 O
) O
and O
( O
8.69 O
) O
derived O
earlier O
for O
the O
sum-product B
algorithm I
, O
we O
can O
readily O
write O
down O
the O
max-sum B
algorithm I
in O
terms O
of O
message B
passing I
simply O
by O
replacing O
‘ O
sum O
’ O
with O
‘ O
max O
’ O
and O
replacing O
products O
with O
sums O
of O
logarithms O
to O
give O
⎡⎣ln O
f O
( O
x O
, O
x1 O
, O
. O
finally O
, O
any O
valid O
distribution O
over O
the O
real O
axis O
( O
such O
as O
a O
gaussian O
) O
can O
be O
turned O
into O
a O
periodic O
distribution O
by O
mapping O
succes- O
sive O
intervals O
of O
width O
2π O
onto O
the O
periodic B
variable I
( O
0 O
, O
2π O
) O
, O
which O
corresponds O
to O
‘ O
wrapping O
’ O
the O
real O
axis O
around O
unit O
circle O
. O
, O
×2 O
× O
1. O
the O
particular O
case O
of O
the O
binomial B
distribution I
for O
n O
= O
1 O
is O
known O
as O
the O
bernoulli O
distribution O
, O
and O
for O
large O
n O
the O
binomial B
distribution I
is O
approximately O
gaussian O
. O
this O
can O
be O
done O
using O
a O
standard O
optimization O
algorithm O
such O
as O
conjugate B
gradients O
or O
quasi-newton O
methods O
. O
( O
7.36 O
) O
again O
, O
a O
numerically O
stable O
solution O
is O
obtained O
by O
averaging O
to O
give O
( O
cid:23 O
) O
( O
cid:22 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
m∈s O
( O
cid:22 O
) O
( O
cid:2 O
) O
m∈s O
b O
= O
1 O
nm O
n∈m O
tn O
− O
( O
cid:4 O
) O
l O
( O
a O
) O
= O
−1 O
2 O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
m=1 O
amtmk O
( O
xn O
, O
xm O
) O
( O
7.37 O
) O
where O
m O
denotes O
the O
set O
of O
indices O
of O
data O
points O
having O
0 O
< O
an O
< O
c. O
an O
alternative O
, O
equivalent O
formulation O
of O
the O
support B
vector I
machine I
, O
known O
as O
the O
ν-svm O
, O
has O
been O
proposed O
by O
sch¨olkopf O
et O
al O
. O
figure O
13.19 O
shows O
the O
corresponding O
graphical B
model I
. O
this O
result O
motivates O
the O
use O
of O
the O
sign O
function O
in O
( O
14.19 O
) O
to O
arrive O
at O
the O
ﬁnal O
classiﬁcation B
decision O
. O
we O
can O
generalize O
the O
k-means O
algorithm O
by O
introducing O
a O
more O
general O
dissimilarity O
measure O
v O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
between O
two O
vectors O
x O
and O
x O
( O
cid:4 O
) O
and O
then O
minimizing O
the O
following O
distortion B
measure I
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
( O
cid:4 O
) O
j O
= O
rnkv O
( O
xn O
, O
µk O
) O
( O
9.6 O
) O
n=1 O
k=1 O
which O
gives O
the O
k-medoids O
algorithm O
. O
similarly O
, O
the O
second O
stage O
of O
weight O
adjustment O
using O
the O
calculated O
derivatives O
can O
be O
tackled O
using O
a O
variety O
of O
optimization O
schemes O
, O
many O
of O
which O
are O
substantially O
more O
powerful O
than O
simple O
gradient B
descent I
. O
( O
10.180 O
) O
note O
that O
this O
has O
precisely O
the O
same O
form O
as O
( O
10.159 O
) O
, O
and O
so O
we O
can O
again O
appeal O
to O
our O
earlier O
result O
( O
10.163 O
) O
, O
which O
can O
be O
obtained O
by O
direct O
optimization O
of O
the O
marginal B
likelihood I
function O
, O
leading O
to O
re-estimation O
equations O
of O
the O
form O
( O
cid:10 O
) O
( O
cid:11 O
) O
( O
ξnew O
n O
) O
2 O
= O
φt O
n O
σn O
+ O
µn O
µt O
n O
φn O
. O
the O
synthetic O
data O
sel O
comprises O
300 O
data O
points O
in O
d O
= O
10 O
dimensions O
sampled O
from O
a O
gaussian O
distribution O
having O
standard B
deviation I
1.0 O
in O
3 O
directions O
and O
standard B
deviation I
0.5 O
in O
the O
remaining O
7 O
directions O
for O
a O
data O
set O
in O
d O
= O
10 O
dimensions O
having O
at O
= O
3 O
directions O
with O
larger O
variance B
than O
the O
remaining O
7 O
directions O
. O
the O
marginal B
distribution O
of O
a O
multivariate O
gaussian O
with O
respect O
to O
a O
subset O
of O
the O
variables O
is O
itself O
gaussian O
, O
and O
similarly O
the O
conditional B
distribution O
is O
also O
gaussian O
. O
we O
can O
also O
use O
this O
synthetic O
data O
set O
to O
illustrate O
the O
‘ O
responsibilities O
’ O
by O
eval- O
uating O
, O
for O
every O
data O
point O
, O
the O
posterior B
probability I
for O
each O
component O
in O
the O
mixture B
distribution I
from O
which O
this O
data O
set O
was O
generated O
. O
however O
, O
these O
bounds O
turn O
out O
to O
be O
too O
loose O
to O
have O
practical O
value O
, O
and O
the O
actual O
performance O
of O
boosting B
is O
much O
better O
than O
the O
bounds O
alone O
would O
suggest O
. O
regression B
shrinkage O
and O
se- O
lection O
via O
the O
lasso B
. O
recall O
that O
the O
principal O
components O
are O
defined O
by O
the O
eigenvectors O
ui O
of O
the O
covariance B
matrix I
where O
i O
= O
1 O
, O
... O
, O
d. O
here O
the O
d O
x O
d O
sample O
covariance O
matrix O
s O
is O
defined O
by O
sui O
= O
aiui O
( O
12.71 O
) O
and O
the O
eigenvectors O
are O
normalized O
such O
that O
ut O
ui O
= O
1. O
now O
consider O
a O
nonlinear O
transformation O
¢ O
( O
x O
) O
into O
an O
m O
-dimensional O
feature B
space I
, O
so O
that O
each O
data O
point O
x O
n O
is O
thereby O
projected O
onto O
a O
point O
¢ O
( O
xn O
) O
. O
for O
this O
purpose O
, O
we O
shall O
assume O
that O
, O
given O
the O
value O
of O
x O
, O
the O
corresponding O
value O
of O
t O
has O
a O
gaussian O
distribution O
with O
a O
mean B
equal O
to O
the O
value O
y O
( O
x O
, O
w O
) O
of O
the O
polynomial O
curve O
given O
by O
( O
1.1 O
) O
. O
again O
, O
we O
can O
verify O
that O
the O
beta O
variables O
themselves O
are O
equivalent O
by O
noting O
that O
( O
8.70 O
) O
implies O
that O
the O
initial O
mes- O
sage O
send O
by O
the O
root O
variable O
node B
is O
µzn→fn O
( O
zn O
) O
= O
1 O
, O
which O
is O
identical O
to O
the O
initialization O
of O
β O
( O
zn O
) O
given O
in O
section O
13.2.2. O
the O
sum-product B
algorithm I
also O
speciﬁes O
how O
to O
evaluate O
the O
marginals O
once O
all O
the O
messages O
have O
been O
evaluated O
. O
consider O
the O
exponential O
error O
function O
deﬁned O
by O
exp O
{ O
−tnfm O
( O
xn O
) O
} O
e O
= O
n O
( O
cid:2 O
) O
n=1 O
( O
14.20 O
) O
where O
fm O
( O
x O
) O
is O
a O
classiﬁer O
deﬁned O
in O
terms O
of O
a O
linear O
combination O
of O
base O
classiﬁers O
yl O
( O
x O
) O
of O
the O
form O
fm O
( O
x O
) O
= O
( O
14.21 O
) O
and O
tn O
∈ O
{ O
−1 O
, O
1 O
} O
are O
the O
training B
set I
target O
values O
. O
the O
relative B
merits O
of O
these O
three O
approaches O
follow O
the O
same O
lines O
as O
for O
classiﬁca- O
tion O
problems O
above O
. O
furthermore O
, O
in O
the O
mixture B
density I
network I
, O
the O
splits O
of O
the O
input O
space O
are O
further O
relaxed O
compared O
to O
the O
hierarchical B
mixture I
of I
experts I
in O
that O
they O
are O
not O
only O
soft B
, O
and O
not O
constrained O
to O
be O
axis O
aligned O
, O
but O
they O
can O
also O
be O
nonlinear O
. O
10.16 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
verify O
the O
results O
( O
10.71 O
) O
and O
( O
10.72 O
) O
for O
the O
ﬁrst O
two O
terms O
in O
the O
lower B
bound I
for O
the O
variational B
gaussian O
mixture B
model I
given O
by O
( O
10.70 O
) O
. O
, O
xm O
) O
= O
1 O
− O
( O
1 O
− O
µ0 O
) O
( O
1 O
− O
µi O
) O
xi O
( O
8.104 O
) O
i=1 O
where O
the O
parameters O
µi O
represent O
the O
probabilities O
p O
( O
xi O
= O
1 O
) O
, O
and O
µ0 O
is O
an O
additional O
parameters O
satisfying O
0 O
( O
cid:1 O
) O
µ0 O
( O
cid:1 O
) O
1. O
the O
conditional B
distribution O
( O
8.104 O
) O
is O
known O
as O
the O
noisy-or O
. O
we O
see O
that O
increasing O
the O
size O
of O
the O
data O
set O
reduces O
the O
over-ﬁtting B
problem O
. O
we O
see O
that O
( O
3.57 O
) O
involves O
the O
convolution O
of O
two O
gaussian O
distributions O
, O
and O
so O
making O
use O
of O
the O
result O
( O
2.115 O
) O
from O
section O
8.1.4 O
, O
we O
see O
that O
the O
predictive B
distribution I
takes O
the O
form O
where O
the O
variance B
σ2 O
p O
( O
t|x O
, O
t O
, O
α O
, O
β O
) O
= O
n O
( O
t|mt O
n O
( O
x O
) O
of O
the O
predictive B
distribution I
is O
given O
by O
n O
φ O
( O
x O
) O
, O
σ2 O
n O
( O
x O
) O
) O
( O
3.58 O
) O
( O
3.59 O
) O
σ2 O
n O
( O
x O
) O
= O
1 O
β O
+ O
φ O
( O
x O
) O
tsn O
φ O
( O
x O
) O
. O
exercises O
419 O
a O
0 O
0 O
0 O
0 O
1 O
1 O
1 O
1 O
b O
0 O
0 O
1 O
1 O
0 O
0 O
1 O
1 O
c O
0 O
1 O
0 O
1 O
0 O
1 O
0 O
1 O
p O
( O
a O
, O
b O
, O
c O
) O
0.192 O
0.144 O
0.048 O
0.216 O
0.192 O
0.064 O
0.048 O
0.096 O
8.3 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
three O
binary O
variables O
a O
, O
b O
, O
c O
∈ O
{ O
0 O
, O
1 O
} O
having O
the O
joint O
distribution O
given O
in O
table O
8.2. O
show O
by O
direct O
evaluation O
that O
this O
distribution O
has O
the O
property O
that O
a O
and O
b O
are O
marginally O
dependent O
, O
so O
that O
p O
( O
a O
, O
b O
) O
( O
cid:9 O
) O
= O
p O
( O
a O
) O
p O
( O
b O
) O
, O
but O
that O
they O
become O
independent B
when O
conditioned O
on O
c O
, O
so O
that O
p O
( O
a O
, O
b|c O
) O
= O
p O
( O
a|c O
) O
p O
( O
b|c O
) O
for O
both O
c O
= O
0 O
and O
c O
= O
1 O
. O
in O
the O
limit O
c O
→ O
∞ O
, O
we O
will O
recover O
the O
earlier O
support B
vector I
machine I
for O
separable O
data O
. O
in O
particular O
, O
the O
distribution O
associated O
with O
a O
particular O
node B
can O
be O
updated O
once O
that O
node B
has O
received O
messages O
from O
all O
of O
its O
parents O
and O
all O
of O
its O
children O
. O
the O
input O
, O
hidden O
, O
and O
output O
variables O
are O
represented O
by O
nodes O
, O
and O
the O
weight O
parameters O
are O
rep- O
resented O
by O
links O
between O
the O
nodes O
, O
in O
which O
the O
bias B
pa- O
rameters O
are O
denoted O
by O
links O
coming O
from O
additional O
input O
and O
hidden O
variables O
x0 O
and O
z0 O
. O
although O
the O
assumption O
of O
a O
linear-gaussian O
model O
leads O
to O
efﬁcient O
algorithms O
for O
inference O
and O
learning B
, O
it O
also O
implies O
that O
the O
marginal B
distribution O
of O
the O
observed O
variables O
is O
simply O
a O
gaussian O
, O
which O
represents O
a O
signiﬁcant O
limitation O
. O
the O
mode O
of O
the O
resulting O
approximation O
to O
the O
posterior O
distribution O
, O
corre- O
sponding O
to O
the O
mean B
of O
the O
gaussian O
approximation O
, O
is O
obtained O
setting O
( O
7.110 O
) O
to O
zero O
, O
giving O
the O
mean B
and O
covariance B
of O
the O
laplace O
approximation O
in O
the O
form O
( O
cid:10 O
) O
w O
( O
cid:1 O
) O
= O
a−1φt O
( O
t O
− O
y O
) O
φtbφ O
+ O
a O
σ O
= O
( O
cid:11 O
) O
−1 O
. O
( O
8.20 O
) O
we O
say O
that O
a O
is O
conditionally O
independent B
of O
b O
given O
c. O
this O
can O
be O
expressed O
in O
a O
slightly O
different O
way O
if O
we O
consider O
the O
joint O
distribution O
of O
a O
and O
b O
conditioned O
on O
c O
, O
which O
we O
can O
write O
in O
the O
form O
p O
( O
a O
, O
b|c O
) O
= O
p O
( O
a|b O
, O
c O
) O
p O
( O
b|c O
) O
= O
p O
( O
a|c O
) O
p O
( O
b|c O
) O
. O
as O
before O
, O
we O
assume O
that O
the O
target O
variable O
t O
is O
given O
by O
a O
deterministic O
func- O
tion O
y O
( O
x O
, O
w O
) O
with O
additive O
gaussian O
noise O
so O
that O
t O
= O
y O
( O
x O
, O
w O
) O
+ O
 O
( O
3.7 O
) O
where O
 O
is O
a O
zero O
mean B
gaussian O
random O
variable O
with O
precision O
( O
inverse B
variance O
) O
β. O
thus O
we O
can O
write O
p O
( O
t|x O
, O
w O
, O
β O
) O
= O
n O
( O
t|y O
( O
x O
, O
w O
) O
, O
β O
−1 O
) O
. O
this O
result O
is O
easily O
extended B
to O
the O
k O
class O
problem O
to O
obtain O
the O
corresponding O
maximum B
likelihood I
solutions O
for O
the O
parameters O
in O
which O
each O
class-conditional O
density B
is O
gaussian O
with O
a O
shared O
covariance O
matrix O
. O
a O
simple O
approach O
to O
ﬁnding O
latent B
variable I
values O
having O
high O
probability B
would O
be O
to O
run O
the O
sum-product B
algorithm I
to O
obtain O
the O
marginals O
p O
( O
xi O
) O
for O
ev- O
ery O
variable O
, O
and O
then O
, O
for O
each O
marginal B
in O
turn O
, O
to O
ﬁnd O
the O
value O
x O
( O
cid:1 O
) O
i O
that O
maximizes O
that O
marginal B
. O
if O
the O
treewidth B
of O
the O
original O
graph O
is O
high O
, O
the O
junction B
tree I
algorithm I
becomes O
impractical O
. O
i O
( O
cid:1 O
) O
si O
and O
αi O
= O
∞ O
, O
then O
the O
basis B
function I
ϕi O
is O
already O
excluded O
in O
practice O
, O
it O
is O
convenient O
to O
evaluate O
the O
quantities O
qi O
= O
ϕt O
si O
= O
ϕt O
i O
c−1t O
i O
c−1ϕi O
. O
4.2 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
the O
minimization O
of O
a O
sum-of-squares B
error I
function O
( O
4.15 O
) O
, O
and O
suppose O
that O
all O
of O
the O
target O
vectors O
in O
the O
training B
set I
satisfy O
a O
linear O
constraint O
attn O
+ O
b O
= O
0 O
( O
4.157 O
) O
where O
tn O
corresponds O
to O
the O
nth O
row O
of O
the O
matrix O
t O
in O
( O
4.15 O
) O
. O
variational B
methods O
have O
broad O
applicability O
and O
include O
such O
areas O
as O
ﬁnite O
element O
methods O
( O
kapur O
, O
1989 O
) O
and O
maximum O
entropy O
( O
schwarz O
, O
1988 O
) O
. O
from O
the O
deﬁnition O
( O
8.69 O
) O
, O
we O
see O
that O
if O
a O
leaf O
node B
is O
a O
variable O
node B
, O
then O
the O
message O
that O
it O
sends O
along O
its O
one O
and O
only O
link B
is O
given O
by O
( O
8.70 O
) O
as O
illustrated O
in O
figure O
8.49 O
( O
a O
) O
. O
in O
order O
to O
start O
this O
recursion O
, O
we O
can O
view O
the O
node B
x O
as O
the O
root O
of O
the O
tree B
and O
begin O
at O
the O
leaf O
nodes O
. O
8.23 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
in O
section O
8.4.4 O
, O
we O
showed O
that O
the O
marginal B
distribution O
p O
( O
xi O
) O
for O
a O
variable O
node B
xi O
in O
a O
factor B
graph I
is O
given O
by O
the O
product O
of O
the O
messages O
arriving O
at O
this O
node B
from O
neighbouring O
factor O
nodes O
in O
the O
form O
( O
8.63 O
) O
. O
zn−1 O
zn O
zn+1 O
xn−1 O
xn O
xn+1 O
requires O
that O
every O
training B
sequence O
be O
evaluated O
under O
each O
of O
the O
models O
in O
or- O
der O
to O
compute O
the O
denominator O
in O
( O
13.73 O
) O
. O
note O
that O
if O
we O
had O
run O
a O
forward O
pass O
of O
max-sum O
message O
passing O
followed O
by O
a O
backward O
pass O
and O
then O
applied O
( O
8.98 O
) O
at O
each O
node B
separately O
, O
we O
could O
end O
up O
selecting O
some O
states O
from O
one O
path O
and O
some O
from O
the O
other O
path O
, O
giving O
an O
overall O
conﬁguration O
that O
is O
not O
a O
global O
maximizer O
. O
( O
1.117 O
) O
we O
can O
apply O
jensen O
’ O
s O
inequality O
in O
the O
form O
( O
1.117 O
) O
to O
the O
kullback-leibler O
divergence O
( O
1.113 O
) O
to O
give O
( O
cid:6 O
) O
( O
cid:12 O
) O
kl O
( O
p O
( O
cid:6 O
) O
q O
) O
= O
− O
p O
( O
x O
) O
ln O
q O
( O
x O
) O
p O
( O
x O
) O
dx O
( O
cid:2 O
) O
− O
ln O
q O
( O
x O
) O
dx O
= O
0 O
( O
1.118 O
) O
( O
cid:1 O
) O
( O
cid:13 O
) O
( O
cid:6 O
) O
1.6. O
information B
theory I
57 O
( O
cid:28 O
) O
where O
we O
have O
used O
the O
fact O
that O
− O
ln O
x O
is O
a O
convex B
function I
, O
together O
with O
the O
nor- O
q O
( O
x O
) O
dx O
= O
1. O
in O
fact O
, O
− O
ln O
x O
is O
a O
strictly O
convex B
function I
, O
malization O
condition O
so O
the O
equality O
will O
hold O
if O
, O
and O
only O
if O
, O
q O
( O
x O
) O
= O
p O
( O
x O
) O
for O
all O
x. O
thus O
we O
can O
in- O
terpret O
the O
kullback-leibler O
divergence O
as O
a O
measure O
of O
the O
dissimilarity O
of O
the O
two O
distributions O
p O
( O
x O
) O
and O
q O
( O
x O
) O
. O
graphically O
, O
we O
say O
that O
node B
c O
is O
head-to-head O
with O
respect O
to O
the O
path O
from O
a O
to O
b O
because O
it O
connects O
to O
the O
heads O
of O
the O
two O
arrows O
. O
this O
framework O
is O
easily O
extended B
to O
regularized O
maximum O
likeli- O
hood O
by O
introducing O
priors O
over O
the O
model O
parameters O
π O
, O
a O
and O
φ O
whose O
values O
are O
then O
estimated O
by O
maximizing O
their O
posterior B
probability I
. O
11.1.5 O
sampling-importance-resampling B
. O
next O
we O
introduce O
priors O
over O
the O
parameters O
µ O
, O
λ O
and O
π. O
the O
analysis O
is O
con- O
siderably O
simpliﬁed O
if O
we O
use O
conjugate B
prior I
distributions O
. O
( O
8.4 O
) O
the O
reader O
should O
take O
a O
moment O
to O
study O
carefully O
the O
correspondence O
between O
( O
8.4 O
) O
and O
figure O
8.2. O
k O
( O
cid:14 O
) O
p O
( O
x O
) O
= O
p O
( O
xk|pak O
) O
we O
can O
now O
state O
in O
general O
terms O
the O
relationship O
between O
a O
given O
directed B
graph O
and O
the O
corresponding O
distribution O
over O
the O
variables O
. O
before O
the O
start O
of O
each O
leapfrog O
integration O
sequence O
, O
we O
choose O
at O
random O
, O
with O
equal O
probability B
, O
whether O
to O
integrate O
forwards O
in O
time O
( O
using O
step O
size O
 O
) O
or O
backwards O
in O
time O
( O
using O
step O
size O
− O
) O
. O
8.22 O
( O
( O
cid:12 O
) O
) O
consider O
a O
tree-structured O
factor B
graph I
, O
in O
which O
a O
given O
subset O
of O
the O
variable O
nodes O
form O
a O
connected O
subgraph O
( O
i.e. O
, O
any O
variable O
node B
of O
the O
subset O
is O
connected O
to O
at O
least O
one O
of O
the O
other O
variable O
nodes O
via O
a O
single O
factor O
node O
) O
. O
values O
for O
the O
parameters O
a O
and O
b O
are O
found O
by O
minimizing O
the O
cross-entropy B
error I
function I
deﬁned O
by O
a O
training B
set I
consisting O
of O
pairs O
of O
values O
y O
( O
xn O
) O
and O
tn O
. O
( O
2.92 O
) O
( O
2.93 O
) O
we O
see O
that O
for O
a O
marginal B
distribution O
, O
the O
mean B
and O
covariance B
are O
most O
simply O
ex- O
pressed O
in O
terms O
of O
the O
partitioned B
covariance O
matrix O
, O
in O
contrast O
to O
the O
conditional B
distribution O
for O
which O
the O
partitioned B
precision O
matrix O
gives O
rise O
to O
simpler O
expres- O
sions O
. O
( O
4.72 O
) O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
1 O
n O
setting O
the O
derivative B
with O
respect O
to O
π O
equal O
to O
zero O
and O
rearranging O
, O
we O
obtain O
tn O
= O
n1 O
n O
= O
n1 O
π O
= O
n=1 O
n1 O
+ O
n2 O
( O
4.73 O
) O
where O
n1 O
denotes O
the O
total O
number O
of O
data O
points O
in O
class O
c1 O
, O
and O
n2 O
denotes O
the O
total O
number O
of O
data O
points O
in O
class O
c2 O
. O
one O
approach O
to O
maxi- O
mizing O
the O
likelihood B
function I
is O
to O
use O
iterative O
numerical O
optimization O
techniques O
( O
fletcher O
, O
1987 O
; O
nocedal O
and O
wright O
, O
1999 O
; O
bishop O
and O
nabney O
, O
2008 O
) O
. O
for O
instance O
, O
to O
ﬁnd O
p O
( O
x1 O
) O
we O
need O
to O
prop- O
agate O
a O
message O
µβ O
( O
· O
) O
from O
node B
xn O
back O
to O
node B
x2 O
. O
bayesian O
gaussian O
process O
mod- O
els O
: O
pac-bayesian O
generalization B
error O
bounds O
and O
sparse O
approximations O
. O
this O
involves O
a O
slight O
modiﬁcation O
of O
the O
usual O
backpropagation B
algorithm O
to O
ensure O
that O
the O
shared-weight O
constraints O
are O
satisﬁed O
. O
the O
pruning O
is O
based O
on O
a O
criterion O
that O
balances O
residual O
error B
against O
a O
measure O
of O
model O
complexity O
. O
due O
to O
the O
presence O
of O
the O
regularizer O
, O
the O
solution O
no O
longer O
interpolates O
the O
training B
data O
exactly O
. O
for O
the O
moment O
, O
however O
, O
we O
shall O
determine O
values O
for O
the O
unknown O
parame- O
ters O
µ O
and O
σ2 O
in O
the O
gaussian O
by O
maximizing O
the O
likelihood B
function I
( O
1.53 O
) O
. O
exercises O
287 O
5.20 O
( O
( O
cid:12 O
) O
) O
derive O
an O
expression O
for O
the O
outer B
product I
approximation I
to O
the O
hessian O
matrix O
for O
a O
network O
having O
k O
outputs O
with O
a O
softmax O
output-unit O
activation B
function I
and O
a O
cross-entropy B
error I
function I
, O
corresponding O
to O
the O
result O
( O
5.84 O
) O
for O
the O
sum-of- O
squares O
error B
function I
. O
figure O
1.13 O
plot O
of O
the O
univariate O
gaussian O
showing O
the O
mean B
µ O
and O
the O
standard B
deviation I
σ. O
n O
( O
x|µ O
, O
σ2 O
) O
1.2. O
probability B
theory O
25 O
2σ O
µ O
dx O
= O
1 O
. O
now O
we O
turn O
to O
a O
discussion O
of O
the O
marginal B
distribution O
given O
by O
( O
cid:6 O
) O
p O
( O
xa O
) O
= O
p O
( O
xa O
, O
xb O
) O
dxb O
( O
2.83 O
) O
which O
, O
as O
we O
shall O
see O
, O
is O
also O
gaussian O
. O
this O
can O
be O
achieved O
if O
the O
clique B
potentials O
of O
the O
undirected B
graph I
are O
given O
by O
the O
conditional B
distributions O
of O
the O
directed B
graph O
. O
the O
network O
is O
again O
trained O
by O
minimization O
of O
the O
error B
function I
( O
12.91 O
) O
. O
the O
corresponding O
lagrangian O
function O
is O
given O
by O
the O
conditions O
for O
this O
lagrangian O
to O
be O
stationary B
with O
respect O
to O
x1 O
, O
x2 O
, O
and O
λ O
give O
the O
following O
coupled O
equations O
: O
1 O
− O
x2 O
l O
( O
x O
, O
λ O
) O
= O
1 O
− O
x2 O
1 O
− O
x2 O
2 O
+ O
λ O
( O
x1 O
+ O
x2 O
− O
1 O
) O
. O
the O
factors O
fs O
( O
xs O
) O
are O
then O
set O
equal O
to O
the O
clique B
potentials O
. O
10.6. O
variational B
logistic O
regression B
503 O
speciﬁcally O
, O
we O
consider O
once O
again O
a O
simple O
isotropic B
gaussian O
prior B
distribu- O
tion O
of O
the O
form O
p O
( O
w|α O
) O
= O
n O
( O
w|0 O
, O
α O
−1i O
) O
. O
parameter O
λ. O
the O
effective B
number I
of I
parameters I
in O
the O
network O
therefore O
grows O
during O
the O
course O
of O
training B
. O
for O
many O
applications O
, O
the O
resulting O
model O
can O
be O
signiﬁcantly O
more O
compact O
, O
and O
hence O
faster O
to O
evaluate O
, O
than O
a O
support B
vector I
machine I
having O
the O
same O
generalization B
performance O
. O
in O
fact O
, O
we O
shall O
see O
that O
some O
properties O
of O
gaussian O
distributions O
are O
most O
naturally O
expressed O
in O
terms O
of O
the O
covariance B
, O
whereas O
others O
take O
a O
simpler O
form O
when O
viewed O
in O
terms O
of O
the O
precision O
. O
support B
vector I
machines O
: O
training B
and O
applications O
. O
by O
deﬁnition O
, O
there O
will O
always O
be O
at O
least O
one O
active B
constraint I
, O
because O
there O
will O
always O
be O
a O
closest O
point O
, O
and O
once O
the O
margin B
has O
been O
maximized O
there O
will O
be O
at O
least O
two O
active O
constraints O
. O
factorization B
properties O
8.3.2 O
. O
520 O
10. O
approximate O
inference B
10.22 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
we O
have O
seen O
that O
each O
mode O
of O
the O
posterior O
distribution O
in O
a O
gaussian O
mix- O
ture O
model O
is O
a O
member O
of O
a O
family O
of O
k O
! O
equivalent O
modes O
. O
( O
9.80 O
) O
n=1 O
9.6 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
special O
case O
of O
a O
gaussian O
mixture B
model I
in O
which O
the O
covari- O
ance O
matrices O
σk O
of O
the O
components O
are O
all O
constrained O
to O
have O
a O
common O
value O
σ. O
derive O
the O
em O
equations O
for O
maximizing O
the O
likelihood B
function I
under O
such O
a O
model O
. O
exercises O
357 O
exercises O
7.1 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
suppose O
we O
have O
a O
data O
set O
of O
input O
vectors O
{ O
xn O
} O
with O
corresponding O
target O
values O
tn O
∈ O
{ O
−1 O
, O
1 O
} O
, O
and O
suppose O
that O
we O
model O
the O
density B
of O
input O
vec- O
tors O
within O
each O
class O
separately O
using O
a O
parzen O
kernel B
density I
estimator I
( O
see O
sec- O
tion O
2.5.1 O
) O
with O
a O
kernel O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
. O
5.38 O
( O
( O
cid:12 O
) O
) O
using O
the O
general O
result O
( O
2.115 O
) O
, O
derive O
the O
predictive B
distribution I
( O
5.172 O
) O
for O
the O
laplace O
approximation O
to O
the O
bayesian O
neural B
network I
model O
. O
12.4.2 O
autoassociative O
neural O
networks O
in O
chapter O
5 O
we O
considered O
neural O
networks O
in O
the O
context O
of O
supervised O
learn O
( O
cid:173 O
) O
ing O
, O
where O
the O
role O
of O
the O
network O
is O
to O
predict O
the O
output O
variables O
given O
values O
12.4. O
nonlinear O
latent B
variable I
models O
593 O
figure O
12.18 O
an O
autoassociative O
multilayer O
perceptron B
having O
two O
layers O
of O
weights O
. O
because O
the O
linear B
dynamical I
system I
is O
a O
linear-gaussian O
model O
, O
the O
joint O
distri- O
bution O
over O
all O
variables O
, O
as O
well O
as O
all O
marginals O
and O
conditionals O
, O
will O
be O
gaussian O
. O
alternatively O
, O
we O
can O
consider O
a O
speciﬁc O
distribution O
and O
ask O
which O
graphs O
have O
the O
appropriate O
conditional B
independence I
properties O
. O
as O
well O
as O
providing O
insight O
into O
the O
origin O
of O
sparsity B
in O
the O
rvm O
, O
this O
analysis O
also O
leads O
to O
a O
practical O
algorithm O
for O
optimizing O
the O
hyperparameters O
that O
has O
signiﬁcant O
speed O
advantages O
. O
2. O
insights O
into O
the O
properties O
of O
the O
model O
, O
including O
conditional B
independence I
properties O
, O
can O
be O
obtained O
by O
inspection O
of O
the O
graph O
. O
this O
means O
that O
the O
neural B
network I
func- O
tion O
is O
differentiable O
with O
respect O
to O
the O
network O
parameters O
, O
and O
this O
property O
will O
play O
a O
central O
role O
in O
network O
training B
. O
if O
m O
= O
0 O
, O
the O
model O
is O
equivalent O
to O
the O
isotropic B
covariance O
case O
. O
similarly O
, O
for O
multiple O
binary O
classiﬁcation O
problems O
, O
each O
output O
unit O
activation O
is O
transformed O
using O
a O
logistic B
sigmoid I
function O
so O
that O
where O
yk O
= O
σ O
( O
ak O
) O
σ O
( O
a O
) O
= O
1 O
1 O
+ O
exp O
( O
−a O
) O
. O
7 O
sparse O
kernel O
machines O
in O
the O
previous O
chapter O
, O
we O
explored O
a O
variety O
of O
learning B
algorithms O
based O
on O
non- O
linear O
kernels O
. O
for O
independent O
data O
, O
this O
error B
function I
takes O
the O
form O
e O
( O
w O
) O
= O
− O
n O
( O
cid:2 O
) O
ln O
( O
cid:24 O
) O
k O
( O
cid:2 O
) O
πk O
( O
xn O
, O
w O
) O
n O
( O
cid:10 O
) O
tn|µk O
( O
xn O
, O
w O
) O
, O
σ2 O
k O
( O
xn O
, O
w O
) O
( O
5.153 O
) O
( O
cid:11 O
) O
( O
cid:25 O
) O
n=1 O
k=1 O
where O
we O
have O
made O
the O
dependencies O
on O
w O
explicit O
. O
5.9 O
( O
( O
cid:12 O
) O
) O
www O
the O
error B
function I
( O
5.21 O
) O
for O
binary O
classiﬁcation B
problems O
was O
de- O
rived O
for O
a O
network O
having O
a O
logistic-sigmoid O
output O
activation B
function I
, O
so O
that O
0 O
( O
cid:1 O
) O
y O
( O
x O
, O
w O
) O
( O
cid:1 O
) O
1 O
, O
and O
data O
having O
target O
values O
t O
∈ O
{ O
0 O
, O
1 O
} O
. O
clearly O
, O
this O
deﬁnition O
will O
ensure O
that O
the O
location O
of O
the O
mean B
is O
independent B
of O
the O
origin O
of O
the O
angular O
coor- O
dinate O
. O
consider O
a O
general O
directed B
graph O
in O
which O
a O
, O
b O
, O
and O
c O
are O
arbi- O
trary O
nonintersecting O
sets O
of O
nodes O
( O
whose O
union O
may O
be O
smaller O
than O
the O
complete O
set O
of O
nodes O
in O
the O
graph O
) O
. O
the O
entropy B
of O
a O
distribution O
p O
( O
x O
) O
is O
given O
by O
( O
cid:6 O
) O
h O
[ O
x O
] O
= O
− O
p O
( O
x O
) O
ln O
p O
( O
x O
) O
dx O
. O
thus O
, O
when O
we O
differentiate O
( O
6.90 O
) O
with O
respect O
to O
θ O
, O
we O
obtain O
two O
sets O
of O
terms O
, O
the O
ﬁrst O
arising O
from O
the O
dependence O
of O
the O
covariance B
matrix I
cn O
on O
θ O
, O
and O
the O
rest O
arising O
from O
dependence O
of O
a O
( O
cid:1 O
) O
n O
on O
θ. O
the O
terms O
arising O
from O
the O
explicit O
dependence O
on O
θ O
can O
be O
found O
by O
using O
( O
6.80 O
) O
together O
with O
the O
results O
( O
c.21 O
) O
and O
( O
c.22 O
) O
, O
and O
are O
given O
by O
∂ O
ln O
p O
( O
tn|θ O
) O
∂θj O
= O
−1 O
a O
( O
cid:1 O
) O
t O
n O
c O
n O
∂cn O
∂θj O
−1 O
n O
a O
( O
cid:1 O
) O
n O
c O
( O
cid:29 O
) O
1 O
2 O
−1 O
2 O
tr O
( O
i O
+ O
cn O
wn O
) O
−1wn O
( O
cid:30 O
) O
∂cn O
∂θj O
. O
in O
order O
to O
minimize O
the O
total O
error B
function I
, O
it O
is O
necessary O
to O
be O
able O
to O
evaluate O
its O
derivatives O
with O
respect O
to O
the O
various O
adjustable O
parameters O
. O
a O
further O
problem O
with O
techniques O
such O
as O
cross-validation B
that O
use O
separate O
data O
to O
assess O
performance O
is O
that O
we O
might O
have O
multiple O
complexity O
parameters O
for O
a O
single O
model O
( O
for O
in- O
stance O
, O
there O
might O
be O
several O
regularization B
parameters O
) O
. O
( O
2.152 O
) O
n O
( O
cid:2 O
) O
n=1 O
λ O
2π O
− O
λµ2 O
2 O
( O
cid:15 O
) O
we O
now O
wish O
to O
identify O
a O
prior B
distribution O
p O
( O
µ O
, O
λ O
) O
that O
has O
the O
same O
functional B
dependence O
on O
µ O
and O
λ O
as O
the O
likelihood B
function I
and O
that O
should O
therefore O
take O
the O
form O
p O
( O
µ O
, O
λ O
) O
∝ O
λ1/2 O
exp O
( O
cid:29 O
) O
( O
cid:12 O
) O
( O
cid:16 O
) O
( O
cid:30 O
) O
β O
( O
cid:13 O
) O
− O
λµ2 O
2 O
( O
µ O
− O
c/β O
) O
2 O
exp O
{ O
cλµ O
− O
dλ O
} O
( O
cid:12 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
( O
cid:13 O
) O
− O
βλ O
2 O
λβ/2 O
exp O
= O
exp O
( O
2.153 O
) O
where O
c O
, O
d O
, O
and O
β O
are O
constants O
. O
again O
, O
this O
leads O
to O
a O
decoupling O
into O
k O
inde- O
pendent O
regression B
problems O
. O
for O
a O
given O
input O
vector O
x O
, O
our O
uncertainty O
in O
the O
true O
class O
is O
expressed O
through O
the O
joint O
probability B
distribution O
p O
( O
x O
, O
ck O
) O
and O
so O
we O
seek O
instead O
to O
minimize O
the O
average O
loss O
, O
where O
the O
average O
is O
computed O
with O
respect O
to O
this O
distribution O
, O
which O
is O
given O
by O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:6 O
) O
e O
[ O
l O
] O
= O
k O
j O
rj O
lkjp O
( O
x O
, O
ck O
) O
dx O
. O
this O
backpropagation B
starts O
at O
the O
output O
units O
for O
which O
the O
required O
derivatives O
can O
be O
found O
directly O
from O
the O
functional B
form O
of O
the O
output-unit O
activation B
function I
. O
most O
of O
the O
discussion O
in O
this O
chapter O
, O
however O
, O
is O
independent B
of O
the O
particular O
choice O
of O
basis B
function I
set O
, O
and O
so O
for O
most O
of O
our O
discussion O
we O
shall O
not O
specify O
the O
particular O
form O
of O
the O
basis O
functions O
, O
except O
for O
the O
purposes O
of O
numerical O
il- O
lustration O
. O
322 O
6. O
kernel O
methods O
6.18 O
( O
( O
cid:12 O
) O
) O
consider O
a O
nadaraya-watson O
model O
with O
one O
input O
variable O
x O
and O
one O
target O
variable O
t O
having O
gaussian O
components O
with O
isotropic B
covariances O
, O
so O
that O
the O
co- O
variance B
matrix O
is O
given O
by O
σ2i O
where O
i O
is O
the O
unit O
matrix O
. O
ensemble O
learning B
in O
bayesian O
neural O
networks O
. O
there O
is O
, O
however O
, O
an O
on-line O
version O
of O
gradient B
descent I
that O
has O
proved O
useful O
in O
practice O
for O
training O
neural O
networks O
on O
large O
data O
sets O
( O
le O
cun O
et O
al. O
, O
1989 O
) O
. O
the O
set O
of O
all O
possible O
probability B
distributions O
p O
( O
x O
) O
that O
pass O
through O
the O
ﬁlter O
is O
denoted O
df O
. O
we O
see O
that O
the O
set O
of O
all O
nodes O
on O
which O
q O
( O
cid:1 O
) O
( O
xj O
) O
depends O
corresponds O
to O
the O
markov O
blanket O
of O
node B
xj O
, O
as O
illustrated O
in O
figure O
8.26. O
thus O
the O
update O
of O
the O
factors O
in O
the O
variational B
posterior O
distribution O
represents O
a O
local B
calculation O
on O
the O
graph O
. O
if O
we O
now O
consider O
an O
inﬁnite O
number O
of O
copies O
of O
each O
data O
point O
, O
each O
of O
which O
is O
perturbed O
by O
the O
transformation O
266 O
5. O
neural O
networks O
in O
which O
the O
parameter O
ξ O
is O
drawn O
from O
a O
distribution O
p O
( O
ξ O
) O
, O
then O
the O
error B
function I
deﬁned O
over O
this O
expanded O
data O
set O
can O
be O
written O
as O
{ O
y O
( O
s O
( O
x O
, O
ξ O
) O
) O
− O
t O
} O
2p O
( O
t|x O
) O
p O
( O
x O
) O
p O
( O
ξ O
) O
dx O
dt O
dξ O
. O
while O
the O
prior B
distribution O
for O
the O
latent O
variables O
is O
the O
same O
as O
for O
the O
mixture O
of O
gaussians O
model O
, O
so O
that O
p O
( O
z|π O
) O
= O
( O
9.53 O
) O
if O
we O
form O
the O
product O
of O
p O
( O
x|z O
, O
µ O
) O
and O
p O
( O
z|π O
) O
and O
then O
marginalize O
over O
z O
, O
then O
we O
recover O
( O
9.47 O
) O
. O
the O
gradient O
of O
this O
error B
function I
with O
respect O
to O
a O
weight O
wji O
is O
given O
by O
which O
can O
be O
interpreted O
as O
a O
‘ O
local B
’ O
computation O
involving O
the O
product O
of O
an O
‘ O
error B
signal O
’ O
ynj O
− O
tnj O
associated O
with O
the O
output O
end O
of O
the O
link B
wji O
and O
the O
variable O
xni O
associated O
with O
the O
input O
end O
of O
the O
link B
. O
we O
have O
succeeded O
in O
approximating O
the O
convex B
function I
f O
( O
x O
) O
by O
a O
simpler O
, O
lin- O
ear O
function O
y O
( O
x O
, O
λ O
) O
. O
) O
, O
learning B
in O
graphical O
models O
, O
pp O
. O
a O
b O
c O
376 O
8. O
graphical O
models O
figure O
8.20 O
as O
in O
figure O
8.19 O
but O
conditioning O
on O
the O
value O
of O
node B
c. O
in O
this O
graph O
, O
the O
act O
of O
conditioning O
induces O
a O
depen- O
dence O
between O
a O
and O
b. O
a O
b O
c O
and O
so O
a O
and O
b O
are O
independent B
with O
no O
variables O
observed O
, O
in O
contrast O
to O
the O
two O
previous O
examples O
. O
146 O
3. O
linear O
models O
for B
regression I
figure O
3.4 O
plot O
of O
the O
contours O
of O
the O
unregularized O
error B
function I
( O
blue O
) O
along O
with O
the O
constraint O
re- O
gion O
( O
3.30 O
) O
for O
the O
quadratic O
regular- O
izer O
q O
= O
2 O
on O
the O
left O
and O
the O
lasso B
regularizer O
q O
= O
1 O
on O
the O
right O
, O
in O
which O
the O
optimum O
value O
for O
the O
pa- O
rameter O
vector O
w O
is O
denoted O
by O
w O
( O
cid:1 O
) O
. O
this O
is O
the O
technique O
of O
weight B
sharing I
that O
was O
discussed O
in O
section O
5.5.6 O
as O
a O
way O
of O
building O
translation B
invariance I
into O
networks O
used O
for O
image O
interpretation O
. O
points O
with O
an O
= O
c O
can O
lie O
inside O
the O
margin B
and O
can O
either O
be O
correctly O
classiﬁed O
if O
ξn O
( O
cid:1 O
) O
1 O
or O
misclassiﬁed O
if O
ξn O
> O
1. O
to O
determine O
the O
parameter O
b O
in O
( O
7.1 O
) O
, O
we O
note O
that O
those O
support O
vectors O
for O
which O
0 O
< O
an O
< O
c O
have O
ξn O
= O
0 O
so O
that O
tny O
( O
xn O
) O
= O
1 O
and O
hence O
will O
satisfy O
( O
cid:23 O
) O
tn O
amtmk O
( O
xn O
, O
xm O
) O
+ O
b O
= O
1 O
. O
com- O
parison O
with O
( O
2.194 O
) O
shows O
that O
next O
consider O
the O
multinomial B
distribution I
that O
, O
for O
a O
single O
observation O
x O
, O
takes O
the O
form O
p O
( O
x|µ O
) O
= O
m O
( O
cid:14 O
) O
u O
( O
x O
) O
= O
x O
h O
( O
x O
) O
= O
1 O
g O
( O
η O
) O
= O
σ O
( O
−η O
) O
. O
furthermore O
, O
the O
number O
of O
independent B
parameters O
to O
be O
learned O
from O
the O
data O
is O
much O
smaller O
still O
, O
due O
to O
the O
substantial O
numbers O
of O
constraints O
on O
the O
weights O
. O
another O
difference O
between O
probabilistic O
pca O
and O
factor B
analysis I
concerns O
their O
different O
behaviour O
under O
transformations O
of O
the O
data O
set O
. O
9.12 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
mixture B
distribution I
of O
the O
form O
p O
( O
x O
) O
= O
πkp O
( O
x|k O
) O
( O
9.82 O
) O
k=1 O
where O
the O
elements O
of O
x O
could O
be O
discrete O
or O
continuous O
or O
a O
combination O
of O
these O
. O
( O
9.13 O
) O
section O
8.1.2 O
j=1 O
we O
shall O
view O
πk O
as O
the O
prior B
probability O
of O
zk O
= O
1 O
, O
and O
the O
quantity O
γ O
( O
zk O
) O
as O
the O
corresponding O
posterior B
probability I
once O
we O
have O
observed O
x. O
as O
we O
shall O
see O
later O
, O
γ O
( O
zk O
) O
can O
also O
be O
viewed O
as O
the O
responsibility B
that O
component O
k O
takes O
for O
‘ O
explain- O
ing O
’ O
the O
observation O
x. O
we O
can O
use O
the O
technique O
of O
ancestral B
sampling I
to O
generate O
random O
samples O
distributed O
according O
to O
the O
gaussian O
mixture B
model I
. O
standard O
techniques O
for O
principal O
component O
analysis O
( O
based O
on O
singular B
value I
decomposition I
) O
are O
guaran O
( O
cid:173 O
) O
teed O
to O
give O
the O
correct O
solution O
in O
finite O
time O
, O
and O
they O
also O
generate O
an O
ordered O
set O
of O
eigenvalues O
with O
corresponding O
orthonormal O
eigenvectors O
. O
the O
ﬁrst O
row O
of O
this O
ﬁgure O
corresponds O
to O
the O
situation O
before O
any O
data O
points O
are O
observed O
and O
shows O
a O
plot O
of O
the O
prior B
distribution O
in O
w O
space O
together O
with O
six O
samples O
of O
the O
function O
y O
( O
x O
, O
w O
) O
in O
which O
the O
values O
of O
w O
are O
drawn O
from O
the O
prior B
. O
( O
5.9 O
) O
j=0 O
i=0 O
as O
can O
be O
seen O
from O
figure O
5.1 O
, O
the O
neural B
network I
model O
comprises O
two O
stages O
of O
processing O
, O
each O
of O
which O
resembles O
the O
perceptron B
model O
of O
section O
4.1.7 O
, O
and O
for O
this O
reason O
the O
neural B
network I
is O
also O
known O
as O
the O
multilayer B
perceptron I
, O
or O
mlp O
. O
derive O
an O
alternative O
formalism O
for O
ﬁnding O
the O
jacobian O
based O
on O
forward B
propagation I
equations O
. O
note O
that O
the O
basis O
functions O
( O
6.41 O
) O
are O
normalized O
, O
so O
that O
( O
cid:5 O
) O
another O
situation O
in O
which O
expansions O
in O
normalized O
radial O
basis O
functions O
arise O
is O
in O
the O
application O
of O
kernel O
density O
estimation O
to O
the O
problem O
of O
regression B
, O
as O
we O
shall O
discuss O
in O
section O
6.3.1. O
because O
there O
is O
one O
basis B
function I
associated O
with O
every O
data O
point O
, O
the O
corre- O
sponding O
model O
can O
be O
computationally O
costly O
to O
evaluate O
when O
making O
predictions O
for O
new O
data O
points O
. O
approach O
2 O
leaves O
the O
data O
set O
unchanged O
but O
modiﬁes O
the O
error B
function I
through O
the O
addition O
of O
a O
regularizer O
. O
for O
large O
n O
, O
this O
distribution O
will O
be O
sharply O
peaked O
around O
the O
mean B
and O
so O
( O
2.244 O
) O
if O
, O
however O
, O
we O
also O
assume O
that O
the O
region O
r O
is O
sufﬁciently O
small O
that O
the O
probability B
density O
p O
( O
x O
) O
is O
roughly O
constant O
over O
the O
region O
, O
then O
we O
have O
k O
( O
cid:7 O
) O
n O
p. O
( O
2.245 O
) O
where O
v O
is O
the O
volume O
of O
r. O
combining O
( O
2.244 O
) O
and O
( O
2.245 O
) O
, O
we O
obtain O
our O
density B
estimate O
in O
the O
form O
. O
evaluation O
of O
gaussian O
processes O
and O
other O
methods O
for O
non-linear O
regression B
. O
show O
that O
if O
we O
assume O
negligible O
overlap O
between O
the O
components O
of O
the O
q O
mixture B
, O
the O
resulting O
lower B
bound I
differs O
from O
that O
for O
a O
single O
component O
q O
distribution O
through O
the O
ad- O
dition O
of O
an O
extra O
term O
ln O
k O
! O
. O
exercises O
321 O
6.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
the O
space O
of O
all O
possible O
subsets O
a O
of O
a O
given O
ﬁxed O
set O
d. O
show O
that O
the O
kernel B
function I
( O
6.27 O
) O
corresponds O
to O
an O
inner O
product O
in O
a O
feature B
space I
of O
dimensionality O
2|d| O
deﬁned O
by O
the O
mapping O
φ O
( O
a O
) O
where O
a O
is O
a O
subset O
of O
d O
and O
the O
element O
φu O
( O
a O
) O
, O
indexed O
by O
the O
subset O
u O
, O
is O
given O
by O
( O
cid:12 O
) O
φu O
( O
a O
) O
= O
if O
u O
⊆ O
a O
; O
1 O
, O
0 O
, O
otherwise O
. O
6.3 O
radial B
basis I
function I
networks O
. O
7.7 O
( O
( O
cid:12 O
) O
) O
consider O
the O
lagrangian O
( O
7.56 O
) O
for O
the O
regression B
support O
vector O
machine O
. O
we O
generate O
synthetic O
data O
from O
the O
function O
f O
( O
x O
, O
a O
) O
= O
a0 O
+ O
a1x O
with O
param- O
eter O
values O
a0 O
= O
−0.3 O
and O
a1 O
= O
0.5 O
by O
ﬁrst O
choosing O
values O
of O
xn O
from O
the O
uniform B
distribution I
u O
( O
x|−1 O
, O
1 O
) O
, O
then O
evaluating O
f O
( O
xn O
, O
a O
) O
, O
and O
ﬁnally O
adding O
gaussian O
noise O
with O
standard B
deviation I
of O
0.2 O
to O
obtain O
the O
target O
values O
tn O
. O
, O
xn O
, O
zn O
) O
representing O
the O
joint O
distri- O
bution O
of O
all O
the O
observations O
up O
to O
xn O
and O
the O
latent B
variable I
zn O
. O
( O
2.135 O
) O
2.3. O
the O
gaussian O
distribution O
97 O
z O
figure O
2.11 O
in O
the O
case O
of O
a O
gaussian O
distribution O
, O
with O
θ O
corresponding O
to O
the O
mean B
µ O
, O
the O
regression B
function I
illustrated O
in O
figure O
2.10 O
takes O
the O
form O
of O
a O
straight O
line O
, O
as O
shown O
in O
red O
. O
together O
these O
distributions O
constitute O
a O
gaussian-gamma O
conjugate B
prior I
distribution O
. O
requiring O
that O
the O
functional B
derivative O
vanishes O
then O
gives O
( O
cid:15 O
) O
( O
cid:16 O
) O
∂g O
∂y O
− O
d O
dx O
∂g O
∂y O
( O
cid:1 O
) O
= O
0 O
which O
are O
known O
as O
the O
euler-lagrange O
equations O
. O
we O
can O
represent O
this O
problem O
using O
a O
graphical B
model I
of O
the O
form O
show O
in O
figure O
8.8. O
the O
graphical B
model I
captures O
the O
causal O
process O
( O
pearl O
, O
1988 O
) O
by O
which O
the O
ob- O
served O
data O
was O
generated O
. O
x1 O
x2 O
fs O
x3 O
exercise O
8.21 O
and O
indeed O
the O
notion O
of O
one O
node B
having O
a O
special O
status O
was O
introduced O
only O
as O
a O
convenient O
way O
to O
explain O
the O
message B
passing I
protocol O
. O
6.4.5 O
gaussian O
processes O
for O
classiﬁcation O
in O
a O
probabilistic O
approach O
to O
classiﬁcation B
, O
our O
goal O
is O
to O
model O
the O
posterior O
probabilities O
of O
the O
target O
variable O
for O
a O
new O
input O
vector O
, O
given O
a O
set O
of O
training B
data O
. O
tutorial O
on O
variational B
ap- O
proximation B
methods O
. O
6.22 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
regression B
problem O
with O
n O
training B
set I
input O
vectors O
x1 O
, O
. O
8.21 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
marginal B
distributions O
p O
( O
xs O
) O
over O
the O
sets O
of O
variables O
xs O
associated O
with O
each O
of O
the O
factors O
fx O
( O
xs O
) O
in O
a O
factor B
graph I
can O
be O
found O
by O
ﬁrst O
running O
the O
sum-product O
message O
passing O
algorithm O
and O
then O
evaluating O
the O
required O
marginals O
using O
( O
8.72 O
) O
. O
the O
green O
curve O
shows O
the O
true O
gaussian O
distribution O
from O
which O
data O
is O
generated O
, O
and O
the O
three O
red O
curves O
show O
the O
gaussian O
distributions O
obtained O
by O
ﬁtting O
to O
three O
data O
sets O
, O
each O
consist- O
ing O
of O
two O
data O
points O
shown O
in O
blue O
, O
us- O
ing O
the O
maximum B
likelihood I
results O
( O
1.55 O
) O
and O
( O
1.56 O
) O
. O
in O
practice O
, O
the O
nonlinearity O
of O
the O
network O
function O
y O
( O
xn O
, O
w O
) O
causes O
the O
error B
e O
( O
w O
) O
to O
be O
nonconvex O
, O
and O
so O
in O
practice O
local B
maxima O
of O
the O
likelihood O
may O
be O
found O
, O
corresponding O
to O
local B
minima O
of O
the O
error B
function I
, O
as O
discussed O
in O
section O
5.2.1. O
having O
found O
wml O
, O
the O
value O
of O
β O
can O
be O
found O
by O
minimizing O
the O
negative O
log O
likelihood O
to O
give O
1 O
βml O
= O
1 O
n O
n O
( O
cid:2 O
) O
n=1 O
{ O
y O
( O
xn O
, O
wml O
) O
− O
tn O
} O
2 O
. O
in O
a O
markov O
chain O
monte O
carlo O
simulation O
, O
the O
goal O
is O
to O
sample O
from O
a O
given O
probability B
distribution O
p O
( O
z O
) O
. O
this O
exercise O
13.10 O
620 O
13. O
sequential B
data I
represents O
a O
vector O
of O
length O
k O
whose O
entries O
correspond O
to O
the O
expected O
values O
of O
znk O
. O
in O
a O
similar O
way O
, O
we O
can O
view O
the O
linear B
dynamical I
system I
as O
a O
generalization B
of O
the O
continuous O
latent B
variable I
models O
of O
chapter O
12 O
such O
as O
probabilistic O
pca O
and O
factor B
analysis I
. O
this O
local B
approximation O
can O
be O
applied O
to O
multiple O
variables O
in O
turn O
until O
a O
tractable O
approximation O
is O
obtained O
, O
and O
in O
section O
10.6.1 O
we O
shall O
give O
a O
practical O
example O
of O
this O
approach O
in O
the O
context O
of O
logistic B
regression I
. O
the O
bottom O
left O
plot O
shows O
the O
next O
misclassiﬁed O
point O
to O
be O
considered O
, O
indicated O
by O
the O
green O
circle O
, O
and O
its O
feature O
vector O
is O
again O
added O
to O
the O
weight B
vector I
giving O
the O
decision B
boundary I
shown O
in O
the O
bottom O
right O
plot O
for O
which O
all O
data O
points O
are O
correctly O
classiﬁed O
. O
( O
2.276 O
) O
( O
2.277 O
) O
( O
2.278 O
) O
verify O
that O
this O
distribution O
is O
normalized O
, O
and O
ﬁnd O
expressions O
for O
its O
mean B
and O
variance B
. O
as O
a O
further O
example O
of O
mixture B
mod- O
elling O
, O
and O
to O
illustrate O
the O
em O
algorithm O
in O
a O
different O
context O
, O
we O
now O
discuss O
mix- O
tures O
of O
discrete O
binary O
variables O
described O
by O
bernoulli O
distributions O
. O
( O
2001 O
) O
, O
we O
will O
say O
that O
a O
( O
variable O
or O
factor O
) O
node B
a O
has O
a O
message O
pending O
on O
its O
link B
to O
a O
node B
b O
if O
node B
a O
has O
received O
any O
message O
on O
any O
of O
its O
other O
links O
since O
the O
last O
time O
it O
send O
a O
message O
to O
b. O
thus O
, O
when O
a O
node B
receives O
a O
message O
on O
one O
of O
its O
links O
, O
this O
creates O
pending O
messages O
on O
all O
of O
its O
other O
links O
. O
analysis O
of O
sparse O
bayesian O
learning B
. O
we O
shall O
suppose O
that O
the O
variables O
have O
been O
ordered O
such O
that O
there O
are O
no O
links O
from O
any O
node B
to O
any O
lower O
numbered O
node B
, O
in O
other O
words O
each O
node B
has O
a O
higher O
number O
than O
any O
of O
its O
parents O
. O
data O
, O
comprise O
a O
sum O
of O
terms O
, O
one O
for O
each O
data O
point O
in O
the O
training B
set I
, O
so O
that O
n O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
i O
e O
( O
w O
) O
= O
( O
5.44 O
) O
here O
we O
shall O
consider O
the O
problem O
of O
evaluating O
∇en O
( O
w O
) O
for O
one O
such O
term O
in O
the O
error B
function I
. O
c4.5 O
: O
programs O
for O
machine O
learning B
. O
decision B
theory I
: O
an O
introduction O
to O
dynamic B
programming I
and O
sequential O
deci- O
sions O
. O
during O
the O
learning B
phase O
, O
a O
set O
of O
training B
data O
is O
used O
either O
to O
obtain O
a O
point O
estimate O
of O
the O
parameter O
vector O
or O
to O
determine O
a O
posterior O
distribution O
over O
this O
vector O
. O
3.3.3 O
equivalent B
kernel I
the O
posterior O
mean O
solution O
( O
3.53 O
) O
for O
the O
linear O
basis O
function O
model O
has O
an O
in- O
teresting O
interpretation O
that O
will O
set O
the O
stage O
for O
kernel O
methods O
, O
including O
gaussian O
processes O
. O
( O
1.45 O
) O
in O
both O
the O
bayesian O
and O
frequentist B
paradigms O
, O
the O
likelihood B
function I
p O
( O
d|w O
) O
plays O
a O
central O
role O
. O
it O
represents O
the O
log O
of O
the O
ratio O
of O
probabilities O
ln O
[ O
p O
( O
c1|x O
) O
/p O
( O
c2|x O
) O
] O
for O
the O
two O
classes O
, O
also O
known O
as O
the O
log B
odds I
. O
note O
that O
l O
( O
q O
, O
θ O
) O
is O
a O
functional B
( O
see O
appendix O
d O
for O
a O
discussion O
of O
functionals O
) O
of O
the O
distribution O
q O
( O
z O
) O
, O
and O
a O
function O
of O
the O
parameters O
θ. O
it O
is O
worth O
studying O
9.4. O
the O
em O
algorithm O
in O
general O
451 O
figure O
9.11 O
illustration O
of O
the O
decomposition O
given O
by O
( O
9.70 O
) O
, O
which O
holds O
for O
any O
choice O
of O
distribution O
q O
( O
z O
) O
. O
evaluation O
of O
the O
outer B
product I
approximation I
for O
the O
hessian O
is O
straightforward O
as O
it O
only O
involves O
ﬁrst O
derivatives O
of O
the O
error B
function I
, O
which O
can O
be O
evaluated O
efﬁciently O
in O
o O
( O
w O
) O
steps O
using O
standard O
backpropagation O
. O
in O
pattern O
recog- O
nition O
, O
however O
, O
we O
typically O
have O
to O
solve O
an O
inverse B
problem I
, O
such O
as O
trying O
to O
predict O
the O
presence O
of O
a O
disease O
given O
a O
set O
of O
symptoms O
. O
here O
σ O
( O
· O
) O
is O
the O
logistic B
sigmoid I
function O
deﬁned O
by O
( O
4.59 O
) O
. O
we O
can O
solve O
the O
problem O
of O
reversing O
the O
conditional B
probability I
by O
using O
bayes O
’ O
theorem O
to O
give O
× O
20 O
9 O
2 O
3 O
. O
graphical O
models O
provide O
a O
general O
technique O
for O
motivating O
, O
describing O
, O
and O
analysing O
such O
structures O
, O
and O
variational B
methods O
provide O
a O
powerful O
framework O
for O
performing O
inference B
in O
those O
models O
for O
which O
exact O
solution O
is O
intractable O
. O
back- O
propagation O
can O
also O
be O
used O
to O
evaluate O
the O
second O
derivatives O
of O
the O
error B
, O
given O
by O
∂2e O
∂wji∂wlk O
. O
furthermore O
, O
show O
that O
this O
stationary B
point O
is O
a O
maximum O
. O
to O
see O
this O
in O
the O
context O
of O
the O
inference B
of O
the O
mean B
of O
a O
gaussian O
, O
we O
write O
the O
posterior O
distribution O
with O
the O
contribution O
from O
the O
ﬁnal O
data O
point O
xn O
separated O
out O
so O
that O
( O
cid:31 O
) O
n−1 O
( O
cid:14 O
) O
p O
( O
µ|d O
) O
∝ O
p O
( O
µ O
) O
p O
( O
xn|µ O
) O
p O
( O
xn|µ O
) O
. O
consider O
, O
for O
instance O
, O
the O
case O
of O
a O
gaussian O
mixture B
, O
and O
suppose O
we O
perform O
an O
update O
for O
data O
point O
m O
in O
which O
the O
corresponding O
old O
and O
new O
values O
of O
the O
responsibilities O
are O
denoted O
γold O
( O
zmk O
) O
and O
γnew O
( O
zmk O
) O
. O
when O
training B
networks O
in O
prac- O
tice O
, O
derivatives O
should O
be O
evaluated O
using O
backpropagation B
, O
because O
this O
gives O
the O
greatest O
accuracy O
and O
numerical O
efﬁciency O
. O
10.25 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
the O
variational B
treatment O
of O
the O
bayesian O
mixture O
of O
gaussians O
, O
discussed O
in O
section O
10.2 O
, O
made O
use O
of O
a O
factorized O
approximation O
( O
10.5 O
) O
to O
the O
posterior O
distribu- O
tion O
. O
however O
, O
the O
model O
is O
easily O
extended B
to O
allow O
for O
labelling O
errors O
. O
the O
number O
of O
basis O
functions O
in O
the O
resulting O
models O
is O
generally O
much O
smaller O
than O
the O
number O
of O
training B
points O
, O
although O
it O
is O
often O
still O
relatively O
large O
and O
typically O
increases O
with O
the O
size O
of O
the O
training B
set I
. O
it O
corresponds O
to O
a O
two-class O
model O
in O
which O
the O
input O
vector O
x O
is O
ﬁrst O
transformed O
using O
a O
ﬁxed O
nonlinear O
transformation O
to O
give O
a O
feature O
vector O
φ O
( O
x O
) O
, O
and O
this O
is O
then O
used O
to O
construct O
a O
generalized B
linear I
model I
of O
the O
form O
( O
cid:10 O
) O
( O
cid:11 O
) O
y O
( O
x O
) O
= O
f O
wtφ O
( O
x O
) O
( O
4.52 O
) O
4.1. O
discriminant O
functions O
193 O
where O
the O
nonlinear O
activation B
function I
f O
( O
· O
) O
is O
given O
by O
a O
step O
function O
of O
the O
form O
f O
( O
a O
) O
= O
+1 O
, O
a O
( O
cid:2 O
) O
0 O
−1 O
, O
a O
< O
0 O
. O
however O
, O
if O
these O
conditionals O
are O
log O
concave O
, O
then O
sampling O
can O
be O
done O
efﬁciently O
using O
adaptive B
rejection I
sampling I
( O
assuming O
the O
corresponding O
variable O
is O
a O
scalar O
) O
. O
a O
generalization B
of O
probabilistic O
latent O
variable O
models O
to O
general O
exponential B
family I
distributions O
is O
described O
in O
collins O
et O
al O
. O
hidden O
markov O
models O
, O
coupled O
with O
discriminative O
training O
methods O
, O
are O
widely O
used O
in O
speech B
recognition I
( O
kapadia O
, O
1998 O
) O
. O
\ii O
) O
( O
12.64 O
) O
where O
ill O
is O
a O
d O
x O
d O
diagonal B
matrix O
. O
10.10 O
( O
( O
cid:12 O
) O
) O
www O
derive O
the O
decomposition O
given O
by O
( O
10.34 O
) O
that O
is O
used O
to O
ﬁnd O
approxi- O
mate O
posterior O
distributions O
over O
models O
using O
variational B
inference I
. O
from O
our O
earlier O
discussion O
, O
we O
see O
that O
the O
e O
step O
involves O
an O
orthogonal O
projection O
of O
the O
data O
points O
onto O
the O
current O
estimate O
for O
the O
principal B
subspace I
. O
1 O
training B
test O
s O
m O
r O
e O
0.5 O
0 O
−35 O
−30 O
ln O
λ O
−25 O
−20 O
12 O
1. O
introduction O
give O
us O
some O
important O
insights O
into O
the O
concepts O
we O
have O
introduced O
in O
the O
con- O
text O
of O
polynomial B
curve I
ﬁtting I
and O
will O
allow O
us O
to O
extend O
these O
to O
more O
complex O
situations O
. O
consider O
a O
general O
bayesian O
inference B
problem O
for O
a O
parameter O
θ O
for O
which O
we O
have O
observed O
a O
data O
set O
d O
, O
de- O
scribed O
by O
the O
joint O
distribution O
p O
( O
θ O
, O
d O
) O
. O
exercises O
13.1 O
( O
( O
cid:12 O
) O
) O
www O
use O
the O
technique O
of O
d-separation B
, O
discussed O
in O
section O
8.2 O
, O
to O
verify O
that O
the O
markov O
model O
shown O
in O
figure O
13.3 O
having O
n O
nodes O
in O
total O
satisﬁes O
the O
conditional B
independence I
properties O
( O
13.3 O
) O
for O
n O
= O
2 O
, O
. O
2.28 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
a O
joint O
distribution O
over O
the O
variable O
( O
cid:15 O
) O
( O
cid:16 O
) O
x O
y O
z O
= O
( O
2.290 O
) O
whose O
mean B
and O
covariance B
are O
given O
by O
( O
2.108 O
) O
and O
( O
2.105 O
) O
respectively O
. O
there O
are O
also O
single-class O
support O
vector O
machines O
, O
which O
solve O
an O
unsuper- O
vised O
learning B
problem O
related O
to O
probability B
density O
estimation O
. O
finally O
, O
we O
seek O
an O
expression O
for O
the O
conditional B
p O
( O
x|y O
) O
. O
( O
11.8 O
) O
exercise O
11.3 O
in O
this O
case O
, O
the O
inverse B
of O
the O
indeﬁnite O
integral O
can O
be O
expressed O
in O
terms O
of O
the O
‘ O
tan O
’ O
function O
. O
the O
result O
, O
as O
we O
shall O
see O
, O
is O
that O
the O
optimization O
of O
these O
parameters O
by O
maximum B
likelihood I
allows O
the O
relative B
importance O
of O
different O
inputs O
to O
be O
inferred O
from O
the O
data O
. O
( O
8.3 O
) O
for O
a O
given O
choice O
of O
k O
, O
we O
can O
again O
represent O
this O
as O
a O
directed B
graph O
having O
k O
nodes O
, O
one O
for O
each O
conditional B
distribution O
on O
the O
right-hand O
side O
of O
( O
8.3 O
) O
, O
with O
each O
node B
having O
incoming O
links O
from O
all O
lower O
numbered O
nodes O
. O
10.6.1 O
variational B
posterior O
distribution O
. O
what O
is O
the O
relationship O
between O
λ O
and O
the O
rejection O
threshold O
θ O
? O
1.25 O
( O
( O
cid:1 O
) O
) O
www O
consider O
the O
generalization B
of O
the O
squared O
loss B
function I
( O
1.87 O
) O
for O
a O
single O
target O
variable O
t O
to O
the O
case O
of O
multiple O
target O
variables O
described O
by O
the O
vector O
t O
given O
by O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
1.151 O
) O
e O
[ O
l O
( O
t O
, O
y O
( O
x O
) O
) O
] O
= O
( O
cid:6 O
) O
y O
( O
x O
) O
− O
t O
( O
cid:6 O
) O
2p O
( O
x O
, O
t O
) O
dx O
dt O
. O
also O
, O
the O
practical O
applicability O
of O
bayesian O
methods O
has O
been O
greatly O
enhanced O
through O
the O
development O
of O
a O
range O
of O
approximate O
inference B
algorithms O
such O
as O
variational B
bayes O
and O
expectation B
propa- O
gation O
. O
finally O
, O
we O
use O
( O
10.208 O
) O
to O
evaluate O
the O
approximation O
to O
the O
model B
evidence I
, O
given O
by O
p O
( O
d O
) O
( O
cid:7 O
) O
( O
2πvnew O
) O
d/2 O
exp O
( O
b/2 O
) O
sn O
( O
2πvn O
) O
−d/2 O
( O
cid:27 O
) O
n O
( O
cid:14 O
) O
( O
cid:26 O
) O
− O
n O
( O
cid:2 O
) O
n=1 O
n=1 O
mt O
nmn O
vn O
. O
figure O
1.25 O
an O
example O
of O
a O
loss B
matrix I
with O
ele- O
ments O
lkj O
for O
the O
cancer O
treatment O
problem O
. O
knowledge O
of O
multivariate O
calculus O
and O
basic O
linear O
algebra O
is O
required O
, O
and O
some O
familiarity O
with O
probabilities O
would O
be O
helpful O
though O
not O
es- O
sential O
as O
the O
book O
includes O
a O
self-contained O
introduction O
to O
basic O
probability B
theory O
. O
the O
conjugate B
prior I
in O
this O
case O
is O
called O
the O
inverse B
gamma I
distribution I
, O
although O
we O
shall O
not O
discuss O
this O
further O
because O
we O
will O
ﬁnd O
it O
more O
convenient O
to O
work O
with O
the O
precision O
. O
the O
likelihood B
function I
is O
deﬁned O
by O
p O
( O
tn|θ O
) O
= O
p O
( O
tn|an O
) O
p O
( O
an|θ O
) O
dan O
. O
note O
that O
, O
unlike O
rejection B
sampling I
, O
all O
of O
the O
samples O
generated O
are O
retained O
. O
because O
each O
data O
point O
has O
a O
probability B
p O
of O
falling O
within O
r O
, O
the O
total O
number O
k O
of O
points O
that O
lie O
inside O
r O
will O
be O
distributed O
according O
to O
the O
binomial B
distribution I
bin O
( O
k|n O
, O
p O
) O
= O
n O
! O
k O
! O
( O
n O
− O
k O
) O
! O
p O
k O
( O
1 O
− O
p O
) O
1−k O
. O
a O
( O
cid:1 O
) O
( O
6.84 O
) O
once O
we O
have O
found O
the O
mode O
a O
( O
cid:1 O
) O
n O
of O
the O
posterior O
, O
we O
can O
evaluate O
the O
hessian O
matrix O
given O
by O
h O
= O
−∇∇ψ O
( O
an O
) O
= O
wn O
+ O
c O
−1 O
n O
( O
6.85 O
) O
where O
the O
elements O
of O
wn O
are O
evaluated O
using O
a O
( O
cid:1 O
) O
proximation B
to O
the O
posterior O
distribution O
p O
( O
an|tn O
) O
given O
by O
n O
. O
thus O
the O
probability B
that O
b O
takes O
the O
value O
r O
is O
denoted O
p O
( O
b O
= O
r O
) O
. O
given O
a O
prior B
distribution O
p O
( O
µ O
) O
= O
n O
( O
µ|µ0 O
, O
σ0 O
) O
, O
ﬁnd O
the O
corresponding O
posterior O
distribution O
p O
( O
µ|x O
) O
. O
here O
we O
have O
used O
the O
property O
( O
4.88 O
) O
for O
the O
derivative B
of O
the O
logistic B
sigmoid I
function O
. O
the O
em O
algorithm O
breaks O
down O
the O
potentially O
difﬁcult O
problem O
of O
maximizing O
the O
likelihood B
function I
into O
two O
stages O
, O
the O
e O
step O
and O
the O
m O
step O
, O
each O
of O
which O
will O
often O
prove O
simpler O
to O
implement O
. O
in O
terms O
of O
the O
gram O
matrix O
, O
the O
sum-of-squares B
error I
function O
can O
be O
written O
as O
j O
( O
a O
) O
= O
1 O
2 O
atkka O
− O
atkt O
+ O
1 O
2 O
ttt O
+ O
λ O
2 O
atka O
. O
8.13 O
( O
( O
cid:12 O
) O
) O
consider O
the O
use O
of O
iterated B
conditional I
modes I
( O
icm O
) O
to O
minimize O
the O
energy B
function I
given O
by O
( O
8.42 O
) O
. O
in O
deriving O
the O
sum-product B
algorithm I
, O
we O
made O
use O
of O
the O
distributive O
law O
( O
8.53 O
) O
for O
multiplication O
. O
this O
is O
illustrated O
for O
the O
case O
of O
a O
one-dimensional O
input O
space O
in O
figure O
6.11 O
in O
which O
the O
probability B
distri- O
314 O
6. O
kernel O
methods O
10 O
5 O
0 O
−5 O
−10 O
−1 O
−0.5 O
0 O
0.5 O
1 O
1 O
0.75 O
0.5 O
0.25 O
0 O
−1 O
−0.5 O
0 O
0.5 O
1 O
figure O
6.11 O
the O
left O
plot O
shows O
a O
sample O
from O
a O
gaussian O
process O
prior B
over O
functions O
a O
( O
x O
) O
, O
and O
the O
right O
plot O
shows O
the O
result O
of O
transforming O
this O
sample O
using O
a O
logistic B
sigmoid I
function O
. O
196 O
4. O
linear O
models O
for O
classification O
figure O
4.8 O
illustration O
of O
the O
mark O
1 O
perceptron B
hardware O
. O
generalized B
dis- O
criminant O
analysis O
using O
a O
kernel O
approach O
. O
5.2 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
maximizing O
the O
likelihood B
function I
under O
the O
conditional B
distribution O
( O
5.16 O
) O
for O
a O
multioutput O
neural B
network I
is O
equivalent O
to O
minimizing O
the O
sum-of-squares B
error I
function O
( O
5.11 O
) O
. O
the O
corresponding O
set O
of O
kkt O
conditions O
are O
given O
by O
7.1. O
maximum B
margin I
classiﬁers O
333 O
an O
( O
cid:2 O
) O
0 O
tny O
( O
xn O
) O
− O
1 O
+ O
ξn O
( O
cid:2 O
) O
0 O
an O
( O
tny O
( O
xn O
) O
− O
1 O
+ O
ξn O
) O
= O
0 O
µn O
( O
cid:2 O
) O
0 O
ξn O
( O
cid:2 O
) O
0 O
µnξn O
= O
0 O
( O
7.23 O
) O
( O
7.24 O
) O
( O
7.25 O
) O
( O
7.26 O
) O
( O
7.27 O
) O
( O
7.28 O
) O
where O
n O
= O
1 O
, O
. O
longer-range O
effects O
could O
in O
principle O
be O
included O
by O
adding O
extra O
links O
to O
the O
graphical B
model I
of O
figure O
13.5. O
one O
way O
to O
address O
this O
is O
to O
generalize O
the O
hmm O
to O
give O
the O
autoregressive O
hidden O
markov O
model O
( O
ephraim O
et O
al. O
, O
1989 O
) O
, O
an O
example O
of O
which O
is O
shown O
in O
figure O
13.17. O
for O
discrete O
observa- O
tions O
, O
this O
corresponds O
to O
expanded O
tables O
of O
conditional B
probabilities O
for O
the O
emis- O
sion B
distributions O
. O
consider O
a O
single O
input O
variable O
x O
, O
a O
single O
target O
variable O
t O
and O
n O
( O
cid:2 O
) O
n=1 O
154 O
3. O
linear O
models O
for B
regression I
a O
linear O
model O
of O
the O
form O
y O
( O
x O
, O
w O
) O
= O
w0 O
+ O
w1x O
. O
in O
figure O
8.53 O
, O
we O
have O
indicated O
two O
paths O
, O
each O
of O
which O
we O
shall O
suppose O
corresponds O
to O
a O
global O
maximum O
of O
the O
joint O
probability B
distribution O
. O
( O
10.5 O
) O
10.1. O
variational B
inference I
465 O
it O
should O
be O
emphasized O
that O
we O
are O
making O
no O
further O
assumptions O
about O
the O
distri- O
bution O
. O
( O
14.53 O
) O
k=1 O
this O
is O
known O
as O
a O
mixture B
of I
experts I
model O
( O
jacobs O
et O
al. O
, O
1991 O
) O
in O
which O
the O
mix- O
ing O
coefﬁcients O
πk O
( O
x O
) O
are O
known O
as O
gating O
functions O
and O
the O
individual O
component O
densities O
pk O
( O
t|x O
) O
are O
called O
experts O
. O
exercises O
289 O
5.27 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
the O
framework O
for O
training O
with O
transformed O
data O
in O
the O
special O
case O
in O
which O
the O
transformation O
consists O
simply O
of O
the O
addition O
of O
random O
noise O
x O
→ O
x O
+ O
ξ O
where O
ξ O
has O
a O
gaussian O
distribution O
with O
zero O
mean B
and O
unit O
covariance B
. O
another O
property O
of O
on-line O
gradient O
descent O
is O
the O
possibility O
of O
escaping O
from O
local B
minima O
, O
since O
a O
stationary B
point O
with O
respect O
to O
the O
error B
function I
for O
the O
whole O
data O
set O
will O
generally O
not O
be O
a O
stationary B
point O
for O
each O
data O
point O
individually O
. O
( O
8.105 O
) O
( O
8.106 O
) O
suppose O
that O
the O
driver O
tells O
us O
that O
the O
fuel O
gauge O
shows O
empty O
, O
in O
other O
words O
that O
we O
observe O
d O
= O
0. O
evaluate O
the O
probability B
that O
the O
tank O
is O
empty O
given O
only O
this O
observation O
. O
we O
can O
use O
the O
result O
( O
4.135 O
) O
to O
obtain O
an O
approximation O
to O
the O
model B
evidence I
which O
, O
as O
discussed O
in O
section O
3.4 O
, O
plays O
a O
central O
role O
in O
bayesian O
model B
comparison I
. O
for O
comparison O
with O
the O
support B
vector I
machine I
, O
we O
ﬁrst O
reformulate O
maximum B
likelihood I
logistic O
regression B
using O
the O
target O
variable O
t O
∈ O
{ O
−1 O
, O
1 O
} O
. O
if O
the O
experts O
are O
also O
linear O
( O
regression B
or O
classiﬁcation B
) O
models O
, O
then O
the O
whole O
model O
can O
be O
ﬁtted O
efﬁciently O
using O
the O
em O
algorithm O
, O
with O
iterative B
reweighted I
least I
squares I
being O
employed O
in O
the O
m O
step O
( O
jordan O
and O
jacobs O
, O
1994 O
) O
. O
in O
this O
latter O
case O
we O
no O
longer O
have O
a O
representation O
in O
terms O
of O
kernel O
func- O
tions O
evaluated O
at O
the O
training B
set I
data O
points O
. O
taking O
the O
exponential O
section O
10.2.5 O
of O
both O
sides O
, O
we O
have O
( O
cid:26 O
) O
( O
cid:26 O
) O
q O
( O
cid:1 O
) O
( O
zn O
) O
= O
h O
( O
xn O
, O
zn O
) O
g O
( O
e O
[ O
η O
] O
) O
exp O
e O
[ O
ηt O
] O
u O
( O
xn O
, O
zn O
) O
( O
10.116 O
) O
where O
the O
normalization O
coefﬁcient O
has O
been O
re-instated O
by O
comparison O
with O
the O
standard O
form O
for O
the O
exponential B
family I
. O
computation O
( O
cid:173 O
) O
ally O
intensive O
nonlinear O
optimization O
techniques O
must O
be O
used O
, O
and O
there O
is O
the O
risk O
of O
finding O
a O
suboptimal O
local B
minimum I
of O
the O
error B
function I
. O
we O
can O
interpret O
the O
value O
of O
t O
as O
the O
probability B
that O
the O
class O
is O
c1 O
, O
with O
the O
values O
of O
probability B
taking O
only O
the O
extreme O
values O
of O
0 O
and O
1. O
for O
k O
> O
2 O
classes O
, O
it O
is O
convenient O
to O
use O
a O
1-of-k O
coding O
scheme O
in O
which O
t O
is O
a O
vector O
of O
length O
k O
such O
that O
if O
the O
class O
is O
cj O
, O
then O
all O
elements O
tk O
of O
t O
are O
zero O
except O
element O
tj O
, O
which O
takes O
the O
value O
1. O
for O
instance O
, O
if O
we O
have O
k O
= O
5 O
classes O
, O
then O
a O
pattern O
from O
class O
2 O
would O
be O
given O
the O
target B
vector I
t O
= O
( O
0 O
, O
1 O
, O
0 O
, O
0 O
, O
0 O
) O
t. O
( O
4.1 O
) O
again O
, O
we O
can O
interpret O
the O
value O
of O
tk O
as O
the O
probability B
that O
the O
class O
is O
ck O
. O
by O
using O
the O
standard O
results O
for O
the O
mean B
and O
variance B
of O
the O
gamma B
distribution I
given O
by O
( O
b.27 O
) O
and O
( O
b.28 O
) O
, O
show O
that O
if O
we O
let O
n O
→ O
∞ O
, O
this O
variational B
posterior O
distribution O
has O
a O
mean B
given O
by O
the O
inverse B
of O
the O
maximum B
likelihood I
estimator O
for O
the O
variance B
of O
the O
data O
, O
and O
a O
variance B
that O
goes O
to O
zero O
. O
derivatives O
of O
this O
regularizer O
with O
respect O
to O
the O
network O
weights O
can O
be O
found O
using O
an O
extended B
backpropagation O
algorithm O
( O
bishop O
, O
1993 O
) O
. O
2.45 O
( O
( O
cid:12 O
) O
) O
verify O
that O
the O
wishart O
distribution O
deﬁned O
by O
( O
2.155 O
) O
is O
indeed O
a O
conjugate B
prior I
for O
the O
precision B
matrix I
of O
a O
multivariate O
gaussian O
. O
13.28 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
special O
case O
of O
the O
linear B
dynamical I
system I
of O
section O
13.3 O
in O
which O
the O
state O
variable O
zn O
is O
constrained O
to O
be O
equal O
to O
the O
previous O
state O
variable O
, O
which O
corresponds O
to O
a O
= O
i O
and O
γ O
= O
0. O
for O
simplicity O
, O
assume O
also O
that O
v0 O
→ O
∞ O
so O
that O
the O
initial O
conditions O
for O
z O
are O
unimportant O
, O
and O
the O
predictions O
are O
determined O
purely O
by O
the O
data O
. O
yj O
ci O
} O
nij O
xi O
} O
rj O
and O
the O
probability B
of O
selecting O
the O
blue O
box O
is O
6/10 O
. O
inverse B
hessian O
. O
the O
data O
point O
circled O
in O
green O
is O
misclassiﬁed O
and O
so O
its O
feature O
vector O
is O
added O
to O
the O
current O
weight B
vector I
, O
giving O
the O
new O
decision B
boundary I
shown O
in O
the O
top O
right O
plot O
. O
suppose O
instead O
we O
ﬁrst O
launch O
a O
message O
µβ O
( O
xn−1 O
) O
starting O
from O
node B
xn O
and O
propagate O
corresponding O
messages O
all O
the O
way O
back O
to O
node B
x1 O
, O
and O
suppose O
we O
similarly O
launch O
a O
message O
µα O
( O
x2 O
) O
starting O
from O
node B
x1 O
and O
propagate O
the O
corre- O
sponding O
messages O
all O
the O
way O
forward O
to O
node B
xn O
. O
another O
quantity O
that O
will O
play O
an O
important O
role O
is O
the O
conditional B
probability I
of O
z O
given O
x. O
we O
shall O
use O
γ O
( O
zk O
) O
to O
denote O
p O
( O
zk O
= O
1|x O
) O
, O
whose O
value O
can O
be O
found O
using O
bayes O
’ O
theorem O
γ O
( O
zk O
) O
≡ O
p O
( O
zk O
= O
1|x O
) O
= O
= O
k O
( O
cid:2 O
) O
p O
( O
zk O
= O
1 O
) O
p O
( O
x|zk O
= O
1 O
) O
p O
( O
zj O
= O
1 O
) O
p O
( O
x|zj O
= O
1 O
) O
k O
( O
cid:2 O
) O
πkn O
( O
x|µk O
, O
σk O
) O
πjn O
( O
x|µj O
, O
σj O
) O
j=1 O
. O
und O
, O
, O
' O
wilh O
`` O
'' O
peel O
to O
appropriate O
prior B
distribution O
' O
. O
11.1.4 O
importance B
sampling I
. O
we O
therefore O
turn O
to O
a O
bayesian O
treatment O
of O
linear B
regression I
, O
which O
will O
avoid O
the O
over-ﬁtting B
problem O
of O
maximum B
likelihood I
, O
and O
which O
will O
also O
lead O
to O
automatic O
methods O
of O
determining O
model O
complexity O
using O
the O
training B
data O
alone O
. O
nonlinear O
optimization O
algorithms O
, O
and O
their O
practical O
application O
to O
neural O
net- O
work O
training B
, O
are O
discussed O
in O
detail O
in O
bishop O
and O
nabney O
( O
2008 O
) O
. O
13.30 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
starting O
from O
the O
result O
( O
13.65 O
) O
for O
the O
pairwise O
posterior O
marginal O
in O
a O
state B
space I
model I
, O
derive O
the O
speciﬁc O
form O
( O
13.103 O
) O
for O
the O
case O
of O
the O
gaussian O
linear B
dynamical I
system I
. O
in O
summary O
, O
we O
can O
view O
each O
step O
of O
the O
particle B
ﬁlter I
algorithm O
as O
comprising O
two O
stages O
. O
if O
one O
or O
both O
of O
the O
weights O
is O
a O
bias B
term O
, O
then O
the O
corresponding O
expressions O
are O
obtained O
simply O
by O
setting O
the O
appropriate O
activation O
( O
s O
) O
to O
1. O
inclusion O
of O
skip-layer O
connections O
is O
straightforward O
. O
for O
the O
particular O
case O
of O
r O
= O
i O
, O
we O
see O
that O
the O
columns O
of O
w O
are O
the O
principal O
component O
eigenvectors O
scaled O
by O
the O
variance B
parameters O
ai O
- O
( O
j'2 O
. O
here O
the O
stochastic B
nodes O
corre- O
spond O
to O
{ O
tn O
} O
, O
w O
and O
( O
cid:1 O
) O
t. O
we O
see O
that O
the O
node B
for O
w O
is O
tail-to-tail O
with O
respect O
to O
the O
path O
from O
( O
cid:1 O
) O
t O
to O
any O
one O
of O
the O
nodes O
tn O
and O
so O
we O
have O
the O
following O
conditional B
( O
cid:1 O
) O
t O
⊥⊥ O
tn O
| O
w. O
( O
cid:1 O
) O
t O
is O
independent B
of O
the O
training B
data O
{ O
t1 O
, O
. O
by O
expressing O
the O
likelihood B
function I
in O
the O
form O
( O
13.42 O
) O
, O
we O
have O
reduced O
the O
computational O
cost O
from O
being O
exponential O
in O
the O
length O
of O
the O
chain O
to O
being O
linear O
by O
swapping O
the O
order O
of O
the O
summation O
and O
multiplications O
, O
so O
that O
at O
each O
time O
step O
n O
we O
sum O
the O
contributions O
from O
all O
paths O
passing O
through O
each O
of O
the O
states O
znk O
to O
give O
the O
intermediate O
quantities O
α O
( O
zn O
) O
. O
there O
is O
an O
algorithm O
for O
exact O
inference B
on O
directed B
graphs O
without O
loops O
known O
as O
belief B
propagation I
( O
pearl O
, O
1988 O
; O
lauritzen O
and O
spiegelhalter O
, O
1988 O
) O
, O
and O
is O
equiv- O
alent O
to O
a O
special O
case O
of O
the O
sum-product B
algorithm I
. O
if O
we O
substitute O
for O
p O
( O
z1|µ0 O
, O
v0 O
) O
in O
( O
13.108 O
) O
using O
( O
13.77 O
) O
, O
and O
then O
take O
the O
expectation B
with O
respect O
to O
z O
, O
we O
obtain O
( O
cid:29 O
) O
( O
cid:30 O
) O
0 O
( O
z1 O
− O
µ0 O
) O
−1 O
+ O
const O
q O
( O
θ O
, O
θold O
) O
= O
−1 O
2 O
ln|v0| O
− O
ez|θold O
( O
z1 O
− O
µ0 O
) O
tv O
1 O
2 O
where O
all O
terms O
not O
dependent O
on O
µ0 O
or O
v0 O
have O
been O
absorbed O
into O
the O
additive O
constant O
. O
there O
is O
a O
complexity O
parameter O
c O
, O
or O
ν O
( O
as O
well O
as O
a O
parameter O
 O
in O
the O
case O
of O
regression B
) O
, O
that O
must O
be O
found O
using O
a O
hold-out O
method O
such O
as O
cross-validation B
. O
ml/2 O
to O
the O
parameter O
b O
, O
where O
σ2 O
section O
2.2 O
instead O
of O
working O
with O
the O
precision O
, O
we O
can O
consider O
the O
variance B
itself O
. O
note O
that O
the O
approach O
of O
ﬁtting O
gaussian O
distributions O
to O
the O
classes O
is O
not O
robust O
to O
outliers B
, O
because O
the O
maximum B
likelihood I
estimation O
of O
a O
gaussian O
is O
not O
robust O
. O
to O
see O
this O
, O
we O
note O
that O
the O
eigendecomposition O
of O
the O
covariance B
matrix I
requires O
o O
( O
d O
3 O
) O
computation O
. O
from O
( O
1.46 O
) O
and O
( O
1.53 O
) O
, O
the O
log O
likelihood O
exercise O
1.11 O
section O
1.1 O
exercise O
1.12 O
function O
can O
be O
written O
in O
the O
form O
( O
cid:10 O
) O
x|µ O
, O
σ2 O
( O
cid:11 O
) O
ln O
p O
= O
− O
1 O
2σ2 O
1.2. O
probability B
theory O
27 O
( O
xn O
− O
µ O
) O
2 O
− O
n O
2 O
ln O
σ2 O
− O
n O
2 O
ln O
( O
2π O
) O
. O
note O
also O
that O
the O
normalization O
constant O
z O
need O
be O
evaluated O
only O
once O
, O
using O
any O
convenient O
node B
. O
for O
instance O
, O
in O
our O
digit O
recog- O
nition O
example O
, O
we O
could O
make O
multiple O
copies O
of O
each O
example O
in O
which O
the O
262 O
5. O
neural O
networks O
figure O
5.13 O
a O
schematic O
illustration O
of O
why O
early B
stopping I
can O
give O
similar O
results O
to O
weight B
decay I
in O
the O
case O
of O
a O
quadratic O
error O
func- O
tion O
. O
consider O
for O
a O
moment O
the O
problem O
of O
minimizing O
kl O
( O
p O
( O
cid:5 O
) O
q O
) O
with O
respect O
to O
q O
( O
z O
) O
when O
p O
( O
z O
) O
is O
a O
ﬁxed O
distribution O
and O
q O
( O
z O
) O
is O
a O
member O
of O
the O
exponential B
family I
and O
so O
, O
from O
( O
2.194 O
) O
, O
can O
be O
written O
in O
the O
form O
( O
cid:26 O
) O
( O
cid:27 O
) O
q O
( O
z O
) O
= O
h O
( O
z O
) O
g O
( O
η O
) O
exp O
ηtu O
( O
z O
) O
. O
we O
shall O
return O
to O
a O
discussion O
of O
the O
number O
of O
independent B
parameters O
in O
this O
model O
later O
. O
exercise O
1.25 O
we O
can O
also O
derive O
this O
result O
in O
a O
slightly O
different O
way O
, O
which O
will O
also O
shed O
light O
on O
the O
nature O
of O
the O
regression B
problem O
. O
( O
13.4 O
) O
again O
, O
using O
d-separation B
or O
by O
direct O
evaluation O
, O
we O
see O
that O
the O
conditional B
distri- O
bution O
of O
xn O
given O
xn−1 O
and O
xn−2 O
is O
independent B
of O
all O
observations O
x1 O
, O
. O
4.4.1 O
model B
comparison I
and O
bic O
. O
this O
property O
of O
distributions O
in O
spaces O
of O
high O
dimensionality O
will O
have O
important O
consequences O
when O
we O
consider O
bayesian O
inference B
of O
model O
parameters O
in O
later O
chapters O
. O
this O
provided O
the O
ﬁrst O
rigorous O
proof O
that O
probability B
theory O
could O
be O
regarded O
as O
an O
extension O
of O
boolean O
logic O
to O
situations O
involving O
uncertainty O
( O
jaynes O
, O
2003 O
) O
. O
two O
approaches O
to O
this O
problem O
using O
support B
vector I
machines O
have O
been O
proposed O
. O
so O
far O
, O
we O
have O
considered O
a O
single O
input O
value O
x. O
if O
we O
substitute O
this O
expansion O
back O
into O
( O
3.37 O
) O
, O
we O
obtain O
the O
following O
decomposition O
of O
the O
expected O
squared O
loss O
expected O
loss O
= O
( O
bias B
) O
2 O
+ O
variance B
+ O
noise O
where O
( O
bias B
) O
2 O
= O
variance B
= O
noise O
= O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
{ O
ed O
[ O
y O
( O
x O
; O
d O
) O
] O
− O
h O
( O
x O
) O
} O
2p O
( O
x O
) O
dx O
( O
cid:8 O
) O
{ O
y O
( O
x O
; O
d O
) O
− O
ed O
[ O
y O
( O
x O
; O
d O
) O
] O
} O
2 O
ed O
{ O
h O
( O
x O
) O
− O
t O
} O
2p O
( O
x O
, O
t O
) O
dx O
dt O
( O
cid:9 O
) O
( O
3.41 O
) O
( O
3.42 O
) O
p O
( O
x O
) O
dx O
( O
3.43 O
) O
( O
3.44 O
) O
and O
the O
bias B
and O
variance B
terms O
now O
refer O
to O
integrated O
quantities O
. O
capturing O
this O
property O
explicitly O
can O
lead O
to O
im O
( O
cid:173 O
) O
proved O
density B
modelling O
compared O
with O
more O
general O
methods O
. O
the O
major O
problem O
with O
diagonal B
approximations O
, O
however O
, O
is O
that O
in O
practice O
the O
hessian O
is O
typically O
found O
to O
be O
strongly O
nondiagonal O
, O
and O
so O
these O
approximations O
, O
which O
are O
driven O
mainly O
be O
computational O
convenience O
, O
must O
be O
treated O
with O
care O
. O
without O
loss O
of O
generality O
, O
we O
can O
write O
the O
probability B
distribution O
p O
( O
z O
) O
in O
the O
form O
p O
( O
z O
) O
= O
exp O
( O
−e O
( O
z O
) O
) O
1 O
zp O
( O
11.54 O
) O
where O
e O
( O
z O
) O
is O
interpreted O
as O
the O
potential B
energy I
of O
the O
system O
when O
in O
state O
z. O
the O
system O
acceleration O
is O
the O
rate O
of O
change O
of O
momentum O
and O
is O
given O
by O
the O
applied O
force O
, O
which O
itself O
is O
the O
negative O
gradient O
of O
the O
potential B
energy I
dri O
dτ O
= O
− O
∂e O
( O
z O
) O
∂zi O
. O
the O
goal O
of O
the O
em O
algorithm O
is O
to O
ﬁnd O
maximum B
likelihood I
solutions O
for O
mod- O
els O
having O
latent O
variables O
. O
for O
example O
, O
if O
we O
choose O
q O
( O
θ O
) O
to O
be O
a O
gaussian O
distribution O
n O
( O
θ|µ O
, O
σ O
) O
, O
then O
\j O
( O
θ O
) O
, O
and O
σ O
is O
µ O
is O
set O
equal O
to O
the O
mean B
of O
the O
( O
unnormalized O
) O
distribution O
fj O
( O
θ O
) O
q O
set O
to O
its O
covariance B
. O
figure O
7.12 O
shows O
the O
relevance B
vector I
machine I
applied O
to O
a O
synthetic O
classiﬁ- O
cation O
data O
set O
. O
show O
that O
variational B
optimization O
of O
this O
factorized B
distribution I
is O
equivalent O
to O
an O
em O
algorithm O
, O
in O
which O
the O
e O
step O
optimizes O
qz O
( O
z O
) O
, O
and O
the O
m O
step O
maximizes O
the O
expected O
complete-data O
log O
posterior O
distribution O
of O
θ O
with O
respect O
to O
θ0 O
. O
the O
variance B
has O
two O
terms O
, O
the O
ﬁrst O
of O
which O
arises O
from O
the O
intrinsic O
noise O
on O
the O
target O
variable O
, O
whereas O
the O
second O
is O
an O
x-dependent O
term O
that O
expresses O
the O
uncertainty O
in O
the O
interpolant O
due O
to O
the O
uncertainty O
in O
the O
model O
parameters O
w. O
this O
should O
be O
compared O
with O
the O
corresponding O
predictive B
distribution I
for O
the O
linear B
regression I
model O
, O
given O
by O
( O
3.58 O
) O
and O
( O
3.59 O
) O
. O
this O
is O
more O
general O
than O
the O
corresponding O
least-squares O
result O
because O
the O
variance B
is O
a O
function O
of O
x. O
we O
have O
seen O
that O
for O
multimodal O
distributions O
, O
the O
conditional B
mean O
can O
give O
a O
poor O
representation O
of O
the O
data O
. O
in O
order O
to O
compare O
different O
models O
, O
for O
example O
neural O
networks O
having O
differ- O
ent O
numbers O
of O
hidden O
units O
, O
we O
need O
to O
evaluate O
the O
model B
evidence I
p O
( O
d O
) O
. O
z=z0 O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:12 O
) O
( O
cid:13 O
) O
( O
cid:13 O
) O
note O
that O
the O
ﬁrst-order O
term O
in O
the O
taylor O
expansion O
does O
not O
appear O
since O
z0 O
is O
a O
local B
maximum O
of O
the O
distribution O
. O
unlike O
gradient B
descent I
, O
these O
algorithms O
have O
the O
property O
that O
the O
error B
function I
always O
decreases O
at O
each O
iteration O
unless O
the O
weight B
vector I
has O
arrived O
at O
a O
local B
or O
global B
minimum I
. O
624 O
13. O
sequential B
data I
this O
completes O
the O
e O
step O
, O
and O
we O
use O
the O
results O
to O
ﬁnd O
a O
revised O
set O
of O
parameters O
θnew O
using O
the O
m-step O
equations O
from O
section O
13.2.1. O
we O
then O
continue O
to O
alternate O
between O
e O
and O
m O
steps O
until O
some O
convergence O
criterion O
is O
satisﬁed O
, O
for O
instance O
when O
the O
change O
in O
the O
likelihood B
function I
is O
below O
some O
threshold O
. O
we O
now O
observe O
that O
any O
factor O
p O
( O
xk|pak O
) O
that O
does O
not O
have O
any O
functional B
dependence O
on O
xi O
can O
be O
taken O
outside O
the O
integral O
over O
xi O
, O
and O
will O
therefore O
cancel O
between O
numerator O
and O
denominator O
. O
chapter O
7 O
section O
12.3 O
section O
6.3 O
6.1. O
dual O
representations O
293 O
6.1. O
dual O
representations O
n O
( O
cid:2 O
) O
( O
cid:26 O
) O
n=1 O
( O
cid:27 O
) O
( O
cid:26 O
) O
( O
cid:27 O
) O
2 O
+ O
λ O
2 O
n O
( O
cid:2 O
) O
( O
cid:27 O
) O
n O
( O
cid:2 O
) O
( O
cid:26 O
) O
many O
linear O
models O
for B
regression I
and O
classiﬁcation B
can O
be O
reformulated O
in O
terms O
of O
a O
dual B
representation I
in O
which O
the O
kernel B
function I
arises O
naturally O
. O
t O
y O
( O
x0 O
) O
1.5. O
decision B
theory I
47 O
y O
( O
x O
) O
p O
( O
t|x0 O
) O
x0 O
x O
which O
is O
the O
conditional B
average O
of O
t O
conditioned O
on O
x O
and O
is O
known O
as O
the O
regression B
function I
. O
−1 O
0 O
+ O
nksk O
+ O
β0nk O
β0 O
+ O
nk O
( O
xk O
− O
m0 O
) O
( O
xk O
− O
m0 O
) O
t O
( O
10.60 O
) O
( O
10.61 O
) O
( O
10.62 O
) O
( O
10.63 O
) O
these O
update O
equations O
are O
analogous O
to O
the O
m-step O
equations O
of O
the O
em O
algorithm O
for O
the O
maximum B
likelihood I
solution O
of O
the O
mixture O
of O
gaussians O
. O
the O
hme O
model O
can O
also O
be O
viewed O
as O
a O
probabilistic O
version O
of O
decision O
trees O
discussed O
in O
section O
14.4 O
and O
can O
again O
be O
trained O
efﬁciently O
by O
maximum B
likelihood I
using O
an O
em O
algorithm O
with O
irls O
in O
the O
m O
step O
. O
the O
log O
likelihood O
for O
this O
model O
is O
maximized O
using O
em O
, O
and O
the O
reconstruction O
of O
the O
latent O
variables O
is O
ap O
( O
cid:173 O
) O
proximated O
using O
a O
variational B
approach O
. O
the O
set O
of O
nodes O
comprising O
the O
parents O
, O
the O
children O
and O
the O
co-parents B
is O
called O
the O
markov O
blanket O
and O
is O
illustrated O
in O
figure O
8.26. O
we O
can O
think O
of O
the O
markov O
blanket O
of O
a O
node B
xi O
as O
being O
the O
minimal O
set O
of O
nodes O
that O
isolates O
xi O
from O
the O
rest O
of O
the O
graph O
. O
it O
is O
possible O
to O
optimize O
the O
ep O
cost B
function I
directly O
, O
in O
which O
case O
it O
is O
guaranteed O
to O
converge O
, O
although O
the O
resulting O
algorithms O
can O
be O
slower O
and O
more O
complex O
to O
implement O
. O
in O
order O
to O
ﬁnd O
a O
sufﬁciently O
good O
minimum O
, O
it O
may O
be O
necessary O
to O
run O
a O
gradient-based O
algorithm O
multiple O
times O
, O
each O
time O
using O
a O
different O
randomly O
cho- O
sen O
starting O
point O
, O
and O
comparing O
the O
resulting O
performance O
on O
an O
independent B
vali- O
dation O
set O
. O
to O
apply O
such O
models O
to O
the O
problem O
of O
density B
estimation I
, O
we O
need O
a O
procedure O
for O
determining O
suitable O
values O
for O
the O
parameters O
, O
given O
an O
observed O
data O
set O
. O
in O
particular O
, O
for O
a O
tree-structured O
graph O
we O
can O
follow O
a O
two-pass O
update O
scheme O
, O
corresponding O
to O
the O
standard O
belief O
propagation O
schedule B
, O
which O
will O
result O
in O
exact O
inference O
of O
the O
variable O
and O
factor O
marginals O
. O
this O
problem O
is O
known O
as O
density B
estimation I
. O
the O
forward O
recursion O
equation O
for O
α O
( O
zn O
) O
is O
illustrated O
using O
a O
lattice B
diagram I
in O
figure O
13.12. O
in O
order O
to O
start O
this O
recursion O
, O
we O
need O
an O
initial O
condition O
that O
is O
given O
by O
zn+1 O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
zn+1 O
zn+1 O
zn+1 O
= O
= O
= O
= O
p O
( O
xn+1 O
, O
. O
again O
, O
this O
clearly O
leaves O
the O
network O
input–output O
mapping O
function O
unchanged O
, O
but O
it O
corre- O
sponds O
to O
a O
different O
choice O
of O
weight B
vector I
. O
if O
the O
graph O
had O
been O
fully B
connected I
, O
there O
would O
have O
been O
no O
conditional B
independence I
properties O
, O
and O
we O
would O
have O
been O
forced O
to O
work O
directly O
with O
the O
full O
joint O
distribution O
. O
this O
multivariate O
probability B
density O
must O
satisfy O
( O
cid:6 O
) O
p O
( O
x O
) O
( O
cid:2 O
) O
0 O
p O
( O
x O
) O
dx O
= O
1 O
( O
1.29 O
) O
( O
1.30 O
) O
in O
which O
the O
integral O
is O
taken O
over O
the O
whole O
of O
x O
space O
. O
we O
have O
seen O
that O
the O
derivative B
of O
the O
log O
likelihood O
function O
for O
a O
linear O
regres- O
sion B
model O
with O
respect O
to O
the O
parameter O
vector O
w O
for O
a O
data O
point O
n O
took O
the O
form O
of O
the O
‘ O
error B
’ O
yn O
− O
tn O
times O
the O
feature O
vector O
φn O
. O
re-writing O
the O
graph O
of O
figure O
8.3 O
in O
this O
way O
, O
we O
obtain O
the O
graph O
shown O
in O
figure O
8.4. O
we O
shall O
sometimes O
ﬁnd O
it O
helpful O
to O
make O
the O
parameters O
of O
a O
model O
, O
as O
well O
as O
its O
stochastic B
variables O
, O
explicit O
. O
( O
11.68 O
) O
where O
the O
factor O
of O
1/2 O
arises O
from O
the O
probability B
of O
choosing O
to O
integrate O
with O
a O
positive O
step O
size O
rather O
than O
a O
negative O
one O
. O
we O
have O
already O
seen O
how O
the O
maximum B
likelihood I
expression O
for O
the O
mean B
of O
a O
gaussian O
can O
be O
re-cast O
as O
a O
sequential O
update O
formula O
in O
which O
the O
mean B
after O
observing O
n O
data O
points O
was O
expressed O
in O
terms O
of O
the O
mean B
after O
observing O
n O
− O
1 O
data O
points O
together O
with O
the O
contribution O
from O
data O
point O
xn O
. O
variational B
mixture O
of O
bayesian O
independent O
component O
an- O
alyzers O
. O
and O
similarly O
the O
inverse B
covariance O
matrix O
σ O
−1 O
can O
be O
expressed O
as O
substituting O
( O
2.49 O
) O
into O
( O
2.44 O
) O
, O
the O
quadratic O
form O
becomes O
d O
( O
cid:2 O
) O
i=1 O
y2 O
i O
λi O
∆2 O
= O
where O
we O
have O
deﬁned O
( O
2.51 O
) O
we O
can O
interpret O
{ O
yi O
} O
as O
a O
new O
coordinate O
system O
deﬁned O
by O
the O
orthonormal O
vectors O
ui O
that O
are O
shifted O
and O
rotated O
with O
respect O
to O
the O
original O
xi O
coordinates O
. O
consider O
ﬁrst O
the O
determi- O
nation O
of O
w. O
maximizing O
the O
likelihood B
function I
is O
equivalent O
to O
minimizing O
the O
sum-of-squares B
error I
function O
given O
by O
e O
( O
w O
) O
= O
1 O
2 O
{ O
y O
( O
xn O
, O
w O
) O
− O
tn O
} O
2 O
( O
5.14 O
) O
n O
( O
cid:2 O
) O
n=1 O
234 O
5. O
neural O
networks O
where O
we O
have O
discarded O
additive O
and O
multiplicative O
constants O
. O
and O
taking O
the O
exponential O
, O
we O
obtain O
an O
upper O
bound O
on O
the O
logistic B
sigmoid I
itself O
of O
the O
form O
σ O
( O
x O
) O
( O
cid:1 O
) O
exp O
( O
λx O
− O
g O
( O
λ O
) O
) O
( O
10.137 O
) O
exercise O
10.31 O
which O
is O
plotted O
for O
two O
values O
of O
λ O
on O
the O
left-hand O
plot O
in O
figure O
10.12. O
we O
can O
also O
obtain O
a O
lower B
bound I
on O
the O
sigmoid B
having O
the O
functional B
form O
of O
a O
gaussian O
. O
however O
, O
they O
do O
not O
represent O
an O
explicit O
solution O
because O
the O
expres- O
sion B
on O
the O
right-hand O
side O
of O
( O
10.9 O
) O
for O
the O
optimum O
q O
( O
cid:1 O
) O
j O
( O
zj O
) O
depends O
on O
expectations O
computed O
with O
respect O
to O
the O
other O
factors O
qi O
( O
zi O
) O
for O
i O
( O
cid:9 O
) O
= O
j. O
we O
will O
therefore O
seek O
a O
consistent B
solution O
by O
ﬁrst O
initializing O
all O
of O
the O
factors O
qi O
( O
zi O
) O
appropriately O
and O
then O
cycling O
through O
the O
factors O
and O
replacing O
each O
in O
turn O
with O
a O
revised O
estimate O
given O
by O
the O
right-hand O
side O
of O
( O
10.9 O
) O
evaluated O
using O
the O
current O
estimates O
for O
all O
of O
the O
other O
factors O
. O
conceptually O
, O
we O
will O
now O
determine O
a O
revised O
form O
of O
the O
factor O
( O
cid:4 O
) O
fj O
( O
θ O
) O
by O
ensuring O
that O
the O
product O
( O
cid:4 O
) O
fi O
( O
θ O
) O
factor O
from O
the O
product O
to O
give O
( O
10.193 O
) O
i O
( O
cid:9 O
) O
=j O
( O
cid:21 O
) O
( O
cid:14 O
) O
qnew O
( O
θ O
) O
∝ O
( O
cid:4 O
) O
fj O
( O
θ O
) O
( O
cid:14 O
) O
( O
cid:4 O
) O
fi O
( O
θ O
) O
fj O
( O
θ O
) O
i O
( O
cid:9 O
) O
=j O
is O
as O
close O
as O
possible O
to O
in O
which O
we O
keep O
ﬁxed O
all O
of O
the O
factors O
( O
cid:4 O
) O
fi O
( O
θ O
) O
for O
i O
( O
cid:9 O
) O
= O
j. O
this O
ensures O
that O
the O
to O
the O
‘ O
clutter B
problem I
’ O
. O
amongst O
his O
many O
contributions O
, O
he O
formulated O
the O
modern O
theory B
of O
the O
function O
, O
he O
developed O
( O
together O
with O
lagrange O
) O
the O
calculus B
of I
variations I
, O
and O
he O
discovered O
the O
formula O
eiπ O
= O
−1 O
, O
which O
relates O
four O
of O
the O
most O
important O
numbers O
in O
mathematics O
. O
next O
, O
we O
generate O
a O
number O
u0 O
from O
the O
uniform B
distribution I
over O
[ O
0 O
, O
kq O
( O
z0 O
) O
] O
. O
one O
approach O
is O
to O
maximize O
the O
likelihood B
function I
given O
by O
p O
( O
tn|θ O
) O
for O
which O
we O
need O
expressions O
for O
the O
log O
likelihood O
and O
its O
gradient O
. O
( O
10.212 O
) O
512 O
10. O
approximate O
inference B
( O
10.213 O
) O
( O
cid:4 O
) O
fn O
( O
θ O
) O
= O
snn O
( O
θ|mn O
, O
vni O
) O
the O
factor O
approximations O
will O
therefore O
take O
the O
form O
of O
exponential-quadratic O
functions O
of O
the O
form O
where O
n O
= O
1 O
, O
. O
( O
8.21 O
) O
where O
we O
have O
used O
the O
product B
rule I
of I
probability I
together O
with O
( O
8.20 O
) O
. O
this O
framework O
is O
often O
called O
vector B
quantization I
, O
and O
the O
vectors O
µk O
are O
called O
code-book B
vectors I
. O
as O
we O
have O
noted O
, O
there O
are O
techniques O
for O
training O
svms O
whose O
cost O
is O
roughly O
quadratic O
in O
n. O
of O
course O
, O
in O
the O
case O
of O
the O
rvm O
we O
always O
have O
the O
option O
of O
starting O
with O
a O
smaller O
number O
of O
basis O
functions O
than O
n O
+ O
1. O
more O
signiﬁcantly O
, O
in O
the O
relevance B
vector I
machine I
the O
parameters O
governing O
complexity O
and O
noise O
variance B
are O
determined O
automatically O
from O
a O
single O
training B
run O
, O
whereas O
in O
the O
support B
vector I
machine I
the O
parameters O
c O
and O
 O
( O
or O
ν O
) O
are O
generally O
found O
using O
cross-validation B
, O
which O
involves O
multiple O
training B
runs O
. O
although O
the O
junction B
tree I
algorithm I
sounds O
complicated O
, O
at O
its O
heart O
is O
the O
simple O
idea O
that O
we O
have O
used O
already O
of O
exploiting O
the O
factorization B
properties O
of O
the O
distribution O
to O
allow O
sums O
and O
products O
to O
be O
interchanged O
so O
that O
partial O
summations O
can O
be O
per- O
formed O
, O
thereby O
avoiding O
having O
to O
work O
directly O
with O
the O
joint O
distribution O
. O
13.2.3 O
the O
sum-product B
algorithm I
for O
the O
hmm O
. O
if O
the O
parameters O
φk O
are O
independent B
for O
the O
different O
components O
, O
then O
this O
term O
decouples O
into O
a O
sum O
of O
terms O
one O
for O
each O
value O
of O
k O
, O
each O
of O
which O
can O
be O
maximized O
independently O
. O
for O
instance O
, O
each O
subsampling B
unit O
might O
take O
inputs O
from O
a O
2 O
× O
2 O
unit O
region O
in O
the O
corresponding O
feature B
map I
and O
would O
compute O
the O
average O
of O
those O
inputs O
, O
multiplied O
by O
an O
adaptive O
weight O
with O
the O
addition O
of O
an O
adaptive O
bias O
parameter O
, O
and O
then O
transformed O
using O
a O
sigmoidal O
nonlinear O
activation B
function I
. O
by O
stopping O
training B
early O
, O
a O
weight B
vector I
ew O
is O
found O
that O
is O
qual- O
itatively O
similar O
to O
that O
obtained O
with O
a O
simple O
weight-decay O
reg- O
ularizer O
and O
training B
to O
the O
mini- O
mum O
of O
the O
regularized O
error O
, O
as O
can O
be O
seen O
by O
comparing O
with O
figure O
3.15. O
w2 O
( O
cid:4 O
) O
w O
wml O
w1 O
digit O
is O
shifted O
to O
a O
different O
position O
in O
each O
image O
. O
8.3.3 O
illustration O
: O
image B
de-noising I
we O
can O
illustrate O
the O
application O
of O
undirected B
graphs O
using O
an O
example O
of O
noise O
removal O
from O
a O
binary O
image O
( O
besag O
, O
1974 O
; O
geman O
and O
geman O
, O
1984 O
; O
besag O
, O
1986 O
) O
. O
( O
13.9 O
) O
k O
( O
cid:14 O
) O
k=1 O
we O
shall O
focuss O
attention O
on O
homogeneous B
models O
for O
which O
all O
of O
the O
condi- O
tional O
distributions O
governing O
the O
latent O
variables O
share O
the O
same O
parameters O
a O
, O
and O
similarly O
all O
of O
the O
emission O
distributions O
share O
the O
same O
parameters O
φ O
( O
the O
extension O
to O
more O
general O
cases O
is O
straightforward O
) O
. O
it O
involves O
a O
specific O
choice O
of O
prior B
over O
w O
that O
allows O
surplus O
dimensions O
in O
the O
principal B
subspace I
to O
be O
pruned O
out O
of O
the O
model O
. O
( O
13.45 O
) O
( O
13.46 O
) O
section O
10.1 O
section O
8.4.4 O
626 O
13. O
sequential B
data I
figure O
13.15 O
a O
simpliﬁed O
form O
of O
fac- O
tor O
graph O
to O
describe O
the O
hidden O
markov O
model O
. O
in O
chapter O
10 O
, O
we O
discussed O
inference B
algorithms O
based O
on O
deterministic O
approximations O
, O
which O
include O
methods O
such O
as O
variational B
bayes O
and O
expectation B
propagation I
. O
( O
11.28 O
) O
we O
can O
use O
sampling B
methods I
to O
approximate O
this O
integral O
by O
a O
ﬁnite O
sum O
over O
sam- O
ples O
{ O
z O
( O
l O
) O
} O
, O
which O
are O
drawn O
from O
the O
current O
estimate O
for O
the O
posterior O
distribution O
p O
( O
z|x O
, O
θold O
) O
, O
so O
that O
q O
( O
θ O
, O
θold O
) O
( O
cid:7 O
) O
1 O
l O
ln O
p O
( O
z O
( O
l O
) O
, O
x|θ O
) O
. O
the O
conditional B
distribution O
p O
( O
t|x O
, O
w O
, O
β O
) O
of O
the O
target O
vari- O
able O
is O
given O
by O
( O
3.8 O
) O
, O
and O
the O
posterior O
weight O
distribution O
is O
given O
by O
( O
3.49 O
) O
. O
for O
each O
pattern O
in O
the O
training B
set I
in O
turn O
, O
we O
ﬁrst O
perform O
a O
forward B
propagation I
using O
aj O
= O
h O
( O
a O
) O
≡ O
tanh O
( O
a O
) O
tanh O
( O
a O
) O
= O
ea O
− O
e O
−a O
ea O
+ O
e−a O
. O
however O
, O
pca O
is O
unsupervised O
and O
depends O
only O
on O
the O
values O
x O
n O
whereas O
fisher O
linear B
discriminant I
also O
uses O
class-label O
information O
. O
consider O
a O
model O
deﬁned O
in O
terms O
of O
a O
linear O
combination O
of O
m O
ﬁxed O
basis O
functions O
given O
by O
the O
elements O
of O
the O
vector O
φ O
( O
x O
) O
so O
that O
y O
( O
x O
) O
= O
wtφ O
( O
x O
) O
( O
6.49 O
) O
where O
x O
is O
the O
input O
vector O
and O
w O
is O
the O
m-dimensional O
weight B
vector I
. O
in O
this O
section O
, O
our O
discussion O
will O
be O
very O
general O
, O
and O
then O
in O
section O
3.5 O
we O
shall O
see O
how O
these O
ideas O
can O
be O
applied O
to O
the O
determination O
of O
regularization B
parameters O
in O
linear B
regression I
. O
in O
the O
case O
of O
the O
gaussian O
prior B
, O
the O
mode O
of O
the O
posterior O
distribution O
was O
equal O
to O
the O
mean B
, O
although O
this O
will O
no O
longer O
hold O
if O
q O
( O
cid:9 O
) O
= O
2 O
. O
stochastic B
models O
, O
estima- O
tion O
and O
control O
. O
the O
error B
function I
for O
untransformed O
inputs O
can O
be O
written O
( O
in O
the O
inﬁnite O
data O
set O
limit O
) O
in O
the O
form O
{ O
y O
( O
x O
) O
− O
t O
} O
2p O
( O
t|x O
) O
p O
( O
x O
) O
dx O
dt O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
5.129 O
) O
e O
= O
1 O
2 O
as O
discussed O
in O
section O
1.5.5. O
here O
we O
have O
considered O
a O
network O
having O
a O
single O
output O
, O
in O
order O
to O
keep O
the O
notation O
uncluttered O
. O
the O
corresponding O
value O
of O
the O
distortion B
measure I
is O
then O
given O
by O
sui O
= O
aiui O
d O
j= O
l O
ai O
i=m+l O
( O
12.18 O
) O
which O
is O
simply O
the O
sum O
of O
the O
eigenvalues O
of O
those O
eigenvectors O
that O
are O
orthogonal O
to O
the O
principal B
subspace I
. O
thus O
, O
for O
translation O
invariance B
in O
an O
im- O
age O
, O
the O
training B
set I
should O
include O
examples O
of O
objects O
at O
many O
different O
positions O
. O
consider O
a O
data O
set O
{ O
xn O
} O
of O
observations O
, O
where O
n O
= O
1 O
, O
... O
, O
n O
, O
in O
a O
space O
of O
dimensionality O
d. O
in O
order O
to O
keep O
the O
notation O
uncluttered O
, O
we O
shall O
assume O
that O
we O
have O
already O
subtracted O
the O
sample B
mean I
from O
each O
of O
the O
vectors O
x O
n O
, O
so O
that O
ln O
x O
n O
= O
o. O
the O
first O
step O
is O
to O
express O
conventional O
pca O
in O
such O
a O
form O
that O
the O
data O
vectors O
{ O
x O
n O
} O
appear O
only O
in O
the O
form O
of O
the O
scalar O
products O
x~ O
x O
m O
. O
the O
difference O
is O
that O
here O
α O
is O
replaced O
by O
its O
expecta- O
tion O
e O
[ O
α O
] O
under O
the O
variational B
distribution O
. O
suppose O
we O
take O
the O
conditional B
distribution O
of O
the O
target B
vector I
to O
be O
an O
isotropic B
gaussian O
of O
the O
form O
p O
( O
t|x O
, O
w O
, O
β O
) O
= O
n O
( O
t|wtφ O
( O
x O
) O
, O
β O
−1i O
) O
. O
graphs O
of O
the O
form O
shown O
in O
figure O
8.38 O
are O
called O
markov O
chains O
, O
and O
the O
corresponding O
message B
passing I
equations O
represent O
an O
example O
of O
the O
chapman- O
kolmogorov O
equations O
for O
markov O
processes O
( O
papoulis O
, O
1984 O
) O
. O
each O
node B
can O
send O
a O
message O
towards O
the O
root O
once O
it O
has O
received O
messages O
from O
all O
of O
its O
other O
neighbours O
. O
a O
+ O
βφtφ O
the O
values O
of O
α O
and O
β O
are O
determined O
using O
type-2 O
maximum B
likelihood I
, O
also O
known O
as O
the O
evidence B
approximation I
, O
in O
which O
we O
maximize O
the O
marginal B
likeli- O
hood O
function O
obtained O
by O
integrating O
out O
the O
weight O
parameters O
p O
( O
t|x O
, O
w O
, O
β O
) O
p O
( O
w|α O
) O
dw O
. O
also O
, O
the O
dimensionality O
of O
the O
subspace O
must O
be O
specified O
before O
training B
the O
network O
. O
this O
leads O
to O
the O
basic O
neural B
network I
model O
, O
which O
can O
be O
described O
a O
series O
of O
functional B
transformations O
. O
most O
of O
our O
discussion O
of O
the O
hidden O
markov O
model O
will O
be O
independent B
of O
the O
particular O
choice O
of O
the O
emission O
probabilities O
. O
an O
important O
and O
elegant O
feature O
of O
graphical O
models O
is O
that O
conditional B
independence I
properties O
of O
the O
joint O
distribution O
can O
be O
read O
directly O
from O
the O
graph O
without O
having O
to O
perform O
any O
analytical O
manipulations O
. O
t O
y O
( O
x0 O
, O
w O
) O
1.2. O
probability B
theory O
29 O
y O
( O
x O
, O
w O
) O
p O
( O
t|x0 O
, O
w O
, O
β O
) O
x0 O
2σ O
x O
we O
now O
use O
the O
training B
data O
{ O
x O
, O
t O
} O
to O
determine O
the O
values O
of O
the O
unknown O
parameters O
w O
and O
β O
by O
maximum B
likelihood I
. O
in O
order O
to O
derive O
the O
rules O
of O
probability B
, O
consider O
the O
slightly O
more O
general O
ex- O
ample O
shown O
in O
figure O
1.10 O
involving O
two O
random O
variables O
x O
and O
y O
( O
which O
could O
for O
instance O
be O
the O
box O
and O
fruit O
variables O
considered O
above O
) O
. O
we O
can O
choose O
any O
other O
kernel B
function I
k O
( O
u O
) O
in O
( O
2.249 O
) O
subject O
to O
the O
condi- O
tions O
( O
cid:6 O
) O
k O
( O
u O
) O
( O
cid:2 O
) O
0 O
, O
k O
( O
u O
) O
du O
= O
1 O
( O
2.251 O
) O
( O
2.252 O
) O
which O
ensure O
that O
the O
resulting O
probability B
distribution O
is O
nonnegative O
everywhere O
and O
integrates O
to O
one O
. O
in O
the O
ﬁrst O
stage O
, O
the O
derivatives O
of O
the O
error B
function I
with O
respect O
to O
the O
weights O
must O
be O
evaluated O
. O
for O
m O
sufﬁciently O
large O
, O
the O
retained O
samples O
will O
for O
all O
practical O
purposes O
be O
independent B
. O
such O
induced O
factorizations O
can O
easily O
be O
detected O
using O
a O
simple O
graphical O
test O
based O
on O
d-separation B
as O
follows O
. O
thus O
( O
z O
+ O
µ O
) O
dz O
ztς O
e O
[ O
x O
] O
= O
µ O
( O
2.59 O
) O
and O
so O
we O
refer O
to O
µ O
as O
the O
mean B
of O
the O
gaussian O
distribution O
. O
also O
, O
local B
features O
that O
are O
useful O
in O
one O
region O
of O
the O
image O
are O
likely O
to O
be O
useful O
in O
other O
regions O
of O
the O
image O
, O
for O
instance O
if O
the O
object O
of O
interest O
is O
translated O
. O
its O
solution O
can O
be O
expressed O
in O
terms O
of O
a O
generalized B
eigenvector O
problem O
. O
by O
contrast O
, O
in O
a O
bayesian O
treatment O
we O
introduce O
prior B
distributions O
over O
the O
parameters O
and O
then O
use O
bayes O
’ O
theorem O
to O
compute O
the O
corresponding O
posterior O
distribution O
given O
the O
observed O
data O
. O
( O
1.13 O
) O
y O
we O
can O
view O
the O
denominator O
in O
bayes O
’ O
theorem O
as O
being O
the O
normalization O
constant O
required O
to O
ensure O
that O
the O
sum O
of O
the O
conditional B
probability I
on O
the O
left-hand O
side O
of O
( O
1.12 O
) O
over O
all O
values O
of O
y O
equals O
one O
. O
( O
4.53 O
) O
( O
cid:12 O
) O
the O
vector O
φ O
( O
x O
) O
will O
typically O
include O
a O
bias B
component O
φ0 O
( O
x O
) O
= O
1. O
in O
earlier O
discussions O
of O
two-class O
classiﬁcation B
problems O
, O
we O
have O
focussed O
on O
a O
target O
coding O
scheme O
in O
which O
t O
∈ O
{ O
0 O
, O
1 O
} O
, O
which O
is O
appropriate O
in O
the O
context O
of O
probabilistic O
models O
. O
consider O
the O
joint O
distribution O
p O
( O
x O
, O
y O
) O
over O
two O
binary O
variables O
x O
, O
y O
∈ O
{ O
0 O
, O
1 O
} O
given O
in O
table O
8.1. O
the O
joint O
distribution O
is O
maximized O
by O
setting O
x O
= O
1 O
and O
y O
= O
0 O
, O
corresponding O
the O
value O
0.4. O
however O
, O
the O
marginal B
for O
p O
( O
x O
) O
, O
obtained O
by O
summing O
over O
both O
values O
of O
y O
, O
is O
given O
by O
p O
( O
x O
= O
0 O
) O
= O
0.6 O
and O
p O
( O
x O
= O
1 O
) O
= O
0.4 O
, O
and O
similarly O
the O
marginal B
for O
y O
is O
given O
by O
p O
( O
y O
= O
0 O
) O
= O
0.7 O
and O
p O
( O
y O
= O
1 O
) O
= O
0.3 O
, O
and O
so O
the O
marginals O
are O
maximized O
by O
x O
= O
0 O
and O
y O
= O
0 O
, O
which O
corresponds O
to O
a O
value O
of O
0.3 O
for O
the O
joint O
distribution O
. O
( O
5.98 O
) O
( O
5.99 O
) O
( O
5.100 O
) O
we O
now O
act O
on O
these O
equations O
using O
the O
r O
{ O
· O
} O
operator O
to O
obtain O
a O
set O
of O
forward B
propagation I
equations O
in O
the O
form O
r O
{ O
aj O
} O
= O
r O
{ O
zj O
} O
= O
h O
r O
{ O
yk O
} O
= O
( O
cid:2 O
) O
( O
cid:2 O
) O
wkjr O
{ O
zj O
} O
+ O
( O
cid:4 O
) O
( O
aj O
) O
r O
{ O
aj O
} O
( O
cid:2 O
) O
( O
5.101 O
) O
( O
5.102 O
) O
( O
5.103 O
) O
vkjzj O
vjixi O
i O
j O
j O
where O
vji O
is O
the O
element O
of O
the O
vector O
v O
that O
corresponds O
to O
the O
weight O
wji O
. O
to O
do O
this O
, O
show O
that O
the O
derivative B
of O
the O
left- O
hand O
side O
with O
respect O
to O
µ O
is O
equal O
to O
the O
derivative B
of O
the O
right-hand O
side O
, O
and O
then O
integrate O
both O
sides O
with O
respect O
to O
µ O
and O
then O
show O
that O
the O
constant O
of O
integration O
vanishes O
. O
n=1 O
k=1 O
in O
order O
to O
maximize O
this O
likelihood B
function I
, O
we O
can O
once O
again O
appeal O
to O
the O
em O
algorithm O
, O
which O
will O
turn O
out O
to O
be O
a O
simple O
extension O
of O
the O
em O
algorithm O
for O
unconditional O
gaussian O
mixtures O
of O
section O
9.2. O
we O
can O
therefore O
build O
on O
our O
expe- O
rience O
with O
the O
unconditional O
mixture B
and O
introduce O
a O
set O
z O
= O
{ O
zn O
} O
of O
binary O
latent O
variables O
where O
znk O
∈ O
{ O
0 O
, O
1 O
} O
in O
which O
, O
for O
each O
data O
point O
n O
, O
all O
of O
the O
elements O
k O
= O
1 O
, O
. O
this O
gives O
− O
d O
dx O
f O
[ O
y O
( O
x O
) O
+ O
η O
( O
x O
) O
] O
= O
f O
[ O
y O
( O
x O
) O
] O
+ O
 O
η O
( O
x O
) O
dx O
+ O
o O
( O
2 O
) O
( O
d.7 O
) O
( O
cid:6 O
) O
( O
cid:12 O
) O
( O
cid:16 O
) O
( O
cid:13 O
) O
∂g O
∂y O
( O
cid:1 O
) O
∂g O
∂y O
( O
cid:15 O
) O
d. O
calculus B
of I
variations I
705 O
from O
which O
we O
can O
read O
off O
the O
functional B
derivative O
by O
comparison O
with O
( O
d.3 O
) O
. O
here O
each O
data O
point O
is O
projected O
onto O
a O
two-dimensional O
( O
m O
= O
2 O
) O
principal B
subspace I
, O
so O
that O
a O
data O
point O
x O
n O
is O
plotted O
at O
cartesian O
coordinates O
given O
by O
x'j O
. O
if O
our O
goal O
is O
to O
predict O
, O
as O
best O
we O
can O
, O
the O
outcome O
of O
the O
next O
trial O
, O
then O
we O
must O
evaluate O
the O
predictive B
distribution I
of O
x O
, O
given O
the O
observed O
data O
set O
d. O
from O
the O
sum O
and O
product O
rules O
of O
probability B
, O
this O
takes O
the O
form O
p O
( O
x O
= O
1|d O
) O
= O
p O
( O
x O
= O
1|µ O
) O
p O
( O
µ|d O
) O
dµ O
= O
µp O
( O
µ|d O
) O
dµ O
= O
e O
[ O
µ|d O
] O
. O
by O
analogy O
with O
the O
hidden O
markov O
model O
, O
this O
problem O
can O
be O
solved O
by O
propagating O
messages O
from O
node B
xn O
back O
to O
node B
x1 O
and O
com- O
bining O
this O
information O
with O
that O
obtained O
during O
the O
forward O
message O
passing O
stage O
used O
to O
compute O
the O
( O
cid:1 O
) O
α O
( O
zn O
) O
. O
14.3 O
( O
( O
cid:12 O
) O
) O
www O
by O
making O
use O
of O
jensen O
’ O
s O
inequality O
( O
1.115 O
) O
, O
for O
the O
special O
case O
of O
the O
convex B
function I
f O
( O
x O
) O
= O
x2 O
, O
show O
that O
the O
average O
expected O
sum-of-squares B
error I
eav O
of O
the O
members O
of O
a O
simple O
committee B
model O
, O
given O
by O
( O
14.10 O
) O
, O
and O
the O
expected O
error B
ecom O
of O
the O
committee B
itself O
, O
given O
by O
( O
14.11 O
) O
, O
satisfy O
ecom O
( O
cid:1 O
) O
eav O
. O
5.4.1 O
diagonal B
approximation I
some O
of O
the O
applications O
for O
the O
hessian O
matrix O
discussed O
above O
require O
the O
inverse B
of O
the O
hessian O
, O
rather O
than O
the O
hessian O
itself O
. O
we O
shall O
focus O
on O
the O
case O
in O
which O
the O
approximating O
distribution O
is O
fully O
fac- O
torized O
, O
and O
we O
shall O
show O
that O
in O
this O
case O
expectation B
propagation I
reduces O
to O
loopy B
belief I
propagation I
( O
minka O
, O
2001a O
) O
. O
as O
in O
chapters O
9 O
and O
12 O
, O
we O
shall O
see O
that O
complex O
models O
can O
thereby O
be O
constructed O
from O
simpler O
components O
( O
in O
particular O
, O
from O
distributions O
belonging O
to O
the O
exponential B
family I
) O
and O
can O
be O
read- O
ily O
characterized O
using O
the O
framework O
of O
probabilistic O
graphical O
models O
. O
this O
is O
not O
the O
case O
, O
however O
, O
because O
there O
are O
constraints O
amongst O
the O
feature O
values O
that O
restrict O
the O
effective O
dimensionality O
of O
feature B
space I
. O
652 O
13. O
sequential B
data I
13.34 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
verify O
the O
results O
( O
13.115 O
) O
and O
( O
13.116 O
) O
for O
the O
m-step O
equations O
for O
c O
and O
σ O
in O
the O
linear B
dynamical I
system I
. O
3.5 O
( O
( O
cid:12 O
) O
) O
www O
using O
the O
technique O
of O
lagrange O
multipliers O
, O
discussed O
in O
appendix O
e O
, O
show O
that O
minimization O
of O
the O
regularized O
error O
function O
( O
3.29 O
) O
is O
equivalent O
to O
mini- O
mizing O
the O
unregularized O
sum-of-squares B
error I
( O
3.12 O
) O
subject O
to O
the O
constraint O
( O
3.30 O
) O
. O
for O
the O
remainder O
of O
this O
chapter O
, O
we O
shall O
include O
a O
ﬁxed O
basis B
function I
transformation O
φ O
( O
x O
) O
, O
as O
this O
will O
highlight O
some O
useful O
similarities O
to O
the O
regression B
models O
discussed O
in O
chapter O
3. O
for O
many O
problems O
of O
practical O
interest O
, O
there O
is O
signiﬁcant O
overlap O
between O
the O
class-conditional O
densities O
p O
( O
x|ck O
) O
. O
also O
, O
for O
large O
k O
this O
approach O
requires O
signiﬁcantly O
more O
training B
time O
than O
the O
one-versus-the-rest O
approach O
. O
the O
sum O
in O
( O
5.48 O
) O
is O
transformed O
by O
a O
nonlinear O
activation B
function I
h O
( O
· O
) O
to O
give O
the O
activation O
zj O
of O
unit O
j O
in O
the O
form O
zj O
= O
h O
( O
aj O
) O
. O
as O
well O
as O
the O
predictive B
distribution I
p O
( O
x O
) O
, O
we O
will O
also O
require O
the O
posterior O
distributionp O
( O
zlx O
) O
, O
which O
can O
again O
be O
written O
down O
directly O
using O
the O
result O
( O
2.116 O
) O
for O
linear-gaussian O
models O
to O
give O
note O
that O
the O
posterior O
mean O
depends O
on O
x O
, O
whereas O
the O
posterior O
covariance O
is O
in O
( O
cid:173 O
) O
dependent O
of O
x O
. O
12.2.2 O
em O
algorithm O
for O
pea O
as O
we O
have O
seen O
, O
the O
probabilistic O
pca O
model O
can O
be O
expressed O
in O
terms O
of O
a O
marginalization O
over O
a O
continuous O
latent O
space O
z O
in O
which O
for O
each O
data O
point O
x O
n O
, O
there O
is O
a O
corresponding O
latent B
variable I
zn O
. O
a O
bayesian O
treatment O
simply O
corresponds O
to O
a O
consistent B
application O
of O
the O
sum O
and O
product O
rules O
of O
probability B
, O
which O
allow O
the O
predictive B
distribution I
to O
be O
written O
in O
the O
form O
( O
cid:6 O
) O
p O
( O
t|x O
, O
x O
, O
t O
) O
= O
p O
( O
t|x O
, O
w O
) O
p O
( O
w|x O
, O
t O
) O
dw O
. O
, O
tn O
) O
t. O
we O
can O
express O
our O
uncertainty O
over O
the O
value O
of O
the O
target O
variable O
using O
a O
probability B
distribution O
. O
, O
xn O
) O
of O
the O
observed O
data O
for O
the O
state B
space I
model I
represented O
by O
the O
directed B
graph O
in O
figure O
13.5 O
does O
not O
satisfy O
any O
conditional B
independence I
properties O
and O
hence O
does O
not O
exhibit O
the O
markov O
property O
at O
any O
ﬁnite O
order O
. O
( O
5.130 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:4 O
) O
e O
= O
1 O
2 O
we O
now O
assume O
that O
the O
distribution O
p O
( O
ξ O
) O
has O
zero O
mean B
with O
small O
variance B
, O
so O
that O
we O
are O
only O
considering O
small O
transformations O
of O
the O
original O
input O
vectors O
. O
notice O
that O
the O
boundary O
between O
the O
red O
and O
green O
classes O
, O
which O
have O
the O
same O
covariance B
matrix I
, O
is O
linear O
, O
whereas O
those O
between O
the O
other O
pairs O
of O
classes O
are O
quadratic O
. O
the O
interpretation O
of O
boosting B
as O
the O
sequential O
optimization O
of O
an O
additive O
model O
under O
an O
exponential O
error O
( O
friedman O
et O
al. O
, O
2000 O
) O
opens O
the O
door O
to O
a O
wide O
range O
of O
boosting-like O
algorithms O
, O
including O
multiclass B
extensions O
, O
by O
altering O
the O
choice O
of O
error B
function I
. O
12.20 O
( O
** O
) O
by O
considering O
second O
derivatives O
, O
show O
that O
the O
only O
stationary B
point O
of O
the O
log O
likelihood O
function O
for O
the O
factor B
analysis I
model O
discussed O
in O
section O
12.2.4 O
with O
respect O
to O
the O
parameter O
j1 O
is O
given O
by O
the O
sample B
mean I
defined O
by O
( O
12.1 O
) O
. O
these O
computations O
mirror O
pre- O
cisely O
those O
that O
arise O
in O
the O
maximum B
likelihood I
em O
algorithm O
, O
and O
so O
there O
is O
little O
computational O
overhead O
in O
using O
this O
bayesian O
approach O
as O
compared O
to O
the O
tradi- O
tional O
maximum B
likelihood I
one O
. O
conversely O
, O
in O
logistic-type O
models O
, O
ep O
often O
out-performs O
both O
local B
variational O
methods O
and O
the O
laplace O
approximation O
( O
kuss O
and O
rasmussen O
, O
2006 O
) O
. O
h O
fn O
z1 O
zn−1 O
zn O
to O
derive O
the O
alpha-beta O
algorithm O
, O
we O
denote O
the O
ﬁnal O
hidden B
variable I
zn O
as O
the O
root B
node I
, O
and O
ﬁrst O
pass O
messages O
from O
the O
leaf O
node B
h O
to O
the O
root O
. O
we O
now O
seek O
a O
more O
principled O
approach O
to O
solving O
problems O
in O
pattern O
recognition O
by O
turning O
to O
a O
discussion O
of O
probability B
theory O
. O
here O
we O
consider O
a O
form O
of O
soft B
weight I
sharing I
( O
nowlan O
and O
hinton O
, O
1992 O
) O
in O
which O
the O
hard O
constraint O
of O
equal O
weights O
is O
replaced O
by O
a O
form O
of O
regularization B
in O
which O
groups O
of O
weights O
are O
encouraged O
to O
have O
similar O
values O
. O
this O
is O
illustrated O
with O
an O
example O
in O
figure O
8.27. O
note O
that O
this O
is O
exactly O
the O
same O
as O
the O
d-separation B
crite- O
rion B
except O
that O
there O
is O
no O
‘ O
explaining B
away I
’ O
phenomenon O
. O
if O
the O
dis- O
tribution O
pg O
is O
one O
for O
which O
the O
partition B
function I
can O
be O
evaluated O
analytically O
, O
for O
example O
a O
gaussian O
, O
then O
the O
absolute O
value O
of O
ze O
can O
be O
obtained O
. O
5 O
neural O
networks O
in O
chapters O
3 O
and O
4 O
we O
considered O
models O
for B
regression I
and O
classiﬁcation B
that O
com- O
prised O
linear O
combinations O
of O
ﬁxed O
basis O
functions O
. O
consider O
ﬁrst O
a O
regression B
problem O
in O
which O
the O
goal O
is O
to O
predict O
a O
single O
target O
variable O
t O
from O
a O
d-dimensional O
vector O
x O
= O
( O
x1 O
, O
. O
we O
also O
wish O
to O
ﬁnd O
the O
sequence O
of O
latent B
variable I
values O
that O
corresponds O
to O
this O
path O
. O
let O
us O
begin O
by O
ﬁnding O
an O
expression O
for O
the O
conditional B
distribution O
p O
( O
xa|xb O
) O
. O
at O
convergence O
of O
the O
irls O
algorithm O
, O
the O
negative O
hessian O
represents O
the O
inverse B
covariance O
matrix O
for O
the O
gaussian O
approximation O
to O
the O
posterior O
distribution O
. O
i O
have O
tried O
to O
use O
a O
consistent B
notation O
throughout O
the O
book O
, O
although O
at O
times O
this O
means O
departing O
from O
some O
of O
the O
conventions O
used O
in O
the O
corresponding O
re- O
search O
literature O
. O
for O
a O
successful O
application O
of O
neural O
networks O
, O
it O
may O
not O
be O
necessary O
to O
ﬁnd O
the O
global B
minimum I
( O
and O
in O
general O
it O
will O
not O
be O
known O
whether O
the O
global B
minimum I
has O
been O
found O
) O
but O
it O
may O
be O
necessary O
to O
compare O
several O
local B
minima O
in O
order O
to O
ﬁnd O
a O
sufﬁciently O
good O
solution O
. O
in O
particular O
, O
we O
shall O
restrict O
our O
attention O
to O
the O
speciﬁc O
class O
of O
neu- O
ral O
networks O
that O
have O
proven O
to O
be O
of O
greatest O
practical O
value O
, O
namely O
the O
multilayer B
perceptron I
. O
forms O
of O
sequential B
data I
, O
not O
just O
temporal O
sequences O
. O
the O
top O
left O
ﬁgure O
shows O
a O
sample O
of O
60 O
points O
drawn O
from O
a O
joint O
probability B
distri- O
bution O
over O
these O
variables O
. O
11.9 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
by O
making O
use O
of O
the O
technique O
discussed O
in O
section O
11.1.1 O
for O
sampling O
from O
a O
single O
exponential B
distribution I
, O
devise O
an O
algorithm O
for O
sampling O
from O
the O
piecewise O
exponential B
distribution I
deﬁned O
by O
( O
11.17 O
) O
. O
13.17 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
directed B
graph O
for O
the O
input-output O
hidden O
markov O
model O
, O
given O
in O
figure O
13.18 O
, O
can O
be O
expressed O
as O
a O
tree-structured O
factor B
graph I
of O
the O
form O
shown O
in O
figure O
13.15 O
and O
write O
down O
expressions O
for O
the O
initial O
factor O
h O
( O
z1 O
) O
and O
for O
the O
general O
factor O
fn O
( O
zn−1 O
, O
zn O
) O
where O
2 O
( O
cid:1 O
) O
n O
( O
cid:1 O
) O
n. O
13.18 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
using O
the O
result O
of O
exercise O
13.17 O
, O
derive O
the O
recursion O
equations O
, O
includ- O
ing O
the O
initial O
conditions O
, O
for O
the O
forward-backward B
algorithm I
for O
the O
input-output O
hidden O
markov O
model O
shown O
in O
figure O
13.18 O
. O
to O
derive O
this O
, O
we O
note O
that O
we O
are O
seeking O
a O
weight B
vector I
w O
such O
that O
patterns O
xn O
in O
class O
c1 O
will O
have O
wtφ O
( O
xn O
) O
> O
0 O
, O
whereas O
patterns O
xn O
in O
class O
c2 O
have O
wtφ O
( O
xn O
) O
< O
0. O
using O
the O
t O
∈ O
{ O
−1 O
, O
+1 O
} O
target O
coding O
scheme O
it O
follows O
that O
we O
would O
like O
all O
patterns O
to O
satisfy O
wtφ O
( O
xn O
) O
tn O
> O
0. O
the O
perceptron B
criterion I
associates O
zero O
error B
with O
any O
pattern O
that O
is O
correctly O
classiﬁed O
, O
whereas O
for O
a O
mis- O
classiﬁed O
pattern O
xn O
it O
tries O
to O
minimize O
the O
quantity O
−wtφ O
( O
xn O
) O
tn O
. O
although O
we O
shall O
devote O
the O
whole O
of O
chapter O
11 O
to O
a O
detailed O
discussion O
of O
sampling B
methods I
, O
it O
is O
instructive O
to O
outline O
here O
one O
technique O
, O
called O
ancestral B
sampling I
, O
which O
is O
particularly O
relevant O
to O
graphical O
models O
. O
102 O
2. O
probability B
distributions O
figure O
2.14 O
contour O
plot O
of O
the O
normal-gamma B
distribution I
( O
2.154 O
) O
for O
parameter O
values O
µ0 O
= O
0 O
, O
β O
= O
2 O
, O
a O
= O
5 O
and O
b O
= O
6 O
. O
each O
pair O
of O
nodes O
{ O
zn O
, O
xn O
} O
represents O
a O
linear-gaussian O
latent B
variable I
13.3. O
linear O
dynamical O
systems O
637 O
model O
for O
that O
particular O
observation O
. O
samples O
from O
this O
prior B
are O
plotted O
for O
various O
values O
of O
the O
parameters O
θ0 O
, O
. O
not O
surprisingly O
, O
the O
only O
distributions O
that O
have O
this O
property O
of O
being O
closed O
under O
multiplication O
are O
those O
belonging O
to O
the O
exponential B
family I
. O
9.23 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
in O
section O
7.2.1 O
we O
used O
direct O
maximization O
of O
the O
marginal B
like- O
lihood O
to O
derive O
the O
re-estimation O
equations O
( O
7.87 O
) O
and O
( O
7.88 O
) O
for O
ﬁnding O
values O
of O
the O
hyperparameters O
α O
and O
β O
for O
the O
regression B
rvm O
. O
( O
13.6 O
) O
using O
the O
d-separation B
criterion O
, O
we O
see O
that O
there O
is O
always O
a O
path O
connecting O
any O
two O
observed O
variables O
xn O
and O
xm O
via O
the O
latent O
variables O
, O
and O
that O
this O
path O
is O
never O
blocked O
. O
( O
10.192 O
) O
note O
that O
this O
is O
the O
reverse O
form O
of O
kl O
divergence O
compared O
with O
that O
used O
in O
varia- O
tional O
inference B
. O
in O
fact O
, O
this O
set O
of O
states O
might O
even O
represent O
a O
sequence O
having O
zero O
probability B
, O
if O
it O
so O
happens O
that O
two O
successive O
states O
, O
which O
in O
isolation O
are O
individually O
the O
most O
probable O
, O
are O
such O
that O
the O
transition O
matrix O
element O
connecting O
them O
is O
zero O
. O
( O
7.64 O
) O
, O
in O
other O
words O
those O
for O
which O
either O
an O
( O
cid:9 O
) O
= O
0 O
or O
( O
cid:1 O
) O
an O
( O
cid:9 O
) O
= O
0. O
these O
are O
points O
that O
are O
incompatible O
, O
as O
is O
easily O
seen O
by O
adding O
them O
together O
and O
noting O
that O
ξn O
and O
the O
support O
vectors O
are O
those O
data O
points O
that O
contribute O
to O
predictions O
given O
by O
lie O
on O
the O
boundary O
of O
the O
-tube B
or O
outside O
the O
tube O
. O
here O
we O
consider O
brieﬂy O
one O
simple O
generalization B
of O
the O
squared O
loss O
, O
called O
the O
minkowski O
loss O
, O
whose O
expectation B
is O
given O
by O
( O
cid:6 O
) O
( O
cid:6 O
) O
|y O
( O
x O
) O
− O
t|qp O
( O
x O
, O
t O
) O
dx O
dt O
e O
[ O
lq O
] O
= O
( O
1.91 O
) O
which O
reduces O
to O
the O
expected O
squared O
loss O
for O
q O
= O
2. O
the O
function O
|y O
− O
t|q O
is O
plotted O
against O
y O
− O
t O
for O
various O
values O
of O
q O
in O
figure O
1.29. O
the O
minimum O
of O
e O
[ O
lq O
] O
is O
given O
by O
the O
conditional B
mean O
for O
q O
= O
2 O
, O
the O
conditional B
median O
for O
q O
= O
1 O
, O
and O
the O
conditional B
mode O
for O
q O
→ O
0. O
section O
5.6 O
exercise O
1.27 O
1.6. O
information B
theory I
in O
this O
chapter O
, O
we O
have O
discussed O
a O
variety O
of O
concepts O
from O
probability B
theory O
and O
decision B
theory I
that O
will O
form O
the O
foundations O
for O
much O
of O
the O
subsequent O
discussion O
in O
this O
book O
. O
both O
the O
logistic O
error O
and O
the O
hinge O
loss O
can O
be O
viewed O
as O
continuous O
approx- O
imations O
to O
the O
misclassiﬁcation O
error B
. O
write O
down O
the O
analogous O
equations O
for O
the O
conditional B
distribution O
and O
the O
m O
step O
equations O
for O
the O
case O
of O
a O
hidden O
markov O
with O
multiple O
binary O
output O
variables O
each O
of O
which O
is O
governed O
by O
a O
bernoulli O
conditional B
dis- O
tribution O
. O
each O
ﬁgure O
shows O
the O
number O
m O
of O
base O
learners O
trained O
so O
far O
, O
along O
with O
the O
decision B
boundary I
of O
the O
most O
recent O
base O
learner O
( O
dashed O
black O
line O
) O
and O
the O
combined O
decision B
boundary I
of O
the O
en- O
semble O
( O
solid O
green O
line O
) O
. O
use O
the O
results O
( O
2.109 O
) O
and O
( O
2.110 O
) O
to O
ﬁnd O
an O
expression O
for O
the O
marginal B
distribution O
p O
( O
y O
) O
by O
considering O
the O
linear-gaussian O
model O
comprising O
the O
product O
of O
the O
marginal B
distribution O
p O
( O
x O
) O
and O
the O
conditional B
distribution O
p O
( O
y|x O
) O
. O
one O
way O
to O
estimate O
a O
ratio O
of O
partition O
functions O
is O
to O
use O
importance B
sampling I
( O
11.72 O
) O
ze O
zg O
= O
( O
cid:5 O
) O
( O
cid:5 O
) O
from O
a O
distribution O
with O
energy B
function I
g O
( O
z O
) O
( O
cid:5 O
) O
z O
exp O
( O
−e O
( O
z O
) O
) O
( O
cid:5 O
) O
z O
exp O
( O
−g O
( O
z O
) O
) O
z O
exp O
( O
−e O
( O
z O
) O
+ O
g O
( O
z O
) O
) O
exp O
( O
−g O
( O
z O
) O
) O
z O
exp O
( O
−g O
( O
z O
) O
) O
( O
cid:2 O
) O
= O
= O
eg O
( O
z O
) O
[ O
exp O
( O
−e O
+ O
g O
) O
] O
( O
cid:7 O
) O
exp O
( O
−e O
( O
z O
( O
l O
) O
) O
+ O
g O
( O
z O
( O
l O
) O
) O
) O
l O
11.6. O
estimating O
the O
partition B
function I
555 O
where O
{ O
z O
( O
l O
) O
} O
are O
samples O
drawn O
from O
the O
distribution O
deﬁned O
by O
pg O
( O
z O
) O
. O
for O
the O
purposes O
of O
this O
chapter O
, O
we O
shall O
assume O
that O
the O
data O
points O
are O
independent B
and O
identically O
distributed O
. O
graphs O
having O
some O
intermediate O
level O
of O
complexity O
correspond O
to O
joint O
gaus- O
sian O
distributions O
with O
partially O
constrained O
covariance B
matrices O
. O
we O
have O
seen O
that O
the O
joint O
probability B
of O
two O
independent B
events O
is O
given O
by O
the O
product O
of O
the O
marginal B
probabilities O
for O
each O
event O
separately O
. O
the O
principal O
disadvantage O
of O
the O
rvm O
compared O
to O
the O
svm O
is O
that O
training B
involves O
optimizing O
a O
nonconvex O
function O
, O
and O
training B
times O
can O
be O
longer O
than O
for O
a O
comparable O
svm O
. O
the O
predicted O
regression B
curve O
is O
shown O
by O
the O
red O
line O
, O
and O
the O
-insensitive O
tube O
corresponds O
to O
the O
shaded O
region O
. O
instead O
of O
introducing O
a O
set O
of O
basis O
functions O
, O
which O
implicitly O
determines O
an O
equivalent B
kernel I
, O
we O
can O
instead O
deﬁne O
a O
localized O
kernel O
directly O
and O
use O
this O
to O
make O
predictions O
for O
new O
input O
vectors O
x O
, O
given O
the O
observed O
training O
set O
. O
we O
can O
formalize O
such O
issues O
through O
the O
introduction O
of O
a O
loss B
function I
, O
also O
called O
a O
cost B
function I
, O
which O
is O
a O
single O
, O
overall O
measure O
of O
loss O
incurred O
in O
taking O
any O
of O
the O
available O
decisions O
or O
actions O
. O
, O
n O
( O
7.20 O
) O
in O
which O
the O
slack O
variables O
are O
constrained O
to O
satisfy O
ξn O
( O
cid:2 O
) O
0. O
data O
points O
for O
which O
ξn O
= O
0 O
are O
correctly O
classiﬁed O
and O
are O
either O
on O
the O
margin B
or O
on O
the O
correct O
side O
of O
the O
margin B
. O
if O
we O
take O
a O
directed B
or O
undirected B
tree O
and O
convert O
it O
into O
a O
factor B
graph I
, O
then O
the O
result O
will O
again O
be O
a O
tree B
( O
in O
other O
words O
, O
the O
factor B
graph I
will O
have O
no O
loops O
, O
and O
there O
will O
be O
one O
and O
only O
one O
path O
connecting O
any O
two O
nodes O
) O
. O
the O
situation O
after O
the O
ﬁrst O
m O
step O
is O
shown O
in O
plot O
( O
c O
) O
, O
in O
which O
the O
mean B
of O
the O
blue O
gaussian O
has O
moved O
to O
the O
mean B
of O
the O
data O
set O
, O
weighted O
by O
the O
probabilities O
of O
each O
data O
point O
belonging O
to O
the O
blue O
cluster O
, O
in O
other O
words O
it O
has O
moved O
to O
the O
centre O
of O
mass O
of O
the O
blue O
ink O
. O
3.5. O
the O
evidence B
approximation I
in O
a O
fully O
bayesian O
treatment O
of O
the O
linear O
basis O
function O
model O
, O
we O
would O
intro- O
duce O
prior B
distributions O
over O
the O
hyperparameters O
α O
and O
β O
and O
make O
predictions O
by O
marginalizing O
with O
respect O
to O
these O
hyperparameters O
as O
well O
as O
with O
respect O
to O
the O
parameters O
w. O
however O
, O
although O
we O
can O
integrate O
analytically O
over O
either O
w O
or O
over O
the O
hyperparameters O
, O
the O
complete O
marginalization O
over O
all O
of O
these O
variables O
is O
analytically O
intractable O
. O
we O
must O
also O
transmit O
the O
k O
code O
book O
vectors O
µk O
, O
which O
requires O
24k O
bits B
, O
and O
so O
the O
total O
number O
of O
bits B
required O
to O
transmit O
the O
image O
is O
24k O
+ O
n O
log2 O
k O
( O
rounding O
up O
to O
the O
nearest O
integer O
) O
. O
, O
n. O
similarly O
, O
show O
that O
a O
model O
described O
by O
the O
graph O
in O
figure O
13.4 O
in O
which O
there O
are O
n O
nodes O
in O
total O
p O
( O
zn|xn O
) O
p O
( O
zn+1|xn O
) O
p O
( O
xn+1|zn+1 O
) O
p O
( O
zn+1|xn+1 O
) O
exercises O
647 O
z O
figure O
13.23 O
schematic O
illustration O
of O
the O
operation O
of O
the O
particle B
ﬁlter I
for O
a O
one-dimensional O
latent O
space O
. O
to O
see O
that O
each O
node B
will O
always O
receive O
enough O
messages O
to O
be O
able O
to O
send O
out O
a O
message O
, O
we O
can O
use O
a O
simple O
inductive O
argument O
as O
follows O
. O
3.2. O
the O
bias-variance O
decomposition O
so O
far O
in O
our O
discussion O
of O
linear O
models O
for B
regression I
, O
we O
have O
assumed O
that O
the O
form O
and O
number O
of O
basis O
functions O
are O
both O
ﬁxed O
. O
a O
b O
c O
d O
8.10 O
( O
( O
cid:12 O
) O
) O
consider O
the O
directed B
graph O
shown O
in O
figure O
8.54 O
in O
which O
none O
of O
the O
variables O
is O
observed O
. O
3.1.5 O
multiple O
outputs O
so O
far O
, O
we O
have O
considered O
the O
case O
of O
a O
single O
target O
variable O
t. O
in O
some O
applica- O
tions O
, O
we O
may O
wish O
to O
predict O
k O
> O
1 O
target O
variables O
, O
which O
we O
denote O
collectively O
by O
the O
target B
vector I
t. O
this O
could O
be O
done O
by O
introducing O
a O
different O
set O
of O
basis O
func- O
tions O
for O
each O
component O
of O
t O
, O
leading O
to O
multiple O
, O
independent B
regression O
problems O
. O
a O
new O
learning B
algorithm O
for O
blind O
signal O
separa- O
tion O
. O
p O
( O
z O
) O
= O
in O
order O
to O
apply O
rejection B
sampling I
, O
we O
need O
some O
simpler O
distribution O
q O
( O
z O
) O
, O
sometimes O
called O
a O
proposal B
distribution I
, O
from O
which O
we O
can O
readily O
draw O
samples O
. O
provided O
we O
store O
all O
of O
the O
intermediate O
messages O
along O
the O
way O
, O
then O
any O
node B
can O
evaluate O
its O
marginal B
sim- O
ply O
by O
applying O
( O
8.54 O
) O
. O
each O
such O
conditional B
distribution O
will O
be O
conditioned O
only O
on O
the O
parents O
of O
the O
corresponding O
node B
in O
the O
graph O
. O
such O
restric- O
tions O
can O
be O
conveniently O
expressed O
using O
the O
framework O
of O
probabilistic O
graphical O
models O
, O
as O
discussed O
in O
chapter O
8. O
here O
we O
use O
a O
factor B
graph I
representation O
because O
this O
encompasses O
both O
directed B
and O
undirected B
graphs O
. O
( O
13.34 O
) O
( O
13.35 O
) O
the O
quantity O
α O
( O
zn O
) O
represents O
the O
joint O
probability B
of O
observing O
all O
of O
the O
given O
data O
up O
to O
time O
n O
and O
the O
value O
of O
zn O
, O
whereas O
β O
( O
zn O
) O
represents O
the O
conditional B
probability I
of O
all O
future O
data O
from O
time O
n O
+ O
1 O
up O
to O
n O
given O
the O
value O
of O
zn O
. O
finally O
, O
the O
output O
unit O
activations O
are O
transformed O
using O
an O
appropriate O
activation B
function I
to O
give O
a O
set O
of O
network O
outputs O
yk O
. O
14.14 O
( O
( O
cid:12 O
) O
) O
use O
the O
technique O
of O
lagrange O
multipliers O
( O
appendix O
e O
) O
to O
show O
that O
the O
m-step O
re-estimation O
equation O
for O
the O
mixing O
coefﬁcients O
in O
the O
mixture O
of O
linear O
regression B
models O
trained O
by O
maximum B
likelihood I
em O
is O
given O
by O
( O
14.38 O
) O
. O
greedy O
function O
approxi- O
mation B
: O
a O
gradient O
boosting O
machine O
. O
for O
many O
of O
the O
optimization O
algorithms O
used O
for O
network O
training B
, O
such O
as O
conjugate B
gradients O
, O
the O
error B
is O
a O
nonincreasing O
function O
of O
the O
iteration O
index O
. O
12.4.1 O
independent B
component I
analysis I
. O
we O
have O
seen O
that O
the O
hidden O
markov O
model O
can O
be O
viewed O
as O
an O
extension O
of O
the O
mixture B
models O
of O
chapter O
9 O
to O
allow O
for O
sequential O
correlations O
in O
the O
data O
. O
if O
both O
the O
mean B
and O
the O
precision O
are O
unknown O
, O
then O
, O
following O
a O
similar O
line O
of O
reasoning O
to O
the O
univariate O
case O
, O
the O
conjugate B
prior I
is O
given O
by O
p O
( O
µ O
, O
λ|µ0 O
, O
β O
, O
w O
, O
ν O
) O
= O
n O
( O
µ|µ0 O
, O
( O
βλ O
) O
−1 O
) O
w O
( O
λ|w O
, O
ν O
) O
( O
2.157 O
) O
which O
is O
known O
as O
the O
normal-wishart O
or O
gaussian-wishart O
distribution O
. O
the O
covariance B
in O
( O
2.88 O
) O
is O
expressed O
in O
terms O
of O
the O
partitioned B
precision O
matrix O
given O
by O
( O
2.69 O
) O
. O
recall O
from O
figure O
3.5 O
656 O
14. O
combining B
models I
that O
when O
we O
trained O
multiple O
polynomials O
using O
the O
sinusoidal B
data I
, O
and O
then O
aver- O
aged O
the O
resulting O
functions O
, O
the O
contribution O
arising O
from O
the O
variance B
term O
tended O
to O
cancel O
, O
leading O
to O
improved O
predictions O
. O
, O
k. O
we O
can O
conveniently O
group O
these O
together O
using O
vector O
nota- O
tion O
so O
that O
( O
4.14 O
) O
y O
( O
x O
) O
= O
, O
wt O
( O
cid:4 O
) O
x O
can O
then O
be O
written O
as O
4.1. O
discriminant O
functions O
185 O
a O
dummy O
input O
x0 O
= O
1. O
this O
representation O
was O
discussed O
in O
detail O
in O
section O
3.1. O
a O
error B
function I
, O
as O
we O
did O
for B
regression I
in O
chapter O
3. O
consider O
a O
training B
data O
set O
{ O
xn O
, O
tn O
} O
where O
n O
= O
1 O
, O
. O
the O
gradient O
of O
the O
log O
likelihood O
function O
( O
3.11 O
) O
takes O
the O
form O
∇ O
ln O
p O
( O
t|w O
, O
β O
) O
= O
tn O
− O
wtφ O
( O
xn O
) O
φ O
( O
xn O
) O
t. O
( O
3.13 O
) O
( O
cid:27 O
) O
3.1. O
linear O
basis O
function O
models O
141 O
( O
3.9 O
) O
( O
cid:6 O
) O
n O
( O
cid:14 O
) O
n=1 O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
( O
cid:26 O
) O
n=1 O
142 O
3. O
linear O
models O
for B
regression I
setting O
this O
gradient O
to O
zero O
gives O
n O
( O
cid:2 O
) O
n=1 O
solving O
for O
w O
we O
obtain O
( O
cid:22 O
) O
n O
( O
cid:2 O
) O
( O
cid:11 O
) O
−1 O
φtt O
n=1 O
( O
cid:10 O
) O
0 O
= O
tnφ O
( O
xn O
) O
t O
− O
wt O
φ O
( O
xn O
) O
φ O
( O
xn O
) O
t O
( O
cid:23 O
) O
. O
if O
we O
have O
only O
a O
discriminant B
function I
, O
then O
any O
change O
to O
the O
loss B
matrix I
would O
require O
that O
we O
return O
to O
the O
training B
data O
and O
solve O
the O
classiﬁcation B
problem O
afresh O
. O
here O
we O
shall O
assume O
that O
an O
algorithm O
has O
been O
provided O
that O
generates O
pseudo-random B
numbers I
distributed O
uniformly O
over O
( O
0 O
, O
1 O
) O
, O
and O
indeed O
most O
software O
environments O
have O
such O
a O
facility O
built O
in O
. O
consider O
ﬁrst O
a O
simple O
linear O
model O
in O
which O
the O
outputs O
yk O
are O
linear O
combina- O
tions O
of O
the O
input O
variables O
xi O
so O
that O
yk O
= O
wkixi O
( O
5.45 O
) O
together O
with O
an O
error B
function I
that O
, O
for O
a O
particular O
input O
pattern O
n O
, O
takes O
the O
form O
en O
= O
1 O
2 O
k O
( O
ynk O
− O
tnk O
) O
2 O
= O
( O
ynj O
− O
tnj O
) O
xni O
∂en O
∂wji O
( O
5.46 O
) O
( O
5.47 O
) O
where O
ynk O
= O
yk O
( O
xn O
, O
w O
) O
. O
in O
graph O
( O
b O
) O
, O
the O
path O
from O
a O
to O
b O
is O
blocked O
by O
node B
f O
because O
this O
is O
a O
tail-to-tail O
node O
that O
is O
observed O
, O
and O
so O
the O
conditional B
independence I
property O
a O
⊥⊥ O
b O
| O
f O
will O
figure O
8.22 O
illustration O
of O
the O
con- O
cept O
of O
d-separation B
. O
5.2.3 O
use O
of O
gradient O
information O
as O
we O
shall O
see O
in O
section O
5.3 O
, O
it O
is O
possible O
to O
evaluate O
the O
gradient O
of O
an O
error B
function I
efﬁciently O
by O
means O
of O
the O
backpropagation B
procedure O
. O
m2 O
d0 O
m3 O
d O
model O
can O
generate O
a O
variety O
of O
different O
data O
sets O
since O
the O
parameters O
are O
governed O
by O
a O
prior B
probability O
distribution O
, O
and O
for O
any O
choice O
of O
the O
parameters O
there O
may O
be O
random O
noise O
on O
the O
target O
variables O
. O
note O
also O
that O
for O
ﬁnite O
n O
, O
if O
we O
take O
the O
limit O
σ2 O
prior B
has O
inﬁnite O
variance B
then O
the O
posterior O
mean O
( O
2.141 O
) O
reduces O
to O
the O
maximum O
n O
= O
σ2/n O
. O
the O
a O
and O
π O
parameters O
are O
often O
initialized O
either O
uniformly O
or O
randomly O
from O
a O
uniform B
distribution I
( O
respecting O
their O
non-negativity O
and O
summa- O
tion O
constraints O
) O
. O
similarly O
, O
p O
( O
c1|x O
) O
is O
the O
corresponding O
probability B
, O
revised O
using O
bayes O
’ O
theorem O
in O
light O
of O
the O
information O
contained O
in O
the O
x-ray O
. O
we O
see O
that O
each O
component O
linear B
regression I
model O
in O
the O
mixture B
, O
governed O
by O
its O
own O
parameter O
vector O
wk O
, O
is O
ﬁtted O
separately O
to O
the O
whole O
data O
set O
in O
the O
m O
step O
, O
but O
with O
each O
data O
point O
n O
weighted O
by O
the O
responsibility B
γnk O
that O
model O
k O
takes O
for O
that O
data O
point O
. O
here O
we O
give O
a O
very O
general O
treatment O
of O
the O
em O
algorithm O
and O
in O
the O
process O
provide O
a O
proof O
that O
the O
em O
algorithm O
derived O
heuristically O
in O
sections O
9.2 O
and O
9.3 O
for O
gaussian O
mixtures O
does O
indeed O
maximize O
the O
likelihood B
function I
( O
csisz`ar O
and O
tusn`ady O
, O
1984 O
; O
hath- O
away O
, O
1986 O
; O
neal O
and O
hinton O
, O
1999 O
) O
. O
one O
is O
that O
the O
splits O
are O
aligned O
with O
the O
axes O
of O
the O
feature B
space I
, O
which O
may O
be O
very O
suboptimal O
. O
we O
can O
then O
write O
the O
conditional B
distribution O
explicitly O
in O
the O
form O
p O
( O
zn|zn−1 O
, O
a O
) O
= O
zn−1 O
, O
j O
znk O
jk O
. O
note O
that O
this O
model O
is O
the O
dual O
of O
the O
bayesian O
logistic B
regression I
problem O
discussed O
in O
section O
4.5. O
in O
the O
case O
of O
logistic B
regression I
we O
have O
n O
observations O
of O
the O
feature O
vector O
< O
l O
> O
n O
which O
are O
parameterized O
by O
a O
single O
parameter O
vector O
w O
, O
whereas O
in O
the O
latent O
space O
visualization B
model O
there O
is O
a O
single O
latent O
space O
variable O
x O
( O
analogous O
to O
< O
1 O
» O
and O
n O
copies O
of O
the O
latent B
variable I
w O
n O
. O
the O
variance B
of O
the O
estimator O
is O
given O
l=1 O
( O
11.2 O
) O
f O
( O
z O
( O
l O
) O
) O
. O
use O
d-separation B
to O
show O
that O
x2 O
⊥⊥ O
x5 O
| O
x3 O
. O
in O
order O
to O
i O
( O
10.191 O
) O
and O
in O
particular O
we O
shall O
assume O
that O
they O
come O
from O
the O
exponential B
family I
. O
( O
a O
) O
samples O
from O
the O
joint O
distribution O
p O
( O
z O
) O
p O
( O
x|z O
) O
in O
which O
the O
three O
states O
of O
z O
, O
corresponding O
to O
the O
three O
components O
of O
the O
mixture B
, O
are O
depicted O
in O
red O
, O
green O
, O
and O
blue O
, O
and O
( O
b O
) O
the O
corresponding O
samples O
from O
the O
marginal B
distribution O
p O
( O
x O
) O
, O
which O
is O
obtained O
by O
simply O
ignoring O
the O
values O
of O
z O
and O
just O
plotting O
the O
x O
values O
. O
it O
is O
independent B
of O
the O
choice O
of O
prior B
and O
of O
the O
likelihood B
function I
and O
depends O
only O
on O
the O
assumption O
of O
i.i.d O
. O
simi- O
these O
are O
precisely O
the O
messages O
obtained O
using O
belief B
propagation I
in O
which O
mes- O
sages O
from O
variable O
nodes O
to O
factor O
nodes O
have O
been O
folded O
into O
the O
messages O
from O
factor O
nodes O
to O
variable O
nodes O
. O
( O
cid:2 O
) O
i O
= O
elbow O
down O
( O
5.147 O
) O
we O
see O
that O
πj O
is O
therefore O
driven O
towards O
the O
average O
posterior B
probability I
for O
com- O
ponent O
j O
. O
first O
we O
introduce O
a O
node B
for O
each O
of O
the O
random O
variables O
a O
, O
b O
, O
and O
c O
and O
associate O
each O
node B
with O
the O
corresponding O
conditional B
distribution O
on O
the O
right-hand O
side O
of O
figure O
8.1 O
a O
directed B
graphical O
model O
representing O
the O
joint O
probabil- O
ity O
distribution O
over O
three O
variables O
a O
, O
b O
, O
and O
c O
, O
correspond- O
ing O
to O
the O
decomposition O
on O
the O
right-hand O
side O
of O
( O
8.2 O
) O
. O
here O
r O
is O
a O
diagonal B
matrix O
with O
elements O
yn O
( O
1 O
− O
yn O
) O
, O
and O
yn O
is O
the O
output O
of O
the O
logistic B
regression I
model O
for O
input O
vector O
xn O
. O
to O
resolve O
this O
, O
we O
suppose O
that O
an O
initial O
message O
given O
by O
the O
unit O
function O
has O
been O
passed O
across O
every O
link B
in O
each O
direction O
. O
8.3.2 O
factorization B
properties O
we O
now O
seek O
a O
factorization B
rule O
for O
undirected O
graphs O
that O
will O
correspond O
to O
the O
above O
conditional B
independence I
test O
. O
one O
advantage O
of O
svms O
is O
that O
, O
although O
the O
training B
involves O
nonlinear O
optimization O
, O
the O
objective O
function O
is O
convex O
, O
and O
so O
the O
solution O
of O
the O
optimization O
problem O
is O
relatively O
straightforward O
. O
2.37 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
using O
an O
analogous O
procedure O
to O
that O
used O
to O
obtain O
( O
2.126 O
) O
, O
derive O
an O
ex- O
pression O
for O
the O
sequential B
estimation I
of O
the O
covariance B
of O
a O
multivariate O
gaussian O
distribution O
, O
by O
starting O
with O
the O
maximum B
likelihood I
expression O
( O
2.122 O
) O
. O
the O
left-hand O
plot O
shows O
the O
likelihood B
function I
for O
the O
20th O
data O
point O
alone O
, O
and O
the O
middle O
plot O
shows O
the O
resulting O
posterior O
distribution O
that O
has O
now O
absorbed O
information O
from O
all O
20 O
observations O
. O
the O
adaptive B
rejection I
sampling I
framework O
can O
also O
be O
extended B
to O
distri- O
butions O
that O
are O
not O
log O
concave O
, O
simply O
by O
following O
each O
rejection B
sampling I
step O
with O
a O
metropolis-hastings O
step O
( O
to O
be O
discussed O
in O
section O
11.2.2 O
) O
, O
giving O
rise O
to O
adaptive O
rejection O
metropolis O
sampling O
( O
gilks O
et O
al. O
, O
1995 O
) O
. O
note O
that O
this O
procedure O
can O
not O
be O
applied O
if O
we O
have O
learned O
a O
discriminant B
function I
directly O
instead O
of O
determining O
posterior O
probabilities O
. O
to O
do O
this O
, O
we O
shall O
make O
use O
of O
the O
derivative B
of O
the O
logistic O
sig- O
moid O
function O
, O
which O
can O
conveniently O
be O
expressed O
in O
terms O
of O
the O
sigmoid B
function O
itself O
( O
cid:10 O
) O
( O
cid:11 O
) O
= O
σ O
( O
1 O
− O
σ O
) O
. O
indeed O
, O
the O
class- O
conditional B
densities O
may O
contain O
a O
lot O
of O
structure O
that O
has O
little O
effect O
on O
the O
pos- O
terior O
probabilities O
, O
as O
illustrated O
in O
figure O
1.27. O
there O
has O
been O
much O
interest O
in O
exploring O
the O
relative B
merits O
of O
generative O
and O
discriminative O
approaches O
to O
machine O
learning O
, O
and O
in O
ﬁnding O
ways O
to O
combine O
them O
( O
jebara O
, O
2004 O
; O
lasserre O
et O
al. O
, O
2006 O
) O
. O
the O
marginal B
distribution O
over O
the O
data O
set O
is O
given O
by O
p O
( O
x O
) O
= O
p O
( O
x|h O
) O
p O
( O
h O
) O
. O
xd O
inputs O
x1 O
x0 O
hidden O
units O
zm O
( O
1 O
) O
m O
d O
w O
( O
2 O
) O
km O
w O
yk O
outputs O
y1 O
( O
2 O
) O
10 O
w O
z1 O
z0 O
and O
follows O
the O
same O
considerations O
as O
for O
linear O
models O
discussed O
in O
chapters O
3 O
and O
4. O
thus O
for O
standard O
regression B
problems O
, O
the O
activation B
function I
is O
the O
identity O
so O
that O
yk O
= O
ak O
. O
implicit O
in O
the O
bayesian O
model B
comparison I
framework O
is O
the O
assumption O
that O
the O
true O
distribution O
from O
which O
the O
data O
are O
generated O
is O
contained O
within O
the O
set O
of O
models O
under O
consideration O
. O
in O
this O
case O
, O
the O
lower B
bound I
will O
equal O
the O
log O
likelihood O
, O
as O
illustrated O
in O
figure O
9.12. O
in O
the O
subsequent O
m O
step O
, O
the O
distribution O
q O
( O
z O
) O
is O
held O
ﬁxed O
and O
the O
lower B
bound I
l O
( O
q O
, O
θ O
) O
is O
maximized O
with O
respect O
to O
θ O
to O
give O
some O
new O
value O
θnew O
. O
since O
then O
, O
there O
has O
been O
considerable O
interest O
in O
this O
topic O
, O
both O
in O
terms O
of O
theory B
and O
applications O
. O
in O
addition O
there O
are O
distributions O
for O
which O
neither O
directed B
nor O
undirected B
graphs O
offer O
a O
perfect B
map I
. O
however O
, O
problems O
with O
least O
squares O
can O
be O
more O
severe O
than O
simply O
lack O
of O
robustness B
, O
as O
illustrated O
in O
figure O
4.5. O
this O
shows O
a O
synthetic O
data O
set O
drawn O
from O
three O
classes O
in O
a O
two-dimensional O
input O
space O
( O
x1 O
, O
x2 O
) O
, O
having O
the O
property O
that O
lin- O
ear O
decision O
boundaries O
can O
give O
excellent O
separation O
between O
the O
classes O
. O
φ O
φtφ O
( O
3.17 O
) O
is O
known O
as O
the O
moore-penrose O
pseudo-inverse B
of O
the O
matrix O
φ O
( O
rao O
and O
mitra O
, O
1971 O
; O
golub O
and O
van O
loan O
, O
1996 O
) O
. O
lagrange O
made O
key O
contributions O
to O
the O
calculus B
of I
variations I
and O
the O
foundations O
of O
dynamics O
. O
the O
conditional B
distribution O
p O
( O
y|x1 O
, O
. O
by O
denoting O
the O
maximum O
value O
of O
the O
index O
j O
by O
m O
− O
1 O
, O
the O
total O
number O
of O
parameters O
in O
this O
model O
will O
be O
m. O
j=1 O
the O
parameter O
w0 O
allows O
for O
any O
ﬁxed O
offset O
in O
the O
data O
and O
is O
sometimes O
called O
a O
bias B
parameter I
( O
not O
to O
be O
confused O
with O
‘ O
bias B
’ O
in O
a O
statistical O
sense O
) O
. O
the O
failure O
of O
least O
squares O
should O
not O
surprise O
us O
when O
we O
recall O
that O
it O
cor- O
responds O
to O
maximum B
likelihood I
under O
the O
assumption O
of O
a O
gaussian O
conditional B
distribution O
, O
whereas O
binary O
target O
vectors O
clearly O
have O
a O
distribution O
that O
is O
far O
from O
gaussian O
. O
such O
a O
graph O
will O
have O
more O
than O
one O
node B
with O
the O
property O
of O
having O
no O
parents O
, O
and O
furthermore O
, O
the O
corresponding O
moralized O
undirected B
graph I
will O
have O
loops O
. O
to O
do O
this O
, O
we O
create O
variable O
nodes O
corresponding O
to O
the O
nodes O
in O
the O
original O
undirected B
graph I
, O
and O
then O
create O
addi- O
tional O
factor O
nodes O
corresponding O
to O
the O
maximal O
cliques O
xs O
. O
because O
θ O
represents O
a O
set O
of O
hyperparameters O
for O
the O
regression B
problem O
, O
this O
can O
be O
viewed O
as O
analogous O
to O
the O
type O
2 O
maximum O
like- O
lihood O
procedure O
for O
linear O
regression B
models O
. O
any O
probability B
density O
p O
( O
θ O
) O
deﬁned O
over O
θ O
must O
not O
only O
be O
nonnegative O
and O
integrate O
2.3. O
the O
gaussian O
distribution O
107 O
figure O
2.18 O
the O
von O
mises O
distribution O
can O
be O
derived O
by O
considering O
a O
two-dimensional O
gaussian O
of O
the O
form O
( O
2.173 O
) O
, O
whose O
density B
contours O
are O
shown O
in O
blue O
and O
conditioning O
on O
the O
unit O
circle O
shown O
in O
red O
. O
again O
, O
it O
is O
convenient O
to O
use O
a O
1-of-k O
coding O
scheme O
, O
as O
used O
for O
mixture O
models O
in O
chapter O
9. O
we O
now O
allow O
the O
probability B
distribution O
of O
zn O
to O
depend O
on O
the O
state O
of O
the O
previous O
latent B
variable I
zn−1 O
through O
a O
conditional B
distribution O
p O
( O
zn|zn−1 O
) O
. O
for O
distributions O
deﬁned O
in O
terms O
of O
a O
graphical B
model I
, O
we O
can O
apply O
the O
impor- O
tance O
sampling O
technique O
in O
various O
ways O
. O
( O
2.29 O
) O
k O
( O
cid:14 O
) O
k=1 O
we O
see O
that O
the O
likelihood B
function I
depends O
on O
the O
n O
data O
points O
only O
through O
the O
k O
quantities O
mk O
= O
xnk O
n O
( O
2.30 O
) O
which O
represent O
the O
number O
of O
observations O
of O
xk O
= O
1. O
these O
are O
called O
the O
sufﬁcient B
statistics I
for O
this O
distribution O
. O
2.38 O
( O
( O
cid:12 O
) O
) O
use O
the O
technique O
of O
completing B
the I
square I
for O
the O
quadratic O
form O
in O
the O
expo- O
nent O
to O
derive O
the O
results O
( O
2.141 O
) O
and O
( O
2.142 O
) O
. O
this O
represents O
a O
weighted B
least I
squares I
14.5. O
conditional O
mixture O
models O
669 O
problem O
, O
in O
which O
the O
term O
corresponding O
to O
the O
nth O
data O
point O
carries O
a O
weighting O
coefﬁcient O
given O
by O
βγnk O
, O
which O
could O
be O
interpreted O
as O
an O
effective O
precision O
for O
each O
data O
point O
. O
in O
particular O
, O
we O
shall O
shortly O
generalize O
the O
message B
passing I
formalism O
derived O
above O
for O
chains O
to O
give O
the O
sum-product B
algorithm I
, O
which O
provides O
an O
efﬁcient O
framework O
for O
exact O
inference B
in O
tree-structured O
graphs O
. O
, O
xm O
, O
we O
can O
model O
the O
joint O
distribution O
using O
a O
directed B
graph O
with O
one O
variable O
corresponding O
to O
each O
node B
. O
( O
5.133 O
) O
thus O
, O
to O
leading O
order O
in O
ξ O
, O
the O
ﬁrst O
term O
in O
the O
regularizer O
vanishes O
and O
we O
are O
left O
with O
p O
( O
x O
) O
dx O
( O
5.134 O
) O
( O
cid:6 O
) O
( O
cid:10 O
) O
( O
cid:11 O
) O
2 O
ω O
= O
1 O
2 O
τ O
t∇y O
( O
x O
) O
exercise O
5.27 O
which O
is O
equivalent O
to O
the O
tangent B
propagation I
regularizer O
( O
5.128 O
) O
. O
from O
our O
earlier O
discussion O
of O
locality O
, O
let O
us O
consider O
some O
small O
region O
r O
containing O
x. O
the O
probability B
mass O
associated O
with O
this O
region O
is O
given O
by O
( O
cid:6 O
) O
p O
= O
r O
p O
( O
x O
) O
dx O
. O
denote O
the O
coefﬁcients O
of O
this O
linear O
combination O
by O
αn O
and O
derive O
a O
formulation O
of O
the O
perceptron B
learning O
algorithm O
, O
and O
the O
predictive O
function O
for O
the O
perceptron B
, O
in O
terms O
of O
the O
αn O
. O
2.44 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
univariate O
gaussian O
distribution O
n O
( O
x|µ O
, O
τ O
−1 O
) O
having O
conjugate B
gaussian-gamma O
prior B
given O
by O
( O
2.154 O
) O
, O
and O
a O
data O
set O
x O
= O
{ O
x1 O
, O
. O
this O
would O
, O
however O
, O
discard O
all O
conditional B
independence I
properties O
and O
so O
would O
be O
vacuous O
. O
thus O
, O
points O
that O
have O
a O
signiﬁcant O
probability B
for O
belonging O
to O
either O
cluster O
appear O
purple O
. O
although O
we O
have O
considered O
each O
node B
to O
correspond O
to O
a O
single O
variable O
, O
we O
can O
equally O
well O
associate O
sets O
of O
variables O
and O
vector-valued O
variables O
with O
the O
nodes O
of O
a O
graph O
. O
11.16 O
( O
( O
cid:12 O
) O
) O
by O
making O
use O
of O
( O
11.56 O
) O
, O
( O
11.57 O
) O
, O
and O
( O
11.63 O
) O
, O
show O
that O
the O
conditional B
dis- O
tribution O
p O
( O
r|z O
) O
is O
a O
gaussian O
. O
the O
markov O
blanket O
for O
an O
undirected B
graph I
takes O
a O
particularly O
simple O
form O
, O
because O
a O
node B
will O
be O
conditionally O
independent B
of O
all O
other O
nodes O
conditioned O
only O
on O
the O
neighbouring O
nodes O
, O
as O
illustrated O
in O
figure O
8.28 O
. O
if O
the O
goal O
is O
sequence O
classiﬁca- O
tion O
, O
there O
can O
be O
signiﬁcant O
beneﬁt O
in O
determining O
the O
parameters O
of O
hidden O
markov O
models O
using O
discriminative O
rather O
than O
maximum B
likelihood I
techniques O
. O
14.5.1 O
mixtures O
of O
linear B
regression I
models O
. O
it O
is O
important O
to O
distinguish O
between O
lossless B
data I
compression I
, O
in O
which O
the O
goal O
is O
to O
be O
able O
to O
reconstruct O
the O
original O
data O
exactly O
from O
the O
compressed O
representation O
, O
and O
lossy B
data I
compression I
, O
in O
which O
we O
accept O
some O
errors O
in O
the O
reconstruction O
in O
return O
for O
higher O
levels O
of O
compression O
than O
can O
be O
achieved O
in O
the O
lossless O
case O
. O
this O
leads O
to O
the O
sim- O
pliﬁed O
factor B
graph I
representation O
in O
figure O
13.15 O
, O
in O
which O
the O
factors O
are O
given O
by O
h O
( O
z1 O
) O
= O
p O
( O
z1 O
) O
p O
( O
x1|z1 O
) O
fn O
( O
zn−1 O
, O
zn O
) O
= O
p O
( O
zn|zn−1 O
) O
p O
( O
xn|zn O
) O
. O
the O
use O
of O
a O
support B
vector I
machine I
to O
solve O
a O
regression B
problem O
is O
illustrated O
using O
the O
sinusoidal B
data I
set O
in O
figure O
7.8. O
here O
the O
parameters O
ν O
and O
c O
have O
been O
chosen O
by O
hand O
. O
( O
cid:6 O
) O
1 O
( O
cid:6 O
) O
1 O
0 O
0 O
( O
2.19 O
) O
using O
the O
result O
( O
2.18 O
) O
for O
the O
posterior O
distribution O
p O
( O
µ|d O
) O
, O
together O
with O
the O
result O
( O
2.15 O
) O
for O
the O
mean B
of O
the O
beta B
distribution I
, O
we O
obtain O
m O
+ O
a O
p O
( O
x O
= O
1|d O
) O
= O
( O
2.20 O
) O
m O
+ O
a O
+ O
l O
+ O
b O
which O
has O
a O
simple O
interpretation O
as O
the O
total O
fraction O
of O
observations O
( O
both O
real O
ob- O
servations O
and O
ﬁctitious O
prior B
observations O
) O
that O
correspond O
to O
x O
= O
1. O
note O
that O
in O
the O
limit O
of O
an O
inﬁnitely O
large O
data O
set O
m O
, O
l O
→ O
∞ O
the O
result O
( O
2.20 O
) O
reduces O
to O
the O
maximum B
likelihood I
result O
( O
2.8 O
) O
. O
now O
assume O
that O
the O
prior B
is O
broad O
so O
that O
v O
is O
small O
and O
the O
second O
term O
on O
the O
right-hand O
side O
above O
can O
be O
neglected O
. O
from O
the O
deﬁnition O
of O
a O
gaussian O
process O
, O
the O
marginal B
distribution O
p O
( O
y O
) O
is O
given O
by O
a O
gaussian O
whose O
mean B
is O
zero O
and O
whose O
covariance B
is O
deﬁned O
by O
a O
gram O
matrix O
k O
so O
that O
p O
( O
y O
) O
= O
n O
( O
y|0 O
, O
k O
) O
. O
this O
is O
in O
contrast O
to O
rejection B
sampling I
, O
where O
re- O
jected O
samples O
are O
simply O
discarded O
. O
( O
10.223 O
) O
( O
10.224 O
) O
where O
b O
= O
( O
mnew O
) O
tmnew O
v O
examples O
factor O
approximations O
for O
the O
clutter B
problem I
with O
a O
one-dimensional O
pa- O
rameter O
space O
θ O
are O
shown O
in O
figure O
10.16. O
note O
that O
the O
factor O
approximations O
can O
have O
inﬁnite O
or O
even O
negative O
values O
for O
the O
‘ O
variance B
’ O
parameter O
vn O
. O
( O
1.67 O
) O
again O
we O
can O
ﬁrst O
determine O
the O
parameter O
vector O
wml O
governing O
the O
mean B
and O
sub- O
sequently O
use O
this O
to O
ﬁnd O
the O
precision O
βml O
as O
was O
the O
case O
for O
the O
simple O
gaussian O
distribution O
. O
em O
optimization O
of O
latent B
variable I
den- O
sity O
models O
. O
figure O
11.16 O
a O
graph O
involving O
an O
observed O
gaussian O
variable O
x O
with O
prior B
distributions O
over O
its O
mean B
µ O
and O
precision O
τ. O
µ O
τ O
x O
558 O
11. O
sampling B
methods I
11.17 O
( O
( O
cid:12 O
) O
) O
www O
verify O
that O
the O
two O
probabilities O
( O
11.68 O
) O
and O
( O
11.69 O
) O
are O
equal O
, O
and O
hence O
that O
detailed O
balance O
holds O
for O
the O
hybrid O
monte O
carlo O
algorithm O
. O
, O
l. O
here O
a O
model O
refers O
to O
a O
probability B
distribution O
over O
the O
observed O
data O
d. O
in O
the O
case O
of O
the O
polynomial O
curve-ﬁtting O
problem O
, O
the O
distribution O
is O
deﬁned O
over O
the O
set O
of O
target O
values O
t O
, O
while O
the O
set O
of O
input O
values O
x O
is O
assumed O
to O
be O
known O
. O
once O
all O
the O
classiﬁers O
have O
been O
trained O
, O
their O
predictions O
are O
then O
combined O
through O
a O
weighted O
majority O
voting O
scheme O
, O
as O
illustrated O
schematically O
in O
figure O
14.1. O
consider O
a O
two-class O
classiﬁcation B
problem O
, O
in O
which O
the O
training B
data O
comprises O
input O
vectors O
x1 O
, O
. O
1.6.1 O
relative B
entropy I
and O
mutual B
information I
so O
far O
in O
this O
section O
, O
we O
have O
introduced O
a O
number O
of O
concepts O
from O
information B
theory I
, O
including O
the O
key O
notion O
of O
entropy B
. O
an O
alternative O
approach O
to O
determining O
a O
suitable O
value O
for O
k O
is O
to O
treat O
the O
mixing O
coefﬁcients O
π O
as O
parameters O
and O
make O
point O
estimates O
of O
their O
values O
by O
maximizing O
the O
lower B
bound I
( O
corduneanu O
and O
bishop O
, O
2001 O
) O
with O
respect O
to O
π O
instead O
of O
maintaining O
a O
probability B
distribution O
over O
them O
as O
in O
the O
fully O
bayesian O
approach O
. O
as O
with O
the O
sum-product B
algorithm I
, O
we O
ﬁrst O
represent O
the O
hidden O
markov O
model O
as O
a O
factor B
graph I
, O
as O
shown O
in O
figure O
13.15. O
again O
, O
we O
treat O
the O
variable O
node B
zn O
as O
the O
root O
, O
and O
pass O
messages O
to O
the O
root O
starting O
with O
the O
leaf O
nodes O
. O
in O
a O
probabilistic B
graphical I
model I
, O
each O
node B
represents O
a O
random O
variable O
( O
or O
group O
of O
random O
variables O
) O
, O
and O
the O
links O
express O
probabilistic O
relation- O
ships O
between O
these O
variables O
. O
given O
a O
set O
of O
independent B
observations O
of O
x O
and O
t O
, O
write O
down O
the O
error B
function I
that O
must O
be O
minimized O
in O
order O
to O
ﬁnd O
the O
maximum B
likelihood I
solution O
for O
w O
, O
if O
we O
assume O
that O
σ O
is O
ﬁxed O
and O
known O
. O
( O
9.83 O
) O
nents O
have O
the O
same O
mean B
µk O
= O
( O
cid:1 O
) O
µ O
for O
k O
= O
1 O
, O
. O
k O
= O
1 O
k O
= O
2 O
k O
= O
3 O
n O
− O
2 O
n O
− O
1 O
n O
n O
+ O
1 O
corresponding O
to O
the O
graph O
shown O
in O
figure O
8.38. O
suppose O
we O
take O
node B
xn O
to O
be O
the O
root B
node I
. O
( O
10.24 O
) O
exercise O
10.6 O
section O
2.3.6 O
exercise O
2.44 O
10.1. O
variational B
inference I
471 O
note O
that O
the O
true O
posterior O
distribution O
does O
not O
factorize O
in O
this O
way O
. O
from O
( O
10.188 O
) O
the O
posterior O
is O
given O
by O
p O
( O
θ|d O
) O
= O
fi O
( O
θ O
) O
( O
10.189 O
) O
( O
cid:14 O
) O
i O
1 O
p O
( O
d O
) O
( O
cid:6 O
) O
( O
cid:14 O
) O
i O
and O
the O
model B
evidence I
is O
given O
by O
p O
( O
d O
) O
= O
fi O
( O
θ O
) O
dθ O
. O
these O
two O
error B
functions O
are O
compared O
in O
figure O
14.4 O
. O
( O
5.128 O
) O
n O
k O
i=1 O
where O
λ O
is O
a O
regularization B
coefﬁcient O
and O
( O
cid:22 O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
n O
k O
ω O
= O
1 O
2 O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
∂ynk O
∂ξ O
ξ=0 O
= O
1 O
2 O
exercise O
5.26 O
the O
regularization B
function O
will O
be O
zero O
when O
the O
network O
mapping O
function O
is O
in- O
variant O
under O
the O
transformation O
in O
the O
neighbourhood O
of O
each O
pattern O
vector O
, O
and O
the O
value O
of O
the O
parameter O
λ O
determines O
the O
balance O
between O
ﬁtting O
the O
training B
data O
and O
learning B
the O
invariance B
property O
. O
( O
14.36 O
) O
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
n=1 O
k=1 O
( O
cid:26 O
) O
( O
cid:27 O
) O
exercise O
14.12 O
exercise O
14.13 O
668 O
14. O
combining B
models I
figure O
14.7 O
probabilistic O
directed O
graph O
representing O
a O
mixture O
of O
linear O
regression B
models O
, O
deﬁned O
by O
( O
14.35 O
) O
. O
( O
13.116 O
) O
exercise O
13.33 O
exercise O
13.34 O
644 O
13. O
sequential B
data I
we O
have O
approached O
parameter O
learning O
in O
the O
linear B
dynamical I
system I
using O
maximum B
likelihood I
. O
we O
ﬁrst O
note O
from O
( O
4.46 O
) O
that O
sb O
is O
composed O
of O
the O
sum O
of O
k O
ma- O
trices O
, O
each O
of O
which O
is O
an O
outer O
product O
of O
two O
vectors O
and O
therefore O
of O
rank O
1. O
in O
addition O
, O
only O
( O
k O
− O
1 O
) O
of O
these O
matrices O
are O
independent B
as O
a O
result O
of O
the O
constraint O
( O
4.44 O
) O
. O
an O
alternative O
bound O
for O
the O
multiclass B
case O
has O
been O
explored O
by O
gibbs O
( O
1997 O
) O
. O
( O
2.280 O
) O
( O
2.281 O
) O
( O
2.282 O
) O
by O
performing O
a O
variational B
maximization O
of O
( O
2.279 O
) O
and O
using O
lagrange O
multipliers O
to O
enforce O
the O
constraints O
( O
2.280 O
) O
, O
( O
2.281 O
) O
, O
and O
( O
2.282 O
) O
, O
show O
that O
the O
maximum B
likelihood I
distribution O
is O
given O
by O
the O
gaussian O
( O
2.43 O
) O
. O
the O
goal O
of O
the O
pac O
framework O
is O
to O
understand O
how O
large O
a O
data O
set O
needs O
to O
be O
in O
order O
to O
give O
good O
generalization B
. O
we O
could O
make O
it O
generative O
by O
introducing O
a O
suitable O
prior B
distribution O
p O
( O
x O
) O
, O
at O
the O
expense O
of O
a O
more O
complex O
model O
. O
in O
order O
to O
ﬁnd O
the O
marginal B
distribution O
p O
( O
t O
) O
, O
conditioned O
on O
the O
input O
values O
x1 O
, O
. O
next O
the O
triangulated B
graph I
is O
used O
to O
construct O
a O
new O
tree-structured O
undirected B
graph I
called O
a O
join B
tree I
, O
whose O
nodes O
correspond O
to O
the O
maximal O
cliques O
of O
the O
triangulated B
graph I
, O
and O
whose O
links O
connect O
pairs O
of O
cliques O
that O
have O
vari- O
ables O
in O
common O
. O
for O
example O
, O
the O
conjugate B
prior I
for O
the O
parameters O
of O
the O
multinomial B
distribution I
is O
called O
the O
dirichlet O
distribution O
, O
while O
the O
conjugate B
prior I
for O
the O
mean B
of O
a O
gaussian O
is O
another O
gaussian O
. O
in O
contrast O
to O
the O
svm O
, O
there O
is O
no O
restriction O
to O
positive- O
deﬁnite O
kernels O
, O
nor O
are O
the O
basis O
functions O
tied O
in O
either O
number O
or O
location O
to O
the O
training B
data O
points O
. O
we O
ﬁrst O
convert O
the O
original O
graph O
into O
a O
factor B
graph I
so O
that O
we O
can O
deal O
with O
both O
directed B
and O
undirected B
models O
using O
the O
same O
framework O
. O
although O
there O
are O
various O
alternative O
ways O
to O
express O
such O
variables O
, O
we O
shall O
see O
shortly O
that O
a O
particularly O
convenient O
represen- O
tation O
is O
the O
1-of-k O
scheme O
in O
which O
the O
variable O
is O
represented O
by O
a O
k-dimensional O
vector O
x O
in O
which O
one O
of O
the O
elements O
xk O
equals O
1 O
, O
and O
all O
remaining O
elements O
equal O
2.2. O
multinomial O
variables O
75 O
k O
( O
cid:14 O
) O
k=1 O
k O
( O
cid:2 O
) O
k=1 O
k O
( O
cid:14 O
) O
( O
cid:2 O
) O
k=1 O
( O
cid:2 O
) O
( O
cid:2 O
) O
x O
n O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
n=1 O
k=1 O
0. O
so O
, O
for O
instance O
if O
we O
have O
a O
variable O
that O
can O
take O
k O
= O
6 O
states O
and O
a O
particular O
observation O
of O
the O
variable O
happens O
to O
correspond O
to O
the O
state O
where O
x3 O
= O
1 O
, O
then O
x O
will O
be O
represented O
by O
note O
that O
such O
vectors O
satisfy O
by O
the O
parameter O
µk O
, O
then O
the O
distribution O
of O
x O
is O
given O
x O
= O
( O
0 O
, O
0 O
, O
1 O
, O
0 O
, O
0 O
, O
0 O
) O
t. O
( O
2.25 O
) O
k=1 O
xk O
= O
1. O
if O
we O
denote O
the O
probability B
of O
xk O
= O
1 O
( O
cid:5 O
) O
k O
p O
( O
x|µ O
) O
= O
µxk O
k O
( O
2.26 O
) O
( O
cid:5 O
) O
where O
µ O
= O
( O
µ1 O
, O
. O
if O
data O
is O
plentiful O
, O
then O
one O
approach O
is O
simply O
to O
use O
some O
of O
the O
available O
data O
to O
train O
a O
range O
of O
models O
, O
or O
a O
given O
model O
with O
a O
range O
of O
values O
for O
its O
complexity O
parameters O
, O
and O
then O
to O
compare O
them O
on O
independent B
data O
, O
sometimes O
called O
a O
validation B
set I
, O
and O
select O
the O
one O
having O
the O
best O
predictive O
performance O
. O
this O
conditional B
independence I
property O
can O
be O
expressed O
as O
p O
( O
xi O
, O
xj|x\ O
{ O
i O
, O
j O
} O
) O
= O
p O
( O
xi|x\ O
{ O
i O
, O
j O
} O
) O
p O
( O
xj|x\ O
{ O
i O
, O
j O
} O
) O
( O
8.38 O
) O
where O
x\ O
{ O
i O
, O
j O
} O
denotes O
the O
set O
x O
of O
all O
variables O
with O
xi O
and O
xj O
removed O
. O
also O
, O
the O
signed O
orthogonal O
distance O
of O
a O
gen- O
eral O
point O
x O
from O
the O
decision B
surface I
is O
given O
by O
y O
( O
x O
) O
/ O
( O
cid:3 O
) O
w O
( O
cid:3 O
) O
. O
these O
partitioned B
matrices O
are O
related O
by O
( O
2.88 O
) O
( O
2.89 O
) O
( O
2.90 O
) O
( O
cid:16 O
) O
−1 O
λaa O
λab O
λba O
λbb O
= O
( O
cid:15 O
) O
( O
cid:10 O
) O
σaa O
σab O
σba O
σbb O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:11 O
) O
−1 O
= O
σaa O
. O
we O
now O
give O
a O
powerful O
interpretation O
of O
this O
calculation O
in O
terms O
of O
the O
passing O
of O
local B
messages O
around O
on O
the O
graph O
. O
( O
10.90 O
) O
this O
can O
be O
represented O
as O
a O
directed B
graphical O
model O
as O
shown O
in O
figure O
10.8 O
. O
if O
a O
box O
is O
chosen O
at O
random O
with O
probabilities O
p O
( O
r O
) O
= O
0.2 O
, O
p O
( O
b O
) O
= O
0.2 O
, O
p O
( O
g O
) O
= O
0.6 O
, O
and O
a O
piece O
of O
fruit O
is O
removed O
from O
the O
box O
( O
with O
equal O
probability B
of O
selecting O
any O
of O
the O
items O
in O
the O
box O
) O
, O
then O
what O
is O
the O
probability B
of O
selecting O
an O
apple O
? O
if O
we O
observe O
that O
the O
selected O
fruit O
is O
in O
fact O
an O
orange O
, O
what O
is O
the O
probability B
that O
it O
came O
from O
the O
green O
box O
? O
1.4 O
( O
( O
cid:1 O
) O
( O
cid:1 O
) O
) O
www O
consider O
a O
probability B
density O
px O
( O
x O
) O
deﬁned O
over O
a O
continuous O
vari- O
able O
x O
, O
and O
suppose O
that O
we O
make O
a O
nonlinear O
change O
of O
variable O
using O
x O
= O
g O
( O
y O
) O
, O
so O
that O
the O
density B
transforms O
according O
to O
( O
1.27 O
) O
. O
10.4.1 O
variational B
message O
passing O
. O
when O
data O
is O
particularly O
scarce O
, O
it O
may O
be O
appropriate O
to O
consider O
the O
case O
s O
= O
n O
, O
where O
n O
is O
the O
total O
number O
of O
data O
points O
, O
which O
gives O
the O
leave-one-out B
technique O
. O
we O
wish O
to O
ascertain O
whether O
a O
particular O
conditional B
independence I
statement O
a O
⊥⊥ O
b O
| O
c O
is O
implied O
by O
a O
given O
directed B
acyclic I
graph I
. O
to O
start O
with O
, O
we O
shall O
assume O
that O
all O
classes O
share O
the O
same O
covariance B
matrix I
. O
fast O
training O
of O
support B
vector I
machines O
using O
sequential B
minimal I
optimization I
. O
( O
10.234 O
) O
( O
10.235 O
) O
516 O
10. O
approximate O
inference B
section O
8.4.4 O
µfb→x2 O
( O
x2 O
) O
sent O
by O
factor O
node O
fb O
to O
variable O
node B
x2 O
and O
is O
given O
by O
( O
8.81 O
) O
. O
7.4 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
value O
ρ O
of O
the O
margin B
for O
the O
maximum-margin O
hyper- O
plane O
is O
given O
by O
n O
( O
cid:2 O
) O
1 O
ρ2 O
= O
( O
7.123 O
) O
where O
{ O
an O
} O
are O
given O
by O
maximizing O
( O
7.10 O
) O
subject O
to O
the O
constraints O
( O
7.11 O
) O
and O
( O
7.12 O
) O
. O
( O
14.55 O
) O
in O
order O
to O
ensure O
that O
the O
predictions O
ycom O
( O
x O
) O
remain O
within O
sensible O
limits O
, O
sup- O
pose O
that O
we O
require O
that O
they O
be O
bounded O
at O
each O
value O
of O
x O
by O
the O
minimum O
and O
maximum O
values O
given O
by O
any O
of O
the O
members O
of O
the O
committee B
, O
so O
that O
m=1 O
ymin O
( O
x O
) O
( O
cid:1 O
) O
ycom O
( O
x O
) O
( O
cid:1 O
) O
ymax O
( O
x O
) O
. O
because O
we O
are O
considering O
a O
sum-of-squares B
error I
function O
, O
we O
have O
the O
fol- O
lowing O
standard O
backpropagation O
expressions O
: O
δk O
= O
yk O
− O
tk O
( O
cid:4 O
) O
( O
aj O
) O
δj O
= O
h O
( O
cid:2 O
) O
wkjδk O
. O
, O
xn O
each O
of O
which O
has O
a O
uniform B
distribution I
over O
the O
interval O
[ O
0 O
, O
1 O
] O
and O
then O
considering O
the O
distribution O
of O
the O
mean B
( O
x1 O
+ O
··· O
+ O
xn O
) O
/n O
. O
the O
gaussian O
process O
regression B
model O
gives O
a O
predictive B
distribution I
whose O
mean B
and O
variance B
are O
functions O
of O
the O
input O
vector O
x. O
however O
, O
we O
have O
assumed O
that O
the O
contribution O
to O
the O
predictive O
variance O
arising O
from O
the O
additive O
noise O
, O
gov- O
erned O
by O
the O
parameter O
β O
, O
is O
a O
constant O
. O
10.3. O
variational B
linear O
regression B
exercise O
10.26 O
as O
a O
second O
illustration O
of O
variational B
inference I
, O
we O
return O
to O
the O
bayesian O
linear B
regression I
model O
of O
section O
3.3. O
in O
the O
evidence O
framework O
, O
we O
approximated O
the O
integration O
over O
α O
and O
β O
by O
making O
point O
estimates O
obtained O
by O
maximizing O
the O
log O
marginal O
likelihood O
. O
the O
message B
passing I
steps O
( O
8.66 O
) O
and O
( O
8.69 O
) O
are O
then O
applied O
recursively O
until O
messages O
have O
been O
propagated O
along O
every O
link B
, O
and O
the O
root B
node I
has O
received O
messages O
from O
all O
of O
its O
neighbours O
. O
( O
10.165 O
) O
our O
analysis O
is O
readily O
extended B
to O
more O
general O
gaussian O
priors O
, O
for O
instance O
if O
we O
wish O
to O
associate O
a O
different O
hyperparameter B
with O
different O
subsets O
of O
the O
parame- O
ters O
wj O
. O
by O
a O
variable O
ξ O
having O
a O
distribution O
ν O
( O
ξ O
) O
, O
then O
the O
sum-of-squares B
error I
function O
becomes O
{ O
y O
( O
xn O
+ O
ξ O
) O
− O
tn O
} O
2 O
ν O
( O
ξ O
) O
dξ O
. O
the O
speciﬁc O
choice O
of O
proposal B
distribution I
can O
have O
a O
marked O
effect O
on O
the O
performance O
of O
the O
algorithm O
. O
( O
2.246 O
) O
p O
( O
cid:7 O
) O
p O
( O
x O
) O
v O
p O
( O
x O
) O
= O
k O
n O
v O
note O
that O
the O
validity O
of O
( O
2.246 O
) O
depends O
on O
two O
contradictory O
assumptions O
, O
namely O
that O
the O
region O
r O
be O
sufﬁciently O
small O
that O
the O
density B
is O
approximately O
constant O
over O
the O
region O
and O
yet O
sufﬁciently O
large O
( O
in O
relation O
to O
the O
value O
of O
that O
density B
) O
that O
the O
number O
k O
of O
points O
falling O
inside O
the O
region O
is O
sufﬁcient O
for O
the O
binomial B
distribution I
to O
be O
sharply O
peaked O
. O
components O
that O
take O
essentially O
no O
responsibility B
for O
ex- O
plaining O
the O
data O
points O
have O
rnk O
( O
cid:7 O
) O
0 O
and O
hence O
nk O
( O
cid:7 O
) O
0. O
from O
( O
10.58 O
) O
, O
we O
see O
that O
αk O
( O
cid:7 O
) O
α0 O
and O
from O
( O
10.60 O
) O
– O
( O
10.63 O
) O
we O
see O
that O
the O
other O
parameters O
revert O
to O
their O
prior B
values O
. O
if O
we O
have O
a O
prior B
p O
( O
m O
) O
over O
graphs O
indexed O
by O
m O
, O
then O
the O
posterior O
distribution O
is O
given O
by O
p O
( O
m|d O
) O
∝ O
p O
( O
m O
) O
p O
( O
d|m O
) O
( O
8.103 O
) O
where O
d O
is O
the O
observed O
data O
set O
. O
the O
hinge B
error I
function I
, O
so-called O
because O
of O
its O
shape O
, O
is O
plotted O
in O
figure O
7.5. O
it O
can O
be O
viewed O
as O
an O
approximation O
to O
the O
misclassiﬁcation O
error B
, O
i.e. O
, O
the O
error B
function I
that O
ideally O
we O
would O
like O
to O
minimize O
, O
which O
is O
also O
shown O
in O
figure O
7.5. O
when O
we O
considered O
the O
logistic B
regression I
model O
in O
section O
4.3.2 O
, O
we O
found O
it O
convenient O
to O
work O
with O
target O
variable O
t O
∈ O
{ O
0 O
, O
1 O
} O
. O
we O
can O
view O
the O
update O
equation O
( O
13.89 O
) O
for O
the O
mean B
of O
the O
hidden B
variable I
distribution O
as O
taking O
the O
predicted O
mean B
aµn−1 O
and O
then O
adding O
a O
correction O
that O
is O
proportional O
to O
the O
error B
xn O
− O
cazn−1 O
between O
the O
predicted O
observation O
and O
the O
actual O
observation O
. O
for O
example O
, O
in O
a O
gaussian O
mixture B
model I
, O
the O
indicator O
variables O
zkn O
( O
which O
specify O
which O
component O
k O
is O
responsible O
for O
generating O
data O
point O
xn O
) O
represent O
the O
latent O
variables O
, O
whereas O
the O
means O
µk O
, O
precisions O
λk O
and O
mixing O
proportions O
πk O
represent O
the O
parameters O
. O
in O
order O
to O
ﬁnd O
a O
generalization B
of O
the O
between-class B
covariance I
matrix O
, O
we O
follow O
duda O
and O
hart O
( O
1973 O
) O
and O
consider O
ﬁrst O
the O
total O
covariance B
matrix I
( O
cid:5 O
) O
where O
m O
is O
the O
mean B
of O
the O
total O
data O
set O
st O
= O
( O
xn O
− O
m O
) O
( O
xn O
− O
m O
) O
t O
( O
4.43 O
) O
m O
= O
1 O
n O
xn O
= O
1 O
n O
k O
( O
cid:2 O
) O
k=1 O
nkmk O
( O
4.44 O
) O
and O
n O
= O
k O
nk O
is O
the O
total O
number O
of O
data O
points O
. O
we O
can O
use O
this O
approach O
to O
deﬁne O
a O
kernel B
function I
measuring O
the O
similarity O
of O
two O
sequences O
x O
and O
x O
( O
cid:4 O
) O
by O
extending O
the O
mixture B
representation O
( O
6.29 O
) O
to O
give O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
p O
( O
x|z O
) O
p O
( O
x O
( O
cid:4 O
) O
|z O
) O
p O
( O
z O
) O
( O
cid:2 O
) O
( O
6.31 O
) O
z O
so O
that O
both O
observed O
sequences O
are O
generated O
by O
the O
same O
hidden O
sequence O
z. O
this O
model O
can O
easily O
be O
extended B
to O
allow O
sequences O
of O
differing O
length O
to O
be O
compared O
. O
for O
instance O
, O
for O
the O
means O
the O
sufﬁcient B
statistics I
are O
deﬁned O
by O
( O
9.17 O
) O
and O
( O
9.18 O
) O
from O
which O
we O
obtain O
( O
cid:16 O
) O
( O
cid:10 O
) O
( O
cid:11 O
) O
µnew O
k O
= O
µold O
k O
+ O
γnew O
( O
zmk O
) O
− O
γold O
( O
zmk O
) O
n O
new O
k O
xm O
− O
µold O
k O
( O
cid:15 O
) O
( O
9.78 O
) O
( O
9.79 O
) O
together O
with O
n O
new O
k O
= O
n O
old O
k O
+ O
γnew O
( O
zmk O
) O
− O
γold O
( O
zmk O
) O
. O
( O
14.43 O
) O
( O
cid:12 O
) O
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
γnk O
n=1 O
k=1 O
setting O
the O
derivative B
with O
respect O
to O
β O
equal O
to O
zero O
, O
and O
rearranging O
, O
we O
obtain O
the O
m-step O
equation O
for O
β O
in O
the O
form O
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
( O
cid:10 O
) O
1 O
β O
= O
1 O
n O
γnk O
n=1 O
k=1 O
tn O
− O
wt O
k O
φn O
( O
cid:11 O
) O
2 O
. O
it O
is O
worth O
noting O
that O
maximum B
likelihood I
can O
exhibit O
severe O
over-ﬁtting B
for O
data O
sets O
that O
are O
linearly B
separable I
. O
2.3. O
the O
gaussian O
distribution O
93 O
marginal B
and O
conditional B
gaussians O
given O
a O
marginal B
gaussian O
distribution O
for O
x O
and O
a O
conditional B
gaussian O
distri- O
bution O
for O
y O
given O
x O
in O
the O
form O
p O
( O
x O
) O
= O
n O
( O
x|µ O
, O
λ O
p O
( O
y|x O
) O
= O
n O
( O
y|ax O
+ O
b O
, O
l−1 O
) O
−1 O
) O
( O
2.113 O
) O
( O
2.114 O
) O
the O
marginal B
distribution O
of O
y O
and O
the O
conditional B
distribution O
of O
x O
given O
y O
are O
given O
by O
p O
( O
y O
) O
= O
n O
( O
y|aµ O
+ O
b O
, O
l−1 O
+ O
aλ O
−1at O
) O
p O
( O
x|y O
) O
= O
n O
( O
x|σ O
{ O
atl O
( O
y O
− O
b O
) O
+ O
λµ O
} O
, O
σ O
) O
where O
σ O
= O
( O
λ O
+ O
atla O
) O
−1 O
. O
we O
therefore O
begin O
our O
discussion O
of O
mixture B
distributions O
by O
considering O
the O
problem O
of O
ﬁnding O
clusters O
in O
a O
set O
of O
data O
points O
, O
which O
we O
approach O
ﬁrst O
using O
a O
nonprobabilistic O
technique O
called O
the O
k-means O
algorithm O
( O
lloyd O
, O
1982 O
) O
. O
likelihood O
result O
, O
while O
from O
( O
2.142 O
) O
the O
posterior O
variance O
is O
given O
by O
σ2 O
2.3. O
the O
gaussian O
distribution O
99 O
figure O
2.12 O
illustration O
of O
bayesian O
inference B
for O
the O
mean B
µ O
of O
a O
gaussian O
distri- O
bution O
, O
in O
which O
the O
variance B
is O
as- O
sumed O
to O
be O
known O
. O
if O
we O
consider O
the O
case O
of O
m O
= O
d O
, O
so O
that O
there O
is O
no O
reduction O
of O
dimension O
( O
cid:173 O
) O
ality O
, O
then O
u O
m O
= O
u O
and O
l O
m O
= O
l. O
making O
use O
of O
the O
orthogonality O
properties O
uut O
= O
i O
and O
rrt O
= O
i O
, O
we O
see O
that O
the O
covariance B
c O
of O
the O
marginal B
distribution O
for O
x O
becomes O
( O
12.47 O
) O
and O
so O
we O
obtain O
the O
standard O
maximum O
likelihood O
solution O
for O
an O
unconstrained O
gaussian O
distribution O
in O
which O
the O
covariance B
matrix I
is O
given O
by O
the O
sample O
covari O
( O
cid:173 O
) O
ance O
. O
this O
book O
places O
a O
strong O
emphasis O
on O
the O
bayesian O
viewpoint O
, O
reﬂecting O
the O
huge O
growth O
in O
the O
practical O
importance O
of O
bayesian O
methods O
in O
the O
past O
few O
years O
, O
while O
also O
discussing O
useful O
frequentist B
concepts O
as O
required O
. O
consider O
a O
data O
set O
comprising O
n O
= O
2 O
observations O
t1 O
and O
t2 O
, O
together O
with O
a O
model O
having O
a O
single O
basis B
function I
φ O
( O
x O
) O
, O
with O
hyperparameter B
α O
, O
along O
with O
isotropic B
noise O
having O
pre- O
cision O
β. O
from O
( O
7.85 O
) O
, O
the O
marginal B
likelihood I
is O
given O
by O
p O
( O
t|α O
, O
β O
) O
= O
n O
( O
t|0 O
, O
c O
) O
in O
which O
the O
covariance B
matrix I
takes O
the O
form O
c O
= O
1 O
β O
i O
+ O
1 O
α O
ϕϕt O
( O
7.92 O
) O
where O
ϕ O
denotes O
the O
n-dimensional O
vector O
( O
φ O
( O
x1 O
) O
, O
φ O
( O
x2 O
) O
) O
t O
, O
and O
similarly O
t O
= O
( O
t1 O
, O
t2 O
) O
t. O
notice O
that O
this O
is O
just O
a O
zero-mean O
gaussian O
process O
model O
over O
t O
with O
covariance B
c. O
given O
a O
particular O
observation O
for O
t O
, O
our O
goal O
is O
to O
ﬁnd O
α O
( O
cid:1 O
) O
and O
β O
( O
cid:1 O
) O
by O
maximizing O
the O
marginal B
likelihood I
. O
( O
8.36 O
) O
thus O
, O
conditioned O
on O
the O
polynomial O
coefﬁcients O
w O
, O
the O
predictive B
distribution I
for O
training B
data O
to O
determine O
the O
posterior O
distribution O
over O
the O
coefﬁcients O
w O
and O
then O
we O
can O
discard O
the O
training B
data O
and O
use O
the O
posterior O
distribution O
for O
w O
to O
make O
a O
related O
graphical O
structure O
arises O
in O
an O
approach O
to O
classiﬁcation B
called O
the O
naive O
bayes O
model O
, O
in O
which O
we O
use O
conditional B
independence I
assumptions O
to O
sim- O
plify O
the O
model O
structure O
. O
in O
this O
case O
, O
the O
variance B
along O
d O
- O
1 O
linearly O
in O
( O
cid:173 O
) O
dependent O
directions O
is O
controlled O
by O
the O
columns O
of O
w O
, O
and O
the O
variance B
along O
the O
remaining O
direction O
is O
given O
by O
a O
2 O
. O
0.04 O
0.02 O
0 O
−1 O
0.04 O
0.02 O
0 O
−1 O
0 O
1 O
0 O
1 O
further O
insight O
into O
the O
role O
of O
the O
equivalent B
kernel I
can O
be O
obtained O
by O
consid- O
ering O
the O
covariance B
between O
y O
( O
x O
) O
and O
y O
( O
x O
( O
cid:4 O
) O
) O
, O
which O
is O
given O
by O
cov O
[ O
y O
( O
x O
) O
, O
y O
( O
x O
( O
cid:4 O
) O
) O
] O
= O
cov O
[ O
φ O
( O
x O
) O
tw O
, O
wtφ O
( O
x O
( O
cid:4 O
) O
) O
] O
= O
φ O
( O
x O
) O
tsn O
φ O
( O
x O
( O
cid:4 O
) O
) O
= O
β O
−1k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
( O
3.63 O
) O
where O
we O
have O
made O
use O
of O
( O
3.49 O
) O
and O
( O
3.62 O
) O
. O
if O
λ O
is O
a O
discrete O
variable O
with O
k O
states O
, O
this O
simply O
amounts O
to O
setting O
the O
prior B
probability O
of O
each O
state O
to O
1/k O
. O
in O
the O
ﬁrst O
, O
we O
simply O
set O
the O
required O
derivatives O
of O
the O
marginal B
likelihood I
to O
zero O
and O
obtain O
the O
following O
re-estimation O
equations O
exercise O
7.12 O
αnew O
i O
( O
βnew O
) O
−1 O
= O
= O
γi O
m2 O
i O
( O
cid:5 O
) O
t O
− O
φm O
( O
cid:5 O
) O
2 O
i O
γi O
n O
− O
( O
cid:5 O
) O
( O
7.87 O
) O
( O
7.88 O
) O
section O
3.5.3 O
where O
mi O
is O
the O
ith O
component O
of O
the O
posterior O
mean O
m O
deﬁned O
by O
( O
7.82 O
) O
. O
if O
we O
relax O
the O
assumption O
of O
a O
shared O
covariance O
matrix O
and O
allow O
each O
class- O
conditional B
density O
p O
( O
x|ck O
) O
to O
have O
its O
own O
covariance B
matrix I
σk O
, O
then O
the O
earlier O
cancellations O
will O
no O
longer O
occur O
, O
and O
we O
will O
obtain O
quadratic O
functions O
of O
x O
, O
giv- O
ing O
rise O
to O
a O
quadratic B
discriminant I
. O
more O
generally O
, O
a O
stochastic B
process I
y O
( O
x O
) O
is O
speciﬁed O
by O
giving O
the O
joint O
probability B
distribution O
for O
any O
ﬁnite O
set O
of O
values O
y O
( O
x1 O
) O
, O
. O
for O
the O
particular O
case O
of O
an O
independent B
, O
identically O
distributed O
data O
set O
, O
x O
will O
comprise O
n O
data O
points O
{ O
xn O
} O
while O
z O
will O
comprise O
n O
corresponding O
latent O
( O
cid:21 O
) O
variables O
{ O
zn O
} O
, O
where O
n O
= O
1 O
, O
. O
speciﬁcally O
, O
we O
shall O
consider O
the O
undirected B
graph I
in O
figure O
8.32 O
( O
b O
) O
. O
next O
consider O
the O
maximization O
with O
respect O
to O
the O
parameter O
vector O
wk O
of O
the O
kth O
linear B
regression I
model O
. O
it O
therefore O
has O
poor O
scaling O
properties O
, O
although O
in O
practice O
it O
is O
very O
useful O
as O
a O
check O
on O
the O
soft- O
ware O
implementation O
of O
backpropagation B
methods O
. O
as O
shown O
in O
figure O
11.14 O
, O
this O
has O
the O
effect O
of O
shearing O
a O
region O
of O
phase B
space I
while O
not O
altering O
its O
volume O
. O
solving O
noisy O
linear O
operator O
equations O
by O
gaussian O
processes O
: O
application O
to O
ordinary O
and O
partial O
differential B
equations O
. O
consider O
ﬁrst O
the O
case O
of O
two O
classes O
, O
each O
having O
a O
gaussian O
class-conditional O
density B
with O
a O
shared O
covariance O
matrix O
, O
and O
suppose O
we O
have O
a O
data O
set O
{ O
xn O
, O
tn O
} O
where O
n O
= O
1 O
, O
. O
a O
comparison O
of O
the O
results O
of O
this O
algorithm O
with O
standard O
probabilistic O
pca O
is O
shown O
in O
figure O
12.14. O
bayesian O
pca O
provides O
an O
opportunity O
to O
illustrate O
the O
gibbs O
sampling O
algo O
( O
cid:173 O
) O
rithm O
discussed O
in O
section O
11.3. O
figure O
12.15 O
shows O
an O
example O
of O
the O
samples O
from O
the O
hyperparameters O
in O
ai O
for O
a O
data O
set O
in O
d O
= O
4 O
dimensions O
in O
which O
the O
di O
( O
cid:173 O
) O
mensionality O
of O
the O
latent O
space O
is O
m O
= O
3 O
but O
in O
which O
the O
data O
set O
is O
generated O
from O
a O
probabilistic O
pca O
model O
having O
one O
direction O
of O
high O
variance B
, O
with O
the O
remaining O
directions O
comprising O
low O
variance B
noise O
. O
for O
instance O
, O
in O
the O
geostatistics O
literature O
gaussian O
process O
regression B
is O
known O
as O
kriging B
( O
cressie O
, O
1993 O
) O
. O
a O
simple O
alternative O
is O
to O
take O
the O
mean B
of O
the O
most O
probable O
component O
( O
i.e. O
, O
the O
one O
with O
the O
largest O
mixing B
coefﬁcient I
) O
at O
each O
value O
of O
x. O
this O
is O
shown O
for O
the O
toy O
data O
set O
in O
figure O
5.21 O
( O
d O
) O
. O
we O
then O
work O
through O
each O
of O
the O
nodes O
in O
or- O
to O
do O
this O
, O
we O
start O
with O
the O
lowest-numbered O
node B
and O
draw O
a O
sample O
from O
the O
der O
, O
so O
that O
for O
node O
n O
we O
draw O
a O
sample O
from O
the O
conditional B
distribution O
p O
( O
xn|pan O
) O
in O
which O
the O
parent O
variables O
have O
been O
set O
to O
their O
sampled O
values O
. O
gam O
( O
τ|a O
, O
b O
) O
= O
1 O
γ O
( O
a O
) O
baτ O
a−1e O
−bτ O
e O
[ O
τ O
] O
= O
a O
b O
var O
[ O
τ O
] O
= O
a O
b2 O
mode O
[ O
τ O
] O
= O
a O
− O
1 O
e O
[ O
ln O
τ O
] O
= O
ψ O
( O
a O
) O
− O
ln O
b O
h O
[ O
τ O
] O
= O
ln O
γ O
( O
a O
) O
− O
( O
a O
− O
1 O
) O
ψ O
( O
a O
) O
− O
ln O
b O
+ O
a O
for O
α O
( O
cid:2 O
) O
1 O
b O
( O
b.26 O
) O
( O
b.27 O
) O
( O
b.28 O
) O
( O
b.29 O
) O
( O
b.30 O
) O
( O
b.31 O
) O
where O
ψ O
( O
· O
) O
is O
the O
digamma B
function I
deﬁned O
by O
( O
b.25 O
) O
. O
then O
it O
can O
be O
shown O
that O
a O
head-to-head B
path I
will O
become O
unblocked O
if O
either O
the O
node B
, O
or O
any O
of O
its O
descendants O
, O
is O
observed O
. O
640 O
13. O
sequential B
data I
zn−1 O
zn O
zn O
figure O
13.21 O
the O
linear B
dynamical I
system I
can O
be O
viewed O
as O
a O
sequence O
of O
steps O
in O
which O
increasing O
un- O
certainty O
in O
the O
state O
variable O
due O
to O
diffusion O
is O
compensated O
by O
the O
arrival O
of O
new O
data O
. O
a O
one- O
dimensional O
transformation O
, O
parameterized O
by O
the O
continuous O
variable O
ξ O
, O
applied O
to O
xn O
causes O
it O
to O
sweep O
out O
a O
one-dimensional O
manifold B
m. O
locally O
, O
the O
effect O
of O
the O
transformation O
can O
be O
approximated O
by O
the O
tangent O
vector O
τ O
n. O
x2 O
τ O
n O
xn O
m O
ξ O
x1 O
264 O
5. O
neural O
networks O
will O
be O
one-dimensional O
, O
and O
will O
be O
parameterized O
by O
ξ. O
let O
the O
vector O
that O
results O
from O
acting O
on O
xn O
by O
this O
transformation O
be O
denoted O
by O
s O
( O
xn O
, O
ξ O
) O
, O
which O
is O
deﬁned O
so O
that O
s O
( O
x O
, O
0 O
) O
= O
x. O
then O
the O
tangent O
to O
the O
curve O
m O
is O
given O
by O
the O
directional O
derivative O
τ O
= O
∂s/∂ξ O
, O
and O
the O
tangent O
vector O
at O
the O
point O
xn O
is O
given O
by O
τ O
n O
= O
∂s O
( O
xn O
, O
ξ O
) O
∂ξ O
. O
648 O
13. O
sequential B
data I
13.5 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
verify O
the O
m-step O
equations O
( O
13.18 O
) O
and O
( O
13.19 O
) O
for O
the O
initial O
state O
probabili- O
ties O
and O
transition B
probability I
parameters O
of O
the O
hidden O
markov O
model O
by O
maximiza- O
tion O
of O
the O
expected O
complete-data O
log O
likelihood O
function O
( O
13.17 O
) O
, O
using O
appropriate O
lagrange O
multipliers O
to O
enforce O
the O
summation O
constraints O
on O
the O
components O
of O
π O
and O
a O
. O
the O
only O
factors O
that O
remain O
will O
be O
the O
conditional B
distribution O
p O
( O
xi|pai O
) O
for O
node O
xi O
itself O
, O
together O
with O
the O
conditional B
distributions O
for O
any O
nodes O
xk O
such O
that O
node B
xi O
is O
in O
the O
conditioning O
set O
of O
p O
( O
xk|pak O
) O
, O
in O
other O
words O
for O
which O
xi O
is O
a O
parent O
of O
xk O
. O
each O
data O
point O
is O
depicted O
by O
a O
circle O
whose O
radius O
indicates O
the O
weight O
assigned O
to O
that O
data O
point O
when O
training B
the O
most O
recently O
added O
base O
learner O
. O
the O
beta B
distribution I
is O
a O
special O
case O
of O
the O
k-state O
dirichlet O
distribution O
for O
k O
= O
2. O
binomial O
the O
binomial B
distribution I
gives O
the O
probability B
of O
observing O
m O
occurrences O
of O
x O
= O
1 O
in O
a O
set O
of O
n O
samples O
from O
a O
bernoulli O
distribution O
, O
where O
the O
probability B
of O
observ- O
ing O
x O
= O
1 O
is O
µ O
∈ O
[ O
0 O
, O
1 O
] O
. O
( O
10.141 O
) O
10.5. O
local B
variational O
methods O
497 O
instead O
of O
thinking O
of O
λ O
as O
the O
variational B
parameter O
, O
we O
can O
let O
ξ O
play O
this O
role O
as O
this O
leads O
to O
simpler O
expressions O
for O
the O
conjugate B
function O
, O
which O
is O
then O
given O
by O
g O
( O
λ O
) O
= O
λ O
( O
ξ O
) O
ξ2 O
− O
f O
( O
ξ O
) O
= O
λ O
( O
ξ O
) O
ξ2 O
+ O
ln O
( O
eξ/2 O
+ O
e O
−ξ/2 O
) O
. O
xm O
µxm→fs O
( O
xm O
) O
fs O
µfs→x O
( O
x O
) O
x O
xm O
gm O
( O
xm O
, O
xsm O
) O
where O
ne O
( O
fs O
) O
denotes O
the O
set O
of O
variable O
nodes O
that O
are O
neighbours O
of O
the O
factor O
node O
fs O
, O
and O
ne O
( O
fs O
) O
\ O
x O
denotes O
the O
same O
set O
but O
with O
node B
x O
removed O
. O
this O
rescaled O
error B
function I
is O
also O
plotted O
in O
figure O
7.5 O
and O
we O
see O
that O
it O
has O
a O
similar O
form O
to O
the O
support B
vector I
error O
function O
. O
to O
obtain O
the O
corresponding O
representation O
for O
the O
next O
time O
step O
, O
we O
ﬁrst O
draw O
l O
samples O
from O
the O
mixture B
distribution I
( O
13.119 O
) O
, O
and O
then O
for O
each O
sample O
we O
use O
the O
new O
obser- O
n+1 O
) O
. O
, O
xm O
) O
, O
where O
xi O
∈ O
{ O
0 O
, O
1 O
} O
, O
could O
be O
reduced O
from O
2m O
to O
m O
+ O
1 O
by O
making O
use O
of O
the O
logistic B
sigmoid I
represen- O
tation O
( O
8.10 O
) O
. O
we O
now O
use O
maximum B
likelihood I
to O
determine O
the O
parameters O
of O
the O
logistic B
regression I
model O
. O
to O
do O
this O
set O
the O
derivative B
of O
l O
( O
ξ O
) O
with O
re- O
spect O
to O
ξn O
equal O
to O
zero O
, O
making O
use O
of O
the O
result O
( O
3.117 O
) O
for O
the O
derivative B
of O
the O
log O
of O
a O
determinant O
, O
together O
with O
the O
expressions O
( O
10.157 O
) O
and O
( O
10.158 O
) O
which O
deﬁne O
the O
mean B
and O
covariance B
of O
the O
variational B
posterior O
distribution O
q O
( O
w O
) O
. O
the O
factor- O
ization O
of O
the O
joint O
distribution O
must O
therefore O
be O
such O
that O
xi O
and O
xj O
do O
not O
appear O
in O
the O
same O
factor O
in O
order O
for O
the O
conditional B
independence I
property O
to O
hold O
for O
all O
possible O
distributions O
belonging O
to O
the O
graph O
. O
the O
situation O
with O
a O
neural B
network I
model O
is O
more O
complex O
, O
however O
, O
due O
to O
the O
multimodality B
of O
the O
posterior O
distribution O
. O
we O
therefore O
maximize O
the O
differential B
entropy I
with O
the O
ludwig O
boltzmann O
1844–1906 O
ludwig O
eduard O
boltzmann O
was O
an O
austrian O
physicist O
who O
created O
the O
ﬁeld O
of O
statistical O
mechanics O
. O
( O
3.110 O
) O
( O
3.111 O
) O
3.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
we O
saw O
in O
section O
2.3.6 O
that O
the O
conjugate B
prior I
for O
a O
gaussian O
distribution O
with O
unknown O
mean B
and O
unknown O
precision O
( O
inverse B
variance O
) O
is O
a O
normal-gamma B
distribution I
. O
13 O
sequential B
data I
so O
far O
in O
this O
book O
, O
we O
have O
focussed O
primarily O
on O
sets O
of O
data O
points O
that O
were O
as- O
sumed O
to O
be O
independent B
and O
identically O
distributed O
( O
i.i.d. B
) O
. O
note O
that O
the O
function O
on O
the O
right-hand O
side O
can O
not O
be O
interpreted O
as O
a O
probability B
density O
because O
it O
is O
not O
normalized O
. O
x1 O
x2 O
x1 O
x2 O
f O
( O
x1 O
, O
x2 O
, O
x3 O
) O
x3 O
( O
a O
) O
x3 O
( O
b O
) O
x1 O
x2 O
x1 O
x2 O
x1 O
fa O
x2 O
8.4. O
inference B
in O
graphical O
models O
403 O
x3 O
( O
a O
) O
f O
( O
x1 O
, O
x2 O
, O
x3 O
) O
x3 O
( O
b O
) O
fb O
fc O
x3 O
( O
c O
) O
figure O
8.45 O
( O
a O
) O
a O
fully B
connected I
undirected O
graph O
. O
in O
particular O
, O
if O
we O
imagine O
conditioning O
on O
zn O
we O
see O
that O
, O
as O
with O
the O
standard O
hmm O
, O
the O
values O
of O
zn−1 O
and O
zn+1 O
are O
independent B
, O
corresponding O
to O
the O
conditional B
independence I
property O
( O
13.5 O
) O
. O
it O
is O
worth O
taking O
a O
moment O
to O
study O
the O
form O
of O
the O
covariance B
matrix I
given O
by O
( O
12.36 O
) O
. O
whereas O
the O
k-means O
algorithm O
performs O
a O
hard O
assignment O
of O
data O
points O
to O
clusters O
, O
in O
which O
each O
data O
point O
is O
associated O
uniquely O
with O
one O
cluster O
, O
the O
em O
algorithm O
makes O
a O
soft B
assignment O
based O
on O
the O
posterior O
probabilities O
. O
we O
can O
understand O
this O
alternative O
view O
of O
entropy B
by O
considering O
a O
set O
of O
n O
identical O
objects O
that O
are O
to O
be O
divided O
amongst O
a O
set O
of O
bins O
, O
such O
that O
there O
are O
ni O
objects O
in O
the O
ith O
bin O
. O
for O
subsequent O
pruning O
of O
the O
tree B
, O
the O
misclassiﬁcation O
rate O
is O
generally O
used O
. O
for O
instance O
, O
in O
regression B
we O
might O
simply O
predict O
a O
constant O
over O
each O
region O
, O
or O
in O
classiﬁcation B
we O
might O
assign O
each O
region O
to O
a O
speciﬁc O
class O
. O
the O
‘ O
sparsity B
’ O
measures O
the O
extent O
to O
which O
basis B
function I
ϕi O
overlaps O
with O
the O
other O
basis O
vectors O
in O
the O
model O
, O
and O
the O
‘ O
quality O
’ O
represents O
a O
measure O
of O
the O
alignment O
of O
the O
basis O
vector O
ϕn O
with O
the O
error B
between O
the O
training B
set I
values O
t O
= O
( O
t1 O
, O
. O
as O
long O
as O
each O
of O
the O
two O
models O
gives O
posterior O
probabilities O
for O
the O
classes O
, O
we O
can O
combine O
the O
outputs O
systematically O
using O
the O
rules O
of O
probability B
. O
the O
mixture O
of O
probabilistic O
pca O
models O
can O
also O
be O
extended B
hierarchically O
to O
produce O
an O
interactive O
data O
visualiza O
( O
cid:173 O
) O
tion O
algorithm O
( O
bishop O
and O
tipping O
, O
1998 O
) O
. O
2.5.2 O
nearest-neighbour B
methods I
one O
of O
the O
difﬁculties O
with O
the O
kernel O
approach O
to O
density B
estimation I
is O
that O
the O
parameter O
h O
governing O
the O
kernel O
width O
is O
ﬁxed O
for O
all O
kernels O
. O
in O
the O
direct O
approach O
, O
we O
are O
maximizing O
a O
likelihood B
function I
deﬁned O
through O
the O
conditional B
distribution O
p O
( O
ck|x O
) O
, O
which O
represents O
a O
form O
of O
discriminative O
training O
. O
for O
example O
, O
if O
the O
prior B
assigns O
zero O
probability B
to O
some O
value O
of O
variable O
, O
then O
the O
posterior O
dis- O
tribution O
will O
necessarily O
also O
assign O
zero O
probability B
to O
that O
value O
, O
irrespective O
of O
118 O
2. O
probability B
distributions O
any O
subsequent O
observations O
of O
data O
. O
this O
degeneracy O
is O
removed O
if O
an O
appropriate O
regularization B
term O
( O
section O
5.5 O
) O
is O
added O
to O
the O
error B
function I
. O
using O
the O
results O
( O
c.21 O
) O
, O
( O
c.26 O
) O
, O
and O
( O
c.28 O
) O
from O
appendix O
c O
, O
show O
that O
the O
covariance B
matrix I
σ O
that O
maximizes O
the O
log O
likelihood O
function O
( O
2.118 O
) O
is O
given O
by O
the O
sample O
covariance O
( O
2.122 O
) O
. O
before O
proceeding O
with O
a O
mathematical O
analysis O
, O
we O
ﬁrst O
give O
some O
informal O
insight O
into O
the O
origin O
of O
sparsity B
in O
bayesian O
linear O
models O
. O
10.6 O
variational B
logistic O
regression B
. O
, O
αk O
) O
t. O
note O
that O
, O
because O
of O
the O
summation O
constraint O
, O
the O
distribution O
over O
the O
space O
of O
the O
{ O
µk O
} O
is O
conﬁned O
to O
a O
simplex B
of O
dimensionality O
k O
− O
1 O
, O
as O
illustrated O
for O
k O
= O
3 O
in O
figure O
2.4. O
exercise O
2.9 O
the O
normalized O
form O
for O
this O
distribution O
is O
by O
dir O
( O
µ|α O
) O
= O
γ O
( O
α0 O
) O
γ O
( O
α1 O
) O
··· O
γ O
( O
αk O
) O
µαk−1 O
k O
( O
2.38 O
) O
k O
( O
cid:14 O
) O
k=1 O
which O
is O
called O
the O
dirichlet O
distribution O
. O
reducing O
multiclass B
to O
binary O
: O
a O
unifying O
ap- O
proach O
for O
margin O
classiﬁers O
. O
, O
m O
: O
( O
a O
) O
fit O
a O
classiﬁer O
ym O
( O
x O
) O
to O
the O
training B
data O
by O
minimizing O
the O
weighted O
error O
function O
jm O
= O
n O
( O
cid:2 O
) O
n=1 O
n O
i O
( O
ym O
( O
xn O
) O
( O
cid:9 O
) O
= O
tn O
) O
w O
( O
m O
) O
( O
14.15 O
) O
where O
i O
( O
ym O
( O
xn O
) O
( O
cid:9 O
) O
= O
tn O
) O
is O
the O
indicator O
function O
and O
equals O
1 O
when O
ym O
( O
xn O
) O
( O
cid:9 O
) O
= O
tn O
and O
0 O
otherwise O
. O
( O
9.43 O
) O
thus O
we O
see O
that O
in O
this O
limit O
, O
maximizing O
the O
expected O
complete-data O
log O
likelihood O
is O
equivalent O
to O
minimizing O
the O
distortion B
measure I
j O
for O
the O
k-means O
algorithm O
given O
by O
( O
9.1 O
) O
. O
( O
cid:15 O
) O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:16 O
) O
σ O
= O
σaa O
σab O
σba O
σbb O
, O
λ O
= O
λaa O
λab O
λba O
λbb O
conditional B
distribution O
: O
p O
( O
xa|xb O
) O
= O
n O
( O
x|µa|b O
, O
λ O
−1 O
aa O
) O
µa|b O
= O
µa O
− O
λ O
aa O
λab O
( O
xb O
− O
µb O
) O
. O
10.6.3 O
inference B
of O
hyperparameters O
10.7 O
expectation B
propagation I
. O
instead O
, O
the O
likelihood O
is O
approximated O
using O
monte O
carlo O
techniques O
by O
drawing O
samples O
from O
the O
gaussian O
prior B
. O
in O
the O
context O
of O
machine O
learning O
, O
a O
widely O
used O
functional B
is O
the O
entropy B
h O
[ O
x O
] O
for O
a O
continuous O
variable O
x O
because O
, O
for O
any O
choice O
of O
probability B
density O
function O
p O
( O
x O
) O
, O
it O
returns O
a O
scalar O
value O
representing O
the O
entropy B
of O
x O
under O
that O
density B
. O
for O
a O
d-dimensional O
vector O
x O
, O
the O
gaussian O
is O
governed O
by O
a O
d-dimensional O
mean B
vector O
µ O
and O
a O
d O
× O
d O
covariance B
matrix I
σ O
that O
must O
be O
symmetric O
and O
b. O
probability B
distributions O
689 O
positive-deﬁnite O
. O
reprinted O
as O
the O
mathematical O
founda- O
tions O
of O
learning B
machines O
, O
morgan O
kaufmann O
, O
( O
1990 O
) O
. O
by O
considering O
we O
wish O
to O
ﬁnd O
the O
density B
with O
respect O
to O
radius O
in O
polar O
coordinates O
in O
which O
the O
direction O
variables O
have O
been O
integrated O
out O
. O
11.6. O
estimating O
the O
partition B
function I
as O
we O
have O
seen O
, O
most O
of O
the O
sampling O
algorithms O
considered O
in O
this O
chapter O
re- O
quire O
only O
the O
functional B
form O
of O
the O
probability B
distribution O
up O
to O
a O
multiplicative O
constant O
. O
figure O
11.9 O
shows O
a O
simple O
illustrative O
exam- O
ple O
of O
sampling O
from O
a O
two-dimensional O
gaussian O
distribution O
using O
the O
metropolis O
algorithm O
in O
which O
the O
proposal B
distribution I
is O
an O
isotropic B
gaussian O
. O
( O
10.198 O
) O
( O
cid:6 O
) O
( O
cid:15 O
) O
’ O
’ O
’ O
’ O
qnew O
( O
θ O
) O
( O
cid:16 O
) O
this O
is O
easily O
solved O
because O
the O
approximating O
distribution O
qnew O
( O
θ O
) O
is O
from O
the O
ex- O
ponential O
family O
, O
and O
so O
we O
can O
appeal O
to O
the O
result O
( O
10.187 O
) O
, O
which O
tells O
us O
that O
the O
parameters O
of O
qnew O
( O
θ O
) O
are O
obtained O
by O
matching O
its O
expected O
sufﬁcient B
statistics I
to O
the O
corresponding O
moments O
of O
( O
10.196 O
) O
. O
( O
2.197 O
) O
( O
2.198 O
) O
comparison O
with O
( O
2.194 O
) O
allows O
us O
to O
identify O
114 O
2. O
probability B
distributions O
which O
we O
can O
solve O
for O
µ O
to O
give O
µ O
= O
σ O
( O
η O
) O
, O
where O
σ O
( O
η O
) O
= O
1 O
1 O
+ O
exp O
( O
−η O
) O
( O
2.199 O
) O
is O
called O
the O
logistic B
sigmoid I
function O
. O
z O
= O
∂ O
ln O
p O
( O
x|µml O
, O
σ2 O
) O
= O
∂µml O
( O
2.136 O
) O
thus O
the O
distribution O
of O
z O
is O
gaussian O
with O
mean B
µ O
− O
µml O
, O
as O
illustrated O
in O
fig- O
ure O
2.11. O
substituting O
( O
2.136 O
) O
into O
( O
2.135 O
) O
, O
we O
obtain O
the O
univariate O
form O
of O
( O
2.126 O
) O
, O
provided O
we O
choose O
the O
coefﬁcients O
an O
to O
have O
the O
form O
an O
= O
σ2/n O
. O
as O
an O
illus- O
tration O
of O
the O
left-to-right B
hidden O
markov O
model O
, O
we O
consider O
an O
example O
involving O
handwritten O
digits O
. O
even O
for O
a O
ﬁxed O
number O
of O
nodes O
in O
the O
tree B
, O
the O
problem O
of O
determining O
the O
optimal O
structure O
( O
including O
choice O
of O
input O
variable O
for O
each O
split O
as O
well O
as O
the O
corresponding O
thresh- O
exercise O
14.10 O
14.4. O
tree-based O
models O
665 O
olds O
) O
to O
minimize O
the O
sum-of-squares B
error I
is O
usually O
computationally O
infeasible O
due O
to O
the O
combinatorially O
large O
number O
of O
possible O
solutions O
. O
it O
is O
aimed O
at O
advanced O
undergraduates O
or O
ﬁrst O
year O
phd O
students O
, O
as O
well O
as O
researchers O
and O
practitioners O
, O
and O
assumes O
no O
previous O
knowledge O
of O
pattern O
recognition O
or O
ma- O
chine O
learning B
concepts O
. O
the O
node B
c O
is O
said O
to O
be O
head-to-tail O
with O
respect O
to O
the O
path O
from O
node B
a O
to O
node B
b. O
such O
a O
path O
connects O
nodes O
a O
and O
b O
and O
renders O
them O
dependent O
. O
8.3. O
markov O
random O
fields O
we O
have O
seen O
that O
directed B
graphical O
models O
specify O
a O
factorization B
of O
the O
joint O
dis- O
tribution O
over O
a O
set O
of O
variables O
into O
a O
product O
of O
local B
conditional O
distributions O
. O
how- O
ever O
, O
a O
simple O
insight O
into O
the O
origins O
of O
maximum B
margin I
has O
been O
given O
by O
tong O
and O
koller O
( O
2000 O
) O
who O
consider O
a O
framework O
for O
classiﬁcation O
based O
on O
a O
hybrid O
of O
generative O
and O
discriminative O
approaches O
. O
a O
key O
point O
about O
gaussian O
stochastic B
processes O
is O
that O
the O
joint O
distribution O
over O
n O
variables O
y1 O
, O
. O
the O
principal O
difference O
between O
boosting B
and O
the O
committee B
methods O
such O
as O
bagging B
discussed O
above O
, O
is O
that O
the O
base O
classiﬁers O
are O
trained O
in O
sequence O
, O
and O
each O
base O
classiﬁer O
is O
trained O
using O
a O
weighted O
form O
of O
the O
data O
set O
in O
which O
the O
weighting O
coefﬁcient O
associated O
with O
each O
data O
point O
depends O
on O
the O
performance O
of O
the O
previous O
classiﬁers O
. O
( O
10.26 O
) O
( O
10.27 O
) O
note O
that O
for O
n O
→ O
∞ O
this O
gives O
the O
maximum B
likelihood I
result O
in O
which O
µn O
= O
x O
and O
the O
precision O
is O
inﬁnite O
. O
another O
quantity O
of O
interest O
is O
the O
predictive B
distribution I
, O
in O
which O
the O
observed O
data O
is O
x O
= O
{ O
x1 O
, O
. O
12.13 O
( O
* O
* O
) O
show O
that O
the O
optimal O
reconstruction O
of O
a O
data O
point O
under O
probabilistic O
pca O
, O
according O
to O
the O
least O
squares O
projection O
cost O
of O
conventional O
pca O
, O
is O
given O
by O
( O
12.94 O
) O
exercises O
601 O
12.14 O
( O
* O
) O
the O
number O
of O
independent B
parameters O
in O
the O
covariance B
matrix I
for O
the O
proba O
( O
cid:173 O
) O
bilistic O
pca O
model O
with O
an O
m O
-dimensional O
latent O
space O
and O
a O
d-dimensional O
data O
space O
is O
given O
by O
( O
12.51 O
) O
. O
, O
wk O
) O
= O
− O
n O
( O
cid:2 O
) O
tnk O
ln O
ynk O
( O
4.108 O
) O
k O
( O
cid:2 O
) O
which O
is O
known O
as O
the O
cross-entropy B
error I
function I
for O
the O
multiclass B
classiﬁcation O
problem O
. O
2.33 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
the O
same O
joint O
distribution O
as O
in O
exercise O
2.32 O
, O
but O
now O
use O
the O
technique O
of O
completing B
the I
square I
to O
ﬁnd O
expressions O
for O
the O
mean B
and O
covariance B
of O
the O
conditional B
distribution O
p O
( O
x|y O
) O
. O
if O
we O
group O
the O
potentials O
and O
summations O
together O
in O
this O
way O
, O
we O
can O
express O
396 O
8. O
graphical O
models O
the O
desired O
marginal B
in O
the O
form O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
··· O
p O
( O
xn O
) O
= O
1 O
xn−1 O
z⎡⎣ O
( O
cid:2 O
) O
( O
⎡⎣ O
( O
cid:2 O
) O
( O
xn+1 O
( O
cid:31 O
) O
( O
cid:2 O
) O
x1 O
ψ2,3 O
( O
x2 O
, O
x3 O
) O
) O
* O
µα O
( O
xn O
) O
x2 O
( O
cid:31 O
) O
( O
cid:2 O
) O
( O
cid:31 O
) O
( O
cid:2 O
) O
) O
* O
xn O
µβ O
( O
xn O
) O
⎤⎦ O
+ O
··· O
ψ1,2 O
( O
x1 O
, O
x2 O
) O
⎤⎦ O
+ O
ψn O
, O
n+1 O
( O
xn O
, O
xn+1 O
) O
··· O
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
··· O
. O
then O
, O
assuming O
p O
( O
x O
) O
is O
continuous O
, O
the O
mean B
value I
theorem I
( O
weisstein O
, O
1999 O
) O
tells O
us O
that O
, O
for O
each O
such O
bin O
, O
there O
must O
exist O
a O
value O
xi O
such O
that O
( O
cid:6 O
) O
( O
i+1 O
) O
∆ O
p O
( O
x O
) O
dx O
= O
p O
( O
xi O
) O
∆ O
. O
458 O
9. O
mixture B
models O
and O
em O
9.18 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
consider O
a O
bernoulli O
mixture B
model I
as O
discussed O
in O
section O
9.3.3 O
, O
together O
with O
a O
prior B
distribution O
p O
( O
µk|ak O
, O
bk O
) O
over O
each O
of O
the O
parameter O
vectors O
µk O
given O
by O
the O
beta B
distribution I
( O
2.13 O
) O
, O
and O
a O
dirichlet O
prior B
p O
( O
π|α O
) O
given O
by O
( O
2.38 O
) O
. O
the O
generalized B
em O
, O
or O
gem O
, O
algorithm O
addresses O
the O
problem O
of O
an O
intractable O
m O
step O
. O
the O
posterior O
distribution O
p O
( O
θ|d O
) O
is O
then O
approximated O
using O
( O
10.191 O
) O
, O
and O
the O
model B
evidence I
p O
( O
d O
) O
can O
be O
approximated O
by O
using O
( O
10.190 O
) O
with O
the O
factors O
fi O
( O
θ O
) O
replaced O
by O
their O
approximations O
( O
cid:4 O
) O
fi O
( O
θ O
) O
. O
, O
xn O
) O
t O
in O
which O
the O
observations O
{ O
xn O
} O
are O
as- O
sumed O
to O
be O
drawn O
independently O
from O
a O
multivariate O
gaussian O
distribution O
, O
we O
can O
estimate O
the O
parameters O
of O
the O
distribution O
by O
maximum B
likelihood I
. O
in O
order O
to O
clarify O
the O
terminology O
, O
it O
is O
useful O
to O
consider O
the O
nature O
of O
the O
training B
process O
more O
care- O
fully O
. O
points O
above O
the O
-tube B
have O
ξ O
> O
0 O
and O
bξ O
= O
0 O
, O
points O
below O
the O
-tube B
have O
ξ O
= O
0 O
and O
bξ O
> O
0 O
, O
and O
points O
inside O
the O
-tube B
have O
ξ O
= O
bξ O
= O
0. O
y O
( O
x O
) O
ξ O
> O
0 O
( O
cid:1 O
) O
ξ O
> O
0 O
y O
+ O
 O
y O
y O
− O
 O
x O
the O
error B
function I
for O
support B
vector I
regression O
can O
then O
be O
written O
as O
n O
( O
cid:2 O
) O
n=1 O
c O
( O
ξn O
+ O
( O
cid:1 O
) O
ξn O
) O
+ O
( O
cid:5 O
) O
w O
( O
cid:5 O
) O
2 O
1 O
2 O
( O
7.55 O
) O
( O
7.53 O
) O
and O
( O
7.54 O
) O
. O
the O
recursions O
are O
therefore O
independent B
of O
the O
type O
or O
dimensionality O
of O
the O
observed O
variables O
or O
the O
form O
of O
this O
conditional B
distribution O
, O
so O
long O
as O
its O
value O
can O
be O
computed O
for O
each O
of O
the O
k O
possible O
states O
of O
zn O
. O
figure O
14.5 O
shows O
an O
illustration O
of O
a O
recursive O
binary O
partitioning O
of O
the O
input O
space O
, O
along O
with O
the O
corresponding O
tree B
structure O
. O
note O
that O
, O
as O
a O
function O
of O
w O
, O
this O
is O
the O
lq O
error B
function I
considered O
in O
section O
1.5.5 O
. O
by O
using O
a O
sufﬁcient O
number O
of O
gaussians O
, O
and O
by O
adjusting O
their O
means O
and O
covariances O
as O
well O
as O
the O
coefﬁcients O
in O
the O
linear O
combination O
, O
almost O
any O
continuous O
density B
can O
be O
approximated O
to O
arbitrary O
accuracy O
. O
these O
are O
deﬁned O
by O
ξn O
= O
0 O
for O
data O
points O
that O
are O
on O
or O
inside O
the O
correct O
margin B
boundary O
and O
ξn O
= O
|tn O
− O
y O
( O
xn O
) O
| O
for O
other O
points O
. O
that O
the O
manifold B
will O
be O
nonlinear O
because O
. O
our O
goal O
is O
to O
infer O
the O
posterior O
distribution O
for O
the O
mean B
µ O
and O
precision O
τ O
, O
given O
a O
data O
set O
d O
= O
{ O
x1 O
, O
. O
5.35 O
( O
( O
cid:12 O
) O
) O
derive O
the O
result O
( O
5.156 O
) O
for O
the O
derivative B
of O
the O
error B
function I
with O
respect O
to O
the O
network O
output O
activations O
controlling O
the O
component O
means O
in O
the O
mixture B
density I
network I
. O
discarding O
terms O
that O
are O
independent B
of O
w O
, O
we O
have O
ln O
q O
( O
w O
) O
= O
eα O
[ O
ln O
{ O
h O
( O
w O
, O
ξ O
) O
p O
( O
w|α O
) O
p O
( O
α O
) O
} O
] O
+ O
const O
= O
ln O
h O
( O
w O
, O
ξ O
) O
+ O
eα O
[ O
ln O
p O
( O
w|α O
) O
] O
+ O
const O
. O
derive O
the O
em O
algorithm O
for O
maximizing O
the O
likelihood B
function I
for O
the O
probabilistic O
pca O
model O
in O
this O
situ O
( O
cid:173 O
) O
ation O
. O
we O
are O
interested O
in O
evaluating O
the O
posterior O
distribution O
p O
( O
θ|d O
) O
for O
the O
purpose O
of O
making O
predictions O
, O
as O
well O
as O
the O
model B
evidence I
p O
( O
d O
) O
for O
the O
purpose O
of O
model B
comparison I
. O
in O
a O
graphical B
model I
, O
we O
will O
denote O
such O
observed O
variables O
by O
shad- O
ing O
the O
corresponding O
nodes O
. O
this O
result O
is O
illustrated O
in O
figure O
1.28. O
it O
can O
readily O
be O
extended B
to O
mul- O
tiple O
target O
variables O
represented O
by O
the O
vector O
t O
, O
in O
which O
case O
the O
optimal O
solution O
is O
the O
conditional B
average O
y O
( O
x O
) O
= O
et O
[ O
t|x O
] O
. O
clustering B
algorithms O
such O
as O
k-means O
have O
also O
been O
used O
, O
which O
give O
a O
set O
of O
basis B
function I
centres O
that O
no O
longer O
coincide O
with O
training B
data O
points O
. O
perhaps O
we O
could O
do O
even O
better O
if O
we O
take O
a O
weighted O
average O
, O
in O
which O
more O
recent O
measurements O
636 O
13. O
sequential B
data I
make O
a O
greater O
contribution O
than O
less O
recent O
ones O
. O
we O
shall O
also O
show O
how O
the O
backpropagation B
framework O
can O
be O
extended B
to O
allow O
other O
derivatives O
to O
be O
evaluated O
, O
such O
as O
the O
jacobian O
and O
hessian O
matrices O
. O
the O
squared O
loss O
is O
not O
the O
only O
possible O
choice O
of O
loss B
function I
for O
regression B
. O
the O
general O
expression O
( O
7.77 O
) O
then O
takes O
the O
svm-like O
form O
y O
( O
x O
) O
= O
wnk O
( O
x O
, O
xn O
) O
+ O
b O
( O
7.78 O
) O
where O
b O
is O
a O
bias B
parameter I
. O
if O
moments O
with O
respect O
to O
the O
distribution O
p O
( O
z O
) O
are O
required O
, O
then O
they O
can O
be O
536 O
11. O
sampling B
methods I
evaluated O
directly O
using O
the O
original O
samples O
together O
with O
the O
weights O
, O
because O
e O
[ O
f O
( O
z O
) O
] O
= O
f O
( O
z O
) O
p O
( O
z O
) O
dz O
( O
cid:6 O
) O
( O
cid:6 O
) O
f O
( O
z O
) O
[ O
( O
cid:4 O
) O
p O
( O
z O
) O
/q O
( O
z O
) O
] O
q O
( O
z O
) O
dz O
( O
cid:6 O
) O
[ O
( O
cid:4 O
) O
p O
( O
z O
) O
/q O
( O
z O
) O
] O
q O
( O
z O
) O
dz O
( O
cid:7 O
) O
l O
( O
cid:2 O
) O
wlf O
( O
zl O
) O
. O
global O
conﬁgurations O
that O
have O
a O
relatively O
high O
probability B
are O
those O
that O
ﬁnd O
a O
good O
balance O
in O
satisfying O
the O
( O
possibly O
conﬂicting O
) O
inﬂuences O
of O
the O
clique B
potentials O
. O
we O
could O
further O
restrict O
the O
covariance B
matrix I
to O
be O
proportional O
to O
the O
identity O
matrix O
, O
σ O
= O
σ2i O
, O
known O
as O
an O
isotropic B
co- O
variance B
, O
giving O
d O
+ O
1 O
independent B
parameters O
in O
the O
model O
and O
spherical O
surfaces O
of O
constant O
density B
. O
10.6. O
variational B
logistic O
regression B
we O
now O
illustrate O
the O
use O
of O
local B
variational O
methods O
by O
returning O
to O
the O
bayesian O
logistic B
regression I
model O
studied O
in O
section O
4.5. O
there O
we O
focussed O
on O
the O
use O
of O
the O
laplace O
approximation O
, O
while O
here O
we O
consider O
a O
variational B
treatment O
based O
on O
the O
approach O
of O
jaakkola O
and O
jordan O
( O
2000 O
) O
. O
( O
3.77 O
) O
exercise O
3.16 O
exercise O
3.17 O
one O
way O
to O
evaluate O
this O
integral O
is O
to O
make O
use O
once O
again O
of O
the O
result O
( O
2.115 O
) O
for O
the O
conditional B
distribution O
in O
a O
linear-gaussian O
model O
. O
in O
practice O
, O
rather O
than O
ﬁxing O
the O
covariance B
function O
, O
we O
may O
prefer O
to O
use O
a O
parametric O
family O
of O
functions O
and O
then O
infer O
the O
parameter O
values O
from O
the O
data O
. O
note O
that O
new O
test O
data O
must O
be O
pre-processed O
using O
the O
same O
steps O
as O
the O
training B
data O
. O
taking O
the O
expectation B
of O
( O
8.14 O
) O
, O
we O
have O
e O
[ O
xi O
] O
= O
wij O
e O
[ O
xj O
] O
+ O
bi O
. O
furthermore O
, O
comparison O
with O
( O
3.13 O
) O
shows O
that O
this O
takes O
precisely O
the O
same O
form O
as O
the O
gradient O
of O
the O
sum-of-squares B
error I
function O
for O
the O
linear B
regression I
model O
. O
7.12 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
show O
that O
direct O
maximization O
of O
the O
log O
marginal O
likelihood O
( O
7.85 O
) O
for O
the O
regression B
relevance O
vector O
machine O
leads O
to O
the O
re-estimation O
equations O
( O
7.87 O
) O
and O
( O
7.88 O
) O
where O
γi O
is O
deﬁned O
by O
( O
7.89 O
) O
. O
maximum B
likelihood I
source O
separation O
: O
a O
context-sensitive O
generalization B
of O
ica O
. O
one O
way O
to O
model O
the O
nonlinear O
structure O
is O
through O
a O
combination O
of O
linear O
models O
, O
so O
that O
we O
make O
a O
piece-wise O
linear O
approximation O
to O
the O
manifold B
. O
( O
10.33 O
) O
section O
1.2.4 O
we O
recognize O
the O
right-hand O
side O
as O
the O
familiar O
unbiased O
estimator O
for O
the O
variance B
of O
a O
univariate O
gaussian O
distribution O
, O
and O
so O
we O
see O
that O
the O
use O
of O
a O
bayesian O
ap- O
proach O
has O
avoided O
the O
bias B
of O
the O
maximum B
likelihood I
solution O
. O
7.1.2 O
relation O
to O
logistic O
regression B
as O
with O
the O
separable O
case O
, O
we O
can O
re-cast O
the O
svm O
for O
nonseparable O
distri- O
butions O
in O
terms O
of O
the O
minimization O
of O
a O
regularized O
error O
function O
. O
given O
a O
training B
set I
comprising O
a O
set O
of O
input O
vectors O
{ O
xn O
} O
, O
where O
n O
= O
1 O
, O
. O
thus O
we O
maximize O
( O
cid:22 O
) O
( O
cid:2 O
) O
( O
cid:23 O
) O
p O
( O
xi O
) O
ln O
p O
( O
xi O
) O
+ O
λ O
i O
i O
p O
( O
xi O
) O
− O
1 O
( O
1.99 O
) O
( O
cid:2 O
) O
( O
cid:4 O
) O
h O
= O
− O
52 O
1. O
introduction O
s O
e O
i O
t O
i O
l O
i O
b O
a O
b O
o O
r O
p O
h O
= O
1.77 O
0.5 O
0.25 O
0 O
s O
e O
i O
t O
i O
l O
i O
b O
a O
b O
o O
r O
p O
h O
= O
3.09 O
0.5 O
0.25 O
0 O
figure O
1.30 O
histograms O
of O
two O
probability B
distributions O
over O
30 O
bins O
illustrating O
the O
higher O
value O
of O
the O
entropy B
h O
for O
the O
broader O
distribution O
. O
4.21 O
( O
( O
cid:12 O
) O
) O
show O
that O
the O
probit B
function I
( O
4.114 O
) O
and O
the O
erf B
function I
( O
4.115 O
) O
are O
related O
by O
( O
4.116 O
) O
. O
the O
initial O
matrix O
h0 O
is O
chosen O
to O
be O
αi O
, O
where O
α O
is O
a O
small O
quantity O
, O
so O
that O
the O
algorithm O
actually O
ﬁnds O
the O
inverse B
of O
h O
+ O
αi O
. O
11.4 O
slice B
sampling I
. O
if O
the O
factors O
are O
parameterized O
functions O
and O
we O
wish O
to O
learn O
the O
values O
of O
the O
parameters O
using O
the O
em O
algorithm O
, O
then O
these O
marginals O
are O
precisely O
the O
quantities O
we O
will O
need O
to O
calculate O
in O
the O
e O
step O
, O
as O
we O
shall O
see O
in O
detail O
when O
we O
discuss O
the O
hidden O
markov O
model O
in O
chapter O
13. O
the O
message O
sent O
by O
a O
variable O
node B
to O
a O
factor O
node O
, O
as O
we O
have O
seen O
, O
is O
simply O
the O
product O
of O
the O
incoming O
messages O
on O
other O
links O
. O
these O
concepts O
are O
illustrated O
in O
figure O
8.41. O
similarly O
, O
to O
convert O
a O
directed B
graph O
to O
a O
factor B
graph I
, O
we O
simply O
create O
variable O
nodes O
in O
the O
factor B
graph I
corresponding O
to O
the O
nodes O
of O
the O
directed B
graph O
, O
and O
then O
create O
factor O
nodes O
corresponding O
to O
the O
conditional B
distributions O
, O
and O
then O
ﬁnally O
add O
the O
appropriate O
links O
. O
prob- O
abilistic O
principal B
component I
analysis I
. O
the O
probability B
distribution O
p O
( O
x|µ O
) O
for O
a O
single O
discrete O
variable O
x O
having O
k O
possible O
states O
( O
using O
the O
1-of-k O
representation O
) O
is O
given O
by O
p O
( O
x|µ O
) O
= O
µxk O
k O
k=1 O
( O
cid:5 O
) O
and O
is O
governed O
by O
the O
parameters O
µ O
= O
( O
µ1 O
, O
. O
= O
= O
we O
shall O
discuss O
the O
probabilistic O
interpretation O
of O
the O
mixture B
distribution I
in O
greater O
detail O
in O
chapter O
9. O
the O
form O
of O
the O
gaussian O
mixture B
distribution I
is O
governed O
by O
the O
parameters O
π O
, O
µ O
and O
σ O
, O
where O
we O
have O
used O
the O
notation O
π O
≡ O
{ O
π1 O
, O
. O
we O
now O
discuss O
the O
modiﬁcations O
to O
282 O
5. O
neural O
networks O
exercise O
5.40 O
exercise O
5.41 O
( O
cid:2 O
) O
this O
framework O
that O
arise O
when O
it O
is O
applied O
to O
classiﬁcation B
. O
the O
third O
order O
( O
m O
= O
3 O
) O
polynomial O
seems O
to O
give O
the O
best O
ﬁt O
to O
the O
function O
sin O
( O
2πx O
) O
of O
the O
examples O
shown O
in O
figure O
1.4. O
when O
we O
go O
to O
a O
much O
higher O
order O
polynomial O
( O
m O
= O
9 O
) O
, O
we O
obtain O
an O
excellent O
ﬁt O
to O
the O
training B
data O
. O
show O
that O
the O
mean B
of O
their O
sum O
y O
= O
x O
+ O
z O
is O
given O
by O
the O
sum O
of O
the O
means O
of O
each O
of O
the O
variable O
separately O
. O
using O
bayes O
’ O
theorem O
, O
we O
have O
w O
( O
l O
) O
n O
f O
( O
z O
( O
l O
) O
n O
) O
( O
13.117 O
) O
where O
{ O
z O
( O
l O
) O
n O
} O
is O
a O
set O
of O
samples O
drawn O
from O
p O
( O
zn|xn−1 O
) O
and O
we O
have O
made O
use O
of O
the O
conditional B
independence I
property O
p O
( O
xn|zn O
, O
xn−1 O
) O
= O
p O
( O
xn|zn O
) O
, O
which O
follows O
from O
the O
graph O
in O
figure O
13.5. O
the O
sampling O
weights O
{ O
w O
n O
} O
are O
deﬁned O
by O
( O
l O
) O
( O
cid:5 O
) O
l O
p O
( O
xn|z O
( O
l O
) O
n O
) O
m=1 O
p O
( O
xn|z O
( O
m O
) O
n O
) O
w O
( O
l O
) O
n O
= O
( O
13.118 O
) O
where O
the O
same O
samples O
are O
used O
in O
the O
numerator O
as O
in O
the O
denominator O
. O
if O
we O
choose O
a O
prior B
to O
be O
proportional O
to O
powers O
of O
µ O
and O
( O
1 O
− O
µ O
) O
, O
then O
the O
posterior O
distribution O
, O
which O
is O
proportional O
to O
the O
product O
of O
the O
prior B
and O
the O
likelihood B
function I
, O
will O
have O
the O
same O
functional B
form O
as O
the O
prior B
. O
thus O
the O
entropy B
of O
p O
( O
x O
) O
could O
equally O
well O
have O
been O
written O
as O
h O
[ O
p O
] O
. O
then O
the O
expectation B
with O
respect O
to O
p O
( O
x O
) O
can O
be O
approximated O
by O
a O
ﬁnite O
sum O
over O
these O
points O
, O
using O
( O
1.35 O
) O
, O
so O
that O
{ O
− O
ln O
q O
( O
xn|θ O
) O
+ O
ln O
p O
( O
xn O
) O
} O
. O
now O
consider O
the O
case O
of O
a O
directed B
graph O
in O
which O
some O
of O
the O
nodes O
are O
in- O
stantiated O
with O
observed O
values O
. O
for O
instance O
, O
cox O
( O
1946 O
) O
showed O
that O
if O
numerical O
values O
are O
used O
to O
represent O
degrees O
of O
belief O
, O
then O
a O
simple O
set O
of O
axioms O
encoding O
common O
sense O
properties O
of O
such O
beliefs O
leads O
uniquely O
to O
a O
set O
of O
rules O
for O
manipulating O
degrees O
of O
belief O
that O
are O
equivalent O
to O
the O
sum O
and O
product O
rules O
of O
probability B
. O
in O
fact O
, O
it O
is O
not O
difﬁcult O
to O
construct O
examples O
for O
which O
the O
set O
of O
individually O
most O
probable O
values O
has O
probability B
zero O
under O
the O
joint O
distribution O
. O
latent B
variable I
models O
and O
factor B
analysis I
. O
this O
is O
reminiscent O
of O
the O
choice O
of O
model O
complexity O
in O
polynomial B
curve I
ﬁtting I
discussed O
in O
chapter O
1 O
where O
the O
degree O
m O
of O
the O
polynomial O
, O
or O
alternatively O
the O
value O
α O
of O
the O
regularization B
parameter O
, O
was O
optimal O
for O
some O
intermediate O
value O
, O
neither O
too O
large O
nor O
too O
small O
. O
our O
goal O
is O
to O
determine O
the O
predictive B
distribution I
p O
( O
tn O
+1|t O
) O
, O
where O
we O
have O
left O
the O
conditioning O
on O
the O
input O
variables O
implicit O
. O
ieee O
transactions O
on O
infor- O
mation B
theory O
51 O
, O
2313–2335 O
. O
we O
begin O
by O
discussing O
the O
kernel O
method O
in O
detail O
, O
and O
to O
start O
with O
we O
take O
the O
region O
r O
to O
be O
a O
small O
hypercube O
centred O
on O
the O
point O
x O
at O
which O
we O
wish O
to O
determine O
the O
probability B
density O
. O
in O
the O
particular O
case O
in O
which O
the O
kernel B
function I
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
is O
deﬁned O
in O
terms O
of O
a O
ﬁnite O
set O
of O
basis O
functions O
, O
we O
can O
derive O
the O
results O
obtained O
previously O
in O
section O
3.3.2 O
for O
linear O
regression B
starting O
from O
the O
gaussian O
process O
viewpoint O
. O
to O
do O
this O
it O
is O
con- O
venient O
to O
regard O
the O
{ O
πj O
} O
as O
prior B
probabilities O
and O
to O
introduce O
the O
corresponding O
posterior O
probabilities O
which O
, O
following O
( O
2.192 O
) O
, O
are O
given O
by O
bayes O
’ O
theorem O
in O
the O
form O
γj O
( O
w O
) O
= O
( O
5.140 O
) O
( O
cid:5 O
) O
πjn O
( O
w|µj O
, O
σ2 O
j O
) O
k O
πkn O
( O
w|µk O
, O
σ2 O
k O
) O
. O
figure O
3.3 O
shows O
con- O
tours O
of O
the O
regularization B
function O
for O
different O
values O
of O
q. O
the O
case O
of O
q O
= O
1 O
is O
know O
as O
the O
lasso B
in O
the O
statistics O
literature O
( O
tibshirani O
, O
it O
has O
the O
property O
that O
if O
λ O
is O
sufﬁciently O
large O
, O
some O
of O
the O
coefﬁcients O
1996 O
) O
. O
1.2.5 O
curve B
ﬁtting I
re-visited O
. O
it O
has O
the O
property O
that O
the O
conditional B
distribution O
of O
xi O
, O
conditioned O
on O
all O
the O
remaining O
variables O
in O
the O
graph O
, O
is O
dependent O
only O
on O
the O
variables O
in O
the O
markov O
blanket O
. O
36 O
, O
823– O
841. O
ing B
theory I
. O
the O
values O
of O
o O
: O
i O
are O
re-estimated O
during O
training B
by O
maximizing O
the O
log O
marginal O
likelihood O
given O
by O
p O
( O
xla O
, O
j-l O
, O
0'2 O
) O
= O
jp O
( O
xiw O
, O
j-l O
, O
o'2 O
) O
p O
( O
wla O
) O
dw O
( O
12.61 O
) O
where O
the O
log O
ofp O
( O
xiw O
, O
j-l O
, O
0'2 O
) O
is O
given O
by O
( O
12.43 O
) O
. O
if O
we O
consider O
more O
restricted O
forms O
for O
the O
nonlinear O
function O
, O
and O
make O
an O
ap O
( O
cid:173 O
) O
propriate O
choice O
of O
the O
latent B
variable I
distribution O
, O
then O
we O
can O
construct O
a O
latent O
vari O
( O
cid:173 O
) O
able O
model O
that O
is O
both O
nonlinear O
and O
efficient O
to O
train O
. O
once O
again O
, O
our O
strategy O
for O
evaluating O
this O
distribution O
efﬁciently O
will O
be O
to O
focus O
on O
the O
quadratic O
form O
in O
the O
exponent O
of O
the O
joint O
distribution O
and O
thereby O
to O
identify O
the O
mean B
and O
covariance B
of O
the O
marginal B
distribution O
p O
( O
xa O
) O
. O
8.2.2 O
d-separation B
we O
now O
give O
a O
general O
statement O
of O
the O
d-separation B
property O
( O
pearl O
, O
1988 O
) O
for O
directed O
graphs O
. O
we O
denote O
the O
functional B
derivative O
of O
e O
[ O
f O
] O
with O
respect O
to O
f O
( O
x O
) O
by O
δf/δf O
( O
x O
) O
, O
and O
deﬁne O
it O
by O
the O
following O
relation O
: O
( O
cid:6 O
) O
f O
[ O
y O
( O
x O
) O
+ O
η O
( O
x O
) O
] O
= O
f O
[ O
y O
( O
x O
) O
] O
+ O
 O
δf O
δy O
( O
x O
) O
η O
( O
x O
) O
dx O
+ O
o O
( O
2 O
) O
. O
here O
the O
param- O
eter O
θ0 O
corresponds O
to O
the O
mean B
of O
the O
distribution O
, O
while O
m O
, O
which O
is O
known O
as O
the O
concentration B
parameter I
, O
is O
analogous O
to O
the O
inverse B
variance O
( O
precision O
) O
for O
the O
gaussian O
. O
for O
instance O
, O
in O
the O
case O
of O
618 O
13. O
sequential B
data I
gaussian O
emission O
densities O
we O
have O
p O
( O
x|φk O
) O
= O
n O
( O
x|µk O
, O
σk O
) O
, O
and O
maximization O
of O
the O
function O
q O
( O
θ O
, O
θold O
) O
then O
gives O
γ O
( O
znk O
) O
xn O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n=1 O
µk O
= O
γ O
( O
znk O
) O
( O
13.20 O
) O
γ O
( O
znk O
) O
( O
xn O
− O
µk O
) O
( O
xn O
− O
µk O
) O
t O
σk O
= O
n=1 O
. O
1.2. O
probability B
theory O
a O
key O
concept O
in O
the O
ﬁeld O
of O
pattern O
recognition O
is O
that O
of O
uncertainty O
. O
this O
factorization B
is O
illustrated O
in O
figure O
8.47. O
note O
that O
the O
set O
of O
variables O
{ O
x O
, O
x1 O
, O
. O
in O
practice O
, O
we O
wish O
to O
eval- O
uate O
this O
function O
at O
speciﬁc O
values O
of O
x O
, O
for O
example O
at O
the O
training B
data O
points O
6.4. O
gaussian O
processes O
305 O
x1 O
, O
. O
in O
the O
ith O
bin O
there O
are O
ni O
! O
ways O
of O
reordering O
the O
objects O
, O
and O
so O
the O
total O
number O
of O
ways O
of O
allocating O
the O
n O
objects O
to O
the O
bins O
is O
given O
by O
w O
= O
n O
! O
( O
cid:21 O
) O
i O
ni O
! O
( O
1.94 O
) O
which O
is O
called O
the O
multiplicity B
. O
in O
such O
cases O
, O
this O
strategy O
can O
fail O
because O
it O
is O
possible O
for O
the O
individual O
variable O
values O
obtained O
by O
maximizing O
the O
product O
of O
messages O
at O
each O
node B
to O
belong O
to O
different O
maximizing O
conﬁgurations O
, O
giving O
an O
overall O
conﬁguration O
that O
no O
longer O
corresponds O
to O
a O
maximum O
. O
this O
was O
a O
consequence O
of O
the O
quadratic O
dependence O
of O
the O
log O
likelihood O
function O
on O
the O
parameter O
vector O
w. O
for O
logistic O
regression B
, O
there O
is O
no O
longer O
a O
closed-form O
solution O
, O
due O
to O
the O
nonlinearity O
of O
the O
logistic B
sigmoid I
function O
. O
here O
we O
shall O
focus O
mainly O
on O
simple O
frequentist B
methods O
. O
this O
corresponds O
to O
propagating O
a O
message O
back O
down O
the O
chain O
using O
n−1 O
= O
φ O
( O
xmax O
xmax O
n O
) O
( O
8.102 O
) O
and O
is O
known O
as O
back-tracking B
. O
gaussian O
ink O
with O
density B
determined O
by O
0- O
the O
accumulated O
ink O
density B
gives O
rise O
to O
a O
'pancake O
' O
shaped O
distribution O
represent O
( O
cid:173 O
) O
ing O
the O
marginal B
density O
p O
( O
x O
) O
. O
consider O
a O
gaussian O
distribution O
over O
two O
variables O
x O
= O
( O
x1 O
, O
x2 O
) O
having O
mean B
µ O
= O
( O
µ1 O
, O
µ2 O
) O
and O
a O
covariance B
matrix I
σ O
= O
σ2i O
where O
i O
is O
the O
2 O
× O
2 O
identity O
matrix O
, O
so O
that O
p O
( O
x1 O
, O
x2 O
) O
= O
1 O
2πσ2 O
exp O
− O
( O
x1 O
− O
µ1 O
) O
2 O
+ O
( O
x2 O
− O
µ2 O
) O
2 O
2σ2 O
. O
probabilis- O
tic O
principal B
component I
analysis I
. O
we O
have O
seen O
that O
for O
data O
points O
that O
are O
on O
the O
correct O
side O
of O
the O
margin B
boundary O
, O
and O
which O
therefore O
satisfy O
yntn O
( O
cid:2 O
) O
1 O
, O
we O
have O
ξn O
= O
0 O
, O
and O
for O
the O
7.1. O
maximum B
margin I
classiﬁers O
337 O
e O
( O
z O
) O
figure O
7.5 O
plot O
of O
the O
‘ O
hinge O
’ O
error B
function I
used O
in O
support B
vector I
machines O
, O
shown O
in O
blue O
, O
along O
with O
the O
error B
function I
for O
logistic B
regression I
, O
rescaled O
by O
a O
factor O
of O
1/ O
ln O
( O
2 O
) O
so O
that O
it O
passes O
through O
the O
point O
( O
0 O
, O
1 O
) O
, O
shown O
in O
red O
. O
hierarchical B
mixtures O
of O
experts O
and O
the O
em O
algorithm O
. O
principal O
curves O
can O
be O
generalized B
to O
multidimensional O
manifolds O
called O
principal O
surfaces O
although O
these O
have O
found O
limited O
use O
due O
to O
the O
difficulty O
of O
data O
smoothing O
in O
higher O
dimensions O
even O
for O
two-dimensional O
manifolds O
. O
an O
infor- O
mation B
maximization O
approach O
to O
blind O
separa- O
tion O
and O
blind O
deconvolution O
. O
it O
can O
be O
regarded O
as O
a O
generalization B
of O
the O
notion O
of O
matrix O
inverse B
to O
nonsquare O
matrices O
. O
in O
many O
applications O
, O
it O
will O
be O
appropriate O
to O
use O
a O
common O
noise O
variance B
, O
governed O
by O
a O
precision B
parameter I
β O
, O
for O
all O
k O
components O
, O
and O
this O
is O
the O
case O
we O
consider O
here O
. O
4.1.3 O
least O
squares O
for O
classiﬁcation O
in O
chapter O
3 O
, O
we O
considered O
models O
that O
were O
linear O
functions O
of O
the O
parame- O
ters O
, O
and O
we O
saw O
that O
the O
minimization O
of O
a O
sum-of-squares B
error I
function O
led O
to O
a O
simple O
closed-form O
solution O
for O
the O
parameter O
values O
. O
10.3.2 O
predictive B
distribution I
the O
predictive B
distribution I
over O
t O
, O
given O
a O
new O
input O
x O
, O
is O
easily O
evaluated O
for O
this O
model O
using O
the O
gaussian O
variational B
posterior O
for O
the O
parameters O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:6 O
) O
p O
( O
t|x O
, O
t O
) O
= O
( O
cid:7 O
) O
p O
( O
t|x O
, O
w O
) O
p O
( O
w|t O
) O
dw O
p O
( O
t|x O
, O
w O
) O
q O
( O
w O
) O
dw O
n O
( O
t|wtφ O
( O
x O
) O
, O
β O
n O
φ O
( O
x O
) O
, O
σ2 O
( O
x O
) O
) O
= O
= O
n O
( O
t|mt O
−1 O
) O
n O
( O
w|mn O
, O
sn O
) O
dw O
( O
10.105 O
) O
10.3. O
variational B
linear O
regression B
489 O
where O
we O
have O
evaluated O
the O
integral O
by O
making O
use O
of O
the O
result O
( O
2.115 O
) O
for O
the O
linear-gaussian O
model O
. O
theory B
of O
probability B
and O
its O
applications O
9 O
( O
1 O
) O
, O
141–142 O
. O
due O
to O
the O
choice O
of O
a O
conjugate B
gaus- O
sian O
prior B
distribution O
, O
the O
posterior O
will O
also O
be O
gaussian O
. O
learning B
the O
similarity O
of O
doc- O
uments O
: O
an O
information-geometric O
approach O
to O
document B
retrieval I
and O
classiﬁcation B
. O
in O
other O
pattern O
recognition O
problems O
, O
the O
training B
data O
consists O
of O
a O
set O
of O
input O
vectors O
x O
without O
any O
corresponding O
target O
values O
. O
in O
order O
to O
obtain O
a O
lower B
bound I
on O
p O
( O
t O
) O
, O
we O
make O
use O
of O
the O
variational B
lower O
bound O
on O
the O
logistic B
sigmoid I
function O
given O
by O
( O
10.144 O
) O
, O
which O
( O
10.148 O
) O
10.6. O
variational B
logistic O
regression B
499 O
( O
cid:27 O
) O
we O
reproduce O
here O
for O
convenience O
σ O
( O
z O
) O
( O
cid:2 O
) O
σ O
( O
ξ O
) O
exp O
( O
cid:26 O
) O
where O
λ O
( O
ξ O
) O
= O
we O
can O
therefore O
write O
p O
( O
t|w O
) O
= O
eatσ O
( O
−a O
) O
( O
cid:2 O
) O
eatσ O
( O
ξ O
) O
exp O
( O
cid:30 O
) O
( O
z O
− O
ξ O
) O
/2 O
− O
λ O
( O
ξ O
) O
( O
z2 O
− O
ξ2 O
) O
( O
cid:29 O
) O
( O
cid:26 O
) O
− O
( O
a O
+ O
ξ O
) O
/2 O
− O
λ O
( O
ξ O
) O
( O
a2 O
− O
ξ2 O
) O
( O
cid:27 O
) O
σ O
( O
ξ O
) O
− O
1 O
2 O
1 O
2ξ O
. O
( O
2.174 O
) O
( O
2.175 O
) O
next O
we O
substitute O
these O
transformations O
into O
the O
two-dimensional O
gaussian O
distribu- O
tion O
( O
2.173 O
) O
, O
and O
then O
condition O
on O
the O
unit O
circle O
r O
= O
1 O
, O
noting O
that O
we O
are O
interested O
only O
in O
the O
dependence O
on O
θ. O
focussing O
on O
the O
exponent O
in O
the O
gaussian O
distribution O
we O
have O
( O
cid:26 O
) O
( O
r O
cos O
θ O
− O
r0 O
cos O
θ0 O
) O
2 O
+ O
( O
r O
sin O
θ O
− O
r0 O
sin O
θ0 O
) O
2 O
( O
cid:27 O
) O
0 O
− O
2r0 O
cos O
θ O
cos O
θ0 O
− O
2r0 O
sin O
θ O
sin O
θ0 O
1 O
+ O
r2 O
( O
cid:27 O
) O
( O
cid:26 O
) O
− O
1 O
2σ2 O
= O
− O
1 O
2σ2 O
= O
r0 O
( O
2.176 O
) O
σ2 O
cos O
( O
θ O
− O
θ0 O
) O
+ O
const O
108 O
2. O
probability B
distributions O
m O
= O
5 O
, O
θ0 O
= O
π/4 O
m O
= O
1 O
, O
θ0 O
= O
3π/4 O
3π/4 O
π/4 O
0 O
2π O
m O
= O
5 O
, O
θ0 O
= O
π/4 O
m O
= O
1 O
, O
θ0 O
= O
3π/4 O
figure O
2.19 O
the O
von O
mises O
distribution O
plotted O
for O
two O
different O
parameter O
values O
, O
shown O
as O
a O
cartesian O
plot O
on O
the O
left O
and O
as O
the O
corresponding O
polar O
plot O
on O
the O
right O
. O
the O
derivative B
of O
the O
function O
then O
describes O
how O
the O
output O
value O
varies O
as O
we O
make O
inﬁnitesimal O
changes O
to O
the O
input O
value O
. O
consider O
the O
set O
of O
distributions O
such O
that O
for O
each O
distribution O
there O
exists O
a O
directed B
graph O
that O
is O
a O
perfect B
map I
. O
, O
yn O
) O
t O
, O
and O
φ O
is O
the O
design B
matrix I
with O
elements O
φni O
= O
φi O
( O
xn O
) O
. O
exercises O
459 O
9.25 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
lower B
bound I
l O
( O
q O
, O
θ O
) O
given O
by O
( O
9.71 O
) O
, O
with O
q O
( O
z O
) O
= O
p O
( O
z|x O
, O
θ O
( O
old O
) O
) O
, O
has O
the O
same O
gradient O
with O
respect O
to O
θ O
as O
the O
log O
likelihood O
function O
ln O
p O
( O
x|θ O
) O
at O
the O
point O
θ O
= O
θ O
( O
old O
) O
. O
( O
11.55 O
) O
it O
is O
convenient O
to O
reformulate O
this O
dynamical B
system I
using O
the O
hamiltonian O
framework O
. O
to O
do O
this O
, O
we O
simply O
make O
use O
of O
the O
back-tracking B
procedure O
discussed O
in O
sec- O
tion O
8.4.5. O
speciﬁcally O
, O
we O
note O
that O
the O
maximization O
over O
zn O
must O
be O
performed O
for O
each O
of O
the O
k O
possible O
values O
of O
zn+1 O
. O
366 O
8. O
graphical O
models O
figure O
8.8 O
a O
graphical B
model I
representing O
the O
process O
by O
which O
images O
of O
objects O
are O
created O
, O
in O
which O
the O
identity O
of O
an O
object O
( O
a O
discrete O
variable O
) O
and O
the O
position O
and O
orientation O
of O
that O
object O
( O
continuous O
variables O
) O
have O
independent B
prior O
probabilities O
. O
( O
5.138 O
) O
i O
j=1 O
the O
total O
error B
function I
is O
then O
given O
by O
( O
cid:4 O
) O
e O
( O
w O
) O
= O
e O
( O
w O
) O
+ O
λω O
( O
w O
) O
( O
5.139 O
) O
where O
λ O
is O
the O
regularization B
coefﬁcient O
. O
8.4.5 O
the O
max-sum B
algorithm I
. O
values O
of O
x2 O
are O
given O
by O
copying O
the O
corresponding O
values O
of O
x1 O
and O
adding O
noise O
, O
and O
values O
of O
x3 O
are O
sampled O
from O
an O
independent B
gaussian O
dis- O
tribution O
. O
the O
cliques O
of O
the O
form O
{ O
xi O
, O
yi O
} O
have O
an O
associated O
energy B
function I
that O
expresses O
the O
correlation O
between O
these O
variables O
. O
then O
the O
probability B
of O
selecting O
an O
apple O
is O
just O
the O
fraction O
of O
apples O
in O
the O
blue O
box O
which O
is O
3/4 O
, O
and O
so O
p O
( O
f O
= O
a|b O
= O
b O
) O
= O
3/4 O
. O
in O
fact O
if O
we O
consider O
the O
limit O
n O
→ O
∞ O
then O
the O
bayesian O
treatment O
converges O
to O
the O
maximum B
likelihood I
em O
algorithm O
. O
10.2 O
( O
( O
cid:12 O
) O
) O
use O
the O
properties O
e O
[ O
z1 O
] O
= O
m1 O
and O
e O
[ O
z2 O
] O
= O
m2 O
to O
solve O
the O
simultaneous O
equa- O
tions O
( O
10.13 O
) O
and O
( O
10.15 O
) O
, O
and O
hence O
show O
that O
, O
provided O
the O
original O
distribution O
p O
( O
z O
) O
is O
nonsingular O
, O
the O
unique O
solution O
for O
the O
means O
of O
the O
factors O
in O
the O
approxi- O
mation B
distribution O
is O
given O
by O
e O
[ O
z1 O
] O
= O
µ1 O
and O
e O
[ O
z2 O
] O
= O
µ2 O
. O
also O
shown O
is O
the O
test B
set I
error O
( O
blue O
curve O
) O
showing O
that O
the O
evidence O
maximum O
occurs O
close O
to O
the O
point O
of O
best O
generalization B
. O
here O
we O
will O
make O
the O
naive O
bayes O
assumption O
in O
which O
the O
feature O
values O
are O
treated O
as O
independent B
, O
conditioned O
on O
the O
class O
ck O
. O
`` O
`` O
e O
ooie O
that O
i O
'' O
< O
tandard O
linear O
i'ca O
, O
we O
often O
retain O
some O
redoce O
< O
l O
num· O
ber O
l O
< O
dof O
eigenvectors O
and O
then O
appro O
, O
lmale O
0 O
data O
vttl O
< O
: O
> O
r O
xn O
b O
} O
' O
its O
projection O
i~ O
0 O
, O
,1 O
'' O
lhe O
l-dimensional O
principal B
subspace I
, O
defined O
by O
, O
i~-l O
: O
« O
`` O
, O
) O
'' O
'' O
( O
12.88 O
) O
i O
'' O
kernell'ca O
. O
a O
better O
approach O
is O
to O
use O
the O
reconstruction O
error B
for O
cluster O
assignment O
( O
kambhatla O
and O
leen O
, O
1997 O
; O
hinton O
et O
al. O
, O
1997 O
) O
as O
then O
a O
common O
cost B
function I
is O
being O
optimized O
in O
each O
stage O
. O
here O
the O
prior B
parameters O
have O
been O
set O
to O
a0 O
= O
b0 O
= O
0 O
, O
corresponding O
to O
the O
noninformative B
prior I
p O
( O
α O
) O
∝ O
1/α O
, O
which O
is O
uniform O
over O
ln O
α O
as O
discussed O
in O
section O
2.3.6. O
as O
we O
saw O
in O
section O
10.1 O
, O
the O
quantity O
l O
represents O
lower B
bound I
on O
the O
log O
marginal O
likelihood O
p O
( O
t|m O
) O
for O
the O
model O
. O
the O
newton-raphson O
update O
, O
for O
minimizing O
a O
function O
e O
( O
w O
) O
, O
takes O
the O
form O
( O
fletcher O
, O
1987 O
; O
bishop O
and O
nabney O
, O
2008 O
) O
( O
4.92 O
) O
where O
h O
is O
the O
hessian O
matrix O
whose O
elements O
comprise O
the O
second O
derivatives O
of O
e O
( O
w O
) O
with O
respect O
to O
the O
components O
of O
w. O
let O
us O
ﬁrst O
of O
all O
apply O
the O
newton-raphson O
method O
to O
the O
linear B
regression I
model O
( O
3.3 O
) O
with O
the O
sum-of-squares B
error I
function O
( O
3.12 O
) O
. O
this O
will O
either O
leave O
the O
probability B
unchanged O
, O
if O
xj O
is O
unchanged O
, O
or O
will O
increase O
it O
. O
ahhough O
the O
data O
space O
comprises O
12 O
measuremenls O
, O
a O
data O
set O
of O
points O
will O
lie O
close O
to O
a O
iwo-dimensional O
manifold B
embedded O
within O
this O
space O
. O
( O
2.292 O
) O
verify O
that O
substituting O
the O
expression O
for O
a O
gaussian O
distribution O
into O
the O
robbins- O
monro O
sequential B
estimation I
formula O
( O
2.135 O
) O
gives O
a O
result O
of O
the O
same O
form O
, O
and O
hence O
obtain O
an O
expression O
for O
the O
corresponding O
coefﬁcients O
an O
. O
this O
conditional B
density O
represents O
a O
complete O
description O
of O
the O
generator O
of O
the O
data O
, O
so O
far O
as O
the O
problem O
of O
predicting O
the O
value O
of O
the O
output O
vector O
is O
concerned O
. O
as O
usual O
, O
we O
can O
deﬁne O
an O
error B
function I
by O
taking O
the O
negative O
logarithm O
of O
the O
likelihood O
, O
which O
gives O
the O
cross- O
entropy B
error O
function O
in O
the O
form O
e O
( O
w O
) O
= O
− O
ln O
p O
( O
t|w O
) O
= O
− O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:14 O
) O
n=1 O
n O
( O
cid:2 O
) O
exercise O
4.13 O
section O
3.1.1 O
exercise O
4.14 O
{ O
tn O
ln O
yn O
+ O
( O
1 O
− O
tn O
) O
ln O
( O
1 O
− O
yn O
) O
} O
( O
4.90 O
) O
where O
yn O
= O
σ O
( O
an O
) O
and O
an O
= O
wtφn O
. O
similarly O
, O
we O
can O
deﬁne O
uf O
to O
be O
the O
set O
of O
such O
distributions O
that O
can O
be O
expressed O
as O
a O
factorization B
of O
the O
form O
( O
8.39 O
) O
with O
respect O
to O
the O
maximal O
cliques O
of O
the O
graph O
. O
5.4. O
the O
hessian O
matrix O
251 O
5.4.2 O
outer B
product I
approximation I
when O
neural O
networks O
are O
applied O
to O
regression B
problems O
, O
it O
is O
common O
to O
use O
a O
sum-of-squares B
error I
function O
of O
the O
form O
( O
yn O
− O
tn O
) O
2 O
( O
5.82 O
) O
n O
( O
cid:2 O
) O
n=1 O
e O
= O
1 O
2 O
n O
( O
cid:2 O
) O
exercise O
5.20 O
an O
analogous O
result O
can O
be O
obtained O
for O
multiclass O
networks O
having O
softmax O
output- O
unit O
activation O
functions O
. O
in O
the O
e O
step O
, O
we O
evaluate O
the O
posterior O
distribution O
over O
w O
given O
by O
( O
10.156 O
) O
, O
in O
which O
the O
mean B
and O
covari- O
ance O
are O
deﬁned O
by O
( O
10.157 O
) O
and O
( O
10.158 O
) O
. O
bayesian O
gaussian O
processes O
for B
regression I
and O
classiﬁcation B
. O
at O
one O
extreme O
we O
have O
a O
fully B
connected I
graph O
that O
exhibits O
no O
conditional B
in- O
dependence O
properties O
at O
all O
, O
and O
which O
can O
represent O
any O
possible O
joint O
probability B
distribution O
over O
the O
given O
variables O
. O
• O
the O
existence O
of O
a O
likelihood B
function I
allows O
direct O
comparison O
with O
other O
probabilistic O
density O
models O
. O
l O
( O
q O
, O
θold O
) O
ln O
p O
( O
x|θold O
) O
shown O
in O
figure O
9.13. O
if O
we O
substitute O
q O
( O
z O
) O
= O
p O
( O
z|x O
, O
θold O
) O
into O
( O
9.71 O
) O
, O
we O
see O
that O
, O
after O
the O
e O
step O
, O
the O
lower B
bound I
takes O
the O
form O
l O
( O
q O
, O
θ O
) O
= O
p O
( O
z|x O
, O
θold O
) O
ln O
p O
( O
x O
, O
z|θ O
) O
− O
p O
( O
z|x O
, O
θold O
) O
ln O
p O
( O
z|x O
, O
θold O
) O
( O
cid:2 O
) O
( O
cid:2 O
) O
z O
= O
q O
( O
θ O
, O
θold O
) O
+ O
const O
z O
( O
9.74 O
) O
where O
the O
constant O
is O
simply O
the O
negative O
entropy B
of O
the O
q O
distribution O
and O
is O
there- O
fore O
independent B
of O
θ. O
thus O
in O
the O
m O
step O
, O
the O
quantity O
that O
is O
being O
maximized O
is O
the O
expectation B
of O
the O
complete-data O
log O
likelihood O
, O
as O
we O
saw O
earlier O
in O
the O
case O
of O
mix- O
tures O
of O
gaussians O
. O
a O
matrix O
a O
is O
said O
to O
be O
positive B
deﬁnite I
, O
denoted O
by O
a O
( O
cid:7 O
) O
0 O
, O
if O
wtaw O
> O
0 O
for O
all O
values O
of O
the O
vector O
w. O
equivalently O
, O
a O
positive B
deﬁnite I
matrix I
has O
λi O
> O
0 O
for O
all O
of O
its O
eigenvalues O
( O
as O
can O
be O
seen O
by O
setting O
w O
to O
each O
of O
the O
eigenvectors O
in O
turn O
, O
and O
by O
noting O
that O
an O
arbitrary O
vector O
can O
be O
expanded O
as O
a O
linear O
combination O
of O
the O
eigenvectors O
) O
. O
for O
this O
reason O
, O
there O
has O
been O
some O
interest O
in O
using O
a O
diagonal B
approximation I
to O
the O
hessian O
, O
in O
other O
words O
one O
that O
simply O
replaces O
the O
off-diagonal O
elements O
with O
zeros O
, O
because O
its O
inverse B
is O
trivial O
to O
evaluate O
. O
there O
will O
be O
many O
possible O
paths O
converging O
on O
the O
correspond- O
ing O
node O
in O
the O
lattice B
diagram I
. O
em O
for O
gaussian O
mixtures O
given O
a O
gaussian O
mixture B
model I
, O
the O
goal O
is O
to O
maximize O
the O
likelihood B
function I
with O
respect O
to O
the O
parameters O
( O
comprising O
the O
means O
and O
covariances O
of O
the O
components O
and O
the O
mixing O
coefﬁcients O
) O
. O
ensem- O
ble O
learning B
for O
blind B
source I
separation I
. O
1.13 O
( O
( O
cid:1 O
) O
) O
suppose O
that O
the O
variance B
of O
a O
gaussian O
is O
estimated O
using O
the O
result O
( O
1.56 O
) O
but O
with O
the O
maximum B
likelihood I
estimate O
µml O
replaced O
with O
the O
true O
value O
µ O
of O
the O
mean B
. O
if O
we O
had O
been O
asked O
which O
box O
had O
been O
chosen O
before O
being O
told O
the O
identity O
of O
the O
selected O
item O
of O
fruit O
, O
then O
the O
most O
complete O
information O
we O
have O
available O
is O
provided O
by O
the O
probability B
p O
( O
b O
) O
. O
similarly O
, O
if O
the O
leaf O
node B
is O
a O
factor O
node O
, O
we O
see O
from O
( O
8.66 O
) O
that O
the O
message O
sent O
should O
take O
the O
form O
µx→f O
( O
x O
) O
= O
1 O
µf→x O
( O
x O
) O
= O
f O
( O
x O
) O
( O
8.71 O
) O
figure O
8.49 O
the O
sum-product B
algorithm I
begins O
with O
messages O
sent O
by O
the O
leaf O
nodes O
, O
which O
de- O
pend O
on O
whether O
the O
leaf O
node B
is O
( O
a O
) O
a O
variable O
node B
, O
or O
( O
b O
) O
a O
factor O
node O
. O
10.6.1 O
variational B
posterior O
distribution O
here O
we O
shall O
make O
use O
of O
a O
variational B
approximation O
based O
on O
the O
local B
bounds O
introduced O
in O
section O
10.5. O
this O
allows O
the O
likelihood B
function I
for O
logistic O
regres- O
sion B
, O
which O
is O
governed O
by O
the O
logistic B
sigmoid I
, O
to O
be O
approximated O
by O
the O
expo- O
nential O
of O
a O
quadratic O
form O
. O
similarly O
, O
show O
that O
the O
maximum B
likelihood I
solution O
for O
the O
shared O
covariance O
matrix O
is O
given O
by O
k O
( O
cid:2 O
) O
k=1 O
σ O
= O
nk O
n O
sk O
where O
sk O
= O
1 O
nk O
tnk O
( O
φn O
− O
µk O
) O
( O
φn O
− O
µk O
) O
t. O
n O
( O
cid:2 O
) O
n=1 O
( O
4.162 O
) O
( O
4.163 O
) O
thus O
σ O
is O
given O
by O
a O
weighted O
average O
of O
the O
covariances O
of O
the O
data O
associated O
with O
each O
class O
, O
in O
which O
the O
weighting O
coefﬁcients O
are O
given O
by O
the O
prior B
probabilities O
of O
the O
classes O
. O
pca O
is O
often O
used O
to O
project O
a O
data O
set O
onto O
a O
lower-dimensional O
space O
, O
for O
ex O
( O
cid:173 O
) O
ample O
two O
dimensional O
, O
for O
the O
purposes O
of O
visualization B
. O
ieee O
transactions O
on O
infor- O
mation B
theory O
it-13 O
, O
260–267 O
. O
( O
1992 O
) O
giving O
rise O
to O
the O
technique O
of O
support B
vector I
machines O
. O
the O
use O
of O
this O
gradient O
information O
can O
lead O
to O
signiﬁcant O
improvements O
in O
the O
speed O
with O
which O
the O
minima O
of O
the O
error B
function I
can O
be O
located O
. O
we O
denote O
the O
probability B
of O
observing O
both O
x1k O
= O
1 O
and O
x2l O
= O
1 O
by O
the O
parameter O
µkl O
, O
where O
x1k O
denotes O
the O
kth O
component O
of O
x1 O
, O
and O
similarly O
for O
x2l O
. O
having O
found O
the O
posterior O
probabilities O
, O
we O
use O
decision B
theory I
to O
determine O
class O
membership O
for O
each O
new O
input O
x. O
approaches O
that O
explicitly O
or O
implicitly O
model O
the O
distribution O
of O
inputs O
as O
well O
as O
outputs O
are O
known O
as O
generative O
models O
, O
because O
by O
sampling O
from O
them O
it O
is O
possible O
to O
generate O
synthetic O
data O
points O
in O
the O
input O
space O
. O
we O
also O
consider O
some O
extensions O
to O
the O
neural B
network I
model O
, O
and O
in O
particular O
we O
describe O
a O
gen- O
eral O
framework O
for O
modelling O
conditional B
probability I
distributions O
known O
as O
mixture O
density O
networks O
. O
( O
4.116 O
) O
the O
generalized B
linear I
model I
based O
on O
a O
probit O
activation O
function O
is O
known O
as O
probit B
regression I
. O
however O
, O
we O
shall O
ﬁnd O
it O
highly O
advantageous O
to O
augment O
the O
analysis O
using O
diagrammatic O
representations O
of O
probability B
distributions O
, O
called O
probabilistic O
graphical O
models O
. O
converting O
from O
an O
undirected B
to O
a O
directed B
representation O
is O
much O
less O
common O
and O
in O
general O
presents O
problems O
due O
to O
the O
normalization O
constraints O
. O
the O
categories O
of O
the O
digits O
in O
the O
training B
set I
are O
known O
in O
advance O
, O
typically O
by O
inspecting O
them O
individually O
and O
hand-labelling O
them O
. O
because O
it O
is O
at O
this O
stage O
that O
errors O
are O
propagated O
backwards O
through O
the O
network O
, O
we O
shall O
use O
the O
term O
backpropagation B
speciﬁcally O
to O
describe O
the O
evaluation O
of O
derivatives O
. O
the O
interesting O
term O
is O
the O
model B
evidence I
p O
( O
d|mi O
) O
which O
expresses O
the O
preference O
shown O
by O
the O
data O
for O
p O
( O
mi|d O
) O
∝ O
p O
( O
mi O
) O
p O
( O
d|mi O
) O
. O
however O
, O
the O
key O
differ- O
ence O
in O
the O
rvm O
is O
that O
we O
introduce O
a O
separate O
hyperparameter B
αi O
for O
each O
of O
the O
weight O
parameters O
wi O
instead O
of O
a O
single O
shared O
hyperparameter O
. O
1.36 O
( O
( O
cid:1 O
) O
) O
a O
strictly O
convex B
function I
is O
deﬁned O
as O
one O
for O
which O
every O
chord O
lies O
above O
the O
function O
. O
4.6 O
( O
( O
cid:12 O
) O
) O
using O
the O
deﬁnitions O
of O
the O
between-class B
and O
within-class B
covariance I
matrices O
given O
by O
( O
4.27 O
) O
and O
( O
4.28 O
) O
, O
respectively O
, O
together O
with O
( O
4.34 O
) O
and O
( O
4.36 O
) O
and O
the O
choice O
of O
target O
values O
described O
in O
section O
4.1.5 O
, O
show O
that O
the O
expression O
( O
4.33 O
) O
that O
minimizes O
the O
sum-of-squares B
error I
function O
can O
be O
written O
in O
the O
form O
( O
4.37 O
) O
. O
, O
tn O
, O
tn O
+1 O
) O
t. O
we O
then O
apply O
the O
results O
from O
section O
2.3.1 O
to O
obtain O
the O
required O
conditional B
distribu- O
tion O
, O
as O
illustrated O
in O
figure O
6.7. O
from O
( O
6.61 O
) O
, O
the O
joint O
distribution O
over O
t1 O
, O
. O
however O
, O
if O
we O
started O
from O
an O
undirected B
graph I
, O
then O
in O
general O
there O
will O
be O
an O
unknown O
normalization O
coefﬁcient O
1/z O
. O
( O
2003 O
) O
that O
provide O
additional O
information O
on O
sampling O
526 O
11. O
sampling B
methods I
methods O
for O
statistical O
inference B
. O
by O
examining O
the O
quadratic O
form O
in O
the O
exponent O
of O
the O
joint O
distribution O
, O
and O
using O
the O
technique O
of O
‘ O
completing B
the I
square I
’ O
discussed O
in O
section O
2.3 O
, O
ﬁnd O
expressions O
for O
the O
mean B
and O
covariance B
of O
the O
marginal B
distribution O
p O
( O
y O
) O
in O
which O
the O
variable O
x O
has O
been O
integrated O
out O
. O
( O
cid:2 O
) O
( O
cid:2 O
) O
xn−1 O
⎡⎣ O
( O
cid:2 O
) O
⎤⎦ O
··· O
along O
the O
chain O
to O
node B
xn O
from O
node B
xn+1 O
. O
thus O
we O
see O
that O
minimizing O
this O
kullback-leibler O
divergence O
is O
equivalent O
to O
maximizing O
the O
likelihood B
function I
. O
then O
to O
transmit O
the O
whole O
image O
directly O
would O
cost O
24n O
bits B
. O
e O
( O
w O
) O
wa O
wb O
w2 O
w1 O
wc O
∇e O
following O
the O
discussion O
of O
section O
4.3.4 O
, O
we O
see O
that O
the O
output O
unit O
activation B
function I
, O
which O
corresponds O
to O
the O
canonical O
link O
, O
is O
given O
by O
the O
softmax B
function I
( O
5.25 O
) O
( O
cid:2 O
) O
j O
exp O
( O
ak O
( O
x O
, O
w O
) O
) O
exp O
( O
aj O
( O
x O
, O
w O
) O
) O
yk O
( O
x O
, O
w O
) O
= O
( O
cid:5 O
) O
which O
satisﬁes O
0 O
( O
cid:1 O
) O
yk O
( O
cid:1 O
) O
1 O
and O
k O
yk O
= O
1. O
note O
that O
the O
yk O
( O
x O
, O
w O
) O
are O
unchanged O
if O
a O
constant O
is O
added O
to O
all O
of O
the O
ak O
( O
x O
, O
w O
) O
, O
causing O
the O
error B
function I
to O
be O
constant O
for O
some O
directions O
in O
weight O
space O
. O
8 O
1. O
introduction O
figure O
1.5 O
graphs O
of O
the O
root-mean-square B
error I
, O
deﬁned O
by O
( O
1.3 O
) O
, O
evaluated O
on O
the O
training B
set I
and O
on O
an O
inde- O
pendent O
test B
set I
for O
various O
values O
of O
m. O
training B
test O
1 O
s O
m O
r O
e O
0.5 O
0 O
0 O
3 O
m O
6 O
9 O
for O
m O
= O
9 O
, O
the O
training B
set I
error O
goes O
to O
zero O
, O
as O
we O
might O
expect O
because O
this O
polynomial O
contains O
10 O
degrees B
of I
freedom I
corresponding O
to O
the O
10 O
coefﬁcients O
w0 O
, O
. O
consider O
the O
graph O
shown O
in O
figure O
8.2. O
this O
is O
not O
a O
fully B
connected I
graph O
because O
, O
for O
instance O
, O
there O
is O
no O
link B
from O
x1 O
to O
x2 O
or O
from O
x3 O
to O
x7 O
. O
now O
assume O
that O
σ O
is O
also O
to O
be O
determined O
from O
the O
data O
, O
and O
write O
down O
an O
expression O
for O
the O
maximum B
likelihood I
solution O
for O
σ. O
note O
that O
the O
optimizations O
of O
w O
and O
σ O
are O
now O
coupled O
, O
in O
contrast O
to O
the O
case O
of O
independent B
target O
variables O
discussed O
in O
section O
5.2 O
. O
544 O
11. O
sampling B
methods I
to O
show O
that O
this O
procedure O
samples O
from O
the O
required O
distribution O
, O
we O
ﬁrst O
of O
all O
note O
that O
the O
distribution O
p O
( O
z O
) O
is O
an O
invariant O
of O
each O
of O
the O
gibbs O
sampling O
steps O
individually O
and O
hence O
of O
the O
whole O
markov O
chain O
. O
using O
the O
orthonormality O
property O
of O
the O
matrix O
u O
, O
we O
see O
that O
the O
square O
of O
the O
determinant O
of O
the O
jacobian O
matrix O
is O
( O
2.54 O
) O
and O
hence O
|j| O
= O
1. O
also O
, O
the O
determinant O
|σ| O
of O
the O
covariance B
matrix I
can O
be O
written O
( O
cid:7 O
) O
( O
cid:7 O
) O
ut O
( O
cid:7 O
) O
( O
cid:7 O
) O
2 O
= O
|j|2 O
= O
( O
cid:7 O
) O
( O
cid:7 O
) O
ut O
( O
cid:7 O
) O
( O
cid:7 O
) O
|u| O
= O
( O
cid:7 O
) O
( O
cid:7 O
) O
utu O
( O
cid:7 O
) O
( O
cid:7 O
) O
= O
|i| O
= O
1 O
82 O
2. O
probability B
distributions O
as O
the O
product O
of O
its O
eigenvalues O
, O
and O
hence O
d O
( O
cid:14 O
) O
j=1 O
|σ|1/2 O
= O
1/2 O
j O
. O
moreover O
there O
will O
be O
multiple O
addilional O
degrees B
of I
freedom I
associaled O
wilh O
more O
complex O
deformations O
due O
to O
the O
variability O
in O
an O
individual O
's O
wriling O
3s O
well O
as O
lhe O
differences O
in O
writing O
slyles O
between O
individuals O
. O
10.19 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
derive O
the O
result O
( O
10.81 O
) O
for O
the O
predictive B
distribution I
in O
the O
variational B
treat- O
ment O
of O
the O
bayesian O
mixture O
of O
gaussians O
model O
. O
( O
8.49 O
) O
we O
shall O
consider O
the O
speciﬁc O
case O
in O
which O
the O
n O
nodes O
represent O
discrete O
vari- O
ables O
each O
having O
k O
states O
, O
in O
which O
case O
each O
potential B
function I
ψn−1 O
, O
n O
( O
xn−1 O
, O
xn O
) O
comprises O
an O
k O
× O
k O
table O
, O
and O
so O
the O
joint O
distribution O
has O
( O
n O
− O
1 O
) O
k O
2 O
parameters O
. O
it O
is O
also O
interesting O
to O
examine O
the O
behaviour O
of O
a O
given O
model O
as O
the O
size O
of O
the O
data O
set O
is O
varied O
, O
as O
shown O
in O
figure O
1.6. O
we O
see O
that O
, O
for O
a O
given O
model O
complexity O
, O
the O
over-ﬁtting B
problem O
become O
less O
severe O
as O
the O
size O
of O
the O
data O
set O
increases O
. O
8.8 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
a O
⊥⊥ O
b O
, O
c O
| O
d O
implies O
a O
⊥⊥ O
b O
| O
d. O
8.9 O
( O
( O
cid:12 O
) O
) O
www O
using O
the O
d-separation B
criterion O
, O
show O
that O
the O
conditional B
distribution O
for O
a O
node B
x O
in O
a O
directed B
graph O
, O
conditioned O
on O
all O
of O
the O
nodes O
in O
the O
markov O
blanket O
, O
is O
independent B
of O
the O
remaining O
variables O
in O
the O
graph O
. O
here O
we O
note O
that O
a O
hidden O
markov O
model O
can O
be O
trained O
effectively O
, O
using O
max- O
imum O
likelihood O
, O
provided O
the O
training B
sequence O
is O
sufﬁciently O
long O
. O
it O
is O
related O
to O
the O
probit B
function I
by O
φ O
( O
a O
) O
= O
1 O
2 O
1√ O
2 O
1 O
+ O
erf O
( O
a O
) O
. O
n=1 O
the O
corresponding O
karush-kuhn-tucker O
( O
kkt O
) O
conditions O
, O
which O
state O
that O
at O
the O
solution O
the O
product O
of O
the O
dual O
variables O
and O
the O
constraints O
must O
vanish O
, O
are O
given O
by O
and O
such O
points O
must O
lie O
either O
on O
or O
below O
the O
lower O
boundary O
of O
the O
-tube B
. O
because O
the O
weighing O
matrix O
r O
is O
not O
constant O
but O
depends O
on O
the O
parameter O
vector O
w O
, O
we O
must O
apply O
the O
normal B
equations I
iteratively O
, O
each O
time O
using O
the O
new O
weight B
vector I
w O
to O
compute O
a O
revised O
weighing O
matrix O
r. O
for O
this O
reason O
, O
the O
algorithm O
is O
known O
as O
iterative B
reweighted I
least I
squares I
, O
or O
irls O
( O
rubin O
, O
1983 O
) O
. O
originally O
designed O
for O
solving O
classiﬁcation B
problems O
, O
boosting B
can O
also O
be O
extended B
to O
regression B
( O
friedman O
, O
2001 O
) O
. O
finally O
, O
if O
we O
allow O
each O
component O
in O
the O
mixture B
model I
to O
be O
itself O
a O
mixture B
of I
experts I
model O
, O
then O
we O
obtain O
a O
hierarchical B
mixture I
of I
experts I
. O
an O
important O
concept O
is O
the O
treewidth B
of O
a O
graph O
( O
bodlaender O
, O
1993 O
) O
, O
which O
is O
de- O
ﬁned O
in O
terms O
of O
the O
number O
of O
variables O
in O
the O
largest O
clique B
. O
( O
2.118 O
) O
by O
simple O
rearrangement O
, O
we O
see O
that O
the O
likelihood B
function I
depends O
on O
the O
data O
set O
only O
through O
the O
two O
quantities O
n O
( O
cid:2 O
) O
xn O
, O
n=1 O
n=1 O
xnxt O
n. O
( O
2.119 O
) O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n O
( O
cid:2 O
) O
n=1 O
appendix O
c O
these O
are O
known O
as O
the O
sufﬁcient B
statistics I
for O
the O
gaussian O
distribution O
. O
another O
linear O
technique O
with O
a O
similar O
aim O
is O
multidimensional B
scaling I
, O
or O
mds O
( O
cox O
and O
cox O
, O
2000 O
) O
. O
we O
see O
that O
the O
decision B
boundary I
lies O
roughly O
mid O
way O
between O
the O
clusters O
of O
data O
points O
, O
and O
that O
the O
contours O
of O
the O
predictive B
distribution I
splay O
out O
away O
from O
the O
data O
reﬂecting O
the O
greater O
uncertainty O
in O
the O
classiﬁcation B
of O
such O
regions O
. O
5.5.6 O
convolutional B
networks O
another O
approach O
to O
creating O
models O
that O
are O
invariant O
to O
certain O
transformation O
of O
the O
inputs O
is O
to O
build O
the O
invariance B
properties O
into O
the O
structure O
of O
a O
neural O
net- O
work O
. O
to O
motivate O
this O
prior B
, O
we O
note O
that O
the O
likelihood B
function I
takes O
the O
form O
of O
the O
product O
of O
factors O
of O
the O
form O
µx O
( O
1 O
− O
µ O
) O
1−x O
. O
probability B
, O
random O
variables O
, O
and O
stochastic B
processes O
( O
second O
ed. O
) O
. O
, O
xm O
and O
a O
sin- O
gle O
child O
y O
, O
used O
to O
illustrate O
the O
idea O
of O
parameterized O
conditional B
distributions O
for O
discrete O
variables O
. O
as O
a O
comparison O
, O
we O
also O
ﬁt O
the O
same O
data O
set O
using O
a O
single O
multivariate O
bernoulli O
distribution O
, O
again O
using O
maximum B
likelihood I
. O
we O
denote O
the O
prior B
class O
probability B
p O
( O
c1 O
) O
= O
π O
, O
so O
that O
p O
( O
c2 O
) O
= O
1 O
− O
π. O
for O
a O
data O
point O
xn O
from O
class O
c1 O
, O
we O
have O
tn O
= O
1 O
and O
hence O
p O
( O
xn O
, O
c1 O
) O
= O
p O
( O
c1 O
) O
p O
( O
xn|c1 O
) O
= O
πn O
( O
xn|µ1 O
, O
σ O
) O
. O
this O
can O
be O
shown O
formally O
by O
including O
identity O
functions O
for O
the O
observed O
variables O
into O
the O
factor O
functions O
, O
as O
we O
did O
for O
the O
sum-product B
algorithm I
. O
suppose O
the O
original O
image O
has O
n O
pixels O
comprising O
{ O
r O
, O
g O
, O
b O
} O
values O
each O
of O
which O
is O
stored O
with O
8 O
bits B
of O
precision O
. O
if O
we O
had O
a O
large O
number O
of O
independent B
training O
sets O
of O
a O
given O
size O
, O
we O
would O
be O
better O
off O
combining O
them O
into O
a O
single O
large O
training O
set O
, O
which O
of O
course O
would O
reduce O
the O
level O
of O
over-ﬁtting B
for O
a O
given O
model O
complexity O
. O
and O
the O
diagonal B
elements O
of O
1j. O
' O
. O
the O
choice O
of O
activation B
function I
is O
determined O
by O
the O
nature O
of O
the O
data O
and O
the O
assumed O
distribution O
of O
target O
variables O
exercise O
5.1 O
228 O
5. O
neural O
networks O
figure O
5.1 O
network O
diagram O
for O
the O
two- O
layer O
neural B
network I
corre- O
sponding O
to O
( O
5.7 O
) O
. O
if O
we O
denote O
the O
starting O
tree B
for O
pruning O
by O
t0 O
, O
then O
we O
deﬁne O
t O
⊂ O
t0 O
to O
be O
a O
subtree O
of O
t0 O
if O
it O
can O
be O
obtained O
by O
pruning O
nodes O
from O
t0 O
( O
in O
other O
words O
, O
by O
collapsing O
internal O
nodes O
by O
combining O
the O
corresponding O
regions O
) O
. O
denote O
the O
mean B
and O
covariance B
of O
p O
( O
x|k O
) O
by O
µk O
and O
σk O
, O
respectively O
. O
exercise O
8.18 O
both O
directed B
and O
undirected B
graphs O
allow O
a O
global O
function O
of O
several O
vari- O
ables O
to O
be O
expressed O
as O
a O
product O
of O
factors O
over O
subsets O
of O
those O
variables O
. O
backpropagation B
applied O
to O
handwritten O
zip O
code O
recognition O
. O
how- O
ever O
, O
if O
the O
mixture B
components O
are O
members O
of O
the O
exponential B
family I
, O
then O
the O
responsibilities O
enter O
only O
through O
simple O
sufﬁcient B
statistics I
, O
and O
these O
can O
be O
up- O
dated O
efﬁciently O
. O
once O
we O
know O
the O
most O
probable O
value O
of O
the O
ﬁ- O
nal O
node B
xn O
, O
we O
can O
then O
simply O
follow O
the O
link B
back O
to O
ﬁnd O
the O
most O
probable O
state O
of O
node B
xn−1 O
and O
so O
on O
back O
to O
the O
initial O
node B
x1 O
. O
in O
section O
4.3.2 O
, O
we O
saw O
how O
a O
similar O
formula O
arises O
with O
the O
logistic B
sigmoid I
activation O
function O
together O
with O
the O
cross O
entropy B
error O
function O
, O
and O
similarly O
for O
the O
softmax O
activation O
function O
together O
with O
its O
matching O
cross-entropy B
error I
function I
. O
2 O
probability B
distributions O
in O
chapter O
1 O
, O
we O
emphasized O
the O
central O
role O
played O
by O
probability B
theory O
in O
the O
solution O
of O
pattern O
recognition O
problems O
. O
12.16 O
( O
* O
* O
* O
) O
in O
figure O
12.11 O
, O
we O
showed O
an O
application O
of O
probabilistic O
pca O
to O
a O
data O
set O
in O
which O
some O
of O
the O
data O
values O
were O
missing B
at I
random I
. O
the O
input O
values O
{ O
xn O
} O
are O
generated O
uniformly O
in O
range O
( O
0 O
, O
1 O
) O
, O
and O
the O
corresponding O
target O
values O
{ O
tn O
} O
are O
obtained O
by O
ﬁrst O
computing O
the O
corresponding O
values O
of O
the O
function O
sin O
( O
2πx O
) O
, O
and O
then O
adding O
random O
noise O
with O
a O
gaussian O
distribution O
having O
standard B
deviation I
0.3. O
various O
forms O
of O
this O
data O
set O
, O
having O
different O
numbers O
of O
data O
points O
, O
are O
used O
in O
the O
book O
. O
because O
the O
parameters O
are O
revised O
after O
each O
data O
point O
, O
rather O
than O
waiting O
until O
after O
the O
whole O
data O
set O
is O
processed O
, O
this O
incremental O
ver- O
sion B
can O
converge O
faster O
than O
the O
batch O
version O
. O
inal O
conditional B
distributions O
( O
conditioned O
on O
the O
parents O
) O
deﬁning O
each O
node B
, O
and O
so O
standard O
sampling O
techniques O
can O
be O
employed O
. O
the O
algorithm O
first O
defines O
the O
neighbourhood O
for O
each O
data O
point O
, O
either O
by O
finding O
the O
k O
nearest O
neighbours O
or O
by O
finding O
all O
points O
within O
a O
sphere O
of O
radius O
e. O
a O
graph O
is O
then O
constructed O
by O
link B
( O
cid:173 O
) O
ing O
all O
neighbouring O
points O
and O
labelling O
them O
with O
their O
euclidean O
distance O
. O
to O
see O
this O
, O
we O
can O
imagine O
taking O
observations O
one O
at O
a O
time O
and O
after O
each O
observation O
updating O
the O
current O
posterior O
prior O
2 O
1 O
0 O
0 O
2 O
1 O
0 O
0 O
0.5 O
µ O
1 O
likelihood B
function I
0.5 O
µ O
1 O
2.1. O
binary O
variables O
73 O
posterior O
2 O
1 O
0 O
0 O
0.5 O
µ O
1 O
figure O
2.3 O
illustration O
of O
one O
step O
of O
sequential O
bayesian O
inference B
. O
3.2. O
the O
bias-variance O
decomposition O
151 O
figure O
3.6 O
plot O
of O
squared O
bias B
and O
variance B
, O
together O
with O
their O
sum O
, O
correspond- O
ing O
to O
the O
results O
shown O
in O
fig- O
ure O
3.5. O
also O
shown O
is O
the O
average O
test B
set I
error O
for O
a O
test O
data O
set O
size O
of O
1000 O
points O
. O
for O
these O
parameter O
values O
, O
we O
can O
run O
the O
inference B
algorithm O
to O
determine O
the O
posterior O
distribution O
of O
the O
latent O
variables O
p O
( O
z|x O
, O
θold O
) O
, O
or O
more O
precisely O
those O
in O
particular O
, O
we O
shall O
local B
posterior O
marginals O
that O
are O
required O
in O
the O
m O
step O
. O
it O
appears O
that O
the O
bias B
parameter I
b O
has O
disappeared O
from O
the O
optimiza- O
tion O
. O
convexity O
also O
plays O
a O
central O
role O
in O
the O
local B
variational O
framework O
. O
furthermore O
, O
if O
data O
points O
arrive O
sequentially O
, O
then O
the O
posterior O
distribution O
at O
any O
stage O
acts O
as O
the O
prior B
distribution O
for O
the O
subsequent O
data O
point O
, O
such O
that O
the O
new O
posterior O
distribution O
is O
again O
given O
by O
( O
3.49 O
) O
. O
a O
tourist O
guide O
through O
treewidth B
. O
orthogonal B
least I
squares I
learning O
algorithm O
for O
radial O
basis B
function I
networks O
. O
t O
the O
outputs O
of O
a O
conventional O
neural B
network I
that O
takes O
x O
as O
its O
input O
. O
8.4.3 O
factor O
graphs O
the O
sum-product B
algorithm I
that O
we O
derive O
in O
the O
next O
section O
is O
applicable O
to O
undirected B
and O
directed B
trees O
and O
to O
polytrees O
. O
5.4. O
the O
hessian O
matrix O
we O
have O
shown O
how O
the O
technique O
of O
backpropagation B
can O
be O
used O
to O
obtain O
the O
ﬁrst O
derivatives O
of O
an O
error B
function I
with O
respect O
to O
the O
weights O
in O
the O
network O
. O
consider O
a O
general O
classiﬁcation B
problem O
with O
k O
classes O
, O
with O
a O
1-of-k O
binary O
coding O
scheme O
for O
the O
target B
vector I
t. O
one O
justiﬁcation O
for O
using O
least O
squares O
in O
such O
a O
context O
is O
that O
it O
approximates O
the O
conditional B
expectation I
e O
[ O
t|x O
] O
of O
the O
target O
values O
given O
the O
input O
vector O
. O
bayes O
’ O
theorem O
, O
represents O
an O
example O
of O
generative O
modelling O
, O
because O
we O
could O
take O
such O
a O
model O
and O
generate O
synthetic O
data O
by O
drawing O
values O
of O
x O
from O
the O
marginal B
distribution O
p O
( O
x O
) O
. O
there O
are O
now O
two O
kinds O
of O
solution O
possible O
, O
according O
to O
whether O
the O
con- O
strained O
stationary B
point O
lies O
in O
the O
region O
where O
g O
( O
x O
) O
> O
0 O
, O
in O
which O
case O
the O
con- O
straint O
is O
inactive O
, O
or O
whether O
it O
lies O
on O
the O
boundary O
g O
( O
x O
) O
= O
0 O
, O
in O
which O
case O
the O
constraint O
is O
said O
to O
be O
active O
. O
monte O
carlo O
implementation O
of O
gaussian O
process O
models O
for O
bayesian O
regression B
and O
classiﬁcation B
. O
networks O
for O
ap- O
proximation B
and O
learning B
. O
we O
see O
that O
the O
mean B
µk O
for O
the O
kth O
gaussian O
component O
is O
obtained O
by O
taking O
a O
weighted O
mean O
of O
all O
of O
the O
points O
in O
the O
data O
set O
, O
in O
which O
the O
weighting O
factor O
for O
data O
point O
xn O
is O
given O
by O
the O
posterior B
probability I
γ O
( O
znk O
) O
that O
component O
k O
was O
responsible O
for O
generating O
xn O
. O
( O
14.5 O
) O
n=1 O
n=1 O
zn O
thus O
we O
see O
that O
each O
observed O
data O
point O
xn O
has O
a O
corresponding O
latent B
variable I
zn O
. O
data O
, O
as O
can O
be O
seen O
by O
comparison O
with O
( O
9.40 O
) O
for O
the O
case O
of O
a O
gaussian O
mixture B
. O
other O
data O
points O
can O
be O
moved O
around O
freely O
( O
so O
long O
as O
they O
remain O
out- O
side O
the O
margin B
region O
) O
without O
changing O
the O
decision B
boundary I
, O
and O
so O
the O
solution O
will O
be O
independent B
of O
such O
data O
points O
. O
by O
contrast O
, O
a O
head-to-head O
node O
blocks O
a O
path O
if O
it O
is O
unobserved O
, O
but O
once O
the O
node B
, O
and/or O
at O
least O
one O
of O
its O
descendants O
, O
is O
observed O
the O
path O
becomes O
unblocked O
. O
thus O
the O
perceptron B
learning O
rule O
is O
not O
guaranteed O
to O
reduce O
the O
total O
error B
function I
at O
each O
stage O
. O
speciﬁcally O
, O
the O
required O
conditional B
probability I
is O
assumed O
to O
be O
of O
the O
form O
p O
( O
t O
= O
1|x O
) O
= O
σ O
( O
ay O
( O
x O
) O
+ O
b O
) O
( O
7.43 O
) O
where O
y O
( O
x O
) O
is O
deﬁned O
by O
( O
7.1 O
) O
. O
use O
the O
calculus O
of O
vari- O
ations O
to O
minimize O
this O
error B
function I
with O
respect O
to O
the O
function O
y O
( O
x O
) O
, O
and O
hence O
show O
that O
the O
optimal O
solution O
is O
given O
by O
an O
expansion O
of O
the O
form O
( O
6.40 O
) O
in O
which O
the O
basis O
functions O
are O
given O
by O
( O
6.41 O
) O
. O
note O
that O
we O
do O
not O
restrict O
the O
choice O
of O
potential O
functions O
to O
those O
that O
have O
a O
speciﬁc O
probabilistic O
interpretation O
as O
marginal B
or O
conditional B
distributions O
. O
the O
factor B
graph I
, O
however O
, O
keeps O
such O
factors O
explicit O
and O
so O
is O
able O
to O
convey O
more O
detailed O
information O
about O
the O
underlying O
factorization B
. O
, O
wk O
) O
= O
− O
n O
( O
cid:2 O
) O
∇wk O
ynk O
( O
ikj O
− O
ynj O
) O
φnφt O
n. O
( O
4.110 O
) O
exercise O
4.20 O
n=1 O
as O
with O
the O
two-class O
problem O
, O
the O
hessian O
matrix O
for O
the O
multiclass B
logistic O
regres- O
sion B
model O
is O
positive B
deﬁnite I
and O
so O
the O
error B
function I
again O
has O
a O
unique O
minimum O
. O
, O
tn O
) O
t. O
we O
can O
maximize O
this O
likelihood B
function I
iteratively O
by O
making O
use O
of O
the O
em O
algorithm O
. O
second O
, O
the O
value O
of O
the O
smoothing B
parameter I
should O
be O
neither O
too O
large O
nor O
too O
small O
in O
order O
to O
obtain O
good O
results O
. O
w O
n O
proximation B
, O
which O
is O
appropriate O
when O
the O
number O
of O
data O
points O
is O
relatively O
large O
and O
the O
corresponding O
posterior O
distribution O
is O
tightly O
peaked O
( O
bishop O
, O
1999a O
) O
. O
figure O
8.36 O
an O
undirected B
graph I
whose O
conditional B
independence I
properties O
can O
not O
be O
expressed O
in O
terms O
of O
a O
directed B
graph O
over O
the O
same O
variables O
. O
for O
large O
values O
of O
m O
, O
there O
is O
a O
clear O
advantage O
in O
working O
with O
the O
logistic B
regression I
model O
directly O
. O
expanding O
around O
this O
stationary B
point O
we O
have O
ln O
f O
( O
z O
) O
( O
cid:7 O
) O
ln O
f O
( O
z0 O
) O
− O
1 O
2 O
( O
z O
− O
z0 O
) O
ta O
( O
z O
− O
z0 O
) O
where O
the O
m O
× O
m O
hessian O
matrix O
a O
is O
deﬁned O
by O
a O
= O
− O
∇∇ O
ln O
f O
( O
z O
) O
|z=z0 O
( O
4.131 O
) O
( O
4.132 O
) O
and O
∇ O
is O
the O
gradient O
operator O
. O
to O
determine O
the O
reﬁned O
term O
( O
cid:4 O
) O
fjl O
( O
θl O
) O
, O
( O
cid:4 O
) O
fik O
( O
θk O
) O
\j O
( O
θ O
) O
∝ O
( O
10.238 O
) O
( O
cid:14 O
) O
( O
cid:14 O
) O
i O
( O
cid:9 O
) O
=j O
k O
q O
we O
need O
only O
consider O
the O
functional B
dependence O
on O
θl O
, O
and O
so O
we O
simply O
ﬁnd O
the O
corresponding O
marginal B
of O
( O
10.239 O
) O
up O
to O
a O
multiplicative O
constant O
, O
this O
involves O
taking O
the O
marginal B
of O
fj O
( O
θj O
) O
multiplied O
\j O
( O
θ O
) O
that O
are O
functions O
of O
any O
of O
the O
variables O
in O
θj O
. O
by O
taking O
the O
log- O
arithm O
and O
then O
exchanging O
maximizations O
and O
summations O
, O
derive O
the O
recursion O
650 O
13. O
sequential B
data I
( O
13.68 O
) O
where O
the O
quantities O
ω O
( O
zn O
) O
are O
deﬁned O
by O
( O
13.70 O
) O
. O
the O
entropy B
is O
then O
deﬁned O
as O
the O
logarithm O
of O
the O
multiplicity B
scaled O
by O
an O
appropriate O
constant O
h O
= O
1 O
n O
ln O
w O
= O
1 O
n O
ln O
n O
! O
− O
1 O
n O
ln O
ni O
! O
. O
here O
δ O
is O
another O
pre-speciﬁed O
parameter O
, O
and O
the O
terminology O
‘ O
probably O
approxi- O
mately O
correct O
’ O
comes O
from O
the O
requirement O
that O
with O
high O
probability B
( O
greater O
than O
1− O
δ O
) O
, O
the O
error B
rate O
be O
small O
( O
less O
than O
 O
) O
. O
the O
con- O
ditional O
expectation B
of O
z O
given O
θ O
deﬁnes O
a O
deterministic O
function O
f O
( O
θ O
) O
that O
is O
given O
by O
( O
cid:6 O
) O
f O
( O
θ O
) O
≡ O
e O
[ O
z|θ O
] O
= O
zp O
( O
z|θ O
) O
dz O
( O
2.127 O
) O
and O
is O
illustrated O
schematically O
in O
figure O
2.10. O
functions O
deﬁned O
in O
this O
way O
are O
called O
regression B
functions O
. O
3.23 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
marginal B
probability I
of O
the O
data O
, O
in O
other O
words O
the O
model B
evidence I
, O
for O
the O
model O
described O
in O
exercise O
3.12 O
is O
given O
by O
p O
( O
t O
) O
= O
1 O
( O
2π O
) O
n/2 O
ba0 O
0 O
ban O
n O
γ O
( O
an O
) O
γ O
( O
a0 O
) O
|sn|1/2 O
|s0|1/2 O
by O
ﬁrst O
marginalizing O
with O
respect O
to O
w O
and O
then O
with O
respect O
to O
β O
. O
in O
section O
14.2 O
, O
we O
dis- O
cuss O
ways O
to O
apply O
the O
committee B
concept O
in O
practice O
, O
and O
we O
also O
give O
some O
insight O
into O
why O
it O
can O
sometimes O
be O
an O
effective O
procedure O
. O
indeed O
, O
in O
a O
bayesian O
model O
the O
effective B
number I
of I
parameters I
adapts O
automatically O
to O
the O
size O
of O
the O
data O
set O
. O
suppose O
that O
the O
marginal B
distributions O
over O
the O
mean B
and O
precision O
are O
given O
by O
n O
( O
µ|µ0 O
, O
s0 O
) O
and O
gam O
( O
τ|a O
, O
b O
) O
, O
where O
gam O
( O
·|· O
, O
· O
) O
denotes O
a O
gamma B
distribution I
. O
by O
deﬁnition O
, O
the O
maximum O
like- O
lihood O
solution O
θml O
is O
a O
stationary B
point O
of O
the O
log O
likelihood O
function O
and O
hence O
satisﬁes O
( O
cid:24 O
) O
( O
2.133 O
) O
exchanging O
the O
derivative B
and O
the O
summation O
, O
and O
taking O
the O
limit O
n O
→ O
∞ O
we O
have O
θml O
n=1 O
= O
0 O
. O
an O
important O
motivation O
for O
such O
models O
is O
that O
many O
data O
sets O
have O
the O
property O
that O
the O
data O
points O
all O
lie O
close O
to O
a O
manifold B
of O
much O
lower O
dimensionality O
than O
that O
of O
the O
original O
data O
space O
. O
consider O
a O
problem O
in O
which O
the O
elements O
of O
the O
loss B
matrix I
are O
subjected O
to O
revision O
from O
time O
to O
time O
( O
such O
as O
might O
occur O
in O
a O
ﬁnancial O
1.5. O
decision B
theory I
45 O
application O
) O
. O
an O
example O
of O
a O
location B
parameter I
would O
be O
the O
mean B
µ O
of O
a O
gaussian O
distribution O
. O
the O
difference O
between O
these O
two O
results O
can O
be O
understood O
by O
noting O
that O
there O
is O
a O
large O
positive O
contribution O
to O
the O
kullback-leibler O
divergence O
kl O
( O
q O
( O
cid:5 O
) O
p O
) O
= O
− O
q O
( O
z O
) O
ln O
dz O
( O
10.18 O
) O
( O
cid:6 O
) O
( O
cid:12 O
) O
( O
cid:13 O
) O
p O
( O
z O
) O
q O
( O
z O
) O
10.1. O
variational B
inference I
469 O
( O
a O
) O
( O
b O
) O
( O
c O
) O
figure O
10.3 O
another O
comparison O
of O
the O
two O
alternative O
forms O
for O
the O
kullback-leibler O
divergence O
. O
thus O
the O
training B
data O
points O
are O
perfectly O
separated O
in O
the O
original O
data O
space O
. O
note O
that O
this O
is O
not O
a O
density B
with O
respect O
to O
zn O
and O
so O
is O
not O
normalized O
to O
one O
. O
( O
cid:26 O
) O
again O
we O
wish O
to O
construct O
a O
scalar O
that O
is O
large O
when O
the O
between-class B
covariance I
is O
large O
and O
when O
the O
within-class B
covariance I
is O
small O
. O
( O
7.51 O
) O
( O
7.52 O
) O
( O
7.53 O
) O
( O
7.54 O
) O
7.1. O
maximum B
margin I
classiﬁers O
341 O
figure O
7.7 O
illustration O
of O
svm O
regression B
, O
showing O
the O
regression B
curve O
together O
with O
the O
- O
insensitive O
‘ O
tube O
’ O
. O
the O
data O
used O
to O
ﬁt O
the O
sigmoid B
needs O
to O
be O
independent B
of O
that O
used O
to O
train O
the O
original O
svm O
in O
order O
to O
avoid O
severe O
over-ﬁtting B
. O
we O
will O
denote O
the O
mean B
and O
covariance B
of O
this O
distribution O
by O
µa|b O
and O
σa|b O
, O
respectively O
. O
even O
if O
the O
joint O
distribution O
p O
( O
x O
, O
z|θ O
) O
belongs O
to O
the O
exponential O
440 O
9. O
mixture B
models O
and O
em O
family O
, O
the O
marginal B
distribution O
p O
( O
x|θ O
) O
typically O
does O
not O
as O
a O
result O
of O
this O
sum- O
mation B
. O
this O
is O
a O
sequential O
selection O
process O
in O
which O
at O
each O
step O
the O
next O
data O
point O
to O
be O
chosen O
as O
a O
basis B
function I
centre O
corresponds O
to O
the O
one O
that O
gives O
the O
greatest O
reduction O
in O
the O
sum-of-squares B
error I
. O
the O
posterior O
covariance O
in O
this O
limit O
is O
2 O
> O
0 O
, O
the O
latent O
projection O
zero O
, O
however O
, O
and O
the O
density B
becomes O
singular O
. O
this O
is O
illustrated O
in O
figure O
8.25. O
alternatively O
, O
we O
can O
use O
the O
graph O
as O
a O
different O
kind O
of O
ﬁlter O
by O
ﬁrst O
listing O
all O
of O
the O
conditional B
independence I
properties O
obtained O
by O
applying O
the O
d-separation B
criterion O
to O
the O
graph O
, O
and O
then O
allowing O
a O
distribution O
to O
pass O
only O
if O
it O
satisﬁes O
all O
of O
these O
properties O
. O
note O
, O
however O
, O
that O
if O
a O
digit O
‘ O
2 O
’ O
is O
written O
in O
the O
reverse O
order O
, O
that O
is O
, O
starting O
at O
the O
bottom O
right O
and O
ending O
at O
the O
top O
left O
, O
then O
even O
though O
the O
pen O
tip O
coordinates O
may O
be O
identical O
to O
an O
example O
from O
the O
training B
set I
, O
the O
probability B
of O
the O
observations O
under O
the O
model O
will O
be O
extremely O
small O
. O
w O
( O
cid:1 O
) O
0 O
w O
( O
cid:1 O
) O
1 O
w O
( O
cid:1 O
) O
2 O
w O
( O
cid:1 O
) O
3 O
w O
( O
cid:1 O
) O
4 O
w O
( O
cid:1 O
) O
5 O
w O
( O
cid:1 O
) O
6 O
w O
( O
cid:1 O
) O
7 O
w O
( O
cid:1 O
) O
8 O
w O
( O
cid:1 O
) O
9 O
0.19 O
m O
= O
0 O
m O
= O
1 O
m O
= O
6 O
0.31 O
7.99 O
-25.43 O
17.37 O
0.82 O
-1.27 O
m O
= O
9 O
0.35 O
232.37 O
-5321.83 O
48568.31 O
-231639.30 O
640042.26 O
-1061800.52 O
1042400.18 O
-557682.99 O
125201.43 O
1.1. O
example O
: O
polynomial O
curve O
fitting O
9 O
t O
1 O
0 O
−1 O
n O
= O
15 O
t O
1 O
0 O
−1 O
n O
= O
100 O
0 O
x O
1 O
0 O
x O
1 O
figure O
1.6 O
plots O
of O
the O
solutions O
obtained O
by O
minimizing O
the O
sum-of-squares B
error I
function O
using O
the O
m O
= O
9 O
polynomial O
for O
n O
= O
15 O
data O
points O
( O
left O
plot O
) O
and O
n O
= O
100 O
data O
points O
( O
right O
plot O
) O
. O
the O
graph O
also O
expresses O
a O
set O
of O
conditional B
independence I
statements O
obtained O
through O
the O
d-separation B
criterion O
, O
and O
the O
d-separation B
theorem O
is O
really O
an O
expression O
of O
the O
equivalence O
of O
these O
two O
properties O
. O
assuming O
for O
the O
moment O
that O
α O
and O
β O
are O
ﬁxed O
, O
we O
can O
ﬁnd O
a O
maximum O
of O
the O
posterior O
, O
which O
we O
denote O
wmap O
, O
by O
standard O
nonlinear O
optimization O
algorithms O
such O
as O
conjugate B
gradients O
, O
using O
error B
backpropagation I
to O
evaluate O
the O
required O
derivatives O
. O
the O
k-means O
algorithm O
is O
illustrated O
using O
the O
old O
faithful O
data O
set O
in O
fig- O
ure O
9.1. O
for O
the O
purposes O
of O
this O
example O
, O
we O
have O
made O
a O
linear O
re-scaling O
of O
the O
data O
, O
known O
as O
standardizing B
, O
such O
that O
each O
of O
the O
variables O
has O
zero O
mean B
and O
unit O
standard B
deviation I
. O
this O
problem O
can O
be O
tackled O
using O
a O
technique O
known O
as O
chaining B
( O
neal O
, O
1993 O
; O
barber O
and O
bishop O
, O
1997 O
) O
, O
which O
involves O
introducing O
a O
succession O
of O
intermediate O
distributions O
p2 O
, O
. O
clearly O
, O
this O
has O
to O
be O
a O
constrained O
maximization O
to O
prevent O
ilulll O
... O
.. O
00. O
the O
appropriate O
constraint O
comes O
from O
the O
normalization O
condition O
uf O
ul O
= O
1. O
to O
enforce O
this O
constraint O
, O
we O
introduce O
a O
lagrange O
multiplier O
that O
we O
shall O
denote O
by O
ai O
, O
and O
then O
make O
an O
unconstrained O
maximization O
of O
( O
12.4 O
) O
by O
setting O
the O
derivative B
with O
respect O
to O
ul O
equal O
to O
zero O
, O
we O
see O
that O
this O
quantity O
will O
have O
a O
stationary B
point O
when O
( O
12.5 O
) O
which O
says O
that O
ul O
must O
be O
an O
eigenvector O
of O
s. O
if O
we O
left-multiply O
by O
uf O
and O
make O
use O
of O
uf O
ul O
= O
1 O
, O
we O
see O
that O
the O
variance B
is O
given O
by O
and O
so O
the O
variance B
will O
be O
a O
maximum O
when O
we O
set O
ul O
equal O
to O
the O
eigenvector O
having O
the O
largest O
eigenvalue O
ai O
. O
( O
1.62 O
) O
n O
( O
cid:2 O
) O
n=1 O
consider O
ﬁrst O
the O
determination O
of O
the O
maximum B
likelihood I
solution O
for O
the O
polyno- O
mial O
coefﬁcients O
, O
which O
will O
be O
denoted O
by O
wml O
. O
the O
joint O
distribution O
over O
latent O
and O
observed O
variables O
can O
be O
represented O
by O
the O
graphical B
model I
shown O
in O
figure O
14.7. O
the O
complete-data O
log O
likelihood O
function O
then O
takes O
the O
form O
ln O
p O
( O
t O
, O
z|θ O
) O
= O
znk O
ln O
πkn O
( O
tn|wt O
k O
φn O
, O
β O
−1 O
) O
. O
the O
simplest O
involves O
constructing O
a O
discriminant B
function I
that O
directly O
assigns O
each O
vector O
x O
to O
a O
speciﬁc O
class O
. O
the O
resulting O
support B
vector I
machine I
will O
give O
exact O
separation O
of O
the O
training B
data O
in O
the O
original O
input O
space O
x O
, O
although O
the O
corresponding O
decision B
boundary I
will O
be O
nonlinear O
. O
5.3.4 O
the O
jacobian O
matrix O
we O
have O
seen O
how O
the O
derivatives O
of O
an O
error B
function I
with O
respect O
to O
the O
weights O
can O
be O
obtained O
by O
the O
propagation O
of O
errors O
backwards O
through O
the O
network O
. O
if O
we O
make O
a O
further O
generalization B
to O
allow O
the O
mixing O
coefﬁcients O
also O
to O
depend O
on O
the O
inputs O
then O
we O
obtain O
a O
mixture B
of I
experts I
model O
. O
independent B
factor I
analysis I
. O
the O
gaussian O
process O
prior B
for O
an O
+1 O
takes O
the O
form O
p O
( O
an O
+1 O
) O
= O
n O
( O
an O
+1|0 O
, O
cn O
+1 O
) O
. O
5.4.1 O
diagonal B
approximation I
. O
10.2.1 O
variational B
distribution O
in O
order O
to O
formulate O
a O
variational B
treatment O
of O
this O
model O
, O
we O
next O
write O
down O
the O
joint O
distribution O
of O
all O
of O
the O
random O
variables O
, O
which O
is O
given O
by O
p O
( O
x O
, O
z O
, O
π O
, O
µ O
, O
λ O
) O
= O
p O
( O
x|z O
, O
µ O
, O
λ O
) O
p O
( O
z|π O
) O
p O
( O
π O
) O
p O
( O
µ|λ O
) O
p O
( O
λ O
) O
( O
10.41 O
) O
in O
which O
the O
various O
factors O
are O
deﬁned O
above O
. O
suppose O
we O
have O
already O
obtained O
the O
inverse B
hessian O
using O
the O
ﬁrst O
l O
data O
points O
. O
the O
corresponding O
likelihood O
k O
φ O
wt O
( O
14.45 O
) O
14.5. O
conditional O
mixture O
models O
671 O
figure O
14.9 O
the O
left O
plot O
shows O
the O
predictive O
conditional O
density B
corresponding O
to O
the O
converged O
solution O
in O
figure O
14.8. O
this O
gives O
a O
log O
likelihood O
value O
of O
−3.0 O
. O
note O
that O
the O
class O
priors O
p O
( O
ck O
) O
can O
often O
be O
esti- O
mated O
simply O
from O
the O
fractions O
of O
the O
training B
set I
data O
points O
in O
each O
of O
the O
classes O
. O
however O
, O
throughout O
this O
book O
we O
shall O
be O
interested O
in O
more O
complex O
models O
with O
many O
parameters O
, O
for O
which O
the O
bias B
problems O
asso- O
ciated O
with O
maximum B
likelihood I
will O
be O
much O
more O
severe O
. O
recall O
that O
to O
generate O
samples O
from O
a O
mixture O
of O
1 O
0.5 O
k O
= O
1 O
0 O
0 O
k O
= O
3 O
k O
= O
2 O
0.5 O
13.2. O
hidden O
markov O
models O
613 O
1 O
0.5 O
1 O
0 O
0 O
0.5 O
1 O
figure O
13.8 O
illustration O
of O
sampling O
from O
a O
hidden O
markov O
model O
having O
a O
3-state O
latent B
variable I
z O
and O
a O
gaussian O
emission O
model O
p O
( O
x|z O
) O
where O
x O
is O
2-dimensional O
. O
thus O
the O
variational B
framework O
assigns O
the O
highest O
probability B
to O
the O
model O
with O
m O
= O
3. O
this O
should O
be O
contrasted O
with O
the O
maximum B
likelihood I
result O
, O
which O
assigns O
ever O
smaller O
residual O
error B
to O
models O
of O
increasing O
complexity O
until O
the O
residual O
error B
is O
driven O
to O
zero O
, O
causing O
maximum B
likelihood I
to O
favour O
severely O
over-ﬁtted O
models O
. O
we O
shall O
use O
γ O
( O
zn O
) O
to O
denote O
the O
marginal B
posterior O
distribution O
of O
a O
latent B
variable I
zn O
, O
and O
ξ O
( O
zn−1 O
, O
zn O
) O
to O
denote O
the O
joint O
posterior O
distribution O
of O
two O
successive O
latent O
variables O
, O
so O
that O
γ O
( O
zn O
) O
= O
p O
( O
zn|x O
, O
θold O
) O
ξ O
( O
zn−1 O
, O
zn O
) O
= O
p O
( O
zn−1 O
, O
zn|x O
, O
θold O
) O
. O
we O
see O
that O
, O
as O
expected O
, O
the O
parameter O
h O
plays O
the O
role O
of O
a O
smoothing B
parameter I
, O
and O
there O
is O
a O
trade-off O
between O
sensitivity O
to O
noise O
at O
small O
h O
and O
over-smoothing O
at O
large O
h. O
again O
, O
the O
optimization O
of O
h O
is O
a O
problem O
in O
model O
complexity O
, O
analogous O
to O
the O
choice O
of O
bin O
width O
in O
histogram B
density I
estimation I
, O
or O
the O
degree O
of O
the O
polynomial O
used O
in O
curve B
ﬁtting I
. O
let O
us O
begin O
with O
a O
simple O
example O
in O
which O
we O
consider O
a O
single O
gaussian O
random O
variable O
x. O
we O
shall O
suppose O
that O
the O
variance B
σ2 O
is O
known O
, O
and O
we O
consider O
the O
task O
of O
inferring O
the O
mean B
µ O
given O
a O
set O
of O
n O
observations O
x O
= O
{ O
x1 O
, O
. O
more O
recently O
, O
highly O
efﬁcient O
deterministic O
approximation O
schemes O
such O
as O
variational B
bayes O
and O
expectation B
propagation I
( O
discussed O
in O
chapter O
10 O
) O
have O
been O
developed O
. O
the O
ﬁrst O
is O
that O
, O
if O
the O
domain O
of O
λ O
is O
unbounded O
, O
this O
prior B
distribution O
can O
not O
be O
correctly O
normalized O
because O
the O
integral O
over O
λ O
diverges O
. O
the O
second O
term O
, O
called O
the O
variance B
, O
measures O
the O
extent O
to O
which O
the O
solutions O
for O
individual O
data O
sets O
vary O
around O
their O
average O
, O
and O
hence O
this O
measures O
the O
extent O
to O
which O
the O
function O
y O
( O
x O
; O
d O
) O
is O
sensitive O
to O
the O
particular O
choice O
of O
data O
set O
. O
although O
there O
is O
nothing O
intrinsically O
approximate O
about O
variational B
methods O
, O
they O
do O
naturally O
lend O
themselves O
to O
ﬁnding O
approximate O
solutions O
. O
probabilistic O
pca O
is O
a O
simple O
example O
of O
the O
linear-gaussian O
framework O
, O
in O
which O
all O
of O
the O
marginal B
and O
conditional B
distributions O
are O
gaussian O
. O
( O
b O
) O
conversion O
to O
a O
fragment O
of O
a O
factor B
graph I
having O
a O
tree B
struc- O
ture O
, O
in O
which O
f O
( O
x1 O
, O
x2 O
, O
x3 O
) O
= O
p O
( O
x1 O
) O
p O
( O
x2|x1 O
) O
p O
( O
x3|x1 O
, O
x2 O
) O
. O
again O
, O
we O
shall O
make O
use O
of O
conditional B
independence I
properties O
, O
in O
particular O
( O
13.25 O
) O
and O
( O
13.26 O
) O
, O
together O
with O
the O
sum O
and O
product O
rules O
, O
allowing O
us O
to O
express O
α O
( O
zn O
) O
in O
terms O
of O
α O
( O
zn−1 O
) O
as O
follows O
α O
( O
zn O
) O
= O
p O
( O
x1 O
, O
. O
in O
this O
represen- O
tation O
, O
the O
multinomial B
distribution I
therefore O
takes O
the O
form O
p O
( O
x|η O
) O
= O
1 O
+ O
exp O
( O
ηk O
) O
exp O
( O
ηtx O
) O
. O
the O
term O
backpropagation B
is O
also O
used O
to O
describe O
the O
training B
of O
a O
multilayer B
perceptron I
us- O
ing O
gradient O
descent O
applied O
to O
a O
sum-of-squares B
error I
function O
. O
11.1.2 O
rejection B
sampling I
the O
rejection B
sampling I
framework O
allows O
us O
to O
sample O
from O
relatively O
complex O
distributions O
, O
subject O
to O
certain O
constraints O
. O
the O
goal O
in O
such O
unsupervised B
learning I
problems O
may O
be O
to O
discover O
groups O
of O
similar O
examples O
within O
the O
data O
, O
where O
it O
is O
called O
clustering B
, O
or O
to O
determine O
the O
distribution O
of O
data O
within O
the O
input O
space O
, O
known O
as O
density B
estimation I
, O
or O
to O
project O
the O
data O
from O
a O
high-dimensional O
space O
down O
to O
two O
or O
three O
dimensions O
for O
the O
purpose O
of O
visualization B
. O
now O
show O
that O
the O
covariance B
matrix I
between O
these O
two O
variables O
is O
again O
diagonal B
. O
in O
the O
linear B
regression I
models O
considered O
in O
chapter O
3 O
, O
the O
model O
prediction O
y O
( O
x O
, O
w O
) O
was O
given O
by O
a O
linear O
function O
of O
the O
parameters O
w. O
in O
the O
simplest O
case O
, O
the O
model O
is O
also O
linear O
in O
the O
input O
variables O
and O
therefore O
takes O
the O
form O
y O
( O
x O
) O
= O
wtx O
+ O
w0 O
, O
so O
that O
y O
is O
a O
real O
number O
. O
both O
types O
of O
multimodality B
were O
encountered O
in O
chapter O
9 O
in O
the O
context O
of O
gaussian O
mixtures O
, O
where O
they O
manifested O
themselves O
as O
multiple O
maxima O
in O
the O
likelihood B
function I
, O
and O
a O
variational B
treatment O
based O
on O
the O
minimization O
of O
kl O
( O
q O
( O
cid:5 O
) O
p O
) O
will O
tend O
to O
ﬁnd O
one O
of O
these O
modes O
. O
the O
joint O
optimization O
of O
the O
choice O
of O
region O
to O
split O
, O
and O
the O
choice O
of O
input O
variable O
and O
threshold O
, O
can O
be O
done O
efﬁciently O
by O
exhaustive O
search O
noting O
that O
, O
for O
a O
given O
choice O
of O
split O
variable O
and O
threshold O
, O
the O
optimal O
choice O
of O
predictive O
variable O
is O
given O
by O
the O
local B
average O
of O
the O
data O
, O
as O
noted O
earlier O
. O
6.1 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
www O
consider O
the O
dual O
formulation O
of O
the O
least O
squares O
linear B
regression I
problem O
given O
in O
section O
6.1. O
show O
that O
the O
solution O
for O
the O
components O
an O
of O
the O
vector O
a O
can O
be O
expressed O
as O
a O
linear O
combination O
of O
the O
elements O
of O
the O
vector O
φ O
( O
xn O
) O
. O
the O
message B
passing I
framework O
can O
be O
generalized B
to O
arbitrary O
graph O
topolo- O
gies O
, O
giving O
an O
exact O
inference O
procedure O
known O
as O
the O
junction B
tree I
algorithm I
( O
lau- O
ritzen O
and O
spiegelhalter O
, O
1988 O
; O
jordan O
, O
2007 O
) O
. O
however O
, O
if O
we O
choose O
the O
density B
pλ O
( O
λ O
) O
to O
be O
constant O
, O
then O
the O
density B
of O
η O
will O
be O
given O
, O
from O
( O
1.27 O
) O
, O
by O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
dλ O
dη O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
= O
pλ O
( O
η2 O
) O
2η O
∝ O
η O
pη O
( O
η O
) O
= O
pλ O
( O
λ O
) O
( O
2.231 O
) O
and O
so O
the O
density B
over O
η O
will O
not O
be O
constant O
. O
a O
f O
a O
f O
8.2. O
conditional B
independence I
379 O
b O
e O
c O
b O
e O
c O
( O
a O
) O
( O
b O
) O
be O
satisﬁed O
by O
any O
distribution O
that O
factorizes O
according O
to O
this O
graph O
. O
in O
the O
context O
of O
maximum B
likelihood I
, O
this O
redundancy O
is O
irrelevant O
because O
the O
parameter O
optimization O
algorithm O
( O
for O
example O
em O
) O
will O
, O
depending O
on O
the O
initial- O
ization O
of O
the O
parameters O
, O
ﬁnd O
one O
speciﬁc O
solution O
, O
and O
the O
other O
equivalent O
solu- O
tions O
play O
no O
role O
. O
in O
fact O
, O
it O
only O
proved O
such O
limitations O
in O
the O
case O
of O
single-layer O
networks O
such O
as O
the O
perceptron B
and O
merely O
conjectured O
( O
in- O
correctly O
) O
that O
they O
applied O
to O
more O
general O
network O
models O
. O
−1/2 O
2 O
λ O
5.2. O
network O
training B
239 O
u2 O
w O
( O
cid:1 O
) O
u1 O
w1 O
−1/2 O
1 O
λ O
because O
the O
eigenvectors O
{ O
ui O
} O
form O
a O
complete O
set O
, O
an O
arbitrary O
vector O
v O
can O
be O
written O
in O
the O
form O
from O
( O
5.33 O
) O
and O
( O
5.34 O
) O
, O
we O
then O
have O
( O
5.38 O
) O
( O
5.39 O
) O
v O
= O
ciui O
. O
the O
set O
{ O
x1 O
, O
x2 O
, O
x3 O
, O
x4 O
} O
is O
not O
a O
clique B
because O
of O
the O
missing O
link O
from O
x1 O
to O
x4 O
. O
2.4.1 O
maximum B
likelihood I
and O
sufﬁcient B
statistics I
2.4.2 O
conjugate B
priors O
2.4.3 O
noninformative B
priors O
. O
now O
consider O
a O
variational B
approximation O
in O
which O
the O
distribution O
q O
( O
x O
) O
is O
assumed O
to O
factorize O
with O
respect O
to O
the O
xi O
so O
that O
q O
( O
x O
) O
= O
qi O
( O
xi O
) O
. O
( O
2.62 O
) O
for O
single O
random O
variables O
, O
we O
subtracted O
the O
mean B
before O
taking O
second O
mo- O
ments O
in O
order O
to O
deﬁne O
a O
variance B
. O
ard O
is O
illustrated O
using O
a O
simple O
synthetic O
data O
set O
having O
three O
inputs O
x1 O
, O
x2 O
and O
x3 O
( O
nabney O
, O
2002 O
) O
in O
figure O
6.10. O
the O
target O
variable O
t O
, O
is O
generated O
by O
sampling O
100 O
values O
of O
x1 O
from O
a O
gaussian O
, O
evaluating O
the O
function O
sin O
( O
2πx1 O
) O
, O
and O
then O
adding O
6.4. O
gaussian O
processes O
313 O
figure O
6.10 O
illustration O
of O
automatic O
rele- O
vance O
determination O
in O
a O
gaus- O
sian O
process O
for O
a O
synthetic O
prob- O
lem O
having O
three O
inputs O
x1 O
, O
x2 O
, O
and O
x3 O
, O
for O
which O
the O
curves O
show O
the O
corresponding O
values O
of O
the O
hyperparameters O
η1 O
( O
red O
) O
, O
η2 O
( O
green O
) O
, O
and O
η3 O
( O
blue O
) O
as O
a O
func- O
tion O
of O
the O
number O
of O
iterations O
when O
optimizing O
the O
marginal B
likelihood I
. O
show O
that O
the O
function O
e O
( O
mn O
) O
deﬁned O
by O
( O
3.82 O
) O
satisﬁes O
the O
relation O
2e O
( O
mn O
) O
= O
n. O
3.16 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
derive O
the O
result O
( O
3.86 O
) O
for O
the O
log O
evidence O
function O
p O
( O
t|α O
, O
β O
) O
of O
the O
linear B
regression I
model O
by O
making O
use O
of O
( O
2.115 O
) O
to O
evaluate O
the O
integral O
( O
3.77 O
) O
directly O
. O
training B
with O
noise O
is O
equiv- O
alent O
to O
tikhonov O
regularization B
. O
for O
example O
, O
in O
chapter O
7 O
we O
shall O
introduce O
the O
relevance B
vector I
machine I
, O
which O
is O
a O
bayesian O
model O
having O
one O
complexity O
parameter O
for O
every O
training B
data O
point O
. O
320 O
6. O
kernel O
methods O
exercises O
by O
working O
directly O
with O
the O
covariance B
function O
we O
have O
implicitly O
marginal- O
ized O
over O
the O
distribution O
of O
weights O
. O
this O
is O
just O
the O
familiar O
result O
( O
3.59 O
) O
obtained O
in O
the O
context O
of O
linear B
regression I
. O
the O
hessian O
plays O
an O
important O
role O
in O
many O
aspects O
of O
neural O
computing O
, O
including O
the O
following O
: O
1. O
several O
nonlinear O
optimization O
algorithms O
used O
for O
training O
neural O
networks O
are O
based O
on O
considerations O
of O
the O
second-order O
properties O
of O
the O
error B
surface O
, O
which O
are O
controlled O
by O
the O
hessian O
matrix O
( O
bishop O
and O
nabney O
, O
2008 O
) O
. O
from O
( O
1.5 O
) O
, O
( O
1.6 O
) O
, O
and O
( O
1.8 O
) O
, O
we O
can O
then O
derive O
the O
following O
relationship O
p O
( O
x O
= O
xi O
, O
y O
= O
yj O
) O
= O
nij O
n O
= O
nij O
ci O
· O
ci O
n O
= O
p O
( O
y O
= O
yj|x O
= O
xi O
) O
p O
( O
x O
= O
xi O
) O
( O
1.8 O
) O
( O
1.9 O
) O
which O
is O
the O
product B
rule I
of I
probability I
. O
in O
the O
case O
of O
the O
cross-entropy B
error I
function I
for O
a O
network O
with O
logistic B
sigmoid I
output-unit O
activation O
functions O
, O
the O
corresponding O
approximation O
is O
given O
by O
h O
( O
cid:7 O
) O
n O
( O
cid:2 O
) O
h O
( O
cid:7 O
) O
n O
( O
cid:2 O
) O
252 O
5. O
neural O
networks O
5.4.3 O
inverse B
hessian O
we O
can O
use O
the O
outer-product O
approximation O
to O
develop O
a O
computationally O
ef- O
ﬁcient O
procedure O
for O
approximating O
the O
inverse B
of O
the O
hessian O
( O
hassibi O
and O
stork O
, O
1993 O
) O
. O
5.28 O
( O
( O
cid:12 O
) O
) O
www O
consider O
a O
neural B
network I
, O
such O
as O
the O
convolutional B
network O
discussed O
in O
section O
5.5.6 O
, O
in O
which O
multiple O
weights O
are O
constrained O
to O
have O
the O
same O
value O
. O
56 O
1. O
introduction O
figure O
1.31 O
a O
convex B
function I
f O
( O
x O
) O
is O
one O
for O
which O
ev- O
ery O
chord O
( O
shown O
in O
blue O
) O
lies O
on O
or O
above O
the O
function O
( O
shown O
in O
red O
) O
. O
a O
ﬁnal O
example O
of O
a O
kernel B
function I
is O
the O
sigmoidal O
kernel O
given O
by O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
tanh O
axtx O
( O
cid:4 O
) O
+ O
b O
( O
6.37 O
) O
( O
cid:10 O
) O
( O
cid:11 O
) O
whose O
gram O
matrix O
in O
general O
is O
not O
positive O
semideﬁnite O
. O
( O
b.52 O
) O
( O
cid:11 O
) O
gaussian-wishart O
this O
is O
the O
conjugate B
prior I
distribution O
for O
a O
multivariate O
gaussian O
n O
( O
x|µ O
, O
λ O
) O
in O
which O
both O
the O
mean B
µ O
and O
the O
precision O
λ O
are O
unknown O
, O
and O
is O
also O
called O
the O
normal-wishart O
distribution O
. O
the O
principal O
disadvantage O
is O
that O
the O
hessian O
matrix O
has O
size O
m O
k×m O
k O
, O
where O
m O
is O
the O
number O
of O
active O
basis O
functions O
, O
which O
gives O
an O
additional O
factor O
of O
k O
3 O
in O
the O
computational O
cost O
of O
training B
compared O
with O
the O
two-class O
rvm O
. O
using O
the O
calculus B
of I
variations I
, O
show O
that O
the O
function O
y O
( O
x O
) O
for O
which O
this O
expected O
loss O
is O
minimized O
is O
given O
by O
y O
( O
x O
) O
= O
et O
[ O
t|x O
] O
. O
( O
10.44 O
) O
substituting O
for O
the O
two O
conditional B
distributions O
on O
the O
right-hand O
side O
, O
and O
again O
absorbing O
any O
terms O
that O
are O
independent B
of O
z O
into O
the O
additive O
constant O
, O
we O
have O
ln O
q O
( O
cid:1 O
) O
( O
z O
) O
= O
znk O
ln O
ρnk O
+ O
const O
( O
10.45 O
) O
n O
( O
cid:2 O
) O
k O
( O
cid:2 O
) O
n=1 O
k=1 O
where O
we O
have O
deﬁned O
ln O
ρnk O
= O
e O
[ O
ln O
πk O
] O
+ O
where O
d O
is O
the O
dimensionality O
of O
the O
data O
variable O
x. O
taking O
the O
exponential O
of O
both O
sides O
of O
( O
10.45 O
) O
we O
obtain O
−1 O
2 O
1 O
2 O
eµk O
, O
λk O
( O
cid:8 O
) O
( O
cid:9 O
) O
2 O
e O
[ O
ln|λk| O
] O
− O
d O
ln O
( O
2π O
) O
( O
xn O
− O
µk O
) O
tλk O
( O
xn O
− O
µk O
) O
k O
( O
cid:14 O
) O
q O
( O
cid:1 O
) O
( O
z O
) O
∝ O
n O
( O
cid:14 O
) O
n O
( O
cid:14 O
) O
k O
( O
cid:14 O
) O
ρznk O
nk O
. O
we O
shall O
see O
that O
these O
two O
stages O
of O
updating O
rnk O
and O
updating O
µk O
correspond O
respectively O
to O
the O
e O
( O
expectation B
) O
and O
m O
( O
maximization O
) O
steps O
of O
the O
em O
algorithm O
, O
and O
to O
emphasize O
this O
we O
shall O
use O
the O
terms O
e O
step O
and O
m O
step O
in O
the O
context O
of O
the O
k-means O
algorithm O
. O
( O
8.45 O
) O
figure O
8.33 O
example O
of O
a O
simple O
directed B
graph O
( O
a O
) O
and O
the O
corre- O
sponding O
moral O
graph O
( O
b O
) O
. O
it O
is O
therefore O
tempting O
to O
see O
if O
we O
can O
apply O
the O
same O
formalism O
to O
classiﬁcation B
problems O
. O
if O
we O
had O
an O
unlimited O
supply O
of O
data O
( O
and O
unlimited O
computational O
resources O
) O
, O
we O
could O
in O
principle O
ﬁnd O
the O
regres- O
sion B
function O
h O
( O
x O
) O
to O
any O
desired O
degree O
of O
accuracy O
, O
and O
this O
would O
represent O
the O
optimal O
choice O
for O
y O
( O
x O
) O
. O
we O
start O
by O
discussing O
regression B
problems O
, O
and O
for O
the O
moment O
we O
consider O
a O
single O
target O
variable O
t O
that O
can O
take O
any O
real O
value O
. O
' O
and O
capturing O
the O
covariance B
between O
variables O
in O
the O
matrix O
w. O
in O
the O
factor B
analysis I
literature O
. O
the O
mean B
of O
this O
distribution O
is O
taken O
to O
be O
a O
linear O
combination O
of O
the O
states O
of O
its O
parent O
nodes O
pai O
of O
node B
i O
⎛⎝xi O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:7 O
) O
( O
cid:2 O
) O
j∈pai O
⎞⎠ O
p O
( O
xi|pai O
) O
= O
n O
wijxj O
+ O
bi O
, O
vi O
( O
8.11 O
) O
where O
wij O
and O
bi O
are O
parameters O
governing O
the O
mean B
, O
and O
vi O
is O
the O
variance B
of O
the O
conditional B
distribution O
for O
xi O
. O
this O
will O
also O
allow O
us O
to O
highlight O
similarities O
, O
and O
differences O
, O
compared O
to O
the O
logistic B
regression I
model O
. O
in O
figure O
1.4 O
, O
we O
show O
four O
examples O
of O
the O
results O
of O
ﬁtting O
polynomials O
having O
orders O
m O
= O
0 O
, O
1 O
, O
3 O
, O
and O
9 O
to O
the O
data O
set O
shown O
in O
figure O
1.2. O
we O
notice O
that O
the O
constant O
( O
m O
= O
0 O
) O
and O
ﬁrst B
order I
( O
m O
= O
1 O
) O
polynomials O
give O
rather O
poor O
ﬁts O
to O
the O
data O
and O
consequently O
rather O
poor O
representations O
of O
the O
function O
sin O
( O
2πx O
) O
. O
4.12 O
( O
( O
cid:12 O
) O
) O
www O
verify O
the O
relation O
( O
4.88 O
) O
for O
the O
derivative B
of O
the O
logistic B
sigmoid I
func- O
tion O
deﬁned O
by O
( O
4.59 O
) O
. O
for O
instance O
, O
in O
section O
9.3.3 O
we O
shall O
consider O
mixtures O
of O
bernoulli O
distributions O
as O
an O
example O
of O
a O
mixture B
model I
for O
discrete O
variables O
. O
such O
points O
will O
be O
strongly O
weighted O
at O
the O
expense O
of O
misclassiﬁed O
points O
, O
and O
so O
if O
the O
objective O
is O
to O
minimize O
the O
mis- O
classiﬁcation B
rate O
, O
then O
a O
monotonically O
decreasing O
error B
function I
would O
be O
a O
better O
choice O
. O
however O
, O
the O
hessian O
can O
also O
be O
calculated O
exactly O
using O
an O
extension O
of O
the O
backpropagation B
technique O
. O
thus O
x1 O
is O
a O
good O
predictor O
of O
t O
, O
x2 O
is O
a O
more O
noisy O
predictor O
of O
t O
, O
and O
x3 O
has O
only O
chance O
correlations O
with O
t. O
the O
marginal B
likelihood I
for O
a O
gaussian O
process O
with O
ard O
parameters O
η1 O
, O
η2 O
, O
η3 O
is O
optimized O
using O
the O
scaled O
conjugate B
gradients O
algorithm O
. O
however O
, O
reducing O
h O
may O
lead O
to O
noisy O
estimates O
elsewhere O
in O
data O
space O
where O
the O
density B
is O
smaller O
. O
for O
instance O
, O
in O
our O
cancer O
example O
, O
we O
might O
have O
a O
loss B
matrix I
of O
the O
form O
shown O
in O
figure O
1.25. O
this O
particular O
loss B
matrix I
says O
that O
there O
is O
no O
loss O
incurred O
if O
the O
correct O
decision O
is O
made O
, O
there O
is O
a O
loss O
of O
1 O
if O
a O
healthy O
patient O
is O
diagnosed O
as O
having O
cancer O
, O
whereas O
there O
is O
a O
loss O
of O
1000 O
if O
a O
patient O
having O
cancer O
is O
diagnosed O
as O
healthy O
. O
7.14 O
( O
( O
cid:12 O
) O
( O
cid:12 O
) O
) O
derive O
the O
result O
( O
7.90 O
) O
for O
the O
predictive B
distribution I
in O
the O
relevance B
vector I
machine I
for O
regression B
. O
it O
can O
therefore O
also O
be O
interpreted O
as O
an O
extension O
of O
a O
mixture B
model I
in O
which O
the O
choice O
of O
mixture B
com- O
ponent O
for O
each O
observation O
is O
not O
selected O
independently O
but O
depends O
on O
the O
choice O
of O
component O
for O
the O
previous O
observation O
. O
we O
recognize O
this O
as O
the O
log O
of O
a O
gamma B
distribution I
, O
and O
so O
we O
obtain O
q O
( O
α O
) O
= O
gam O
( O
α|an O
, O
bn O
) O
= O
where O
0 O
αa0−1e O
−b0α O
1 O
γ O
( O
a0 O
) O
ab0 O
( O
cid:8 O
) O
an O
= O
a0 O
+ O
m O
2 O
1 O
2 O
ew O
bn O
= O
b0 O
+ O
( O
cid:9 O
) O
. O
by O
now O
, O
a O
message O
will O
have O
passed O
in O
both O
directions O
across O
every O
link B
in O
the O
graph O
, O
and O
every O
node B
will O
have O
received O
a O
message O
from O
all O
of O
its O
neighbours O
. O
( O
12.31 O
) O
similarly O
, O
the O
conditional B
distribution O
of O
the O
observed B
variable I
x O
, O
conditioned O
on O
the O
value O
of O
the O
latent B
variable I
z O
, O
is O
again O
gaussian O
, O
of O
the O
form O
p O
( O
xlz O
) O
= O
n O
( O
xlwz O
+ O
j-l O
, O
a O
2i O
) O
( O
12.32 O
) O
in O
which O
the O
mean B
of O
x O
is O
a O
general O
linear O
function O
of O
z O
governed O
by O
the O
d O
x O
m O
matrix O
wand O
the O
d-dimensional O
vector O
j-l. O
note O
that O
this O
factorizes O
with O
respect O
to O
the O
elements O
of O
x O
, O
in O
other O
words O
this O
is O
an O
example O
of O
the O
naive O
bayes O
model O
. O
we O
have O
already O
encountered O
an O
example O
of O
a O
regression B
problem O
when O
we O
considered O
polynomial B
curve I
ﬁtting I
in O
chapter O
1. O
the O
polynomial O
is O
a O
speciﬁc O
example O
of O
a O
broad O
class O
of O
functions O
called O
linear B
regression I
models O
, O
which O
share O
the O
property O
of O
being O
linear O
functions O
of O
the O
adjustable O
parameters O
, O
and O
which O
will O
form O
the O
focus O
of O
this O
chapter O
. O
in O
( O
13.89 O
) O
, O
we O
can O
view O
the O
quantity O
aµn−1 O
as O
the O
prediction O
of O
the O
mean B
over O
zn O
obtained O
by O
simply O
taking O
the O
mean B
over O
zn−1 O
and O
projecting O
it O
forward O
one O
step O
using O
the O
transition B
probability I
matrix O
a. O
this O
predicted O
mean B
would O
give O
a O
predicted O
observation O
for O
xn O
given O
by O
cazn−1 O
obtained O
by O
applying O
the O
emission B
probability I
matrix O
c O
to O
the O
predicted O
hidden O
state O
mean B
. O
14.9 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
the O
sequential O
minimization O
of O
the O
sum-of-squares B
error I
func- O
tion O
for O
an O
additive O
model O
of O
the O
form O
( O
14.21 O
) O
in O
the O
style O
of O
boosting B
simply O
involves O
ﬁtting O
each O
new O
base O
classiﬁer O
to O
the O
residual O
errors O
tn−fm−1 O
( O
xn O
) O
from O
the O
previous O
model O
. O
one O
approach O
is O
simply O
to O
replace O
the O
expectation B
in O
the O
deﬁnition O
of O
the O
fisher O
informa- O
tion O
with O
the O
sample O
average O
, O
giving O
g O
( O
θ O
, O
xn O
) O
g O
( O
θ O
, O
xn O
) O
t. O
( O
6.35 O
) O
n O
( O
cid:2 O
) O
n=1 O
f O
( O
cid:7 O
) O
1 O
n O
6.3. O
radial B
basis I
function I
networks O
299 O
section O
12.1.3 O
this O
is O
the O
covariance B
matrix I
of O
the O
fisher O
scores O
, O
and O
so O
the O
fisher O
kernel O
corre- O
sponds O
to O
a O
whitening B
of O
these O
scores O
. O
simplifying O
neural O
networks O
by O
soft B
weight I
sharing I
. O
many O
have O
the O
property O
of O
being O
a O
function O
only O
of O
the O
difference O
between O
the O
arguments O
, O
so O
that O
k O
( O
x O
, O
x O
( O
cid:4 O
) O
) O
= O
k O
( O
x O
− O
x O
( O
cid:4 O
) O
) O
, O
which O
are O
known O
as O
stationary B
kernels O
because O
they O
are O
invariant O
to O
translations O
in O
input O
space O
. O
we O
now O
look O
at O
the O
moments O
of O
the O
gaussian O
distribution O
and O
thereby O
provide O
an O
interpretation O
of O
the O
parameters O
µ O
and O
σ. O
the O
expectation B
of O
x O
under O
the O
gaussian O
distribution O
is O
given O
by O
( O
cid:6 O
) O
( O
cid:6 O
) O
( O
cid:12 O
) O
( O
cid:12 O
) O
−1 O
2 O
−1 O
2 O
( O
cid:13 O
) O
−1z O
exp O
( O
x O
− O
µ O
) O
tς O
−1 O
( O
x O
− O
µ O
) O
x O
dx O
= O
exp O
|σ|1/2 O
( O
2π O
) O
d/2 O
( O
2.58 O
) O
where O
we O
have O
changed O
variables O
using O
z O
= O
x O
− O
µ. O
we O
now O
note O
that O
the O
exponent O
is O
an O
even O
function O
of O
the O
components O
of O
z O
and O
, O
because O
the O
integrals O
over O
these O
are O
taken O
over O
the O
range O
( O
−∞ O
, O
∞ O
) O
, O
the O
term O
in O
z O
in O
the O
factor O
( O
z O
+ O
µ O
) O
will O
vanish O
by O
symmetry O
. O
in O
particular O
, O
the O
contribution O
to O
the O
gradient O
from O
data O
point O
n O
is O
given O
by O
the O
‘ O
error B
’ O
yn O
− O
tn O
between O
the O
target O
value O
and O
the O
prediction O
of O
the O
model O
, O
times O
the O
basis B
function I
vector O
φn O
. O
12.2.1 O
maximum B
likelihood I
pca O
. O
8.1. O
bayesian O
networks O
in O
order O
to O
motivate O
the O
use O
of O
directed B
graphs O
to O
describe O
probability B
distributions O
, O
consider O
ﬁrst O
an O
arbitrary O
joint O
distribution O
p O
( O
a O
, O
b O
, O
c O
) O
over O
three O
variables O
a O
, O
b O
, O
and O
c. O
note O
that O
at O
this O
stage O
, O
we O
do O
not O
need O
to O
specify O
anything O
further O
about O
these O
vari- O
ables O
, O
such O
as O
whether O
they O
are O
discrete O
or O
continuous O
. O
because O
we O
are O
restricted O
to O
potential O
functions O
which O
are O
strictly O
positive O
it O
is O
convenient O
to O
express O
them O
as O
exponentials O
, O
so O
that O
ψc O
( O
xc O
) O
= O
exp O
{ O
−e O
( O
xc O
) O
} O
( O
8.41 O
) O
where O
e O
( O
xc O
) O
is O
called O
an O
energy B
function I
, O
and O
the O
exponential O
representation O
is O
called O
the O
boltzmann O
distribution O
. O
for O
each O
state O
of O
a O
given O
variable O
, O
there O
is O
a O
unique O
state O
of O
the O
previous O
variable O
that O
maximizes O
the O
probability B
( O
ties O
are O
broken O
either O
systematically O
or O
at O
random O
) O
, O
corresponding O
to O
the O
function O
φ O
( O
xn O
) O
given O
by O
( O
8.101 O
) O
, O
and O
this O
is O
indicated O
8.4. O
inference B
in O
graphical O
models O
415 O
by O
the O
lines O
connecting O
the O
nodes O
. O
in O
uppsala O
symposium O
on O
psycho- O
logical O
factor B
analysis I
, O
number O
3 O
in O
nordisk O
psykologi O
monograph O
series O
, O
pp O
. O
then O
another O
that O
z O
( O
cid:4 O
) O
( O
cid:4 O
) O
548 O
11. O
sampling B
methods I
candidate O
point O
is O
drawn O
uniformly O
from O
this O
reduced O
region O
and O
so O
on O
, O
until O
a O
value O
of O
z O
is O
found O
that O
lies O
within O
the O
slice O
. O
because O
the O
absorbtion O
properties O
of O
different O
materials O
vary O
dif- O
ferently O
as O
a O
function O
of O
energy O
, O
measurement O
of O
the O
attenuations O
at O
the O
two O
energies O
provides O
two O
independent B
pieces O
of O
information O
. O
again O
, O
using O
the O
product B
rule I
p O
( O
x O
, O
ck O
) O
= O
p O
( O
ck|x O
) O
p O
( O
x O
) O
, O
and O
noting O
that O
the O
factor O
of O
p O
( O
x O
) O
is O
common O
to O
all O
terms O
, O
we O
see O
that O
each O
x O
should O
be O
assigned O
to O
the O
class O
having O
the O
largest O
posterior B
probability I
p O
( O
ck|x O
) O
. O
, O
xn O
} O
together O
with O
their O
class O
labels O
, O
then O
we O
can O
ﬁt O
the O
naive O
bayes O
model O
to O
the O
training B
data O
8.2. O
conditional B
independence I
381 O
using O
maximum B
likelihood I
assuming O
that O
the O
data O
are O
drawn O
independently O
from O
the O
model O
. O
j=1 O
9.20 O
( O
( O
cid:12 O
) O
) O
www O
show O
that O
maximization O
of O
the O
expected O
complete-data O
log O
likelihood O
function O
( O
9.62 O
) O
for O
the O
bayesian O
linear B
regression I
model O
leads O
to O
the O
m O
step O
re- O
estimation O
result O
( O
9.63 O
) O
for O
α O
. O
data O
points O
that O
are O
correctly O
classiﬁed O
by O
ym O
( O
x O
) O
, O
and O
if O
we O
denote O
the O
remaining O
misclassiﬁed O
points O
by O
mm O
, O
then O
we O
can O
in O
turn O
rewrite O
the O
error B
function I
in O
the O
14.3. O
boosting B
661 O
( O
cid:2 O
) O
( O
cid:2 O
) O
n∈mm O
w O
( O
m O
) O
n O
form O
e O
= O
e O
−αm/2 O
w O
( O
m O
) O
n O
+ O
eαm/2 O
n∈tm O
= O
( O
eαm/2 O
− O
e O
−αm/2 O
) O
n O
( O
cid:2 O
) O
n=1 O
n O
i O
( O
ym O
( O
xn O
) O
( O
cid:9 O
) O
= O
tn O
) O
+ O
e O
w O
( O
m O
) O
−αm/2 O
n O
( O
cid:2 O
) O
n=1 O
w O
( O
m O
) O
n O
. O
this O
is O
repeated O
for O
all O
possible O
choices O
of O
variable O
to O
be O
split O
, O
and O
the O
one O
that O
gives O
the O
smallest O
residual O
sum-of-squares B
error I
is O
retained O
. O
deﬁne O
the O
following O
partitions O
( O
cid:15 O
) O
( O
cid:16 O
) O
( O
cid:16 O
) O
x O
= O
xa O
xb O
( O
cid:15 O
) O
σ O
= O
σaa O
σab O
σba O
σbb O
, O
µ O
= O
, O
λ O
= O
( O
cid:16 O
) O
( O
cid:15 O
) O
( O
cid:15 O
) O
µa O
µb O
λaa O
λab O
λba O
λbb O
then O
the O
conditional B
distribution O
p O
( O
xa|xb O
) O
is O
given O
by O
−1 O
aa O
) O
p O
( O
xa|xb O
) O
= O
n O
( O
x|µa|b O
, O
λ O
µa|b O
= O
µa O
− O
λ O
aa O
λab O
( O
xb O
− O
µb O
) O
−1 O
( O
cid:16 O
) O
( O
b.47 O
) O
( O
b.48 O
) O
( O
b.49 O
) O
( O
b.50 O
) O
690 O
b. O
probability B
distributions O
and O
the O
marginal B
distribution O
p O
( O
xa O
) O
is O
given O
by O
p O
( O
xa O
) O
= O
n O
( O
xa|µa O
, O
σaa O
) O
. O
if O
we O
µml O
, O
which O
we O
will O
denote O
by O
µ O
( O
n O
) O
2.3. O
the O
gaussian O
distribution O
95 O
figure O
2.10 O
a O
schematic O
illustration O
of O
two O
correlated O
ran- O
dom O
variables O
z O
and O
θ O
, O
together O
with O
the O
regression B
function I
f O
( O
θ O
) O
given O
by O
the O
con- O
ditional O
expectation B
e O
[ O
z|θ O
] O
. O
