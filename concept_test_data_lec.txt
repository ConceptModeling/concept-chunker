we O O
will O O
see O O
that O O
in O O
text B B
retrieval I I
the O O
data O O
is O O
unstructured O O
it O O
s O O
free O O
text O B
. O O
in O O
this O O
lecture O O
we O O
re O O
going O O
to O O
continue O O
the O O
discussion O O
of O O
text B B
retrieval B I
methods I I
. O O
first O O
these O O
models O O
are O O
all O O
based O O
on O O
the O O
assumption O O
of O O
using O O
bag B B
of I I
words I I
for O O
representing O O
text O B
. O O
finally O O
we O O
re O O
going O O
to O O
cover O O
the O O
relation O O
between O O
natural B B
language I I
processing I I
and O O
text B O
retrieval I I
. O O
the O O
second O O
is O O
a O O
article O O
that O O
has O O
a O O
survey O O
of O O
statistical B B
language B B
models I I
with O O
other O O
pointers O O
to O O
research O O
work O O
. O O
so O O
to O O
summarize O O
web O B
scale O O
indexing O O
requires O O
some O O
new O O
techniques O O
that O O
go O O
beyond O O
the O O
standard O O
traditional O O
indexing B O
techniques I O
. O O
because O O
now O O
d O O
here O O
which O O
did O O
not O O
have O O
a O O
very O O
high O O
score O O
with O O
our O O
simplest O O
vector B B
space I I
model I I
. O O
so O O
b O O
in O O
this O O
sense O O
controls O O
the O O
length B B
normalization I I
where O O
as O O
if O O
we O O
set O O
d O O
to O O
a O O
non-zero O O
value O O
then O O
the O O
normalizer O O
will O O
look O O
like O O
this O O
right O O
. O O
we O O
also O O
need O O
a O O
documents O O
and O O
that O O
s O O
called O O
the O O
document O O
collection O O
and O O
on O O
the O O
right O O
side O O
you O O
see O O
we O O
need O O
relevance B O
judgment I B
. O O
so O O
if O O
i O O
plot O O
the O O
word B O
frequencies I O
in O O
sorted O O
order O O
then O O
you O O
can O O
see O O
this O O
more O O
easily O O
. O O
let O O
s O O
build O O
a O O
language B B
model I I
. O O
we O O
talked O O
about O O
the O O
different O O
ways O O
of O O
combining O O
precision B B
and I I
recall B B
. O O
and O O
the O O
third O O
line O O
of O O
techniques O O
is O O
link B O
analysis I I
. O O
so O O
in O O
that O O
sense O O
these O O
functions O O
are O O
less O O
heuristic O O
compared O O
with O O
the O O
vector B B
space I I
model I I
. O O
we O O
also O O
show O O
that O O
such O O
a O O
single O O
vector B O
space I I
model I I
still O O
doesn O O
t O O
work O O
well O O
and O O
we O O
need O O
to O O
improve O O
it O O
. O O
there O O
is O O
another O O
reason O O
why O O
we O O
have O O
not O O
used O O
the O O
sophisticated O O
nlp B O
techniques O O
in O O
modern O O
search B B
engines I I
and O O
that O O
s O O
because O O
some O O
retrieval B O
techniques I O
actually O O
naturally O O
solve O O
the O O
problem O O
of O O
nlp B O
. O O
there O O
that O O
same O O
web B B
search I I
may O O
be O O
broken O O
but O O
that O O
it O O
doesn O O
t O O
matter O O
. O O
these O O
so O O
this O O
is O O
how O O
pseudo-feedback B O
works O O
. O O
the O O
first O O
one O O
s O O
called O O
a O O
pivoted O O
length B B
normalization I I
vector B O
space I I
model I I
. O O
we O O
re O O
going O O
to O O
look O O
at O O
another O O
kind O O
of O O
very O O
different O O
way O O
to O O
design O O
ranking B B
functions I I
then O O
the O O
vector B B
space I I
model I I
that O O
we O O
discussed O O
before O O
. O O
another O O
trend O O
we O O
can O O
expect O O
to O O
see O O
is O O
the O O
search B B
engine I I
will O O
be O O
able O O
to O O
learn O O
over O O
time O O
. O O
so O O
let O O
s O O
see O O
what O O
the O O
is O O
the O O
scale O O
of O O
divergence B B
which O O
we O O
will O O
model O O
. O O
and O O
you O O
might O O
also O O
realize O O
we O O
needed O O
this O O
count O O
of O O
documents O O
or O O
document B B
frequency I I
for O O
computing O O
some O O
statistics O O
to O O
be O O
used O O
in O O
the O O
vector B B
space I I
model I I
. O O
imagine O O
if O O
we O O
drop O O
this O O
logarithm O O
we O O
would O O
still O O
have O O
tf B B
and O O
idf B B
weighting O O
. O O
in O O
this O O
case O O
then O O
the O O
search B B
engine I I
can O O
keep O O
track O O
of O O
the O O
clicked O O
documents O O
and O O
see O O
if O O
one O O
method O O
has O O
contributed O O
more O O
to O O
the O O
clicked O O
documents O O
. O O
now O O
of O O
curse O O
the O O
vector B B
space I I
model I I
is O O
a O O
special O O
case O O
of O O
this O O
. O O
so O O
this O O
key-value O O
pair O O
will O O
be O O
sent O O
to O O
a O O
map B B
function O O
. O O
in O O
sum O O
in O O
this O O
lecture O O
we O O
ll O O
talk O O
about O O
what O O
is O O
nlp B O
and O O
we O O
ve O O
talked O O
about O O
the O O
state O O
of O O
the O O
art O O
techniques O O
what O O
we O O
can O O
do O O
what O O
we O O
cannot O O
do O O
. O O
the O O
issue O O
of O O
document O B
length B B
normalization I I
. O O
so O O
you O O
can O O
imagine O O
now O O
normalization O O
essentially O O
is O O
to O O
compare O O
the O O
actual O O
dcg B B
with O O
the O O
best O O
decision O O
you O O
can O O
possibly O O
get O O
for O O
this O O
topic O O
. O O
now O O
over O O
many O O
decades O O
researchers O O
have O O
designed O O
many O O
different O O
kinds O O
of O O
retrieval B B
models I I
and O O
they O O
fall O O
into O O
different O O
categories O O
. O O
so O O
this O O
shows O O
that O O
by O O
using O O
this O O
vector B B
space I I
representation O O
we O O
can O O
actually O O
capture O O
the O O
differences O O
between O O
topics O O
of O O
documents O O
. O O
we O O
often O O
are O O
interested O O
in O O
to O O
the O O
precision B B
up O O
to O O
ten O O
documents O O
for O O
web B B
search I I
. O O
as O O
more O O
people O O
are O O
using O O
it O O
the O O
search B B
engine I I
will O O
become O O
better O O
and O O
better O O
and O O
this O O
is O O
already O O
happening O O
because O O
the O O
search B B
engines I I
can O O
learn O O
from O O
the O O
of O O
feedback O O
. O O
like O O
map B B
or O O
ndcg B B
. O O
in O O
the O O
pull O O
mode O O
we O O
have O O
further O O
distinguished O O
querying B B
and O O
browsing O O
. O O
note O O
that O O
we O O
don O O
t O O
have O O
to O O
attach O O
any O O
document O B
that O O
that O O
didn O O
t O O
match O O
any O O
query B B
term I I
but O O
this O O
is O O
why O O
it O O
s O O
fast O O
. O O
so O O
if O O
you O O
rank O O
the O O
answers O O
then O O
your O O
goal O O
is O O
to O O
rank O O
that O O
one O O
particular O O
answer O O
on O O
top O O
right O O
so O O
in O O
this O O
case O O
you O O
can O O
easily O O
verify O O
the O O
average O B
position O O
will O O
basically O O
boil O O
down O O
to O O
reciprocal B O
rank I O
. O O
we O O
ll O O
talk O O
about O O
f B O
measure I O
as O O
a O O
way O O
to O O
combine O O
precision B B
and I I
recall B B
. O O
so O O
as O O
a O O
result O O
we O O
have O O
robust O O
and O O
general O O
natural B B
language I I
processing I I
techniques I I
that O O
can O O
process O O
a O O
lot O O
of O O
text O B
data O O
in O O
a O O
shallow O O
way O O
meaning O O
we O O
only O O
do O O
superficial O O
analysis O O
. O O
so O O
do O O
you O O
use O O
map B B
or O O
gmap O O
again O O
that O O
s O O
important O O
question O O
. O O
and O O
this O O
is O O
just O O
like O O
a O O
different O O
a O O
ways O O
to O O
place O O
a O O
doc O O
in O O
the O O
vector O B
in O O
the O O
vector B B
space I I
. O O
so O O
it O O
s O O
a O O
sum O O
of O O
all O O
of O O
the O O
query B O
words I I
and O O
inside O O
the O O
sum O O
that O O
is O O
log O O
of O O
the O O
probability O O
of O O
this O O
word O O
given O O
by O O
the O O
document O O
. O O
second O O
tf B B
and O O
the O O
document B O
frequency I I
of O O
words O I
. O O
and O O
the O O
third O O
is O O
implicit B B
feedback I O
where O O
we O O
use O O
clickthroughs O O
. O O
and O O
in O O
this O O
case O O
you O O
can O O
easy O O
to O O
see O O
this O O
a O O
precisely O O
a O O
vector B B
space I I
model I I
because O O
this O O
part O O
is O O
a O O
sum O O
over O O
all O O
the O O
matched O O
query B B
terms I I
this O O
is O O
an O O
element O O
of O O
the O O
query O B
map B B
. O O
it O O
s O O
very O O
easy O O
to O O
extend O O
the O O
the O O
computation O O
to O O
include O O
other O O
weights O O
like O O
the O O
transformation O O
of O O
tf B B
or O O
document O B
or O O
idf B B
weighting O O
. O O
the O O
reality O O
however O O
high O O
recall B B
tends O O
to O O
be O O
associated O O
with O O
low O O
precision B B
and O I
you O O
can O O
imagine O O
why O O
that O O
is O O
the O O
case O O
. O O
in O O
particular O O
the O O
anchor B O
text I B
base O O
is O O
very O O
useful O O
. O O
in O O
particular O O
you O O
can O O
see O O
the O O
difference O O
in O O
the O O
standard O O
idf B B
and O O
we O O
somehow O O
have O O
a O O
point O O
here O O
. O O
we O O
have O O
also O O
considered O O
a O O
it O O
s O O
global O O
statistics O O
such O O
as O O
idf B B
in O O
words O O
document B O
frequency I I
. O O
in O O
terms O O
of O O
semantic B B
analysis I I
we O O
are O O
far O O
from O O
being O O
able O O
to O O
do O O
a O O
complete O O
understanding O O
of O O
a O O
sentence O O
. O O
and O O
then O O
there O O
is O O
another O O
scenario O O
that O O
s O O
incremental O O
updating O O
of O O
the O O
crawl O O
data O O
or O O
incremental O O
crawling B B
. O O
you O O
need O O
a O O
topical O O
map B B
to O O
tour O O
the O O
information O B
space O I
. O O
you O O
can O O
see O O
in O O
the O O
center O O
there O O
s O O
a O O
triangle O O
that O O
connects O O
keyword B O
queries I O
to O O
search O O
a O O
bag B B
of I I
words I I
representation I I
. O O
and O O
we O O
then O O
talked O O
about O O
a O O
high-level O O
strategies O O
for O O
text O B
access O O
and O O
we O O
talked O O
about O O
push B B
versus O O
pull O O
in O O
plural O O
. O O
and O O
recommend O O
the O O
system O O
is O O
it O O
will O O
help O O
users O O
in O O
the O O
push B B
mode O O
. O O
the O O
topic O O
of O O
learning B O
to I O
rank I O
is O O
still O O
active O O
research O O
. O O
more O O
specifically O O
we O O
will O O
cover O O
how O O
search B B
engines I I
work O O
. O O
so O O
in O O
this O O
case O O
the O O
system O O
a O O
has O O
a O O
precision B B
of O O
two O O
out O O
of O O
three O O
. O O
so O O
naturally O O
it O O
would O O
be O O
interesting O O
to O O
combine O O
them O O
and O O
here O O
s O O
one O O
measure O O
that O O
s O O
often O O
used O O
called O O
f B O
measure I O
. O O
and O O
this O O
is O O
called O O
a O O
maximum B B
likelihood I I
. O O
and O O
this O O
is O O
partly O O
because O O
the O O
search B B
task I O
is O O
not O O
all O O
that O O
difficult O O
. O O
one O O
is O O
text B B
retrieval I I
which O O
is O O
covered O O
in O O
this O O
course O O
. O O
and O O
they O O
are O O
all O O
excellent O O
in O O
covering O O
a O O
broad O O
review O O
of O O
information B B
retrieval I I
and O O
evaluation O O
. O O
if O O
we O O
look O O
back O O
at O O
the O O
assumptions O O
we O O
have O O
made O O
while O O
substantiating O O
the O O
vector B B
space I I
model I I
we O O
will O O
realize O O
that O O
the O O
problem O O
is O O
really O O
coming O O
from O O
some O O
of O O
the O O
assumptions O O
. O O
so O O
as O O
a O O
effective O O
original O O
function O O
bm O O
should O O
probably O O
use O O
a O O
heuristic O O
modification O O
of O O
the O O
idf B B
to O O
make O O
that O O
even O O
more O O
like O O
a O O
vector B O
space I I
model I I
. O O
we O O
generally O O
would O O
just O O
look O O
up O O
a O O
a O O
sequence O O
of O O
document O O
ids O O
and O O
frequencies O O
for O O
all O O
of O O
the O O
documents O O
that O O
match O O
a O O
query B O
term I I
. O O
but O O
in O O
some O O
other O O
languages O O
we O O
may O O
need O O
to O O
do O O
some O O
natural B B
language I I
processing I I
to O O
figure O O
out O O
the O O
where O O
are O O
the O O
boundaries O O
for O O
words O O
. O O
this O O
will O O
give O O
us O O
all O O
the O O
documents O O
that O O
match O O
this O O
query B O
term I I
and O O
that O O
includes O O
d O O
f O O
and O O
so O O
d O O
and O O
fn O O
. O O
this O O
is O O
a O O
popular O O
measure O O
that O O
is O O
often O O
used O O
as O O
a O O
combined O O
precision B B
and I I
recall B B
. O O
and O O
here O O
we O O
see O O
that O O
it O O
s O O
basically O O
the O O
tfidf B O
weighting O O
model O O
that O O
we O O
have O O
discussed O O
. O O
now O O
in O O
the O O
case O O
of O O
text B B
retrieval I I
what O I
should O O
be O O
the O O
right O O
answers O O
to O O
a O O
query O O
is O O
not O O
very O O
well O O
specified O O
as O O
we O O
just O O
discussed O O
. O O
so O O
one O O
challenge O O
in O O
crawling B B
is O O
to O O
find O O
the O O
new O O
pages O O
that O O
people O O
have O O
created O B
and O O
people O O
probably O O
are O O
creating O O
new O O
pages O O
all O O
the O O
time O O
and O O
this O O
is O O
very O O
challenging O O
if O O
the O O
new O O
pages O O
have O O
not O O
been O O
actually O O
linked O O
to O O
any O O
old O O
page O O
. O O
so O O
we O O
either O O
used O O
jm B O
smoothing O O
or O O
dirichlet B O
prior I B
smoothing I I
. O O
so O O
to O O
summarise O O
this O O
lecture O O
the O O
main O O
point O O
is O O
that O O
we O O
need O O
to O O
do O O
some O O
sub O O
linearity O O
of O O
tf B B
transformation O O
. O O
so O O
we O O
can O O
modify O O
this O O
slightly O O
to O O
actually O O
build O O
a O O
inverted B B
index I I
in O O
parallel O O
. O O
because O O
this O O
is O O
a O O
very O O
important O O
topic O O
for O O
search B B
engines I I
. O O
one O O
is O O
content O O
based O O
one O O
is O O
collaborative B O
filtering I O
and O O
they O O
can O O
be O O
combined O O
together O O
. O O
and O O
here O O
the O O
main O O
technique O O
that O O
we O O
talked O O
about O O
how O O
to O O
construct O O
an O O
inverted B B
index I I
. O O
but O O
as O O
we O O
will O O
see O O
later O O
when O O
we O O
consider O O
some O O
specific O O
smoothing B O
methods I I
it O O
turns O O
out O O
that O O
they O O
do O O
penalize O O
long O O
documents O O
. O O
and O O
we O O
also O O
talk O O
about O O
the O O
pagerank B B
and O O
hits B B
algorithm I O
as O O
two O O
major O O
link B O
analysis I I
algorithms O O
. O O
in O O
the O O
previously O O
lecture O O
we O O
talked O O
about O O
natural B B
language I I
content O O
analysis O O
. O O
and O O
then O O
if O O
you O O
re O O
thinking O O
this O O
way O O
basically O O
we O O
can O O
do O O
exactly O O
the O O
same O O
as O O
what O O
we O O
did O O
before O O
we O O
re O O
going O O
to O O
use O O
maximum B B
likelihood I I
estimator I I
to O O
adjust O O
this O O
model O O
to O O
estimate O O
the O O
parameters O O
. O O
so O O
this O O
transformation O O
function O O
is O O
going O O
to O O
turn O O
the O O
raw O O
count O O
of O O
word O O
into O O
a O O
term B O
frequency I I
weight O O
for O O
the O O
word O O
in O O
the O O
document O O
. O O
so O O
for O O
example O O
a O O
news O O
filter O O
or O O
news O O
recommender B B
system I O
could O O
monitor O O
the O O
news O O
stream O O
and O O
identify O O
interest O O
in O O
news O O
to O O
you O O
and O O
simply O O
push B B
the O O
news O O
articles O O
to O O
you O O
. O O
and O O
this O O
will O O
control O O
the O O
influence O O
of O O
really O O
high O O
weight O O
because O O
it O O
s O O
going O O
to O O
lower O O
its O O
inference B B
yet O O
it O O
will O O
retain O O
the O O
inference B B
of O O
small O O
count O O
. O O
in O O
this O O
lecture O O
we O O
are O O
going O O
to O O
talk O O
about O O
one O O
of O O
the O O
most O O
important O O
applications O O
of O O
text B B
retrieval I I
web B B
search B I
engines I I
. O O
a O O
rare O O
term O O
will O O
match O O
fewer O O
documents O O
and O O
then O O
the O O
score O O
confusion O O
will O O
be O O
higher O O
because O O
the O O
idf B B
value O O
will O O
be O O
higher O O
and O O
and O O
then O O
it O O
allows O O
us O O
to O O
attach O O
the O O
most O O
diplomacy O O
documents O O
first O O
. O O
for O O
example O O
the O O
user O O
may O O
type O O
in O O
a O O
query O O
and O O
then O O
browse O O
results O O
to O O
find O O
the O O
relevant B B
information I I
. O O
now O O
this O O
is O O
called O O
a O O
kl-divergence B B
because O O
this O O
can O O
be O O
interpreted O O
as O O
measuring O O
the O O
kl-divergence B B
of O O
two O O
distributions O O
. O O
now O O
in O O
the O O
rocchio B B
feedback O O
method O O
we O O
re O O
going O O
to O O
combine O O
all O O
these O O
with O O
original O O
query O B
vector O I
which O O
is O O
this O O
. O O
first O O
what O O
is O O
natural B B
language I I
processing I I
which O O
is O O
a O O
main O O
technique O O
for O O
processing O O
natural B B
language I I
to O O
obtain O O
understanding O O
the O O
second O O
is O O
the O O
state O O
of O O
the O O
art O O
in O O
nlp B O
which O O
stands O O
for O O
natural B B
language I I
processing I I
. O O
now O O
this O O
preference O O
also O O
has O O
a O O
theoretical O O
justification O O
and O O
this O O
is O O
given O O
by O O
the O O
probability B O
ranking I B
principle I O
. O O
and O O
this O O
could O O
cause O O
underflow O O
and O O
we O O
might O O
lose O O
precision B B
by O O
transforming O O
the O O
value O O
as O O
a O O
logarithm O O
function O O
. O O
yet O O
in O O
some O O
domains O O
perhaps O O
in O O
limited O O
domains O O
when O O
you O O
have O O
a O O
lot O O
of O O
restrictions O O
on O O
the O O
world O O
of O O
users O O
you O O
may O O
be O O
to O O
may O O
be O O
able O O
to O O
perform O O
inference B B
to O O
some O O
extent O O
but O O
in O O
general O O
we O O
cannot O O
really O O
do O O
that O O
reliably O O
. O O
and O O
after O O
we O O
cover O O
that O O
we O O
are O O
going O O
to O O
cover O O
a O O
number O O
of O O
topics O O
all O O
about O O
the O O
search B B
engines I I
. O O
so O O
this O O
is O O
the O O
idea O O
of O O
the O O
statistical B B
significance B B
test I I
. O O
and O O
we O O
also O O
have O O
a O O
formula O O
that O O
intuitively O O
makes O O
a O O
lot O O
of O O
sense O O
and O O
does O O
tf-idf B B
weighting O O
and O O
documenting O O
and O O
some O O
others O O
. O O
we O O
have O O
to O O
somehow O O
be O O
able O O
to O O
estimate O O
this O O
conditional B B
probability I I
without O O
relying O O
on O O
this O O
big O O
table O O
. O O
in O O
this O O
lecture O O
we O O
re O O
going O O
to O O
discuss O O
another O O
subclass O O
in O O
this O O
big O O
class O O
called O O
a O O
language B B
modeling I I
approaches O O
to O O
retrieval O O
. O O
it O O
s O O
used O O
for O O
measuring O O
relevance O B
based O O
on O O
much O O
more O O
level O O
relevance B O
judgments I O
. O O
one O O
is O O
classic O O
probabilistic B B
model I I
another O O
is O O
language B B
model I I
yet O O
another O O
is O O
divergence-from-randomness O O
model O O
. O O
and O O
in O O
fact O O
wherever O O
you O O
have O O
a O O
lot O O
of O O
text O B
data O O
you O O
would O O
have O O
a O O
search B B
engine I I
. O O
so O O
most O O
specific O O
idf B B
can O O
be O O
defined O O
as O O
the O O
logarithm O O
of O O
m O O
plus O O
one O O
divided O O
by O O
k O O
where O O
m O O
is O O
the O O
total O O
number O O
of O O
documents O O
in O O
the O O
collection O O
k O O
is O O
df B O
or O O
document B B
frequency I I
. O O
so O O
the O O
ranking O B
of O O
systems O O
based O O
on O O
only O O
dcg B B
would O O
be O O
exactly O O
the O O
same O O
. O O
so O O
to O O
summarize O O
this O O
lecture O O
we O O
ve O O
talked O O
about O O
how O O
to O O
improve O O
this O O
vector B O
space I I
model I I
. O O
in O O
this O O
lecture O O
we O O
re O O
going O O
to O O
talk O O
about O O
how O O
would O O
you O O
use O O
tf B B
transformation O O
to O O
solve O O
this O O
problem O O
. O O
one O O
is O O
the O O
talking O O
the O O
language B B
model I I
here O O
. O O
and O O
the O O
first O O
component O O
is O O
the O O
crawler O O
that O O
with O O
the O O
crawl O O
pages O O
and O O
the O O
second O O
component O O
is O O
indexer B O
. O O
which O O
is O O
based O O
on O O
the O O
bit B B
vector I O
representation O O
dot B I
product I I
similarity O O
and O O
bag B B
of I I
words I I
instantiation O O
. O O
so O O
this O O
was O O
a O O
basically O O
the O O
main O O
idea O O
of O O
the O O
the O O
vector B B
space I I
model I I
. O O
and O O
if O O
the O O
query O O
comes O O
popular O O
you O O
will O O
assume O O
it O O
will O O
fetch O O
the O O
inverted B B
index I I
for O O
the O O
same O O
term O O
again O O
. O O
so O O
you O O
can O O
see O O
this O O
is O O
already O O
very O O
similar O O
to O O
building O O
a O O
inverted B B
index I I
and O O
if O O
you O O
think O O
about O O
it O O
the O O
output O O
here O O
is O O
indexed O O
by O O
a O O
word O O
and O O
we O O
have O O
already O O
got O O
a O O
dictionary O O
basically O O
. O O
and O O
a O O
programmer O O
would O O
specify O O
these O O
two O O
functions O O
to O O
program O O
on O O
top O O
of O O
mapreduce B B
. O O
we O O
also O O
later O O
talked O O
about O O
leverageable O O
learning O I
approach O O
and O O
that O O
s O O
probabilistic B O
model I O
. O O
and O O
then O O
you O O
merge O O
in O O
step O O
three O O
with O O
do O O
pair-wise O O
merging O O
of O O
these O O
runs O O
and O O
here O O
you O O
eventually O O
merge O O
all O O
the O O
runs O O
we O O
generate O O
a O O
single O O
inverted B B
index I I
. O O
of O I
course O O
within O O
such O O
a O O
short O O
amount O O
of O O
time O O
we O O
can O O
t O O
really O O
give O O
you O O
a O O
a O O
complete O O
view O O
of O O
any O O
of O O
it O O
which O O
is O O
a O O
big O O
field O O
and O O
either O O
expect O O
that O O
to O O
have O O
to O O
see O O
multiple O O
courses O O
on O O
natural B B
language I I
processing I I
topic O O
itself O O
. O O
so O O
far O O
in O O
the O O
lectures O O
about O O
the O O
vector B B
space I I
model I I
we O O
have O O
used O O
the O O
various O O
signals O O
from O O
the O O
document O O
to O O
assess O O
the O O
matching O O
of O O
the O O
document O O
though O O
with O O
a O O
preorder O O
. O O
the O O
first O O
one O O
is O O
a O O
classic O O
textbook O O
on O O
the O O
scare O O
the O O
efficiency O O
of O O
inverted B B
index I I
and O O
the O O
compression O O
techniques O O
and O O
how O O
to O O
in O O
general O O
build O O
a O O
efficient O O
search B B
engine I I
in O O
terms O O
of O O
the O O
space O O
overhead O O
and O O
speed O O
. O O
we O O
only O O
need O O
to O O
process O O
the O O
documents O O
that O O
tap O O
that O O
match O O
at O O
least O O
one O O
query B B
term I I
. O O
if O O
it O O
is O O
the O O
system O O
hasn O O
t O O
seen O O
any O O
relevant B B
items I I
to O O
your O O
interest O O
the O O
system O O
could O O
then O O
take O O
the O O
initiative O O
to O O
recommend O O
information O O
to O O
you O O
. O O
and O O
you O O
can O O
see O O
again O O
b O O
is O O
set O O
to O O
and O O
there O O
there O O
is O O
no O O
length B B
normalization I I
. O O
then O O
we O O
process O O
the O O
security O O
all O O
right O O
let O O
s O O
think O O
about O O
the O O
what O O
should O O
be O O
the O O
order O O
of O O
processing O O
here O O
when O O
we O O
consider O O
query B B
terms I I
it O O
might O O
make O O
difference O O
especially O O
if O O
we O O
don O O
t O O
want O O
to O O
keep O O
to O O
keep O O
all O O
the O O
score O O
accumulators O O
. O O
so O O
this O O
is O O
a O O
one O O
reason O O
why O O
we O O
tend O O
to O O
map B B
these O O
streams O O
into O O
integers O O
so O O
that O O
so O O
that O O
we O O
don O O
t O O
have O O
to O O
carry O O
these O O
streams O O
around O O
. O O
so O O
this O O
analogy O O
also O O
tells O O
us O O
that O O
today O O
we O O
have O O
very O O
good O O
support O O
for O O
querying B B
but O O
we O O
don O O
t O O
really O O
have O O
good O O
support O O
for O O
browsing O O
. O O
this O O
lecture O O
is O O
a O O
overview O O
of O O
text B B
retrieval B I
methods I I
. O O
but O O
the O O
fundamental O O
reason O O
why O O
the O O
natural B B
language I I
processing I I
is O O
difficult O O
for O O
computers O O
is O O
simple O O
because O O
natural B B
language I I
has O O
not O O
been O O
designed O O
for O O
computers O O
. O O
first O O
bag B B
of I I
words I I
representation I I
. O O
so O O
now O O
lets O O
see O O
if O O
our O O
vector B O
space I I
model I I
could O O
do O O
the O O
same O O
or O O
could O O
do O O
something O O
closer O O
. O O
of O O
course O O
if O O
we O O
assume O O
the O O
user O O
stops O O
at O O
the O O
ten O O
documents O O
and O O
we O O
re O O
looking O O
at O O
the O O
cutoff O B
at O O
ten O O
we O O
can O O
look O O
after O O
the O O
total O O
gain B B
of O O
the O O
user O O
. O O
so O O
now O O
a O O
natural O B
question O O
is O O
which O O
model O O
works O O
the O O
best O O
now O O
it O O
turns O O
out O O
that O O
many O O
models O O
work O O
equally O O
well O O
so O O
here O O
i O O
listed O O
the O O
four O O
major O O
models O O
that O O
are O O
generally O O
regarded O O
as O O
a O O
state O O
of O O
the O O
art O O
retrieval B O
models I I
. O O
first O O
we O O
can O O
already O O
see O O
it O O
has O O
a O O
frequency O O
of O O
the O O
word O O
in O O
the O O
query O O
just O O
like O O
in O O
the O O
vector B B
space I I
model I I
. O O
the O O
general O O
idea O O
of O O
smoothing O O
in O O
retrieval O B
is O O
to O O
use O O
the O O
connection O O
language B B
model I I
to O O
give O O
us O O
some O O
clue O O
about O O
which O O
unseen O O
word O O
would O O
have O O
a O O
higher O O
probability O O
. O O
in O O
fact O O
most O O
search B B
engines I I
today O O
use O O
something O O
called O O
a O O
bag B B
of I I
words I I
representation I I
. O O
that O O
is O O
to O O
turn O O
text O B
data O O
into O O
simply O O
a O O
bag B B
of I I
words I I
. O O
the O O
collector O O
would O O
then O O
try O O
to O O
sort O O
all O O
these O O
key O O
value O O
pairs O O
from O O
different O O
map B B
functions O O
. O O
this O O
is O O
a O O
slide O O
that O O
you O O
have O O
seen O O
earlier O O
in O O
the O O
lecture O O
where O O
we O O
talk O O
about O O
the O O
grand O O
evaluation B B
methodology I O
. O O
in O O
this O O
lecture O O
we O O
re O O
going O O
to O O
talk O O
about O O
how O O
to O O
instantiate O O
a O O
vector B B
space I I
model I I
so O O
that O O
we O O
can O O
get O O
a O O
very O O
specific O O
ranking B B
function I I
. O O
and O O
this O O
would O O
then O O
give O O
us O O
an O O
updated O O
model O O
just O O
like O O
again O O
in O O
rocchio B B
. O O
what O O
we O O
could O O
do O O
is O O
do O O
it O O
just O O
normalize O O
the O O
word B O
frequency I I
here O O
. O O
so O O
most O O
specifically O O
we O O
will O O
be O O
very O O
interested O O
in O O
the O O
following O O
conditional B B
probability I I
as O O
i O O
show O O
you O O
you O O
this O O
here O O
. O O
so O O
to O O
summarize O O
all O O
what O O
we O O
have O O
said O O
about O O
the O O
vector B B
space I I
model I I
. O O
now O O
many O O
information B O
retrieval I O
algorithms O O
had O O
been O O
developed O B
at O O
the O O
before O O
the O O
web O B
was O O
born O O
. O O
this O O
lecture O O
is O O
about O O
some O O
practical O O
issues O O
that O O
you O O
would O O
have O O
to O O
address O O
in O O
evaluation B O
of I O
text B B
retrieval B I
systems I I
. O O
so O O
again O O
think O O
about O O
the O O
average O B
of O O
mean O O
reciprocal B O
rank I I
versus O O
average O B
of O O
just O O
r O O
. O O
so O O
what O O
are O O
the O O
major O O
crawling B O
strategies O O
in O O
general O O
breadth-first O O
is O O
most O O
common O O
because O O
it O O
naturally O O
balance O O
balances O O
server O O
load O O
. O O
and O O
this O O
is O O
basically O O
the O O
idea O O
of O O
rocchio B B
of O O
course O O
you O O
then O O
can O O
see O O
that O O
the O O
centroid O O
of O O
negative O O
documents O O
. O O
so O O
the O O
back O O
of O O
words O O
replenishing O O
is O O
generally O O
the O O
main O O
method O O
used O O
in O O
modern O O
search B B
engines I I
and O O
it O O
s O O
often O O
sufficient O O
for O O
most O O
of O O
the O O
search B B
tasks I I
. O O
but O O
intuitively O O
it O O
still O O
implements O O
tfidf B O
waiting O O
and O O
document O O
lens O O
rendition O O
again O O
the O O
form O O
of O O
the O O
function O O
is O O
dictated O O
by O O
the O O
probabilistic O B
reasoning O O
and O O
assumptions O O
that O O
we O O
have O O
made O O
. O O
so O O
for O O
example O O
in O O
the O O
zero-one O O
bit B B
vector I I
retentation O I
we O O
actually O O
use O O
the O O
suchier O O
transformation O O
function O O
as O O
shown O O
here O O
. O O
and O O
here O O
indeed O O
we O O
can O O
see O O
it O O
actually O O
encodes O O
a O O
weight O O
that O O
has O O
similar O O
factor O O
to O O
tf-idf B O
weighting O O
. O O
basically O O
you O O
overlook O O
kinds O O
of O O
terms O O
in O O
a O O
small O O
set O O
of O O
documents O O
and O O
and O O
then O O
once O O
you O O
collect O O
those O O
counts O O
you O O
can O O
sort O O
those O O
counts O O
based O O
on O O
terms O O
so O O
that O O
you O O
build O O
a O O
local O O
a O O
partial O B
inverted B B
index I I
. O O
web B I
search B I
engines I I
ipad O O
all O O
of O O
you O O
are O O
using O O
google O O
or O O
bing O O
or O O
another O O
web B B
search B I
engine I I
all O O
the O O
time O O
. O O
now O O
inside O O
this O O
h O O
function O O
there O O
are O O
functions O O
that O O
would O O
compute O O
the O O
weights O O
of O O
the O O
contribution O O
of O O
a O O
matched O O
query B B
term I I
t O O
i O O
. O O
so O O
this O O
would O O
give O O
us O O
what O O
s O O
called O O
a O O
mean O O
average O B
position O O
or O O
map B B
. O O
in O O
fact O O
you O O
may O O
recall B B
that O O
we O O
talked O O
about O O
using O O
language B B
model I I
to O O
analyze O O
word O O
association O O
to O O
learn O O
related O O
words O O
to O O
the O O
word O O
computer O O
. O O
so O O
for O O
example O O
cosine O O
measure O O
can O O
be O O
regarded O O
as O O
the O O
dot B B
product I I
of O O
two O O
normalized O O
vectors O O
. O O
so O O
for O O
example O O
you O O
can O O
connect O O
everyone O O
with O O
web O B
pages O O
and O O
the O O
support O O
search O B
and O O
browsing O O
what O O
do O O
you O O
get O O
well O O
that O O
s O O
web B B
search I I
right O O
what O O
if O O
we O O
connect O O
uiuc O O
employees O O
with O O
organization O O
documents O O
or O O
enterprise O O
documents O O
to O O
support O O
the O O
search O B
and O O
browsing O O
but O O
that O O
s O O
enterprise O O
search O B
. O O
in O O
the O O
future O O
we O O
would O O
like O O
to O O
have O O
knowledge O O
representation O O
where O O
we O O
can O O
add O O
perhaps O O
inference B B
rules I O
and O O
then O O
the O O
search B B
engine I I
would O O
become O O
more O O
intelligent O O
. O O
the O O
most O O
effective O O
method O O
in O O
the O O
vector B B
space I I
model I I
of O O
feedback O O
is O O
called O O
rocchio B B
feedback O O
which O O
was O O
actually O O
proposed O O
several O O
decades O O
ago O O
. O O
now O O
in O O
this O O
course O O
we O O
re O O
going O O
to O O
talk O O
more O O
mostly O O
about O O
the O O
effectiveness O O
and O O
accuracy O O
measures O O
because O O
the O O
efficiency O O
and O O
usability O O
dimensions O O
are O O
not O O
really O O
unique O O
to O O
search B B
engines I I
and O O
so O O
they O O
are O O
needed O O
for O O
evaluating O O
any O O
other O O
software O O
systems O I
. O O
so O O
more O O
specifically O O
we O O
can O O
assume O O
the O O
input O O
to O O
each O O
map B B
function O O
is O O
a O O
key O O
value O O
pair O O
that O O
represents O O
the O O
line O O
number O O
and O O
the O O
stream O O
on O O
that O O
line O O
. O O
so O O
the O O
reason O O
why O O
i O O
ve O O
put O O
it O O
in O O
the O O
vector B B
space I I
model I I
is O O
first O O
the O O
ranking B B
function I I
actually O O
has O O
a O O
nice O O
interpretation O O
in O O
the O O
vector B B
space I I
model I I
. O O
so O O
in O O
this O O
sense O O
r O O
is O O
also O O
a O O
meaningful O O
measure O O
and O O
the O O
reciprocal B O
rank I I
will O O
take O O
the O O
reciprocal O O
of O O
r O O
instead O O
of O O
using O O
r O O
directly O O
. O O
and O O
if O O
we O O
do O O
that O O
then O O
we O O
get O O
normalized B O
dcg B B
. O O
note O O
that O O
if O O
we O O
only O O
want O O
to O O
heuristically O O
implement O O
a O O
tf B B
weighting O O
and O O
idf B B
weighting O O
we O O
don O O
t O O
necessarily O O
have O O
to O O
have O O
a O O
logarithm O O
here O O
. O O
in O O
this O O
lecture O O
we O O
re O O
going O O
to O O
talk O O
about O O
some O O
possible O O
future O O
trends O O
of O O
web B B
search I I
and O O
intelligent B O
information B O
retrieval B O
systems I I
in O O
general O O
. O O
from O O
the O O
major O O
search B B
engine I I
. O O
the O O
vector B O
space I I
model I I
is O O
really O O
a O O
framework O O
. O O
the O O
second O O
reason O O
is O O
because O O
the O O
original O O
bm O O
has O O
a O O
somewhat O O
different O O
from O O
of O O
idf B B
. O O
so O O
overall O O
this O O
shows O O
by O O
using O O
a O O
probabilistic B B
model I I
we O O
follow O O
very O O
different O O
strategies O O
then O O
the O O
vector B B
space I I
model I I
. O O
if O O
you O O
think O O
about O O
the O O
different O O
ways O O
naturally O O
you O O
will O O
probably O O
be O O
able O O
to O O
think O O
about O O
another O O
way O O
which O O
is O O
geometric B O
mean I O
. O O
right O O
it O O
basically O O
means O O
if O O
a O O
document O O
matches O O
more O O
unique O O
query B B
terms I I
then O O
the O O
document O O
will O O
be O O
assuming O O
to O O
be O O
more O O
relevant O B
. O O
so O O
this O O
is O O
called O O
relevance B B
feedback I I
. O O
so O O
here O O
s O O
how O O
mapreduce B B
works O O
. O O
so O O
here O O
i O O
show O O
in O O
x-axis O O
that O O
raw O O
count O O
and O O
in O O
y-axis O O
i O O
show O O
the O O
term B O
frequency I I
weight O O
. O O
so O O
in O O
this O O
case O O
in O O
the O O
case O O
of O O
browsing O O
the O O
users O O
would O O
simply O O
navigate O O
into O O
the O O
relevant B B
information I I
by O O
following O O
the O O
path O O
that O O
s O O
supported O O
by O O
the O O
structures O O
on O O
the O O
documents O O
. O O
so O O
this O O
calls O O
for O O
large-scale O O
semantic B B
analysis I I
and O O
perhaps O O
this O O
is O O
more O O
feasible O O
for O O
vertical O O
search B B
engines I I
. O O
and O O
and O O
so O O
we O O
can O O
t O O
even O O
do O O
part B O
of I O
speech I I
tagging I I
. O O
search B I
engines I I
are O O
the O O
main O O
tools O O
for O O
supporting O O
the O O
pull O O
mode O O
. O O
and O O
the O O
ro O O
rocchio B B
method O I
is O O
usually O O
robust O O
and O O
effective O O
. O O
that O O
includes O O
text B B
retrieval I I
problem O O
text B O
retrieval B I
methods I I
how O O
to O O
evaluate O O
these O O
methods O O
implementation O O
of O O
the O O
system O O
and O O
web B B
search I I
applications O O
. O O
so O O
in O O
comparison O O
search B B
tasks I I
are O O
solved O O
relatively O O
easy O O
such O O
a O O
representation O O
is O O
often O O
sufficient O O
. O O
and O O
in O O
fact O O
we O O
re O O
going O O
to O O
place O O
all O O
the O O
documents O O
in O O
this O O
vector B B
space I I
. O O
we O O
usually O O
think O O
of O O
this O O
as O O
processing O O
of O O
natural B B
languages I I
but O O
you O O
could O O
also O O
think O O
of O O
this O O
as O O
you O O
say O O
language O B
process O O
is O O
natural O B
. O O
now O O
if O O
we O O
re O O
plug O O
in O O
this O O
function O O
into O O
our O O
tf-idf B B
weighting O O
vector B O
space I I
model I I
then O O
we O O
would O O
end O O
up O O
having O O
the O O
following O O
ranking B B
function I I
which O O
has O O
a O O
bm O O
tf B B
component O O
. O O
so O O
because O O
of O O
these O O
challenges O O
and O O
opportunities O O
there O O
are O O
new O O
techniques O O
that O O
have O O
been O O
developed O B
for O O
web B B
search I I
or O O
due O O
to O O
the O O
need O O
of O O
a O O
web B B
search I I
. O O
we O O
re O O
going O O
to O O
talk O O
about O O
what O O
is O O
a O O
language B B
model I I
and O O
then O O
we O O
re O O
going O O
to O O
talk O O
about O O
the O O
simplest O O
language B B
model I I
called O O
a O O
unigram O O
language B B
model I I
. O O
by O O
adjusting O O
b O O
which O O
varies O O
from O O
zero O B
to O O
one O O
we O O
can O O
control O O
the O O
degree O O
of O O
length B B
normalization I I
. O O
so O O
in O O
this O O
case O O
we O O
can O O
assume O O
the O O
input O O
to O O
a O O
map B B
function O O
is O O
a O O
pair O O
of O O
a O O
key O O
which O O
denotes O O
the O O
document O O
id O B
and O O
the O O
value O O
denoting O O
the O O
string O O
for O O
that O O
document O O
. O O
so O O
naturally O O
that O O
implies O O
that O O
the O O
query O B
and O O
document O O
must O O
be O O
represented O O
in O O
the O O
same O O
way O O
and O O
in O O
this O O
case O O
we O O
represent O O
them O O
as O O
vectors O O
in O O
high O O
dimensional B O
vector B O
space I I
. O O
actually O O
the O O
use O O
of O O
machine B B
learning I I
in O O
information B O
retrieval I I
has O O
started O O
since O O
many O O
decades O O
ago O O
. O O
now O O
to O O
understand O O
this O O
idea O O
let O O
s O O
take O O
a O O
look O O
at O O
the O O
general O O
idea O O
or O O
the O O
basic O O
idea O O
of O O
probabilistic B B
retrieval B B
models I I
. O O
interestingly O I
we O O
also O O
have O O
something O O
related O O
to O O
the O O
length B B
normalization I I
. O O
the O O
first O O
part O O
is O O
the O O
indexer B O
and O O
the O O
second O O
part O O
is O O
the O O
scorer O O
that O O
responds O O
to O O
the O O
user O O
s O O
query O O
. O O
so O O
how O O
do O O
we O O
compute O O
this O O
query O O
like O O
if O O
we O O
make O O
this O O
sum O O
where O O
it O O
involves O O
two O O
steps O O
right O O
the O O
first O O
is O O
the O O
computer O O
s O O
model O O
and O O
we O O
call O O
it O O
talking O O
the O O
language B B
model I I
here O O
. O O
this O O
is O O
very O O
similar O O
to O O
rocchio B O
which O O
updates O O
the O O
query O O
vector O O
. O O
to O O
summarize O O
we O O
talk O O
about O O
precision B B
which O O
addresses O O
the O O
question O O
are O O
the O O
retrieval O O
results O O
all O O
relevant O B
we O O
ll O O
also O O
talk O O
about O O
the O O
recall B B
which O O
addresses O O
the O O
question O O
have O O
all O O
the O O
relevant B B
documents I I
been O O
retrieved O I
these O O
two O O
are O O
the O O
two O O
basic O O
measures O O
in O O
testing O O
retrieval O O
in O O
variation O O
. O O
it O O
means O O
we O O
are O O
to O O
make O O
judgements O O
on O O
relevant B B
documents I I
because O O
those O O
are O O
the O O
most O O
useful O O
documents O O
from O O
the O O
users O O
perspective O O
. O O
we O O
ll O O
talk O O
about O O
how O O
we O O
will O O
construct O O
the O O
inverted B B
index I I
when O O
the O O
data O O
can O O
fit O O
into O O
the O O
memory O O
. O O
in O O
the O O
next O O
lecture O O
we O O
re O O
going O O
to O O
talk O O
about O O
collaborative B O
filtering I B
. O O
the O O
first O O
is O O
a O O
paper O O
about O O
the O O
pivoted O O
length B B
normalization I I
. O O
if O O
you O O
use O O
more O O
queries O O
then O O
you O O
will O O
also O O
have O O
to O O
take O O
the O O
average O B
of O O
the O O
average B B
precision B B
over O O
all O O
these O O
queries O O
. O O
in O O
this O O
case O O
you O O
know O O
a O O
new O O
system O O
might O O
be O O
penalized O O
because O O
it O O
might O O
have O O
nominated O O
some O O
relevant B B
documents I I
that O O
have O O
not O O
been O O
judged O O
. O O
for O O
example O O
part O O
of O O
s O O
of O O
speech O O
tagging O I
or O O
partial B B
parsing B B
or O O
recognizing O O
sentiment O O
. O O
and O O
then O O
we O O
re O O
going O O
to O O
use O O
the O O
background O O
language B B
model I I
. O O
this O O
map B B
shows O O
the O O
major O O
topics O O
we O O
have O O
covered O O
in O O
this O O
course O O
. O O
and O O
then O O
we O O
re O O
going O O
to O O
talk O O
about O O
two O O
ways O O
to O O
implement O O
the O O
pull O O
mode O O
querying B B
vs O O
browsing O O
. O O
in O O
particular O O
we O O
re O O
going O O
to O O
discuss O O
the O O
query O B
likelihood O O
retrieval B O
model I I
which O O
is O O
one O O
of O O
the O O
most O O
effective O O
models O O
in O O
probabilistic B B
models I I
. O O
the O O
tf B B
is O O
for O O
news O O
. O O
this O O
measures O O
the O O
completeness O O
of O O
coverage O O
of O O
relevant B B
documents I I
in O O
your O O
retriever O O
result O O
. O O
at O O
different O O
estimation O O
methods O O
here O O
would O O
lead O O
to O O
different O O
ranking B B
functions I I
. O O
first O O
we O O
re O O
going O O
to O O
talk O O
about O O
the O O
crawler O O
also O O
called O O
a O O
spider O O
or O O
a O O
software O O
robot O O
that O O
would O O
do O O
something O O
like O O
a O O
crawling B B
pages O O
on O O
the O O
web O O
. O O
we O O
talked O O
about O O
the O O
three O O
major O O
feedback O O
scenarios O O
relevance B O
feedback I I
pseudo-feedback B O
and O O
implicit B O
feedback I I
. O O
the O O
third O O
trend O O
might O O
be O O
to O O
the O O
integration O O
of O O
bottles O O
of O O
information B B
access I I
. O O
note O O
that O O
in O O
the O O
vector B B
space I I
model I I
we O O
assume O O
they O O
are O O
vectors O O
. O O
because O O
d O O
matched O O
the O O
three O O
distinctive O O
query B B
words I I
news O O
presidential O O
campaign O O
. O O
for O O
example O O
you O O
might O O
consider O O
title O O
field O O
the O O
abstract O O
or O O
body O O
of O O
the O O
reasearch O O
article O O
or O O
even O O
anchor B O
text I B
on O O
the O O
web O O
pages O O
. O O
and O O
we O O
generally O O
need O O
to O O
use O O
a O O
lot O O
of O O
heuristics O O
to O O
design O O
a O O
ranking B B
function I I
. O O
here O O
we O O
say O O
inverse B O
document B B
frequency I I
because O O
we O O
actually O O
want O O
to O O
reword O O
a O O
word O O
that O O
doesn O O
t O O
occur O O
in O O
many O O
documents O O
. O O
in O O
order O O
to O O
consider O O
the O O
difference O O
between O O
a O O
document O O
where O O
a O O
query B O
term I I
occurred O O
multiple O O
times O O
and O O
the O O
one O O
where O O
the O O
query B B
term I I
occurred O O
just O O
once O O
. O O
so O O
in O O
the O O
previous O O
ranking B B
functions I I
we O O
actually O O
have O O
increasingly O O
used O O
some O O
kind O O
of O O
transformation O O
. O O
and O O
these O O
are O O
the O O
output O O
that O O
you O O
see O O
here O O
on O O
this O O
slide O O
from O O
this O O
map B B
function O O
. O O
it O O
can O O
control O O
the O O
emphasis O O
on O O
precision B B
or O O
recall B B
. O O
all O O
right O O
so O O
we O O
going O O
to O O
see O O
precision B B
and I I
recall B B
is O O
all O O
focused O O
on O O
looking O O
at O O
the O O
a O O
that O O
s O O
the O O
number O O
of O O
retrieval O B
relevant B B
documents I I
but O O
we O O
re O O
going O O
to O O
use O O
different O O
denominators O O
. O O
so O O
how O O
to O O
construct O O
such O O
a O O
topical O O
map B B
is O O
in O O
fact O O
a O O
very O O
interesting O O
research O O
question O O
that O O
likely O O
will O O
bring O O
us O O
more O O
interesting O O
browsing O O
experience O O
on O O
the O O
web O B
or O O
in O O
other O O
applications O O
. O O
now O O
we O O
see O O
very O O
clearly O O
this O O
tf B B
weighting O O
here O O
. O O
so O O
cumulative B O
gain B B
gives O O
us O O
some O O
idea O O
about O O
how O O
much O O
total O O
gain B B
the O O
user O O
would O O
have O O
if O O
the O O
user O O
examines O O
all O O
these O O
documents O O
. O O
now O O
this O O
is O O
not O O
the O O
desirable O O
because O O
one O O
can O O
easily O O
have O O
a O O
perfect O O
recall B B
. O O
would O O
lead O O
it O O
to O O
a O O
different O O
ranking B B
function I I
in O O
the O O
vector B B
space I I
model I I
. O O
in O O
this O O
case O O
this O O
is O O
the O O
sum O O
of O O
precision B B
and I I
recall B B
. O O
if O O
our O O
scoring O O
function O O
is O O
just O O
a O O
a O O
simple O O
sum O O
of O O
tf B B
values O O
. O O
and O O
we O O
have O O
seen O O
that O O
this O O
improved O O
model O O
indeed O O
works O O
better O O
than O O
the O O
simplest O O
vector B B
space I I
model I I
but O O
it O O
also O O
still O O
has O O
some O O
problems O O
. O O
before O O
we O O
discuss O O
the O O
details O O
let O O
s O O
take O O
a O O
look O O
at O O
the O O
formula O O
for O O
this O O
symbol O O
here O O
for O O
idf B B
weighting O O
ranking B B
function I I
and O O
see O O
why O O
this O O
document O B
has O O
received O O
such O O
a O O
high O O
score O O
. O O
so O O
this O O
is O O
how O O
we O O
use O O
the O O
kl-divergence B B
model O I
to O O
then O O
do O O
feedback O O
. O O
and O O
the O O
second O O
assumption O O
with O O
med O O
is O O
are O O
query B O
words I I
are O O
generated O O
independently O O
that O O
allows O O
us O O
to O O
decompose O O
the O O
probability O O
of O O
the O O
whole O O
query O O
into O O
a O O
product O O
of O O
probabilities O O
of O O
old O O
words O O
in O O
the O O
query O O
. O O
and O O
there O O
are O O
of O O
course O O
calls O O
are O O
more O O
advanced O O
than O O
machine B B
learning I I
projects O I
that O O
have O O
been O O
proposed O O
for O O
solving O O
these O O
problems O O
. O O
jelinek-mercer B O
which O O
is O O
doing O O
the O O
fixed O O
coefficient O O
linear B O
interpolation B O
. O O
we O O
can O O
use O O
this O O
text O B
to O O
estimate O O
a O O
language B B
model I I
. O O
so O O
this O O
gain B B
basically O O
can O O
mesh O O
your O O
how O O
much O O
gain B B
of O O
random O O
information O B
a O O
user O O
can O O
obtain O O
by O O
looking O O
at O O
each O O
document O O
alright O O
. O O
and O O
that O O
s O O
why O O
it O O
s O O
very O O
tricky O O
how O O
to O O
design O O
effective O O
ranking B B
function I I
. O O
and O O
this O O
is O O
called O O
implicit B B
feedback I I
and O O
we O O
can O O
again O O
use O O
the O O
information O B
to O O
update O O
the O O
query O O
. O O
and O O
we O O
ll O O
talk O O
about O O
that O O
in O O
the O O
natural B B
language I I
processing I I
lecture O O
. O O
eventually O O
we O O
will O O
get O O
a O O
single O O
inverted B B
index I I
where O O
the O O
their O O
entries O O
are O O
sorted O O
based O O
on O O
term O O
ids O O
. O O
so O O
this O O
is O O
how O O
you O O
can O O
decode O O
a O O
gamma B B
code O O
. O O
it O O
s O O
very O O
similar O O
to O O
the O O
case O O
of O O
jm B O
smoothing O B
. O O
another O O
thing O O
is O O
to O O
avoid O O
over-fitting O O
that O O
means O O
we O O
have O O
to O O
keep O O
relatively O O
high O O
weight O O
on O O
the O O
original O O
query B B
terms I I
. O O
so O O
now O O
as O O
a O O
result O O
of O O
idf B B
weighting O O
we O O
will O O
have O O
d O O
to O O
be O O
ranked O O
above O O
d O O
. O O
now O O
why O O
do O O
we O O
want O O
to O O
do O O
this O O
well O O
by O O
doing O O
this O O
we O O
ll O O
map B B
the O O
dcg B B
values O I
in O O
to O O
a O O
range O O
of O O
zero O O
through O O
one O O
so O O
the O O
best O O
value O O
or O O
the O O
highest O O
value O O
for O O
every O O
query O B
would O O
be O O
one O O
. O O
so O O
here O O
are O O
some O O
examples O O
of O O
the O O
feedback O O
model O O
learned O O
from O O
a O O
web O B
document O B
collection O I
and O O
we O O
do O O
pseudo-feedback B O
. O O
so O O
computers O O
have O O
to O O
understand O O
natural B B
languages I I
to O O
some O O
extent O O
in O O
order O O
to O O
make O O
use O O
of O O
the O O
data O O
so O O
that O O
s O O
the O O
topic O O
of O O
this O O
lecture O O
. O O
in O O
particular O O
if O O
you O O
look O O
at O O
these O O
three O O
documents O O
they O O
will O O
all O O
get O O
the O O
same O O
score O O
because O O
they O O
match O O
the O O
three O O
unique O O
query B B
words I I
. O O
and O O
those O O
additional O O
words O O
could O O
be O O
related O O
to O O
the O O
query B O
words I I
. O O
which O O
is O O
one O O
particular O O
approach O O
to O O
design O O
ranking B B
function I I
. O O
here O O
you O O
can O O
see O O
this O O
is O O
original O O
query O B
vector O I
and O O
this O O
average O B
basically O O
is O O
the O O
centroid O O
vector O B
of O O
relevant B B
documents I I
. O O
the O O
first O O
is O O
the O O
classic O O
paper O O
on O O
probability B O
ranking I B
principle I O
. O O
to O O
summarize O O
in O O
this O O
lecture O O
we O O
talked O O
about O O
how O O
to O O
instantiate O O
a O O
vector B B
space I I
model I I
. O O
and O O
this O O
process O O
of O O
relevant B B
documents I I
right O O
and O O
these O O
are O O
random O O
documents O O
for O O
example O O
that O O
s O O
relevant O B
etc O O
. O O
and O O
such O O
a O O
system O O
can O O
actually O O
be O O
based O O
on O O
a O O
search B B
engine I I
system O O
by O O
adding O O
a O O
threshold O O
mechanism O O
and O O
adding O O
adaptive O O
learning B B
algorithm I O
to O O
allow O O
the O O
system O O
to O O
learn O O
from O O
long O O
term O O
feedback O O
from O O
the O O
user O O
. O O
