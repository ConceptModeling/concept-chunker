by O O
taking O O
the O O
complex O O
conjugate B B
of O O
this O O
equation O O
and O O
subtracting O O
the O O
original O O
equation O O
, O O
and O O
then O O
forming O O
the O O
inner O O
product O O
with O O
eigenvector O O
ui O O
, O O
show O O
that O O
the O O
eigenvalues O O
λi O O
are O O
real O O
. O O
the O O
parameter O O
ν O O
is O O
called O O
the O O
degrees B B
of I I
freedom I I
, O O
and O O
its O O
effect O O
is O O
illustrated O O
in O O
figure O O
2.15. O O
for O O
the O O
particular O O
case O O
of O O
ν O O
= O O
1 O O
, O O
the O O
t-distribution O O
reduces O O
to O O
the O O
cauchy O O
distribution O O
, O O
while O O
in O O
the O O
limit O O
ν O O
→ O O
∞ O O
the O O
t-distribution O O
st O O
( O O
x|µ O O
, O O
λ O O
, O O
ν O O
) O O
becomes O O
a O O
gaussian O O
n O O
( O O
x|µ O O
, O O
λ O O
−1 O O
) O O
with O O
mean B B
µ O O
and O O
precision O O
λ. O O
from O O
( O O
2.158 O O
) O O
, O O
we O O
see O O
that O O
student O O
’ O O
s O O
t-distribution O O
is O O
obtained O O
by O O
adding O O
up O O
an O O
inﬁnite O O
number O O
of O O
gaussian O O
distributions O O
having O O
the O O
same O O
mean B B
but O O
different O O
preci- O O
sions O O
. O O
from O O
this O O
we O O
obtain O O
σ O O
( O O
wtφ O O
) O O
= O O
( O O
4.146 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
σ O O
( O O
wtφ O O
) O O
q O O
( O O
w O O
) O O
dw O O
= O O
σ O O
( O O
a O O
) O O
p O O
( O O
a O O
) O O
da O O
( O O
4.147 O O
) O O
where O O
( O O
cid:6 O O
) O O
p O O
( O O
a O O
) O O
= O O
4.5. O O
bayesian O O
logistic B B
regression I I
219 O O
δ O O
( O O
a O O
− O O
wtφ O O
) O O
q O O
( O O
w O O
) O O
dw O O
. O O
9.13 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
using O O
the O O
re-estimation O O
equations O O
for O O
the O O
em O O
algorithm O O
, O O
show O O
that O O
a O O
mix- O O
ture O O
of O O
bernoulli O O
distributions O O
, O O
with O O
its O O
parameters O O
set O O
to O O
values O O
corresponding O O
to O O
a O O
maximum O B
of O O
the O O
likelihood B B
function I I
, O O
has O O
the O O
property O O
that O O
k O O
( O O
cid:2 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
e O O
[ O O
x O O
] O O
= O O
1 O O
n O O
xn O O
≡ O O
x O O
. O O
the O O
latter O B
problem O O
can O O
be O O
solved O O
by O O
ﬁrst O O
running O O
the O O
forward-backward O B
( O O
sum-product O O
) O O
algorithm O O
to O O
ﬁnd O O
the O O
latent B B
variable I I
marginals O O
γ O O
( O O
zn O O
) O O
and O O
then O O
maximizing O O
each O O
of O O
these O O
individually O O
( O O
duda O O
et O O
al. O O
, O O
2001 O O
) O O
. O O
, O O
n O O
, O O
together O O
with O O
a O O
cor- O O
responding O O
sum-of-squares B B
error I I
function O O
deﬁned O O
by O O
averaging O O
over O O
the O O
distribution O O
of O O
input O O
noise O O
to O O
give O O
( O O
cid:6 O O
) O O
n O O
( O O
cid:2 O O
) O O
e O O
= O O
1 O O
2 O O
n=1 O O
{ O O
y O O
( O O
xn O O
− O O
ξn O O
) O O
− O O
tn O O
} O O
2 O O
g O O
( O O
ξn O O
) O O
dξn O O
. O O
because O O
βφtφ O O
is O O
a O O
positive B O
deﬁnite I I
matrix I O
, O O
it O O
will O O
have O O
positive O O
eigenvalues O O
, O O
and O O
so O O
the O O
ratio O O
λi/ O O
( O O
λi O O
+ O O
α O O
) O O
will O O
lie O O
between O O
0 O O
and O O
1. O O
consequently O O
, O O
the O O
quantity O O
γ O O
deﬁned O O
by O O
( O O
3.91 O O
) O O
will O O
lie O O
in O O
the O O
range O O
0 O O
( O O
cid:1 O O
) O O
γ O O
( O O
cid:1 O O
) O O
m. O O
for O O
directions O O
in O O
which O O
λi O O
( O O
cid:12 O O
) O O
α O O
, O O
the O O
corresponding O O
parameter O O
wi O O
will O O
be O O
close O O
to O O
its O O
maximum B B
likelihood I I
value O O
, O O
and O O
the O O
ratio O O
λi/ O O
( O O
λi O O
+ O O
α O O
) O O
will O O
be O O
close O O
to O O
1. O O
such O O
parameters O O
are O O
called O O
well O O
determined O O
because O O
their O O
values O O
are O O
tightly O O
constrained O O
by O O
the O O
data O O
. O O
we O O
can O O
, O O
however O O
, O O
obtain O O
a O O
good O O
approx- O O
imation O O
( O O
spiegelhalter O O
and O O
lauritzen O O
, O O
1990 O O
; O O
mackay O O
, O O
1992b O O
; O O
barber O O
and O O
bishop O O
, O O
1998a O O
) O O
by O O
making O O
use O O
of O O
the O O
close O O
similarity O O
between O O
the O O
logistic B B
sigmoid I I
function O O
σ O O
( O O
a O O
) O O
deﬁned O O
by O O
( O O
4.59 O O
) O O
and O O
the O O
probit B B
function I I
φ O O
( O O
a O O
) O O
deﬁned O O
by O O
( O O
4.114 O O
) O O
. O O
the O O
results O O
( O O
6.66 O O
) O O
and O O
( O O
6.67 O O
) O O
deﬁne O O
the O O
predictive B B
distribution I I
for O O
gaussian O O
pro- O O
cess O O
regression B B
with O O
an O O
arbitrary O O
kernel B O
function I O
k O O
( O O
xn O O
, O O
xm O O
) O O
. O O
60 O O
1. O O
introduction O O
1.12 O O
( O O
( O O
cid:1 O O
) O O
( O O
cid:1 O O
) O O
) O O
www O O
using O O
the O O
results O O
( O O
1.49 O O
) O O
and O O
( O O
1.50 O O
) O O
, O O
show O O
that O O
e O O
[ O O
xnxm O O
] O O
= O O
µ2 O O
+ O O
inmσ2 O O
( O O
1.130 O O
) O O
where O O
xn O O
and O O
xm O O
denote O O
data O O
points O O
sampled O O
from O O
a O O
gaussian O O
distribution O O
with O O
mean B B
µ O O
and O O
variance B B
σ2 O O
, O O
and O O
inm O O
satisﬁes O O
inm O O
= O O
1 O O
if O O
n O O
= O O
m O O
and O O
inm O O
= O O
0 O O
otherwise O O
. O O
the O O
geodesic B O
distance I O
between O O
any O O
pair O O
of O O
points O O
is O O
then O O
approximated O O
by O O
the O O
sum O O
of O O
the O O
arc B O
lengths O O
along O O
the O O
shortest O O
path O O
connecting O O
them O O
( O O
which O O
itself O O
is O O
found O O
using O O
standard O O
algorithms O O
) O O
. O O
many O O
of O O
the O O
modern O O
approaches O O
to O O
computer O O
vision O O
exploit O O
this O O
property O O
by O O
extracting O O
local B O
features O O
that O O
depend O O
only O O
on O O
small O O
subregions O O
of O O
the O O
image O O
. O O
classical B O
dynamics O O
is O O
de- O O
scribed O O
by O O
newton O O
’ O O
s O O
second O O
law O O
of O O
motion O O
in O O
which O O
the O O
acceleration O O
of O O
an O O
object O O
is O O
proportional O O
to O O
the O O
applied O O
force O O
, O O
corresponding O O
to O O
a O O
second-order O O
differential B O
equa- O O
tion O O
over O O
time O O
. O O
to O O
determine O O
the O O
corresponding O O
weight O B
associ- O O
ated O O
with O O
a O O
sample O O
z O O
( O O
l O O
) O O
, O O
we O O
note O O
that O O
the O O
sampling O O
distribution O O
( O O
cid:4 O O
) O O
q O O
( O O
z O O
) O O
is O O
uniform O B
over O O
the O O
possible O O
choices O O
for O O
z O O
, O O
and O O
that O O
( O O
cid:4 O O
) O O
p O O
( O O
z|x O O
) O O
= O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
) O O
, O O
where O O
x O O
denotes O O
the O O
subset O O
of O O
variables O O
that O O
are O O
observed O O
, O O
and O O
the O O
equality O O
follows O O
from O O
the O O
fact O O
that O O
every O O
sample O O
z O O
that O O
is O O
generated O O
is O O
necessarily O O
consistent B B
with O O
the O O
evidence O B
. O O
we O O
begin O O
by O O
considering O O
the O O
functional B B
form O O
of O O
the O O
network O O
model O O
, O O
including O O
the O O
speciﬁc O O
parameterization O O
of O O
the O O
basis O O
functions O O
, O O
and O O
we O O
then O O
discuss O O
the O O
prob- O O
lem O O
of O O
determining O O
the O O
network O O
parameters O O
within O O
a O O
maximum B B
likelihood I I
frame- O O
work O O
, O O
which O O
involves O O
the O O
solution O O
of O O
a O O
nonlinear O O
optimization O O
problem O O
. O O
this O O
leads O O
naturally O O
to O O
a O O
generative O O
view O O
of O O
such O O
models O O
in O O
which O O
we O O
first O O
select O O
a O O
poinl O O
within O O
the O O
manifold B B
according O O
to O O
some O O
latent B B
variable I I
distribution O O
and O O
then O O
generate O O
an O O
observed O O
data O O
point O O
by O O
: O O
ldding O O
noise O O
, O O
drawn O O
from O O
some O O
conditional B B
distribution O O
of O O
the O O
data O O
varillbles O O
given O O
the O O
latent O B
varillbles O O
. O O
the O O
cross O O
entropy B B
and O O
the O O
gini O O
index O O
are O O
better O O
measures O O
than O O
the O O
misclassiﬁcation O O
rate O O
for O O
growing O O
the O O
tree B B
because O O
they O O
are O O
more O O
sensitive O O
to O O
the O O
node B B
probabilities O O
. O O
we O O
then O O
use O O
this O O
posterior O O
distri- O O
bution O O
to O O
evaluate O O
the O O
expectation B B
of O O
the O O
logarithm O O
of O O
the O O
complete-data O O
likelihood B B
function I I
, O O
as O O
a O O
function O O
of O O
the O O
parameters O O
θ O O
, O O
to O O
give O O
the O O
function O O
q O O
( O O
θ O O
, O O
θold O O
) O O
deﬁned O O
by O O
( O O
cid:2 O O
) O O
q O O
( O O
θ O O
, O O
θold O O
) O O
= O O
p O O
( O O
z|x O O
, O O
θold O O
) O O
ln O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
. O O
for O O
the O O
conditional B B
distribution O O
given O O
by O O
( O O
5.12 O O
) O O
, O O
it O O
is O O
sufﬁcient O O
to O O
take O O
the O O
output O O
unit O O
activation B B
function I I
to O O
be O O
the O O
identity O O
, O O
because O O
such O O
a O O
network O O
can O O
approximate O O
any O O
continuous O O
function O O
from O O
x O O
to O O
y. O O
given O O
a O O
data O O
set O O
of O O
n O O
independent B B
, O O
identically O O
distributed O O
observations O O
x O O
= O O
{ O O
x1 O O
, O O
. O O
note O O
that O O
we O O
are O O
only O O
interested O O
in O O
the O O
functional B B
dependence O O
of O O
the O O
right-hand O O
side O O
on O O
the O O
variable O O
z. O O
thus O O
any O O
terms O O
that O O
do O O
not O O
depend O O
on O O
z O O
can O O
be O O
absorbed O O
into O O
the O O
additive O O
normalization O O
constant O O
, O O
giving O O
ln O O
q O O
( O O
cid:1 O O
) O O
( O O
z O O
) O O
= O O
eπ O O
[ O O
ln O O
p O O
( O O
z|π O O
) O O
] O O
+ O O
eµ O O
, O O
λ O O
[ O O
ln O O
p O O
( O O
x|z O O
, O O
µ O O
, O O
λ O O
) O O
] O O
+ O O
const O O
. O O
we O O
now O O
omit O O
where O O
we O O
have O O
used O O
the O O
second O O
term O O
− O O
ln O O
∆ O O
on O O
the O O
right-hand O O
side O O
of O O
( O O
1.102 O O
) O O
and O O
then O O
consider O O
the O O
limit O O
1.6. O O
information B O
theory I I
53 O O
∆ O O
→ O O
0. O O
the O O
ﬁrst O O
term O O
on O O
the O O
right-hand O O
side O O
of O O
( O O
1.102 O O
) O O
will O O
approach O O
the O O
integral O O
of O O
p O O
( O O
x O O
) O O
ln O O
p O O
( O O
x O O
) O O
in O O
this O O
limit O O
so O O
that O O
( O O
cid:25 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:24 O O
) O O
( O O
cid:2 O O
) O O
i O O
lim O O
∆→0 O O
p O O
( O O
xi O O
) O O
∆ O O
ln O O
p O O
( O O
xi O O
) O O
= O O
− O O
p O O
( O O
x O O
) O O
ln O O
p O O
( O O
x O O
) O O
dx O O
( O O
1.103 O O
) O O
where O O
the O O
quantity O O
on O O
the O O
right-hand O O
side O O
is O O
called O O
the O O
differential B B
entropy I I
. O O
however O O
, O O
in O O
a O O
batch O O
setting O O
we O O
have O O
the O O
opportunity O O
to O O
re-use O O
the O O
data O O
points O O
many O O
times O O
in O O
order O O
to O O
achieve O O
improved O O
ac- O O
curacy O O
, O O
and O O
it O O
is O O
this O O
idea O O
that O O
is O O
exploited O O
in O O
expectation B B
propagation I I
. O O
in O O
section O O
1.5.5 O O
, O O
when O O
we O O
discussed O O
decision B O
theory I I
for O O
regression B B
problems O O
, O O
we O O
considered O O
various O O
loss O O
functions O O
each O O
of O O
which O O
leads O O
to O O
a O O
corresponding O O
optimal O O
prediction O O
once O O
we O O
are O O
given O O
the O O
conditional B B
distribution O O
p O O
( O O
t|x O O
) O O
. O O
a O O
general O O
feature O O
of O O
re- O O
inforcement O O
learning B B
is O O
the O O
trade-off O O
between O O
exploration B O
, O O
in O O
which O O
the O O
system O O
tries O O
out O O
new O O
kinds O O
of O O
actions O O
to O O
see O O
how O O
effective O O
they O O
are O O
, O O
and O O
exploitation B O
, O O
in O O
which O O
the O O
system O O
makes O O
use O O
of O O
actions O O
that O O
are O O
known O O
to O O
yield O O
a O O
high O O
reward O O
. O O
9 O O
mixture B B
models O O
and O O
em O O
9.1 O O
k-means O O
clustering B B
. O O
note O O
that O O
it O O
is O O
not O O
sufﬁcient O O
to O O
include O O
only O O
the O O
parents O O
and O O
children O O
of O O
node B B
xi O O
because O O
the O O
phenomenon O O
of O O
explaining B O
away I O
means O O
that O O
observations O O
of O O
the O O
child O O
nodes O O
will O O
not O O
block O O
paths O O
to O O
the O O
co-parents B O
. O O
as O O
before O O
, O O
we O O
can O O
maximize O O
the O O
lower B B
bound I I
l O O
( O O
q O O
) O O
by O O
optimization O O
with O O
respect O O
to O O
the O O
distribution O O
q O O
( O O
z O O
) O O
, O O
which O O
is O O
equivalent O O
to O O
minimizing O O
the O O
kl O O
divergence O O
. O O
as O O
we O O
have O O
seen O O
, O O
the O O
parameter O O
α0 O O
can O O
be O O
interpreted O O
as O O
the O O
effective O B
prior O O
number O O
of O O
observations O O
associated O O
with O O
each O O
component O O
of O O
the O O
mixture B B
. O O
of O O
course O O
, O O
it O O
may O O
hold O O
for O O
a O O
particular O O
distribution O O
by O O
virtue O O
of O O
the O O
speciﬁc O O
numerical O O
values O O
associated O O
with O O
the O O
various O O
conditional B B
probabilities O O
, O O
but O O
it O O
does O O
not O O
follow O O
in O O
general O O
from O O
the O O
structure O O
of O O
the O O
graph O O
. O O
( O O
3.116 O O
) O O
n=1 O O
3.15 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
consider O O
a O O
linear O O
basis O O
function O O
model O O
for B O
regression I I
in O O
which O O
the O O
pa- O O
rameters O O
α O O
and O O
β O O
are O O
set O O
using O O
the O O
evidence O B
framework O O
. O O
section O O
2.1.1 O O
exercise O O
9.18 O O
exercise O O
9.19 O O
additional O O
effective O O
observations O O
of O O
x. O O
we O O
can O O
similarly O O
introduce O O
priors O O
into O O
the O O
bernoulli O O
mixture B B
model I I
, O O
and O O
use O O
em O O
to O O
maximize O O
the O O
posterior B O
probability I I
distri- O O
butions O O
. O O
if O O
jlml O O
' O O
w O O
ml O O
and O O
< O O
pml O O
represent O O
the O O
maximum B B
likelihood I I
solution O O
corresponding O O
to O O
the O O
original O O
untransformed O O
data O O
, O O
show O O
that O O
ajlml O O
' O O
awml O O
, O O
and O O
a O O
< O O
pmlat O O
will O O
rep O O
( O O
cid:173 O O
) O O
resent O O
the O O
corresponding O O
maximum B B
likelihood I I
solution O O
for O O
the O O
transformed O O
data O O
set O O
. O O
representation O O
of O O
functions O O
by O O
su- O O
perpositions O O
of O O
a O O
step O O
or O O
sigmoid B O
function O O
and O O
their O O
applications O O
to O O
neural B O
network I I
theory O O
. O O
these O O
various O O
choices O O
of O O
basis B O
function I I
are O O
illustrated O O
in O O
figure O O
3.1. O O
yet O O
another O O
possible O O
choice O O
of O O
basis B B
function I I
is O O
the O O
fourier O O
basis O O
, O O
which O O
leads O O
to O O
an O O
expansion O O
in O O
sinusoidal O O
functions O O
. O O
the O O
intuition O O
behind O O
this O O
result O O
is O O
given O O
by O O
figure O O
1.15. O O
from O O
( O O
1.58 O O
) O O
it O O
follows O O
that O O
the O O
following O O
estimate O O
for O O
the O O
variance B B
parameter O I
is O O
unbiased O O
( O O
cid:4 O O
) O O
σ2 O O
= O O
n O O
n O O
− O O
1 O O
σ2 O O
ml O O
= O O
1 O O
n O O
− O O
1 O O
( O O
xn O O
− O O
µml O O
) O O
2 O O
. O O
, O O
tn O O
) O O
t. O O
thus O O
, O O
the O O
likelihood B B
function I I
is O O
given O O
by O O
n O O
( O O
cid:14 O O
) O O
n=1 O O
m O O
( O O
cid:14 O O
) O O
p O O
( O O
t|x O O
, O O
w O O
, O O
β O O
) O O
= O O
p O O
( O O
tn|xn O O
, O O
w O O
, O O
β O O
−1 O O
) O O
. O O
the O O
mds O O
concept O O
can O O
be O O
extended B B
to O O
a O O
wide O O
variety O O
of O O
data O O
types O O
specified O O
in O O
terms O O
of O O
a O O
similarity O O
matrix O O
, O O
giving O O
nonmetric O O
mds O O
. O O
by O O
a O O
similar O O
argument O O
, O O
the O O
corresponding O O
result O O
for O O
µ2 O O
is O O
given O O
by O O
n=1 O O
tnxn O O
µ1 O O
= O O
µ2 O O
= O O
1 O O
n2 O O
( O O
1 O O
− O O
tn O O
) O O
xn O O
( O O
4.76 O O
) O O
which O O
again O O
is O O
the O O
mean B B
of O O
all O O
the O O
input O O
vectors O O
xn O O
assigned O O
to O O
class O O
c2 O O
. O O
furthermore O O
the O O
observed O O
data O O
are O O
corrupted O O
with O O
noise O O
, O O
and O O
so O O
for O O
a O O
given O O
( O O
cid:1 O O
) O O
x O O
there O O
is O O
uncertainty O O
as O O
to O O
the O O
appropriate O O
value O O
for O O
( O O
cid:1 O O
) O O
t. O O
probability B B
theory O O
, O O
discussed O O
later O O
, O O
this O O
involves O O
implicitly O O
trying O O
to O O
discover O O
the O O
underlying O O
function O O
sin O O
( O O
2πx O O
) O O
. O O
for O O
example O O
, O O
in O O
the O O
classiﬁcation B B
of O O
objects O O
in O O
two-dimensional O O
images O O
, O O
such O O
as O O
handwritten O O
digits O O
, O O
a O O
particular O O
object O O
should O O
be O O
assigned O O
the O O
same O O
classiﬁcation B B
irrespective O O
of O O
its O O
position O O
within O O
the O O
image O O
( O O
translation B O
invariance I O
) O O
or O O
of O O
its O O
size O O
( O O
scale B O
invariance I O
) O O
. O O
this O O
leaves O O
the O O
issue O O
of O O
deciding O O
the O O
appropriate O O
model O O
complexity O O
for O O
the O O
par- O O
ticular O O
problem O O
, O O
which O O
can O O
not O O
be O O
decided O O
simply O O
by O O
maximizing O O
the O O
likelihood O B
func- O O
tion O O
, O O
because O O
this O O
always O O
leads O O
to O O
excessively O O
complex O O
models O O
and O O
over-ﬁtting B B
. O O
principal B I
component I I
analysis I I
( O O
second O O
ed. O O
) O O
. O O
by O O
analogy O O
with O O
( O O
3.92 O O
) O O
, O O
we O O
obtain O O
βhui O O
= O O
λiui O O
α O O
= O O
γ O O
wt O O
mapwmap O O
( O O
5.178 O O
) O O
section O O
3.5.3 O O
section O O
5.1.1 O O
5.7. O O
bayesian O O
neural O O
networks O O
281 O O
where O O
γ O O
represents O O
the O O
effective B O
number I I
of I I
parameters I I
and O O
is O O
deﬁned O O
by O O
w O O
( O O
cid:2 O O
) O O
i=1 O O
γ O O
= O O
λi O O
α O O
+ O O
λi O O
. O O
we O O
have O O
already O O
seen O O
that O O
the O O
minimizer O O
y O O
( O O
x O O
) O O
of O O
the O O
cross-entropy O B
error O I
( O O
4.90 O O
) O O
for O O
two-class O O
classiﬁcation B B
is O O
given O O
by O O
the O O
posterior O O
class O O
probability B B
. O O
starting O O
with O O
the O O
leaf O O
nodes O O
, O O
we O O
then O O
have O O
the O O
following O O
sequence O O
of O O
six O O
messages O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
x4 O O
x2 O O
x3 O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
x2 O O
x2 O O
( O O
8.74 O O
) O O
( O O
8.75 O O
) O O
( O O
8.76 O O
) O O
( O O
8.77 O O
) O O
( O O
8.78 O O
) O O
( O O
8.79 O O
) O O
( O O
8.80 O O
) O O
( O O
8.81 O O
) O O
( O O
8.82 O O
) O O
( O O
8.83 O O
) O O
( O O
8.84 O O
) O O
( O O
8.85 O O
) O O
the O O
direction O O
of O O
ﬂow O O
of O O
these O O
messages O O
is O O
illustrated O O
in O O
figure O O
8.52. O O
once O O
this O O
mes- O O
sage O O
propagation O O
is O O
complete O O
, O O
we O O
can O O
then O O
propagate O O
messages O O
from O O
the O O
root B B
node I I
out O O
to O O
the O O
leaf O O
nodes O O
, O O
and O O
these O O
are O O
given O O
by O O
µx1→fa O O
( O O
x1 O O
) O O
= O O
1 O O
µfa→x2 O O
( O O
x2 O O
) O O
= O O
x1 O O
µx4→fc O O
( O O
x4 O O
) O O
= O O
1 O O
µfc→x2 O O
( O O
x2 O O
) O O
= O O
fa O O
( O O
x1 O O
, O O
x2 O O
) O O
fc O O
( O O
x2 O O
, O O
x4 O O
) O O
µx2→fb O O
( O O
x2 O O
) O O
= O O
µfa→x2 O O
( O O
x2 O O
) O O
µfc→x2 O O
( O O
x2 O O
) O O
fb O O
( O O
x2 O O
, O O
x3 O O
) O O
µx2→fb O O
. O O
, O O
k. O O
extensions O O
to O O
constrained O O
functional B B
derivatives O O
are O O
similarly O O
straightforward O O
. O O
each O O
message O O
sent O O
from O O
a O O
node B B
replaces O O
any O O
previous O O
message O O
sent O I
in O O
the O O
same O O
direction O O
across O O
the O O
same O O
link B B
and O O
will O O
itself O O
be O O
a O O
function O O
only O O
of O O
the O O
most O O
recent O O
messages O O
received O O
by O O
that O O
node B B
at O O
previous O O
steps O O
of O O
the O O
algorithm O O
. O O
the O O
ﬁrst O O
of O O
these O O
is O O
a O O
regression B B
problem O O
, O O
based O O
on O O
the O O
sinusoidal O B
func- O O
tion O O
, O O
shown O O
in O O
figure O O
a.6 O O
. O O
also O O
shown O O
are O O
the O O
decision B B
boundary I I
, O O
the O O
margin B B
boundaries O O
, O O
and O O
the O O
sup- O O
port O O
vectors O O
. O O
3.24 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
repeat O O
the O O
previous O O
exercise O O
but O O
now O O
use O O
bayes O O
’ O O
theorem O O
in O O
the O O
form O O
p O O
( O O
t O O
) O O
= O O
p O O
( O O
t|w O O
, O O
β O O
) O O
p O O
( O O
w O O
, O O
β O O
) O O
p O O
( O O
w O O
, O O
β|t O O
) O O
( O O
3.118 O O
) O O
( O O
3.119 O O
) O O
and O O
then O O
substitute O O
for O O
the O O
prior B B
and O O
posterior O O
distributions O O
and O O
the O O
likelihood O B
func- O O
tion O O
in O O
order O O
to O O
derive O O
the O O
result O O
( O O
3.118 O O
) O O
. O O
( O O
2.5 O O
) O O
n=1 O O
n=1 O O
in O O
a O O
frequentist B B
setting O O
, O O
we O O
can O O
estimate O O
a O O
value O O
for O O
µ O O
by O O
maximizing O O
the O O
likelihood B B
function I I
, O O
or O O
equivalently O O
by O O
maximizing O O
the O O
logarithm O O
of O O
the O O
likelihood O B
. O O
here O O
we O O
shall O O
focus O O
on O O
the O O
stationary B B
case O O
. O O
section O O
10.1 O O
consider O O
a O O
probabilistic O O
model O O
in O O
which O O
we O O
collectively O O
denote O O
all O O
of O O
the O O
ob- O O
served O O
variables O O
by O O
x O O
and O O
all O O
of O O
the O O
hidden O O
variables O O
by O O
z. O O
the O O
joint O O
distribution O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
is O O
governed O O
by O O
a O O
set O O
of O O
parameters O O
denoted O O
θ. O O
our O O
goal O O
is O O
to O O
maximize O O
the O O
likelihood B B
function I I
that O O
is O O
given O O
by O O
p O O
( O O
x|θ O O
) O O
= O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
. O O
in O O
general O O
, O O
the O O
full O O
conditional B B
distributions O O
will O O
be O O
of O O
a O O
complex O O
form O O
that O O
does O O
not O O
permit O O
the O O
use O O
of O O
standard O O
sam- O O
pling O O
algorithms O O
. O O
each O O
variable O O
xi O O
has O O
( O O
conditional B B
on O O
the O O
states O O
of O O
its O O
parents O O
) O O
a O O
gaussian O O
distribution O O
of O O
the O O
form O O
( O O
8.11 O O
) O O
and O O
so O O
√ O O
xi O O
= O O
wijxj O O
+ O O
bi O O
+ O O
vii O O
( O O
8.14 O O
) O O
where O O
i O O
is O O
a O O
zero O O
mean B B
, O O
unit O O
variance B B
gaussian O O
random O O
variable O O
satisfying O O
e O O
[ O O
i O O
] O O
= O O
0 O O
and O O
e O O
[ O O
ij O O
] O O
= O O
iij O O
, O O
where O O
iij O O
is O O
the O O
i O O
, O O
j O O
element O O
of O O
the O O
identity O O
matrix O O
. O O
thus O O
the O O
probability B B
of O O
selecting O O
the O O
red O O
box O O
is O O
4/10 O O
figure O O
1.9 O O
we O O
use O O
a O O
simple O O
example O O
of O O
two O O
coloured O O
boxes O O
each O O
containing O O
fruit O O
( O O
apples O O
shown O O
in O O
green O O
and O O
or- O O
anges O O
shown O O
in O O
orange O O
) O O
to O O
intro- O O
duce O O
the O O
basic O O
ideas O O
of O O
probability B B
. O O
at O O
each O O
step O O
, O O
we O O
absorb O O
the O O
effect O O
of O O
observation O O
xn+1 O O
through O O
the O O
emission B O
probability I I
p O O
( O O
xn+1|zn+1 O O
) O O
, O O
multiply O O
by O O
the O O
transition O O
matrix O O
p O O
( O O
zn+1|zn O O
) O O
, O O
and O O
then O O
marginalize O O
out O O
zn+1 O O
. O O
another O O
difference O O
between O O
variational B B
bayes O O
and O O
ep O O
arises O O
from O O
the O O
form O O
of O O
kl O O
divergence O O
that O O
is O O
minimized O O
by O O
the O O
two O O
algorithms O O
, O O
because O O
the O O
former O O
mini- O O
mizes O O
kl O O
( O O
q O O
( O O
cid:5 O O
) O O
p O O
) O O
whereas O O
the O O
latter O O
minimizes O O
kl O O
( O O
p O O
( O O
cid:5 O O
) O O
q O O
) O O
. O O
at O O
this O O
point O O
, O O
the O O
root B O
node I O
will O O
have O O
received O O
messages O O
from O O
all O O
of O O
its O O
neighbours O O
. O O
6.4.3 O O
learning B B
the O O
hyperparameters O O
. O O
we O O
ﬁrst O O
note O O
that O O
the O O
maximum B B
likelihood I I
solutions O O
µml O O
and O O
σ2 O O
ml O O
are O O
functions O O
of O O
the O O
data O O
set O O
values O O
x1 O O
, O O
. O O
because O O
it O O
is O O
independent B B
of O O
y O O
( O O
x O O
) O O
, O O
it O O
represents O O
the O O
irreducible O O
minimum O O
value O O
of O O
the O O
loss B B
function I I
. O O
however O O
, O O
these O O
lead O O
to O O
difﬁculties O O
when O O
comparing O O
different O O
models O O
, O O
and O O
indeed O O
bayesian O O
methods O O
based O O
on O O
poor O O
choices O O
of O O
prior B B
can O O
give O O
poor O O
results O O
with O O
high O O
conﬁdence O O
. O O
( O O
2.279 O O
) O O
we O O
wish O O
to O O
maximize O O
h O O
[ O O
x O O
] O O
over O O
all O O
distributions O O
p O O
( O O
x O O
) O O
subject O O
to O O
the O O
constraints O O
that O O
p O O
( O O
x O O
) O O
be O O
normalized O O
and O O
that O O
it O O
have O O
a O O
speciﬁc O O
mean B B
and O O
covariance B B
, O O
so O O
that O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
p O O
( O O
x O O
) O O
dx O O
= O O
1 O O
p O O
( O O
x O O
) O O
x O O
dx O O
= O O
µ O O
p O O
( O O
x O O
) O O
( O O
x O O
− O O
µ O O
) O O
( O O
x O O
− O O
µ O O
) O O
t O O
dx O O
= O O
σ O O
. O O
9.4. O O
the O O
em O O
algorithm O O
in O O
general O O
the O O
expectation B B
maximization I I
algorithm O O
, O O
or O O
em O O
algorithm O O
, O O
is O O
a O O
general O O
technique O O
for O O
ﬁnding O O
maximum B B
likelihood I I
solutions O O
for O O
probabilistic O O
models O O
having O O
latent O B
vari- O O
ables O O
( O O
dempster O O
et O O
al. O O
, O O
1977 O O
; O O
mclachlan O O
and O O
krishnan O O
, O O
1997 O O
) O O
. O O
if O O
we O O
take O O
the O O
derivative B B
with O O
respect O O
to O O
the O O
parameter O O
vector O O
w O O
of O O
the O O
contribution O O
to O O
the O O
error B B
function I I
from O O
a O O
data O O
point O O
n O O
, O O
this O O
takes O O
the O O
form O O
of O O
the O O
‘ O O
error B B
’ O O
yn O O
− O O
tn O O
times O O
the O O
feature O O
vector O O
φn O O
, O O
where O O
yn O O
= O O
wtφn O O
. O O
regression B B
with O O
input-dependent O O
noise O O
: O O
a O O
gaussian O O
process O O
treatment O O
. O O
9.1. O O
k-means O O
clustering B B
we O O
begin O O
by O O
considering O O
the O O
problem O O
of O O
identifying O O
groups O O
, O O
or O O
clusters O O
, O O
of O O
data O O
points O O
in O O
a O O
multidimensional O O
space O O
. O O
in O O
practice O O
, O O
we O O
are O O
usually O O
interested O O
in O O
ﬁnding O O
the O O
most O O
probable O O
sequence O O
of O O
states O O
, O O
and O O
this O O
can O O
be O O
solved O O
efﬁciently O O
using O O
the O O
max-sum B B
algorithm I I
, O O
which O O
in O O
the O O
context O O
of O O
hidden O O
markov O O
models O O
is O O
known O O
as O O
the O O
viterbi O O
algorithm O O
( O O
viterbi O O
, O O
1967 O O
) O O
. O O
4. O O
evaluate O O
the O O
approximation O O
to O O
the O O
model B O
evidence I I
( O O
cid:4 O O
) O O
fj O O
( O O
θ O O
) O O
= O O
zj O O
( O O
cid:6 O O
) O O
( O O
cid:14 O O
) O O
p O O
( O O
d O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:4 O O
) O O
fi O O
( O O
θ O O
) O O
dθ O O
. O O
the O O
formulation O O
of O O
this O O
variational B B
approach O O
for O O
the O O
sequential O O
case O O
is O O
straightforward O O
. O O
5.1. O O
feed-forward O O
network O O
functions O O
231 O O
( O O
a O O
) O O
( O O
c O O
) O O
( O O
b O O
) O O
( O O
d O O
) O O
will O O
show O O
that O O
there O O
exist O O
effective O O
solutions O O
to O O
this O O
problem O O
based O O
on O O
both O O
maximum B B
likelihood I I
and O O
bayesian O O
approaches O O
. O O
of O O
course O O
, O O
as O O
we O O
have O O
already O O
noted O O
, O O
for O O
a O O
multimodal O O
distribution O O
the O O
conditional B B
mean O O
is O O
of O O
limited O O
value O O
. O O
this O O
is O O
achieved O O
in O O
the O O
blocking B O
gibbs O O
sampling O O
algorithm O O
by O O
choosing O O
blocks O O
of O O
variables O O
, O O
not O O
necessarily O O
disjoint O O
, O O
and O O
then O O
sampling O O
jointly O O
from O O
the O O
variables O O
in O O
each O O
block O O
in O O
turn O O
, O O
conditioned O O
on O O
the O O
remaining O O
variables O O
( O O
jensen O O
et O O
al. O O
, O O
1995 O O
) O O
. O O
these O O
concepts O O
are O O
illustrated O O
in O O
figure O O
2.27. O O
in O O
figure O O
2.28 O O
, O O
we O O
show O O
the O O
results O O
of O O
applying O O
the O O
k-nearest-neighbour O O
algo- O O
rithm O O
to O O
the O O
oil B O
ﬂow I O
data I O
, O O
introduced O O
in O O
chapter O O
1 O O
, O O
for O O
various O O
values O O
of O O
k. O O
as O O
expected O O
, O O
we O O
see O O
that O O
k O O
controls O O
the O O
degree O O
of O O
smoothing O O
, O O
so O O
that O O
small O O
k O O
produces O O
many O O
small O O
regions O O
of O O
each O O
class O O
, O O
whereas O O
large O O
k O O
leads O O
to O O
fewer O O
larger O O
regions O O
. O O
because O O
the O O
negative O O
logarithm O O
is O O
a O O
monotonically O O
de- O O
creasing O O
function O O
, O O
maximizing O O
the O O
likelihood O B
is O O
equivalent O O
to O O
minimizing O O
the O O
error B B
. O O
for O O
the O O
purposes O O
of O O
this O O
chapter O O
, O O
however O O
, O O
we O O
shall O O
return O O
to O O
the O O
two-class O O
case O O
, O O
and O O
again O O
remain O O
within O O
the O O
frame- O O
work O O
of O O
generalized O B
linear O I
models O O
so O O
that O O
p O O
( O O
t O O
= O O
1|a O O
) O O
= O O
f O O
( O O
a O O
) O O
where O O
a O O
= O O
wtφ O O
, O O
and O O
f O O
( O O
· O O
) O O
is O O
the O O
activation B B
function I I
. O O
' O O
'' O O
ch O O
'' O O
l'l~f O O
j O O
s~etioo O O
/.4 O O
the O O
no O O
'' O O
liotar O O
mapping O O
is O O
gi O O
, O O
'en O O
by O O
a O O
linear B O
regression I I
model O O
thai O O
allow O O
, O O
for O O
general O O
iio/llinearily O O
while O O
being O O
a O O
linear O O
fuoction O O
of O O
tile O O
adapli'-e O O
parameler O O
< O O
, O O
noie O O
thai O O
tilt O O
usual O O
limitation O O
of O O
linear B O
regression I I
models O O
arising O O
from O O
the O O
en O B
'' O O
'' O O
, O O
of O O
dimen O O
, O O
iooalily O O
does O O
1101 O O
arise O O
in O O
the O O
contr~1 O O
of O O
lhe O O
gt~i O O
si O O
'' O O
'' O O
'e O O
the O O
`` O O
\3nifold O O
generall O O
) O O
' O O
ha O O
< O O
t O O
, O O
,'o O O
di O O
'' O O
ltn· O O
sions O O
irrespecti'-e O O
of O O
the O O
dimensionality O O
of O O
the O O
data O O
space O O
, O O
a O O
coo O O
'' O O
'' O O
! O O
`` O O
, O O
,nce O O
of O O
illese O O
11 O O
'' O O
0 O O
cooices O O
is O O
that O O
the O O
likelihood O B
funclion O O
can O O
be O O
e~pressed O O
analytically O O
in O O
dosed O O
form O O
and O O
can O O
be O O
optimilc O O
< O O
. O O
10.15 O O
( O O
( O O
cid:12 O O
) O O
) O O
using O O
the O O
result O O
( O O
b.17 O O
) O O
, O O
show O O
that O O
the O O
expected O O
value O O
of O O
the O O
mixing O O
coefﬁcients O O
in O O
the O O
variational B B
mixture O O
of O O
gaussians O O
is O O
given O O
by O O
( O O
10.69 O O
) O O
. O O
on O O
the O O
right O O
is O O
a O O
plot O O
of O O
the O O
true O O
posterior O O
probabilities O O
, O O
shown O O
on O O
a O O
colour O O
scale O O
going O O
from O O
pure O O
red O O
denoting O O
probability B B
of O O
the O O
red O O
class O O
is O O
1 O O
to O O
pure O O
blue O O
denoting O O
probability B B
of O O
the O O
red O O
class O O
is O O
0. O O
because O O
these O O
probabilities O O
are O O
known O O
, O O
the O O
optimal O O
decision B B
boundary I I
for O O
minimizing O O
the O O
misclassiﬁcation O O
rate O O
( O O
which O O
corresponds O O
to O O
the O O
contour O O
along O O
which O O
the O O
posterior O O
probabilities O O
for O O
each O O
class O O
equal O O
0.5 O O
) O O
can O O
be O O
evaluated O O
and O O
is O O
shown O O
by O O
the O O
green O O
curve O O
. O O
defined O I
as O O
t~ O O
mean B B
squa.-ed O O
distance O O
! O O
letween O O
the O O
data O O
[ O O
> O O
oint O O
< O O
and O O
tbeir O O
p O O
< O O
ojtttioo O O
, O O
( O O
pearson O O
, O O
19 O O
( O O
1 O O
) O O
. O O
3 O O
linear O O
models O O
for B O
regression I I
the O O
focus O O
so O O
far O O
in O O
this O O
book O O
has O O
been O O
on O O
unsupervised B O
learning I I
, O O
including O O
topics O O
such O O
as O O
density B B
estimation I I
and O O
data O O
clustering O O
. O O
similarly O O
, O O
the O O
message O B
µβ O O
( O O
xn O O
) O O
can O O
be O O
evaluated O O
recursively O O
by O O
starting O O
with O O
node B B
xn O O
and O O
using O O
µβ O O
( O O
xn O O
) O O
= O O
= O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
xn+1 O O
⎡⎣ O O
( O O
cid:2 O O
) O O
⎤⎦ O O
··· O O
ψn+1 O O
, O O
n O O
( O O
xn+1 O O
, O O
xn O O
) O O
xn+2 O O
ψn+1 O O
, O O
n O O
( O O
xn+1 O O
, O O
xn O O
) O O
µβ O O
( O O
xn+1 O O
) O O
. O O
6.4.2 O O
gaussian O O
processes O O
for B O
regression I I
. O O
consider O O
ﬁrst O O
the O O
case O O
of O O
two O O
classes O O
, O O
and O O
suppose O O
we O O
take O O
the O O
d- O O
4.1. O O
discriminant O O
functions O O
187 O O
6 O O
4 O O
2 O O
0 O O
−2 O O
−4 O O
−6 O O
−6 O O
−4 O O
−2 O O
0 O O
2 O O
4 O O
6 O O
6 O O
4 O O
2 O O
0 O O
−2 O O
−4 O O
−6 O O
−6 O O
−4 O O
−2 O O
0 O O
2 O O
4 O O
6 O O
figure O O
4.5 O O
example O O
of O O
a O O
synthetic O O
data O O
set O O
comprising O O
three O O
classes O O
, O O
with O O
training B B
data O O
points O O
denoted O O
in O O
red O O
( O O
× O O
) O O
, O O
green O O
( O O
+ O O
) O O
, O O
and O O
blue O O
( O O
◦ O O
) O O
. O O
this O O
is O O
an O O
example O O
of O O
a O O
credit B O
assignment I O
problem O O
. O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
0 O O
0 O O
1 O O
2 O O
3 O O
4 O O
if O O
the O O
value O O
of O O
θ O O
is O O
drawn O O
from O O
a O O
probability B B
density O O
p O O
( O O
θ O O
) O O
, O O
then O O
the O O
corresponding O O
activation B B
function I I
will O O
be O O
given O O
by O O
the O O
cumulative B O
distribution I O
function I O
( O O
cid:6 O O
) O O
a O O
f O O
( O O
a O O
) O O
= O O
p O O
( O O
θ O O
) O O
dθ O O
−∞ O O
( O O
4.113 O O
) O O
as O O
illustrated O O
in O O
figure O O
4.13. O O
as O O
a O O
speciﬁc O O
example O O
, O O
suppose O O
that O O
the O O
density B B
p O O
( O O
θ O O
) O O
is O O
given O O
by O O
a O O
zero O O
mean B B
, O O
unit O O
variance B B
gaussian O O
. O O
thus O O
our O O
density B B
model O O
is O O
obtained O O
by O O
placing O O
a O O
gaussian O O
over O O
each O O
data O O
point O O
and O O
then O O
adding O O
up O O
the O O
contributions O O
over O O
the O O
whole O O
data O O
set O O
, O O
and O O
then O O
dividing O O
by O O
n O O
so O O
that O O
the O O
den- O O
sity O O
is O O
correctly O O
normalized O O
. O O
the O O
term O O
‘ O O
neural B B
network I I
’ O O
has O O
its O O
origins O O
in O O
attempts O B
to O O
ﬁnd O O
mathematical O O
rep- O O
resentations O O
of O O
information O O
processing O O
in O O
biological O B
systems O O
( O O
mcculloch O O
and O O
pitts O O
, O O
1943 O O
; O O
widrow O O
and O O
hoff O O
, O O
1960 O O
; O O
rosenblatt O O
, O O
1962 O O
; O O
rumelhart O O
et O O
al. O O
, O O
1986 O O
) O O
. O O
7.1. O O
maximum B B
margin I I
classiﬁers O O
339 O O
another O O
approach O O
is O O
to O O
train O O
k O O
( O O
k O O
−1 O O
) O O
/2 O O
different O O
2-class O O
svms O O
on O O
all O O
possible O O
pairs O O
of O O
classes O O
, O O
and O O
then O O
to O O
classify O O
test O O
points O O
according O O
to O O
which O O
class O O
has O O
the O O
high- O O
est O O
number O O
of O O
‘ O O
votes O O
’ O O
, O O
an O O
approach O O
that O O
is O O
sometimes O O
called O O
one-versus-one O O
. O O
( O O
10.35 O O
) O O
ln O O
p O O
( O O
x O O
) O O
= O O
lm O O
− O O
q O O
( O O
z|m O O
) O O
q O O
( O O
m O O
) O O
ln O O
where O O
the O O
lm O O
is O O
a O O
lower B B
bound I I
on O O
ln O O
p O O
( O O
x O O
) O O
and O O
is O O
given O O
by O O
lm O O
= O O
q O O
( O O
z|m O O
) O O
q O O
( O O
m O O
) O O
ln O O
p O O
( O O
z O O
, O O
x O O
, O O
m O O
) O O
q O O
( O O
z|m O O
) O O
q O O
( O O
m O O
) O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
m O O
z O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
m O O
z O O
here O O
we O O
are O O
assuming O O
discrete O O
z O O
, O O
but O O
the O O
same O O
analysis O O
applies O O
to O O
continuous O O
latent O O
variables O O
provided O O
the O O
summations O O
are O O
replaced O O
with O O
integrations O O
. O O
at O O
the O O
opposite O O
extreme O O
, O O
if O O
we O O
could O O
draw O O
samples O O
directly O O
from O O
the O O
joint O O
distribution O O
( O O
an O O
operation O O
that O O
we O O
are O O
supposing O O
is O O
intractable O O
) O O
, O O
then O O
successive O O
samples O O
would O O
be O O
independent B B
. O O
exercise O O
10.18 O O
10.2.3 O O
predictive O O
density O O
in O O
applications O O
of O O
the O O
bayesian O O
mixture O B
of O O
gaussians O O
model O O
we O O
will O O
often O O
be O O
interested O O
in O O
the O O
predictive O B
density O O
for O O
a O O
new O O
value O O
( O O
cid:1 O O
) O O
x O O
of O O
the O O
observed B O
variable I I
. O O
marginalizing O O
both O O
sides O O
of O O
( O O
8.28 O O
) O O
over O O
c O O
we O O
obtain O O
p O O
( O O
a O O
, O O
b O O
) O O
= O O
p O O
( O O
a O O
) O O
p O O
( O O
b O O
) O O
figure O O
8.19 O O
the O O
last O O
of O O
our O O
three O O
examples O O
of O O
3-node O O
graphs O O
used O O
to O O
explore O O
conditional B B
independence I I
properties O O
in O O
graphi- O B
cal O O
models O O
. O O
we O O
therefore O O
also O O
introduce O O
the O O
partitioned B B
form O O
of O O
the O O
precision B O
matrix I I
( O O
cid:15 O O
) O O
λ O O
= O O
λaa O O
λab O O
λba O O
λbb O O
exercise O O
2.22 O O
corresponding O O
to O O
the O O
partitioning O O
( O O
2.65 O O
) O O
of O O
the O O
vector O O
x. O O
because O O
the O O
inverse B B
of O O
a O O
symmetric O O
matrix O O
is O O
also O O
symmetric O O
, O O
we O O
see O O
that O O
λaa O O
and O O
λbb O O
are O O
symmetric O O
, O O
while O O
λt O O
ab O O
= O O
λba O O
. O O
by O O
working O O
directly O O
in O O
terms O O
of O O
the O O
kernel B O
function I I
, O O
without O O
introducing O O
the O O
feature B O
space I I
explicitly O O
, O O
it O O
might O O
there- O O
fore O O
seem O O
that O O
support B B
vector I I
machines O O
somehow O O
manage O O
to O O
avoid O O
the O O
curse O B
of O I
di- O O
336 O O
7. O O
sparse O O
kernel O O
machines O O
section O O
1.4 O O
section O O
4.3.2 O O
mensionality O O
. O O
however O O
, O O
the O O
test B O
set I I
error O O
has O O
become O O
very O O
large O O
and O O
, O O
as O O
we O O
saw O O
in O O
figure O O
1.4 O O
, O O
the O O
corresponding O O
function O O
y O O
( O O
x O O
, O O
w O O
( O O
cid:1 O O
) O O
) O O
exhibits O O
wild O O
oscillations O O
. O O
2.5 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
in O O
this O O
exercise O O
, O O
we O O
prove O O
that O O
the O O
beta B B
distribution I I
, O O
given O O
by O O
( O O
2.13 O O
) O O
, O O
is O O
correctly O O
normalized O O
, O O
so O O
that O O
( O O
2.14 O O
) O O
holds O O
. O O
the O O
lower O B
three O O
plots O O
show O O
the O O
corresponding O O
responsibilities O O
plotted O O
as O O
a O O
vertical O O
line O O
for O O
each O O
data O O
point O O
in O O
which O O
the O O
length O O
of O O
the O O
blue O O
segment O O
gives O O
the O O
posterior B O
probability I I
of O O
the O O
blue O O
line O O
for O O
that O O
data O O
point O O
( O O
and O O
similarly O O
for O O
the O O
red O O
segment O O
) O O
. O O
with O O
regularized B O
least I I
squares I I
, O O
the O O
regularization B B
coefﬁcient O O
λ O O
also O O
controls O O
the O O
effective O O
complexity O O
of O O
the O O
model O O
, O O
whereas O O
for O O
more O O
complex O O
models O O
, O O
such O O
as O O
mixture B B
distributions O O
or O O
neural O O
networks O O
there O O
may O O
be O O
multiple O O
pa- O O
rameters O O
governing O O
complexity O O
. O O
( O O
6.60 O O
) O O
the O O
kernel B O
function I I
that O O
determines O O
k O O
is O O
typically O O
chosen O O
to O O
express O O
the O O
property O O
that O O
, O O
for O O
points O O
xn O O
and O O
xm O O
that O O
are O O
similar O O
, O O
the O O
corresponding O O
values O O
y O O
( O O
xn O O
) O O
and O O
y O O
( O O
xm O O
) O O
will O O
be O O
more O O
strongly O O
correlated O O
than O O
for O O
dissimilar O O
points O O
. O O
using O O
the O O
sum O O
and O O
product O O
rules O O
, O O
we O O
see O O
that O O
the O O
posterior B O
probability I I
( O O
cid:21 O O
) O O
that O O
is O O
evaluated O O
in O O
the O O
e O O
step O O
takes O O
the O O
form O O
( O O
cid:2 O O
) O O
p O O
( O O
z|x O O
, O O
θ O O
) O O
= O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
= O O
z O O
z O O
n=1 O O
n O O
( O O
cid:14 O O
) O O
n O O
( O O
cid:14 O O
) O O
( O O
cid:2 O O
) O O
n=1 O O
p O O
( O O
xn O O
, O O
zn|θ O O
) O O
p O O
( O O
xn O O
, O O
zn|θ O O
) O O
n O O
( O O
cid:14 O O
) O O
n=1 O O
= O O
p O O
( O O
zn|xn O O
, O O
θ O O
) O O
( O O
9.75 O O
) O O
and O O
so O O
the O O
posterior O O
distribution O O
also O O
factorizes O O
with O O
respect O O
to O O
n. O O
in O O
the O O
case O O
of O O
the O O
gaussian O O
mixture B B
model I I
this O O
simply O O
says O O
that O O
the O O
responsibility B O
that O O
each O O
of O O
the O O
mixture B B
components O O
takes O O
for O O
a O O
particular O O
data O O
point O O
xn O O
depends O O
only O O
on O O
the O O
value O O
of O O
xn O O
and O O
on O O
the O O
parameters O O
θ O O
of O O
the O O
mixture B B
components O O
, O O
not O O
on O O
the O O
values O O
of O O
the O O
other O O
data O O
points O O
. O O
the O O
estimate O O
of O O
the O O
density B B
p O O
( O O
x O O
) O O
is O O
then O O
given O O
by O O
( O O
2.246 O O
) O O
with O O
v O O
set O O
to O O
the O O
volume O O
of O O
the O O
resulting O O
sphere O O
. O O
it O O
is O O
easily O O
veriﬁed O O
that O O
this O O
distribution O O
is O O
normalized O O
and O O
that O O
it O O
has O O
mean B B
and O O
variance B B
given O O
by O O
e O O
[ O O
x O O
] O O
= O O
µ O O
var O O
[ O O
x O O
] O O
= O O
µ O O
( O O
1 O O
− O O
µ O O
) O O
. O O
we O O
saw O O
that O O
in O O
going O O
from O O
a O O
directed B B
to O O
an O O
undirected B B
representation O O
we O O
had O O
to O O
discard O O
some O O
conditional B B
independence I I
properties O O
from O O
the O O
graph O O
. O O
the O O
prior B B
over O O
θ O O
is O O
taken O O
to O O
be O O
gaussian O O
p O O
( O O
θ O O
) O O
= O O
n O O
( O O
θ|0 O O
, O O
bi O O
) O O
( O O
10.210 O O
) O O
and O O
minka O O
( O O
2001a O O
) O O
chooses O O
the O O
parameter O O
values O O
a O O
= O O
10 O O
, O O
b O O
= O O
100 O O
and O O
w O O
= O O
0.5. O O
the O O
joint O O
distribution O O
of O O
n O O
observations O O
d O O
= O O
{ O O
x1 O O
, O O
. O O
this O O
can O O
be O O
expressed O O
in O O
terms O O
of O O
the O O
parameters O O
{ O O
an O O
} O O
and O O
the O O
kernel B O
function I I
by O O
substituting O O
for O O
w O O
using O O
( O O
7.8 O O
) O O
to O O
give O O
n O O
( O O
cid:2 O O
) O O
y O O
( O O
x O O
) O O
= O O
antnk O O
( O O
x O O
, O O
xn O O
) O O
+ O O
b O O
. O O
in O O
this O O
chapter O O
, O O
we O O
shall O O
see O O
that O O
mixture B O
distributions O O
, O O
such O O
as O O
the O O
gaussian O O
mixture B B
discussed O O
in O O
section O O
2.3.9 O O
, O O
can O O
be O O
interpreted O O
in O O
terms O O
of O O
discrete O O
latent O O
variables O O
. O O
its O O
conjugate B B
prior I I
for O O
µ O O
is O O
the O O
beta B B
distribution I I
. O O
typical O O
applications O O
involved O O
learning B B
to O O
discriminate O O
simple O O
shapes O O
or O O
characters O O
. O O
( O O
1.108 O O
) O O
( O O
cid:27 O O
) O O
−∞ O O
( O O
cid:26 O O
) O O
−1 O O
+ O O
λ1 O O
+ O O
λ2x O O
+ O O
λ3 O O
( O O
x O O
− O O
µ O O
) O O
2 O O
( O O
cid:13 O O
) O O
( O O
cid:12 O O
) O O
− O O
( O O
x O O
− O O
µ O O
) O O
2 O O
2σ2 O O
( O O
2πσ2 O O
) O O
1/2 O O
exp O O
1 O O
the O O
lagrange O O
multipliers O O
can O O
be O O
found O O
by O O
back O O
substitution O O
of O O
this O O
result O O
into O O
the O O
three O O
constraint O O
equations O O
, O O
leading O O
ﬁnally O O
to O O
the O O
result O O
p O O
( O O
x O O
) O O
= O O
( O O
1.109 O O
) O O
and O O
so O O
the O O
distribution O O
that O O
maximizes O O
the O O
differential B B
entropy I I
is O O
the O O
gaussian O O
. O O
in O O
the O O
over-relaxation B B
frame- O I
work O O
, O O
the O O
value O O
of O O
zi O O
is O O
replaced O O
with O O
i O O
= O O
µi O O
+ O O
α O O
( O O
zi O O
− O O
µi O O
) O O
+ O O
σi O O
( O O
1 O O
− O O
α2 O O
( O O
cid:4 O O
) O O
i O O
) O O
1/2ν O O
z O O
( O O
11.50 O O
) O O
i O O
, O O
then O O
so O O
too O O
does O O
z O O
where O O
ν O O
is O O
a O O
gaussian O O
random O O
variable O O
with O O
zero O O
mean B B
and O O
unit O O
variance B B
, O O
and O O
α O O
is O O
a O O
parameter O O
such O O
that O O
−1 O O
< O O
α O O
< O O
1. O O
for O O
α O O
= O O
0 O O
, O O
the O O
method O O
is O O
equivalent O O
to O O
standard O O
gibbs O O
sampling O O
, O O
and O O
for O O
α O O
< O O
0 O O
the O O
step O O
is O O
biased O O
to O O
the O O
opposite O O
side O O
of O O
the O O
mean B B
. O O
the O O
evaluation O O
of O O
the O O
lower B B
bound I I
can O O
also O O
be O O
simpliﬁed O O
because O O
many O O
of O O
the O O
required O O
quantities O O
are O O
already O O
evaluated O O
as O O
part O O
of O O
the O O
message B B
passing I I
scheme O O
. O O
for O O
the O O
case O O
of O O
k O O
= O O
2 O O
classes O O
, O O
we O O
can O O
alternatively O O
consider O O
the O O
logistic B B
sigmoid I I
formulation O O
given O O
by O O
( O O
4.57 O O
) O O
. O O
) O O
, O O
maxi- O O
mum O O
entropy B B
and O O
bayesian O O
methods O O
, O O
pp O O
. O O
( O O
5.107 O O
) O O
( O O
cid:2 O O
) O O
k O O
finally O O
, O O
we O O
have O O
the O O
usual O O
equations O O
for O O
the O O
ﬁrst O O
derivatives O O
of O O
the O O
error B B
∂e O O
∂wkj O O
∂e O O
∂wji O O
= O O
δkzj O O
= O O
δjxi O O
( O O
5.108 O O
) O O
( O O
5.109 O O
) O O
256 O O
5. O O
neural O O
networks O O
and O O
acting O O
on O O
these O O
with O O
the O O
r O O
{ O O
· O O
} O O
operator O O
, O O
we O O
obtain O O
expressions O O
for O O
the O O
elements O O
of O O
the O O
vector O O
vth O O
= O O
r O O
{ O O
δk O O
} O O
zj O O
+ O O
δkr O O
{ O O
zj O O
} O O
= O O
xir O O
{ O O
δj O O
} O O
. O O
what O O
would O O
be O O
the O O
appropriate O O
choice O O
of O O
output O O
unit O O
activation B B
function I I
? O O
5.10 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
consider O O
a O O
hessian O O
matrix O O
h O O
with O O
eigenvector O O
equation O O
( O O
5.33 O O
) O O
. O O
we O O
now O O
take O O
the O O
expectation B B
of O O
the O O
complete-data O O
log O O
likelihood O O
with O O
respect O O
to O O
the O O
posterior O O
distri- O O
bution O O
p O O
( O O
z|x O O
, O O
θold O O
) O O
which O O
deﬁnes O O
the O O
function O O
q O O
( O O
θ O O
, O O
θold O O
) O O
= O O
ez|θold O O
[ O O
ln O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
] O O
. O O
1.5.5 O O
loss O B
functions O O
for B O
regression I I
. O O
this O O
may O O
seem O O
rather O O
pointless O O
because O O
we O O
have O O
already O O
obtained O O
an O O
exact O O
closed-form O O
so O O
( O O
cid:173 O O
) O O
lution O O
for O O
the O O
maximum B B
likelihood I I
parameter O O
values O O
. O O
162 O O
3. O O
linear O O
models O O
for B O
regression I I
different O O
models O O
, O O
and O O
we O O
shall O O
examine O O
this O O
term O O
in O O
more O O
detail O O
shortly O O
. O O
12.4 O O
nonlinear O O
latent B B
variable I I
models O O
. O O
4.11 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
consider O O
a O O
classiﬁcation B B
problem O O
with O O
k O O
classes O O
for O O
which O O
the O O
feature O O
vector O O
φ O O
has O O
m O O
components O O
each O O
of O O
which O O
can O O
take O O
l O O
discrete O O
states O O
. O O
mk O O
k=1 O O
µmk O O
k O O
( O O
2.34 O O
) O O
which O O
is O O
known O O
as O O
the O O
multinomial B B
distribution I I
. O O
thus O O
the O O
sum-of-squares B B
error I I
function O O
has O O
arisen O O
as O O
a O O
consequence O O
of O O
maximizing O O
likelihood O O
under O O
the O O
assumption O O
of O O
a O O
gaussian O O
noise O O
distribution O O
. O O
we O O
shall O O
refer O O
to O O
this O O
as O O
the O O
classical B B
or O O
frequentist B B
interpretation O O
of O O
probability B B
. O O
here O O
we O O
show O O
how O O
a O O
multivariate O O
gaussian O O
can O O
be O O
expressed O O
as O O
a O O
directed B B
graph O O
corresponding O O
to O O
a O O
linear-gaussian O O
model O O
over O O
the O O
component O O
vari- O O
ables O O
. O O
if O O
desired O O
, O O
suitable O O
regularization B B
terms O O
can O O
also O O
be O O
added O O
, O O
leading O O
to O O
a O O
penalized O O
maximum B B
likelihood I I
solution O O
. O O
however O O
, O O
by O O
drawing O O
sam- O O
ples O O
from O O
the O O
posterior O O
distribution O O
over O O
w O O
, O O
and O O
plotting O O
the O O
corresponding O O
model O O
functions O O
y O O
( O O
x O O
, O O
w O O
) O O
as O O
in O O
figure O O
3.9 O O
, O O
we O O
are O O
visualizing O O
the O O
joint O O
uncertainty O O
in O O
the O O
posterior O O
distribution O O
between O O
the O O
y O O
values O O
at O O
two O O
( O O
or O O
more O O
) O O
x O O
values O O
, O O
as O O
governed O O
by O O
the O O
equivalent B O
kernel I I
. O O
6.27 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
derive O O
the O O
result O O
( O O
6.90 O O
) O O
for O O
the O O
log O O
likelihood O O
function O O
in O O
the O O
laplace O O
approx- O O
imation O O
framework O O
for O O
gaussian O O
process O O
classiﬁcation B B
. O O
by O O
separating O O
off O O
the O O
contribution O O
from O O
data O O
point O O
l O O
+ O O
1 O O
, O O
we O O
obtain O O
n=1 O O
in O O
order O O
to O O
evaluate O O
the O O
inverse B B
of O O
the O O
hessian O O
, O O
we O O
now O O
consider O O
the O O
matrix O O
identity O O
( O O
cid:10 O O
) O O
m O O
+ O O
vvt O O
l+1 O O
. O O
the O O
following O O
result O O
where O O
eθ O O
[ O O
θ O O
] O O
≡ O O
ed O O
[ O O
eθ O O
[ O O
θ|d O O
] O O
] O O
≡ O O
( O O
2.21 O O
) O O
( O O
2.22 O O
) O O
( O O
2.23 O O
) O O
( O O
cid:13 O O
) O O
p O O
( O O
d O O
) O O
dd O O
θp O O
( O O
θ|d O O
) O O
dθ O O
eθ O O
[ O O
θ O O
] O O
= O O
ed O O
[ O O
eθ O O
[ O O
θ|d O O
] O O
] O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:6 O O
) O O
p O O
( O O
θ O O
) O O
θ O O
dθ O O
says O O
that O O
the O O
posterior O O
mean O O
of O O
θ O O
, O O
averaged O O
over O O
the O O
distribution O O
generating O O
the O O
data O O
, O O
is O O
equal O O
to O O
the O O
prior B B
mean O O
of O O
θ. O O
similarly O O
, O O
we O O
can O O
show O O
that O O
varθ O O
[ O O
θ O O
] O O
= O O
ed O O
[ O O
varθ O O
[ O O
θ|d O O
] O O
] O O
+ O O
vard O O
[ O O
eθ O O
[ O O
θ|d O O
] O O
] O O
. O O
this O O
is O O
known O O
as O O
an O O
autoregressive B O
or O O
ar O O
model O O
( O O
box O O
et O O
al. O O
, O O
1994 O O
; O O
thiesson O O
et O O
al. O O
, O O
2004 O O
) O O
. O O
the O O
likelihood B B
function I I
for O O
λ O O
takes O O
the O O
form O O
p O O
( O O
x|λ O O
) O O
= O O
−1 O O
) O O
∝ O O
λn/2 O O
exp O O
n O O
( O O
xn|µ O O
, O O
λ O O
n O O
( O O
cid:2 O O
) O O
( O O
xn O O
− O O
µ O O
) O O
2 O O
. O O
n O O
= O O
s O O
s O O
s O O
−1 O O
0 O O
m0 O O
+ O O
βφtt O O
( O O
3.49 O O
) O O
( O O
3.50 O O
) O O
( O O
3.51 O O
) O O
where O O
note O O
that O O
because O O
the O O
posterior O O
distribution O O
is O O
gaussian O O
, O O
its O O
mode O O
coincides O O
with O O
its O O
mean B B
. O O
8.5 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
draw O O
a O O
directed B B
probabilistic O O
graphical B O
model I I
corresponding O O
to O O
the O O
relevance B B
vector I I
machine I I
described O O
by O O
( O O
7.79 O O
) O O
and O O
( O O
7.80 O O
) O O
. O O
each O O
of O O
them O O
is O O
then O O
transformed O O
using O O
a O O
differentiable O O
, O O
nonlinear O O
activation B B
function I I
h O O
( O O
· O O
) O O
to O O
give O O
zj O O
= O O
h O O
( O O
aj O O
) O O
. O O
from O O
( O O
7.19 O O
) O O
we O O
see O O
that O O
in O O
the O O
case O O
of O O
separable O O
classes O O
, O O
we O O
implicitly O O
used O O
an O O
error B B
function I I
that O O
gave O O
inﬁnite O O
error B B
if O O
a O O
data O O
point O O
was O O
misclassiﬁed O O
and O O
zero O O
error B B
if O O
it O O
was O O
classiﬁed O O
correctly O O
, O O
and O O
then O O
optimized O O
the O O
model O O
parameters O O
to O O
maximize O O
the O O
margin B B
. O O
( O O
14.8 O O
) O O
the O O
average O O
sum-of-squares B B
error I I
then O O
takes O O
the O O
form O O
ex O O
( O O
14.9 O O
) O O
where O O
ex O O
[ O O
· O O
] O O
denotes O O
a O O
frequentist B B
expectation O O
with O O
respect O O
to O O
the O O
distribution O O
of O O
the O O
input O O
vector O O
x. O O
the O O
average O O
error B B
made O O
by O O
the O O
models O O
acting O O
individually O O
is O O
therefore O O
= O O
ex O O
m O O
( O O
x O O
) O O
2 O O
m O O
( O O
x O O
) O O
2 O O
. O O
we O O
recall O O
from O O
( O O
1.56 O O
) O O
that O O
the O O
maximum B B
likelihood I I
estimate O O
of O O
the O O
variance B B
for O O
a O O
gaussian O O
distribution O O
over O O
a O O
3.5. O O
the O O
evidence B B
approximation I I
171 O O
single O O
variable O O
x O O
is O O
given O O
by O O
σ2 O O
ml O O
= O O
1 O O
n O O
n O O
( O O
cid:2 O O
) O O
( O O
xn O O
− O O
µml O O
) O O
2 O O
n=1 O O
( O O
3.96 O O
) O O
and O O
that O O
this O O
estimate O O
is O O
biased O O
because O O
the O O
maximum B B
likelihood I I
solution O O
µml O O
for O O
the O O
mean B B
has O O
ﬁtted O O
some O O
of O O
the O O
noise O O
on O O
the O O
data O O
. O O
this O O
can O O
be O O
done O O
by O O
representing O O
the O O
states O O
{ O O
a O O
, O O
b O O
, O O
c O O
, O O
d O O
, O O
e O O
, O O
f O O
, O O
g O O
, O O
h O O
} O O
using O O
, O O
for O O
instance O O
, O O
the O O
following O O
set O O
of O O
code O O
strings O O
: O O
0 O O
, O O
10 O O
, O O
110 O O
, O O
1110 O O
, O O
111100 O O
, O O
111101 O O
, O O
111110 O O
, O O
111111. O O
the O O
average O O
length O O
of O O
the O O
code O B
that O O
has O O
to O O
be O O
transmitted O O
is O O
then O O
average O O
code O O
length O O
= O O
1 O O
2 O O
× O O
1 O O
+ O O
1 O O
4 O O
× O O
2 O O
+ O O
1 O O
8 O O
× O O
3 O O
+ O O
1 O O
16 O O
× O O
4 O O
+ O O
4 O O
× O O
1 O O
64 O O
× O O
6 O O
= O O
2 O O
bits B B
which O O
again O O
is O O
the O O
same O O
as O O
the O O
entropy B B
of O O
the O O
random O O
variable O O
. O O
the O O
conditional B B
p O O
( O O
xi|pai O O
) O O
will O O
depend O O
on O O
the O O
parents O O
of O O
node B B
xi O O
, O O
whereas O O
the O O
conditionals O O
p O O
( O O
xk|pak O O
) O O
will O O
depend O O
on O O
the O O
children O O
8.3. O O
markov O O
random O O
fields O O
383 O O
figure O O
8.26 O O
the O O
markov O O
blanket O O
of O O
a O O
node B B
xi O O
comprises O O
the O O
set O O
of O O
parents O O
, O O
children O O
and O O
co-parents B O
of O O
the O O
node B B
. O O
it O O
is O O
therefore O O
sufﬁcient O O
that O O
the O O
kernel O O
matrix O O
eigenvalue O O
of O O
c O O
will O O
be O O
λi O O
+ O O
β O O
k O O
( O O
xn O O
, O O
xm O O
) O O
be O O
positive O O
semideﬁnite O O
for O O
any O O
pair O O
of O O
points O O
xn O O
and O O
xm O O
, O O
so O O
that O O
λi O O
( O O
cid:2 O O
) O O
0 O O
, O O
because O O
any O O
eigenvalue O O
λi O O
that O O
is O O
zero O O
will O O
still O O
give O O
rise O O
to O O
a O O
positive O B
eigenvalue O O
for O O
c O O
because O O
β O O
> O O
0. O O
this O O
is O O
the O O
same O O
restriction O O
on O O
the O O
kernel B O
function I I
discussed O O
earlier O O
, O O
and O O
so O O
we O O
can O O
again O O
exploit O O
all O O
of O O
the O O
techniques O O
in O O
section O O
6.2 O O
to O O
construct O O
figure O O
6.6 O O
illustration O O
of O O
the O O
sampling O O
of O O
data O O
points O O
{ O O
tn O O
} O O
from O O
a O O
gaussian O O
process O O
. O O
( O O
cid:6 O O
) O O
∞ O O
−∞ O O
n O O
( O O
cid:10 O O
) O O
x|µ O O
, O O
σ2 O O
( O O
cid:11 O O
) O O
x O O
( O O
1.48 O O
) O O
thus O O
( O O
1.46 O O
) O O
satisﬁes O O
the O O
two O O
requirements O O
for O O
a O O
valid O O
probability B I
density O O
. O O
in O O
figure O O
2.25 O O
, O O
we O O
apply O O
the O O
model O O
( O O
2.250 O O
) O O
to O O
the O O
data O O
124 O O
2. O O
probability B B
distributions O O
figure O O
2.25 O O
illustration O O
of O O
the O O
kernel O O
density O I
model O O
( O O
2.250 O O
) O O
applied O O
to O O
the O O
same O O
data O O
set O O
used O O
to O O
demonstrate O O
the O O
histogram O O
approach O O
in O O
figure O O
2.24. O O
we O O
see O O
that O O
h O O
acts O O
as O O
a O O
smoothing B B
parameter I I
and O O
that O O
if O O
it O O
is O O
set O O
too O O
small O O
( O O
top O O
panel O O
) O O
, O O
the O O
result O O
is O O
a O O
very O O
noisy O O
density B B
model O O
, O O
whereas O O
if O O
it O O
is O O
set O O
too O O
large O O
( O O
bottom O O
panel O O
) O O
, O O
then O O
the O O
bimodal O O
nature O O
of O O
the O O
underlying O O
distribution O O
from O O
which O O
the O O
data O O
is O O
generated O O
( O O
shown O O
by O O
the O O
green O O
curve O O
) O O
is O O
washed O O
out O O
. O O
) O O
, O O
on- O O
line O O
learning B B
in O O
neural O O
networks O O
, O O
pp O O
. O O
if O O
a1 O O
and O O
a2 O O
are O O
two O O
such O O
subsets O O
then O O
one O O
simple O O
choice O O
of O O
kernel O O
would O O
be O O
k O O
( O O
a1 O O
, O O
a2 O O
) O O
= O O
2|a1∩a2| O O
( O O
6.27 O O
) O O
where O O
a1 O O
∩ O O
a2 O O
denotes O O
the O O
intersection O O
of O O
sets O O
a1 O O
and O O
a2 O O
, O O
and O O
|a| O O
denotes O O
the O O
number O O
of O O
subsets O O
in O O
a. O O
this O O
is O O
a O O
valid O O
kernel B O
function I O
because O O
it O O
can O O
be O O
shown O O
to O O
correspond O O
to O O
an O O
inner O O
product O O
in O O
a O O
feature B O
space I I
. O O
5.4.2 O O
outer B O
product I O
approximation I I
. O O
5.36 O O
( O O
( O O
cid:12 O O
) O O
) O O
derive O O
the O O
result O O
( O O
5.157 O O
) O O
for O O
the O O
derivative B B
of O O
the O O
error B B
function I I
with O O
respect O O
to O O
the O O
network O O
output O O
activations O O
controlling O O
the O O
component O O
variances O O
in O O
the O O
mixture B B
density I I
network I I
. O O
applications O O
in O O
which O O
the O O
training B B
data O O
comprises O O
examples O O
of O O
the O O
input O O
vectors O O
along O O
with O O
their O O
corresponding O O
target O O
vectors O O
are O O
known O O
as O O
supervised B B
learning I I
prob- O O
lems O O
. O O
48 O O
1. O O
introduction O O
( O O
b O O
) O O
first O O
solve O O
the O O
inference B B
problem O O
of O O
determining O O
the O O
conditional B B
density O O
p O O
( O O
t|x O O
) O O
, O O
and O O
then O O
subsequently O O
marginalize O O
to O O
ﬁnd O O
the O O
conditional B B
mean O O
given O O
by O O
( O O
1.89 O O
) O O
. O O
then O O
the O O
conditional B B
distribution O O
p O O
( O O
xn|xn−1 O O
) O O
in O O
a O O
ﬁrst-order O B
markov O O
chain O O
will O O
be O O
speciﬁed O O
by O O
a O O
set O O
of O O
k O O
− O O
1 O O
parameters O O
for O O
each O O
of O O
the O O
k O O
states O O
of O O
xn−1 O O
giving O O
a O O
total O O
of O O
k O O
( O O
k O O
− O O
1 O O
) O O
parameters O O
. O O
we O O
can O O
obtain O O
a O O
smoother O O
density O B
model O O
if O O
we O O
choose O O
a O O
smoother O O
kernel O O
function O O
, O O
and O O
a O O
common O O
choice O O
is O O
the O O
gaussian O O
, O O
which O O
gives O O
rise O O
to O O
the O O
following O O
kernel O O
density O O
model O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
p O O
( O O
x O O
) O O
= O O
1 O O
n O O
1 O O
( O O
2πh2 O O
) O O
1/2 O O
exp O O
( O O
cid:12 O O
) O O
− O O
( O O
cid:5 O O
) O O
x O O
− O O
xn O O
( O O
cid:5 O O
) O O
2 O O
( O O
cid:13 O O
) O O
2h2 O O
( O O
2.250 O O
) O O
where O O
h O O
represents O O
the O O
standard B O
deviation I I
of O O
the O O
gaussian O O
components O O
. O O
the O O
conditional B B
distribution O O
p O O
( O O
an O O
+1|an O O
) O O
is O O
obtained O O
by O O
invoking O O
the O O
results O O
( O O
6.66 O O
) O O
and O O
( O O
6.67 O O
) O O
for O O
gaussian O O
pro- O O
cess O O
regression B B
, O O
to O O
give O O
p O O
( O O
an O O
+1|an O O
) O O
= O O
n O O
( O O
an O O
+1|ktc O O
n O O
an O O
, O O
c O O
− O O
ktc O O
−1 O O
−1 O O
n O O
k O O
) O O
. O O
an O O
important O O
example O O
concerns O O
situations O O
in O O
which O O
the O O
conditional B B
distribution O O
p O O
( O O
t|x O O
) O O
is O O
multimodal O O
, O O
as O O
often O O
arises O O
in O O
the O O
solution O O
of O O
inverse B B
problems O O
. O O
verify O O
that O O
in O O
the O O
case O O
of O O
m O O
= O O
d O O
- O O
1 O O
, O O
the O O
number O O
of O O
independent B B
parameters O O
is O O
the O O
same O O
as O O
in O O
a O O
general O O
covariance B B
gaussian O O
, O O
whereas O O
for O O
m O O
= O O
°it O O
is O O
the O O
same O O
as O O
for O O
a O O
gaussian O O
with O O
an O O
isotropic B B
covariance O O
. O O
in O O
practice O O
, O O
we O O
typically O O
wish O O
to O O
ﬁnd O O
the O O
set O O
of O O
values O O
that O O
jointly O O
have O O
the O O
largest O O
probability B B
, O O
in O O
other O O
words O O
the O O
vector O O
xmax O O
that O O
maximizes O O
the O O
joint O O
distribution O O
, O O
so O O
that O O
xmax O O
= O O
arg O O
max O O
p O O
( O O
x O O
) O O
for O O
which O O
the O O
corresponding O O
value O O
of O O
the O O
joint O O
probability B B
will O O
be O O
given O O
by O O
p O O
( O O
xmax O O
) O O
= O O
max O O
x O O
p O O
( O O
x O O
) O O
. O O
thus O O
the O O
graph O O
corresponding O O
to O O
figure O O
8.5 O O
in O O
which O O
the O O
variables O O
{ O O
tn O O
} O O
are O O
observed O O
is O O
shown O O
in O O
figure O O
8.6. O O
note O O
that O O
the O O
value O O
of O O
w O O
is O O
not O O
observed O O
, O O
and O O
so O O
w O O
is O O
an O O
example O O
of O O
a O O
latent B B
variable I I
, O O
also O O
known O O
as O O
a O O
hidden B O
variable I O
. O O
if O O
we O O
have O O
a O O
sequence O O
of O O
updates O O
in O O
which O O
every O O
site O O
is O O
visited O O
at O O
least O O
once O O
, O O
and O O
in O O
which O O
no O O
changes O O
to O O
the O O
variables O O
are O O
made O O
, O O
then O O
by O O
deﬁnition O O
the O O
algorithm O O
exercise O O
8.13 O O
390 O O
8. O O
graphical O O
models O O
figure O O
8.32 O O
( O O
a O O
) O O
example O O
of O O
a O O
directed B B
graph O O
. O O
after O O
each O O
application O O
of O O
the O O
leapfrog O O
algorithm O O
, O O
the O O
resulting O O
candidate O O
state O O
is O O
accepted O O
or O O
rejected O O
according O O
to O O
the O O
metropolis O O
criterion O O
based O O
on O O
the O O
value O O
of O O
the O O
hamiltonian O O
h. O O
thus O O
if O O
( O O
z O O
, O O
r O O
) O O
is O O
the O O
initial O O
state O O
and O O
( O O
z O O
( O O
cid:1 O O
) O O
, O O
r O O
( O O
cid:1 O O
) O O
) O O
is O O
the O O
state O O
after O O
the O O
leapfrog O O
integration O O
, O O
then O O
this O O
candidate O O
state O O
is O O
accepted O O
with O O
probability B B
min O O
( O O
1 O O
, O O
exp O O
{ O O
h O O
( O O
z O O
, O O
r O O
) O O
− O O
h O O
( O O
z O O
( O O
cid:1 O O
) O O
, O O
r O O
( O O
cid:1 O O
) O O
) O O
} O O
) O O
. O O
derivative-free O O
adaptive B O
rejection I I
sampling I I
for O O
gibbs O O
in O O
j. O O
bernardo O O
, O O
j. O O
berger O O
, O O
a. O O
p. O O
dawid O O
, O O
and O O
a. O O
f. O O
m. O O
smith O O
( O O
eds O O
. O O
figure O O
10.6 O O
shows O O
the O O
results O O
of O O
applying O O
this O O
approach O O
to O O
the O O
rescaled O O
old O O
faith- O O
ful O O
data O O
set O O
for O O
a O O
gaussian O O
mixture B B
model I I
having O O
k O O
= O O
6 O O
components O O
. O O
( O O
2.257 O O
) O O
( O O
2.258 O O
) O O
( O O
2.259 O O
) O O
show O O
that O O
the O O
entropy B B
h O O
[ O O
x O O
] O O
of O O
a O O
bernoulli O O
distributed O O
random O O
binary O O
variable O O
x O O
is O O
given O O
by O O
h O O
[ O O
x O O
] O O
= O O
−µ O O
ln O O
µ O O
− O O
( O O
1 O O
− O O
µ O O
) O O
ln O O
( O O
1 O O
− O O
µ O O
) O O
. O O
a O O
bayesian O O
ap- O O
proach O O
to O O
on-line B O
learning I B
. O O
one O O
approach O O
to O O
reducing O O
random O O
walk O O
behaviour O O
in O O
gibbs O O
sampling O O
is O O
called O O
over-relaxation B B
( O O
adler O O
, O O
1981 O O
) O O
. O O
the O O
set O O
df O O
will O O
contain O O
all O O
possible O O
distribu- O O
382 O O
8. O O
graphical O O
models O O
p O O
( O O
x O O
) O O
df O O
figure O O
8.25 O O
we O O
can O O
view O O
a O O
graphical B B
model I I
( O O
in O O
this O O
case O O
a O O
directed B B
graph O O
) O O
as O O
a O O
ﬁlter O O
in O O
which O O
a O O
prob- O O
ability O O
distribution O O
p O O
( O O
x O O
) O O
is O O
allowed O O
through O O
the O O
ﬁlter O O
if O O
, O O
and O O
only O O
if O O
, O O
it O O
satisﬁes O O
the O O
directed B B
factorization I I
property O O
( O O
8.5 O O
) O O
. O O
the O O
mapping O B
f. O O
then O O
defines O O
a O O
projectiorl O O
of O O
points O O
in O O
the O O
original O O
d-dimensional O O
space O O
into O O
the O O
m O O
-dimensional O O
subspace O O
s. O O
12.4. O O
nonlinear O O
latent B B
variable I I
models O O
595 O O
it O O
has O O
the O O
advantage O O
of O O
not O O
being O O
limited O O
to O O
linear O O
transformations O O
, O O
although O O
it O O
con O O
( O O
cid:173 O O
) O O
tains O O
standard O O
principal O O
component O I
analysis O I
as O O
a O O
special O O
case O O
. O O
8.3. O O
markov O O
random O O
fields O O
385 O O
figure O O
8.28 O O
for O O
an O O
undirected B B
graph I I
, O O
the O O
markov O O
blanket O O
of O O
a O O
node B B
xi O O
consists O O
of O O
the O O
set O O
of O O
neighbouring O O
nodes O O
. O O
( O O
4.25 O O
) O O
exercise O O
4.5 O O
we O O
can O O
make O O
the O O
dependence O O
on O O
w O O
explicit O O
by O O
using O O
( O O
4.20 O O
) O O
, O O
( O O
4.23 O O
) O O
, O O
and O O
( O O
4.24 O O
) O O
to O O
rewrite O O
the O O
fisher O O
criterion O O
in O O
the O O
form O O
4.1. O O
discriminant O B
functions O O
189 O O
j O O
( O O
w O O
) O O
= O O
wtsbw O O
wtsww O O
where O O
sb O O
is O O
the O O
between-class B B
covariance I O
matrix O O
and O O
is O O
given O O
by O O
sb O O
= O O
( O O
m2 O O
− O O
m1 O O
) O O
( O O
m2 O O
− O O
m1 O O
) O O
t O O
and O O
sw O O
is O O
the O O
total O O
within-class B B
covariance I I
matrix O O
, O O
given O O
by O O
( O O
4.26 O O
) O O
( O O
4.27 O O
) O O
sw O O
= O O
( O O
xn O O
− O O
m1 O O
) O O
( O O
xn O O
− O O
m1 O O
) O O
t O O
+ O O
( O O
xn O O
− O O
m2 O O
) O O
( O O
xn O O
− O O
m2 O O
) O O
t. O O
( O O
4.28 O O
) O O
( O O
cid:2 O O
) O O
n∈c1 O O
( O O
cid:2 O O
) O O
n∈c2 O O
differentiating O O
( O O
4.26 O O
) O O
with O O
respect O O
to O O
w O O
, O O
we O O
ﬁnd O O
that O O
j O O
( O O
w O O
) O O
is O O
maximized O O
when O O
( O O
wtsbw O O
) O O
sww O O
= O O
( O O
wtsww O O
) O O
sbw O O
. O O
the O O
d-separation B B
theorem O O
says O O
that O O
it O O
is O O
the O O
same O O
set O O
of O O
distributions O O
df O O
that O O
will O O
be O O
allowed O O
through O O
this O O
second O O
kind O O
of O O
ﬁlter O O
. O O
one O O
major O O
drawback O O
of O O
cross-validation B O
is O O
that O O
the O O
number O O
of O O
training B B
runs O O
that O O
must O O
be O O
performed O O
is O O
increased O O
by O O
a O O
factor O B
of O O
s O O
, O O
and O O
this O O
can O O
prove O O
problematic O O
for O O
models O O
in O O
which O O
the O O
training B B
is O O
itself O O
computationally O O
expensive O O
. O O
in O O
contrast O O
to O O
the O O
svm O O
we O O
shall O O
ﬁnd O O
it O O
more O O
convenient O O
to O O
introduce O O
the O O
regres- O O
sion B B
form O O
of O O
the O O
rvm O O
ﬁrst O O
and O O
then O O
consider O O
the O O
extension O O
to O O
classiﬁcation B B
tasks O O
. O O
it O O
is O O
therefore O O
common O O
to O O
include O O
separate O O
priors O O
for O O
the O O
biases O O
( O O
which O O
then O O
break O O
shift O O
invariance B O
) O O
having O O
their O O
own O O
hyperparameters O O
. O O
5.2. O O
network O O
training B B
so O O
far O O
, O O
we O O
have O O
viewed O O
neural O O
networks O O
as O O
a O O
general O O
class O O
of O O
parametric O O
nonlinear O O
functions O O
from O O
a O O
vector O O
x O O
of O O
input O O
variables O O
to O O
a O O
vector O O
y O O
of O O
output O O
variables O O
. O O
finally O O
, O O
consider O O
the O O
case O O
in O O
which O O
the O O
mixing O O
coefﬁcients O O
at O O
both O O
levels O O
of O O
the O O
hi- O O
erarchical O O
mixture B O
are O O
constrained O O
to O O
be O O
linear O O
classiﬁcation O B
( O O
logistic O O
or O O
softmax O O
) O O
models O O
. O O
however O O
, O O
this O O
minimum O O
level O O
is O O
nonzero O O
, O O
and O O
it O O
should O O
be O O
emphasized O O
that O O
a O O
good O O
grasp O O
of O O
calculus O O
, O O
linear O O
algebra O O
, O O
and O O
probability B B
theory O O
is O O
essential O O
for O O
a O O
clear O O
understanding O O
of O O
modern O O
pattern O O
recog- O O
nition O O
and O O
machine O O
learning O O
techniques O O
. O O
( O O
10.164 O O
) O O
mt O O
0 O O
s O O
−1 O O
0 O O
m0 O O
( O O
cid:13 O O
) O O
1 O O
2 O O
−1 O O
n O O
mn O O
+ O O
2 O O
ξn O O
− O O
λ O O
( O O
ξn O O
) O O
ξ2 O O
n O O
this O O
variational B B
framework O O
can O O
also O O
be O O
applied O O
to O O
situations O O
in O O
which O O
the O O
data O O
is O O
arriving O O
sequentially O O
( O O
jaakkola O O
and O O
jordan O O
, O O
2000 O O
) O O
. O O
regression B B
functions O O
, O O
such O O
as O O
this O O
, O O
which O O
make O O
predictions O O
by O O
taking O O
linear O O
combinations O O
of O O
the O O
training B B
set I I
target O O
values O O
are O O
known O O
as O O
linear O B
smoothers O O
. O O
note O O
that O O
there O O
is O O
no O O
need O O
to O O
consider O O
all O O
of O O
the O O
terms O O
in O O
the O O
lower B B
bound I I
but O O
only O O
the O O
dependence O O
of O O
the O O
bound O O
on O O
the O O
{ O O
πk O O
} O O
. O O
n O O
( O O
cid:14 O O
) O O
( O O
cid:24 O O
) O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:25 O O
) O O
98 O O
2. O O
probability B B
distributions O O
conjugate B B
distribution O O
for O O
this O O
likelihood B B
function I I
because O O
the O O
corresponding O O
poste- O O
rior O O
will O O
be O O
a O O
product O O
of O O
two O O
exponentials O O
of O O
quadratic O O
functions O O
of O O
µ O O
and O O
hence O O
will O O
also O O
be O O
gaussian O O
. O O
5.40 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
outline O O
the O O
modiﬁcations O O
needed O O
to O O
the O O
framework O O
for O O
bayesian O O
neural O O
networks O O
, O O
discussed O O
in O O
section O O
5.7.3 O O
, O O
to O O
handle O O
multiclass B B
problems O O
using O O
networks O O
having O O
softmax O O
output-unit O O
activation O B
functions O O
. O O
14.13 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
verify O O
that O O
the O O
complete-data O O
log O O
likelihood O O
function O O
for O O
the O O
mixture O B
of O O
linear O O
regression B I
models O O
is O O
given O O
by O O
( O O
14.36 O O
) O O
. O O
the O O
origins O O
of O O
the O O
sparsity B B
when O I
optimizing O O
with O O
respect O O
to O O
hyperparameters O O
is O O
discussed O O
in O O
detail O O
in O O
the O O
context O O
of O O
the O O
relevance B B
vector I I
machine I I
. O O
show O O
that O O
this O O
estimator O O
has O O
the O O
property O O
that O O
its O O
expectation B B
is O O
given O O
by O O
the O O
true O O
variance B B
σ2 O O
. O O
this O O
result O O
accords O O
with O O
our O O
intuition O O
that O O
, O O
in O O
order O O
to O O
minimize O O
the O O
average O O
squared O O
projection O O
distance O O
, O O
we O O
should O O
choose O O
the O O
principal O O
component O I
subspace O O
to O O
pass O O
through O O
the O O
mean B B
of O O
the O O
data O O
points O O
and O O
to O O
be O O
aligned O O
with O O
the O O
directions O O
of O O
maximum O B
variance O O
. O O
( O O
4.31 O O
) O O
the O O
sum-of-squares B B
error I I
function O O
can O O
be O O
written O O
wtxn O O
+ O O
w0 O O
− O O
tn O O
e O O
= O O
( O O
cid:11 O O
) O O
2 O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:10 O O
) O O
n=1 O O
1 O O
2 O O
( O O
cid:10 O O
) O O
n O O
( O O
cid:2 O O
) O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:10 O O
) O O
n=1 O O
n=1 O O
( O O
cid:11 O O
) O O
( O O
cid:11 O O
) O O
wtxn O O
+ O O
w0 O O
− O O
tn O O
= O O
0 O O
wtxn O O
+ O O
w0 O O
− O O
tn O O
xn O O
= O O
0 O O
. O O
a22 O O
a21 O O
a12 O O
k O O
= O O
2 O O
a32 O O
a23 O O
k O O
= O O
1 O O
a11 O O
k O O
= O O
3 O O
a31 O O
a13 O O
a33 O O
has O O
k O O
( O O
k−1 O O
) O O
independent B B
parameters O O
. O O
data O O
for O O
this O O
problem O O
is O O
generated O O
by O O
sampling O O
a O O
variable O O
x O O
uniformly O O
over O O
the O O
interval O O
( O O
0 O O
, O O
1 O O
) O O
, O O
to O O
give O O
a O O
set O O
of O O
values O O
{ O O
xn O O
} O O
, O O
and O O
the O O
corresponding O O
target O O
values O O
tn O O
are O O
obtained O O
figure O O
5.19 O O
on O O
the O O
left O O
is O O
the O O
data O O
set O O
for O O
a O O
simple O O
‘ O O
forward B B
problem I I
’ O O
in O O
which O O
the O O
red O O
curve O O
shows O O
the O O
result O O
of O O
ﬁtting O O
a O O
two-layer O O
neural B B
network I I
by O O
minimizing O O
the O O
sum-of-squares B B
error I I
function O O
. O O
similarly O O
in O O
speech B B
recognition I I
, O O
small O O
levels O O
of O O
nonlinear O O
warping O O
along O O
the O O
time O O
axis O O
, O O
which O O
preserve O O
temporal O O
ordering O O
, O O
should O O
not O O
change O O
the O O
interpretation O O
of O O
the O O
signal O O
. O O
676 O O
14. O O
combining B B
models I I
14.16 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
extend O O
the O O
logistic B B
regression I I
mixture O O
model O O
of O O
section O O
14.5.2 O O
to O O
a O O
mixture O B
of O O
softmax O O
classiﬁers O O
representing O O
c O O
( O O
cid:2 O O
) O O
2 O O
classes O O
. O O
the O O
battery O O
is O O
either O O
charged O O
or O O
ﬂat O O
, O O
and O O
independently O O
the O O
fuel O O
tank O O
is O O
either O O
full O O
or O O
empty O O
, O O
with O O
prior B B
probabilities O O
p O O
( O O
b O O
= O O
1 O O
) O O
= O O
0.9 O O
p O O
( O O
f O O
= O O
1 O O
) O O
= O O
0.9. O O
given O O
the O O
state O O
of O O
the O O
fuel O O
tank O O
and O O
the O O
battery O O
, O O
the O O
fuel O O
gauge O O
reads O O
full O O
with O O
proba- O O
bilities O O
given O O
by O O
p O O
( O O
g O O
= O O
1|b O O
= O O
1 O O
, O O
f O O
= O O
1 O O
) O O
= O O
0.8 O O
p O O
( O O
g O O
= O O
1|b O O
= O O
1 O O
, O O
f O O
= O O
0 O O
) O O
= O O
0.2 O O
p O O
( O O
g O O
= O O
1|b O O
= O O
0 O O
, O O
f O O
= O O
1 O O
) O O
= O O
0.2 O O
p O O
( O O
g O O
= O O
1|b O O
= O O
0 O O
, O O
f O O
= O O
0 O O
) O O
= O O
0.1 O O
so O O
this O O
is O O
a O O
rather O O
unreliable O O
fuel O O
gauge O O
! O O
all O O
remaining O O
probabilities O O
are O O
determined O O
by O O
the O O
requirement O O
that O O
probabilities O O
sum O O
to O O
one O O
, O O
and O O
so O O
we O O
have O O
a O O
complete O O
speciﬁ- O O
cation O O
of O O
the O O
probabilistic O O
model O O
. O O
in O O
the O O
example O O
of O O
figure O O
1.27 O O
, O O
this O O
would O O
correspond O O
to O O
ﬁnding O O
the O O
value O O
of O O
x O O
shown O O
by O O
the O O
vertical O O
green O O
line O O
, O O
because O O
this O O
is O O
the O O
decision B B
boundary I I
giving O O
the O O
minimum O O
probability O I
of O O
misclassiﬁcation O O
. O O
in O O
this O O
ﬁnal O O
section O O
, O O
we O O
consider O O
some O O
nonparametric O O
approaches O O
to O O
density B B
es- O O
timation O O
that O O
make O O
few O O
assumptions O O
about O O
the O O
form O O
of O O
the O O
distribution O O
. O O
the O O
exponential B O
family I I
. O O
as O O
usual O O
, O O
it O O
is O O
convenient O O
to O O
maximize O O
the O O
logarithm O O
of O O
the O O
posterior O O
, O O
which O O
can O O
be O O
written O O
in O O
the O O
5.7. O O
bayesian O O
neural O O
networks O O
279 O O
form O O
ln O O
p O O
( O O
w|d O O
) O O
= O O
− O O
α O O
2 O O
wtw O O
− O O
β O O
2 O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
{ O O
y O O
( O O
xn O O
, O O
w O O
) O O
− O O
tn O O
} O O
2 O O
+ O O
const O O
( O O
5.165 O O
) O O
which O O
corresponds O O
to O O
a O O
regularized O O
sum-of-squares O O
error B O
function I O
. O O
in O O
that O O
case O O
, O O
the O O
apparent O O
variances O O
of O O
rl O O
and O O
rlf O O
( O O
z O O
( O O
l O O
) O O
) O O
may O O
be O O
small O O
even O O
though O O
the O O
estimate O O
of O O
the O O
expectation B B
may O O
be O O
severely O O
wrong O O
. O O
at O O
this O O
point O O
, O O
we O O
can O O
gain O O
some O O
insight O O
into O O
the O O
role O O
of O O
the O O
bias B B
parameter I I
w0 O O
. O O
note O O
that O O
m O O
controls O O
the O O
number O O
of O O
parameters O O
( O O
weights O O
and O O
biases O O
) O O
in O O
the O O
network O O
, O O
and O O
so O O
we O O
might O O
expect O O
that O O
in O O
a O O
maximum B B
likelihood I I
setting O O
there O O
will O O
be O O
an O O
optimum O O
value O O
of O O
m O O
that O O
gives O O
the O O
best O O
generalization B B
performance O O
, O O
corresponding O O
to O O
the O O
optimum O O
balance O O
between O O
under-ﬁtting O O
and O O
over-ﬁtting B B
. O O
( O O
2.214 O O
) O O
this O O
is O O
the O O
standard O O
form O O
of O O
the O O
exponential B B
family I I
, O O
with O O
parameter O O
vector O O
η O O
= O O
( O O
η1 O O
, O O
. O O
the O O
ﬁlter O O
will O O
allow O O
this O O
distribution O O
to O O
pass O O
through O O
if O O
, O O
and O O
only O O
if O O
, O O
it O O
can O O
be O O
expressed O O
in O O
terms O O
of O O
the O O
factorization B B
( O O
8.5 O O
) O O
implied O O
by O O
the O O
graph O O
. O O
( O O
5.67 O O
) O O
5.3.3 O O
efﬁciency O O
of O O
backpropagation B B
one O O
of O O
the O O
most O O
important O O
aspects O O
of O O
backpropagation B B
is O O
its O O
computational O O
efﬁ- O O
ciency O O
. O O
probability B B
densities O O
1.2.2 O O
expectations O O
and O O
covariances O O
1.2.3 O O
bayesian O O
probabilities O O
. O O
they O O
also O O
deﬁne O O
a O O
set O O
of O O
conditional B B
independence I I
properties O O
that O O
must O O
be O O
satisﬁed O O
by O O
any O O
distribution O O
that O O
factorizes O O
according O O
to O O
the O O
graph O O
. O O
from O O
data O O
distributions O O
to O O
regu- O O
larization O O
in O O
invariant O O
learning B O
. O O
indeed O O
, O O
it O O
is O O
simply O O
another O O
beta B O
distribution I I
, O O
and O O
its O O
normalization O O
coefﬁcient O O
can O O
therefore O O
be O O
obtained O O
by O O
comparison O O
with O O
( O O
2.13 O O
) O O
to O O
give O O
p O O
( O O
µ|m O O
, O O
l O O
, O O
a O O
, O O
b O O
) O O
= O O
γ O O
( O O
m O O
+ O O
a O O
+ O O
l O O
+ O O
b O O
) O O
γ O O
( O O
m O O
+ O O
a O O
) O O
γ O O
( O O
l O O
+ O O
b O O
) O O
µm+a−1 O O
( O O
1 O O
− O O
µ O O
) O O
l+b−1 O O
. O O
this O O
is O O
illustrated O O
with O O
a O O
simple O O
example O O
in O O
figure O O
5.2. O O
each O O
( O O
hidden O O
or O O
output O O
) O O
unit O O
in O O
such O O
a O O
network O O
computes O O
a O O
function O O
given O O
by O O
( O O
cid:22 O O
) O O
( O O
cid:2 O O
) O O
( O O
cid:23 O O
) O O
zk O O
= O O
h O O
wkjzj O O
j O O
( O O
5.10 O O
) O O
where O O
the O O
sum O O
runs O O
over O O
all O O
units O O
that O O
send O O
connections O O
to O O
unit O O
k O O
( O O
and O O
a O O
bias B B
param- O O
eter O O
is O O
included O O
in O O
the O O
summation O O
) O O
. O O
, O O
µd O O
) O O
t. O O
we O O
see O O
that O O
the O O
individual O O
variables O O
xi O O
are O O
independent B B
, O O
given O O
µ. O O
the O O
mean B B
and O O
covariance B B
of O O
this O O
distribution O O
are O O
easily O O
seen O O
to O O
be O O
e O O
[ O O
x O O
] O O
= O O
µ O O
cov O O
[ O O
x O O
] O O
= O O
diag O O
{ O O
µi O O
( O O
1 O O
− O O
µi O O
) O O
} O O
. O O
applying O O
these O O
results O O
to O O
( O O
2.105 O O
) O O
and O O
( O O
2.108 O O
) O O
we O O
see O O
that O O
the O O
conditional B B
distribution O O
p O O
( O O
x|y O O
) O O
has O O
mean B B
and O O
covariance B B
given O O
by O O
e O O
[ O O
x|y O O
] O O
= O O
( O O
λ O O
+ O O
atla O O
) O O
−1 O O
cov O O
[ O O
x|y O O
] O O
= O O
( O O
λ O O
+ O O
atla O O
) O O
−1 O O
. O O
we O O
now O O
discuss O O
an O O
analogous O O
class O O
of O O
models O O
for O O
solving O O
classiﬁcation B B
problems O O
. O O
, O O
xn O O
) O O
for O O
observation O O
xn+1 O O
given O O
all O O
previous O O
observations O O
does O O
not O O
exhibit O O
any O O
conditional B B
independence I I
prop- O O
erties O O
, O O
and O O
so O O
our O O
predictions O O
for O O
xn+1 O O
depends O O
on O O
all O O
previous O O
observations O O
. O O
the O O
interpretation O O
of O O
these O O
scaling O O
factors O O
is O O
clear O O
once O O
we O O
recognize O O
that O O
for O O
a O O
convolution O O
of O O
independent B B
gaussian O O
distributions O O
( O O
in O O
this O O
case O O
the O O
latent O B
space O O
distribution O O
and O O
the O O
noise O O
model O O
) O O
the O O
variances O O
are O O
additive O O
. O O
( O O
10.123 O O
) O O
( O O
cid:14 O O
) O O
i O O
( O O
cid:14 O O
) O O
i O O
( O O
cid:31 O O
) O O
( O O
cid:2 O O
) O O
ln O O
p O O
( O O
xi|pai O O
) O O
note O O
that O O
for O O
observed O O
nodes O O
, O O
there O O
is O O
no O O
factor O O
q O O
( O O
xi O O
) O O
in O O
the O O
variational B B
distribution O O
. O O
σ2 O O
( O O
5.188 O O
) O O
finally O O
, O O
to O O
obtain O O
the O O
predictive B B
distribution I I
, O O
we O O
must O O
marginalize O O
over O O
a O O
using O O
p O O
( O O
t O O
= O O
1|x O O
, O O
d O O
) O O
= O O
σ O O
( O O
a O O
) O O
p O O
( O O
a|x O O
, O O
d O O
) O O
da O O
. O O
it O O
has O O
the O O
property O O
, O O
as O O
we O O
shall O O
show O O
later O O
, O O
that O O
each O O
cycle O O
of O O
em O O
will O O
increase O O
the O O
incomplete-data O O
log O O
likelihood O O
( O O
unless O O
it O O
is O O
already O O
at O O
a O O
local B B
maximum O O
) O O
. O O
( O O
6.26 O O
) O O
an O O
important O O
contribution O O
to O O
arise O O
from O O
the O O
kernel O O
viewpoint O O
has O O
been O O
the O O
exten- O O
sion B O
to O O
inputs O O
that O O
are O O
symbolic O O
, O O
rather O O
than O O
simply O O
vectors O O
of O O
real O O
numbers O O
. O O
one O O
approach O O
to O O
determining O O
frequentist B B
error O I
bars O O
is O O
the O O
bootstrap B O
( O O
efron O O
, O O
1979 O O
; O O
hastie O O
et O O
al. O O
, O O
2001 O O
) O O
, O O
in O O
which O O
multiple O O
data O O
sets O O
are O O
created O O
as O O
follows O O
. O O
k O O
= O O
1 O O
k O O
= O O
5 O O
k O O
= O O
30 O O
5 O O
0 O O
5 O O
0 O O
0 O O
5 O O
0 O O
0 O O
0 O O
0.5 O O
0.5 O O
0.5 O O
1 O O
1 O O
1 O O
exercise O O
2.61 O O
density B B
p O O
( O O
x O O
) O O
, O O
and O O
we O O
allow O O
the O O
radius O O
of O O
the O O
sphere O O
to O O
grow O O
until O O
it O O
contains O O
precisely O O
k O O
data O O
points O O
. O O
1.2.6 O O
bayesian O O
curve B B
ﬁtting I I
although O O
we O O
have O O
included O O
a O O
prior B B
distribution O O
p O O
( O O
w|α O O
) O O
, O O
we O O
are O O
so O O
far O O
still O O
mak- O O
ing O O
a O O
point O O
estimate O O
of O O
w O O
and O O
so O O
this O O
does O O
not O O
yet O O
amount O O
to O O
a O O
bayesian O O
treatment O O
. O O
although O O
neglected O O
for O O
many O O
years O O
, O O
it O O
was O O
re-introduced O O
into O O
machine O O
learning O B
in O O
the O O
context O O
of O O
large- O O
margin B B
classiﬁers O O
by O O
boser O O
et O O
al O O
. O O
however O O
, O O
the O O
number O O
of O O
components O O
in O O
the O O
mixture B B
model I I
can O O
be O O
smaller O O
than O O
the O O
number O O
of O O
training B B
set I I
points O O
, O O
resulting O O
in O O
a O O
model O O
that O O
is O O
faster O O
to O O
evaluate O O
for O O
test O O
data O O
points O O
. O O
all O O
of O O
these O O
distributions O O
are O O
examples O O
of O O
the O O
exponential B B
family I I
of O O
distributions O O
, O O
which O O
possess O O
a O O
number O O
of O O
important O O
properties O O
, O O
and O O
which O O
will O O
be O O
discussed O O
in O O
some O O
detail O O
. O O
this O O
error B B
is O O
minimized O O
both O O
with O O
respect O O
to O O
the O O
weights O O
wi O O
and O O
with O O
respect O O
to O O
the O O
parameters O O
{ O O
πj O O
, O O
µj O O
, O O
σj O O
} O O
of O O
the O O
mixture B B
model I I
. O O
we O O
can O O
easily O O
modify O O
the O O
ep O O
procedure O O
to O O
give O O
the O O
standard O O
form O O
of O O
the O O
sum-product B B
algorithm I I
by O O
updating O O
just O O
one O O
of O O
the O O
we O O
are O O
reﬁning O O
only O O
one O O
term O O
at O O
a O O
time O O
, O O
then O O
we O O
can O O
choose O O
the O O
order O O
in O O
which O O
the O O
reﬁnements O O
are O O
done O O
as O O
we O O
wish O O
. O O
( O O
cid:24 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
( O O
2.148 O O
) O O
consider O O
a O O
prior B B
distribution O O
gam O O
( O O
λ|a0 O O
, O O
b0 O O
) O O
. O O
( O O
11.13 O O
) O O
figure O O
11.4 O O
in O O
the O O
rejection B B
sampling I I
method O O
, O O
samples O O
are O O
drawn O O
from O O
a O O
sim- O O
ple O O
distribution O O
q O O
( O O
z O O
) O O
and O O
rejected O O
if O O
they O O
fall O O
in O O
the O O
grey O O
area O O
be- O O
tween O O
the O O
unnormalized O O
distribu- O O
tion O O
ep O O
( O O
z O O
) O O
and O O
the O O
scaled O O
distribu- O O
tion O O
kq O O
( O O
z O O
) O O
. O O
of O O
particular O O
interest O O
are O O
symmetric O O
matrices O O
, O O
which O O
arise O O
as O O
covariance B B
ma- O O
trices O O
, O O
kernel O O
matrices O O
, O O
and O O
hessians O O
. O O
) O O
, O O
generalization B O
in O O
neural O O
networks O O
and O O
machine O O
learning O O
, O O
pp O O
. O O
by O O
contrast O O
, O O
in O O
the O O
basis B B
function I I
model O O
we O O
have O O
to O O
invert O O
a O O
matrix O O
sn O O
of O O
size O O
m O O
× O O
m O O
, O O
which O O
has O O
o O O
( O O
m O O
3 O O
) O O
computational O O
complexity O O
. O O
for O O
each O O
new O O
start O O
, O O
the O O
weight B B
vector I I
was O O
initial- O O
ized O O
by O O
sampling O O
from O O
an O O
isotropic B B
gaussian O O
distribution O O
having O O
a O O
mean B B
of O O
zero O O
and O O
a O O
variance B B
of O O
10 O O
. O O
a O O
simple O O
approximate O O
solution O O
is O O
to O O
add O O
a O O
term O O
ln O O
k O O
! O O
onto O O
the O O
lower B B
bound I I
when O O
used O O
for O O
model O O
comparison O O
and O O
averaging O O
. O O
32 O O
1. O O
introduction O O
figure O O
1.17 O O
the O O
predictive B B
distribution I I
result- O O
ing O O
from O O
a O O
bayesian O O
treatment O O
of O O
polynomial B O
curve I I
ﬁtting I I
using O O
an O O
m O O
= O O
9 O O
polynomial O O
, O O
with O O
the O O
ﬁxed O O
parameters O O
α O O
= O O
5 O O
× O O
10−3 O O
and O O
β O O
= O O
11.1 O O
( O O
corresponding O O
to O O
the O O
known O O
noise O O
variance B B
) O O
, O O
in O O
which O O
the O O
red O O
curve O O
denotes O O
the O O
mean B B
of O O
the O O
predictive B B
distribution I I
and O O
the O O
red O O
region O O
corresponds O O
to O O
±1 O O
stan- O O
dard O O
deviation O I
around O O
the O O
mean B B
. O O
finally O O
, O O
it O O
allows O O
missing B O
data I O
to O O
be O O
handled O O
in O O
a O O
principled O O
way O O
. O O
) O O
, O O
which O O
is O O
a O O
vector O O
each O O
of O O
whose O O
elements O O
is O O
a O O
function O O
of O O
the O O
scalar O O
) O O
.. O O
there O O
are O O
many O O
possible O O
ways O O
to O O
parameterize O O
the O O
curve O O
, O O
of O O
which O O
a O O
natural O O
choice O O
is O O
the O O
arc B O
length O O
along O O
the O O
curve O O
. O O
cases O O
such O O
as O O
the O O
digit O O
recognition O I
example O O
, O O
in O O
which O O
the O O
aim O O
is O O
to O O
assign O O
each O O
input O O
vector O O
to O O
one O O
of O O
a O O
ﬁnite O O
number O O
of O O
discrete O O
categories O O
, O O
are O O
called O O
classiﬁcation B B
problems O O
. O O
( O O
9.64 O O
) O O
m O O
( O O
cid:2 O O
) O O
λi O O
+ O O
α O O
i=1 O O
at O O
a O O
stationary B B
point O O
of O O
the O O
evidence B O
function I I
, O O
the O O
re-estimation O O
equation O O
( O O
3.92 O O
) O O
will O O
be O O
self-consistently O O
satisﬁed O O
, O O
and O O
hence O O
we O O
can O O
substitute O O
for O O
γ O O
to O O
give O O
n O O
mn O O
= O O
γ O O
= O O
m O O
− O O
αtr O O
( O O
sn O O
) O O
αmt O O
( O O
9.65 O O
) O O
and O O
solving O O
for O O
α O O
we O O
obtain O O
( O O
9.63 O O
) O O
, O O
which O O
is O O
precisely O O
the O O
em O O
re-estimation O O
equation O O
. O O
the O O
binary O O
target O O
variables O O
tk O O
∈ O O
{ O O
0 O O
, O O
1 O O
} O O
have O O
a O O
1-of-k O O
coding O O
scheme O O
indicating O O
the O O
class O O
, O O
and O O
the O O
network O O
outputs O O
are O O
interpreted O O
as O O
yk O B
( O O
x O O
, O O
w O O
) O O
= O O
p O O
( O O
tk O O
= O O
1|x O O
) O O
, O O
leading O O
to O O
the O O
following O O
error B O
function I O
e O O
( O O
w O O
) O O
= O O
− O O
n O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
n=1 O O
k=1 O O
tkn O O
ln O O
yk O O
( O O
xn O O
, O O
w O O
) O O
. O O
computer O O
vi- O O
sion B O
: O O
a O O
modern O O
approach O O
. O O
once O O
the O O
root B O
node I I
has O O
received O O
messages O O
from O O
all O O
of O O
its O O
neighbours O O
, O O
the O O
required O O
marginal B B
can O O
be O O
evaluated O O
using O O
( O O
8.63 O O
) O O
. O O
for O O
an O O
undirected B B
graph I I
this O O
com- O O
prises O O
the O O
set O O
of O O
neighbours O O
, O O
as O O
shown O O
on O O
the O O
left O O
, O O
while O O
for O O
a O O
directed B B
graph O O
the O O
markov O O
blanket O O
comprises O O
the O O
parents O O
, O O
the O O
children O O
, O O
and O O
the O O
co-parents B O
, O O
as O O
shown O O
on O O
the O O
right O O
. O O
the O O
inverse B B
problem I O
is O O
then O O
obtained O O
by O O
keeping O O
the O O
same O O
data O O
points O O
but O O
exchanging O O
the O O
roles O O
of O O
x O O
and O O
t. O O
figure O O
5.19 O O
shows O O
the O O
data O O
sets O O
for O O
the O O
forward O B
and O O
inverse B B
problems O O
, O O
along O O
with O O
the O O
results O O
of O O
ﬁtting O O
two-layer O O
neural O O
networks O O
having O O
6 O O
hidden O O
units O O
and O O
a O O
single O O
linear O B
output O O
unit O O
by O O
minimizing O O
a O O
sum- O O
of-squares O O
error B B
function I I
. O O
bayes O O
’ O O
theorem O O
, O O
which O O
takes O O
the O O
form O O
p O O
( O O
w|d O O
) O O
= O O
p O O
( O O
d|w O O
) O O
p O O
( O O
w O O
) O O
p O O
( O O
d O O
) O O
( O O
1.43 O O
) O O
then O O
allows O O
us O O
to O O
evaluate O O
the O O
uncertainty O O
in O O
w O O
after O O
we O O
have O O
observed O O
d O O
in O O
the O O
form O O
of O O
the O O
posterior B O
probability I I
p O O
( O O
w|d O O
) O O
. O O
an O O
example O O
of O O
a O O
periodic B B
variable I I
would O O
be O O
the O O
wind O O
direction O O
at O O
a O O
particular O O
geographical O O
location O O
. O O
these O O
properties O O
are O O
illustrated O O
for O O
the O O
case O O
of O O
d O O
= O O
2 O O
in O O
figure O O
4.1. O O
furthermore O O
, O O
we O O
note O O
that O O
the O O
value O O
of O O
y O O
( O O
x O O
) O O
gives O O
a O O
signed O O
measure O O
of O O
the O O
per- O O
pendicular O O
distance O O
r O O
of O O
the O O
point O O
x O O
from O O
the O O
decision B B
surface I I
. O O
he O O
also O O
created O O
an O O
early O O
formulation O O
of O O
non-euclidean O O
geometry O O
( O O
a O O
self-consistent O O
geometrical O O
theory B B
that O O
vi- O O
olates O O
the O O
axioms O O
of O O
euclid O O
) O O
but O O
was O O
reluctant O O
to O O
dis- O O
cuss O O
it O O
openly O O
for O O
fear O O
that O O
his O O
reputation O O
might O O
suffer O O
if O O
it O O
were O O
seen O O
that O O
he O O
believed O O
in O O
such O O
a O O
geometry O O
. O O
3.3.2 O O
predictive B O
distribution I I
in O O
practice O O
, O O
we O O
are O O
not O O
usually O O
interested O O
in O O
the O O
value O O
of O O
w O O
itself O O
but O O
rather O O
in O O
making O O
predictions O O
of O O
t O O
for O O
new O O
values O O
of O O
x. O O
this O O
requires O O
that O O
we O O
evaluate O O
the O O
predictive B B
distribution I I
deﬁned O O
by O O
p O O
( O O
t|t O O
, O O
α O O
, O O
β O O
) O O
= O O
p O O
( O O
t|w O O
, O O
β O O
) O O
p O O
( O O
w|t O O
, O O
α O O
, O O
β O O
) O O
dw O O
( O O
3.57 O O
) O O
( O O
cid:6 O O
) O O
exercise O O
3.10 O O
exercise O O
3.11 O O
in O O
which O O
t O O
is O O
the O O
vector O O
of O O
target O O
values O O
from O O
the O O
training B B
set I I
, O O
and O O
we O O
have O O
omitted O O
the O O
corresponding O O
input O O
vectors O O
from O O
the O O
right-hand O O
side O O
of O O
the O O
conditioning O O
statements O O
to O O
simplify O O
the O O
notation O O
. O O
this O O
represents O O
an O O
exam- O O
ple O O
in O O
the O O
gaussian O O
process O O
context O O
of O O
automatic B O
relevance I I
determination I O
, O O
or O O
ard O O
, O O
which O O
was O O
originally O O
formulated O O
in O O
the O O
framework O O
of O O
neural O B
networks O O
( O O
mackay O O
, O O
1994 O O
; O O
neal O O
, O O
1996 O O
) O O
. O O
note O O
that O O
in O O
contrast O O
to O O
the O O
mixture O B
of O O
gaussians O O
, O O
there O O
are O O
no O O
singularities B B
in O O
which O O
the O O
likelihood B B
function I I
goes O O
to O O
inﬁnity O O
. O O
11.1.4 O O
importance B B
sampling I I
one O O
of O O
the O O
principal O B
reasons O O
for O O
wishing O O
to O O
sample O O
from O O
complicated O O
probability B B
distributions O O
is O O
to O O
be O O
able O O
to O O
evaluate O O
expectations O O
of O O
the O O
form O O
( O O
11.1 O O
) O O
. O O
we O O
therefore O O
have O O
∇f O O
( O O
x O O
) O O
= O O
−λ∇g O O
( O O
x O O
) O O
for O O
some O O
value O O
of O O
λ O O
> O O
0. O O
for O O
either O O
of O O
these O O
two O O
cases O O
, O O
the O O
product O O
λg O O
( O O
x O O
) O O
= O O
0. O O
thus O O
the O O
solution O O
to O O
the O O
figure O O
e.3 O O
illustration O O
of O O
f O O
( O O
x O O
) O O
subject O O
g O O
( O O
x O O
) O O
( O O
cid:2 O O
) O O
0. O O
the O O
problem O O
of O O
maximizing O O
to O O
the O O
inequality B O
constraint I O
∇f O O
( O O
x O O
) O O
xa O O
∇g O O
( O O
x O O
) O O
xb O O
g O O
( O O
x O O
) O O
> O O
0 O O
g O O
( O O
x O O
) O O
= O O
0 O O
710 O O
e. O O
lagrange O O
multipliers O O
problem O O
of O O
maximizing O O
f O O
( O O
x O O
) O O
subject O O
to O O
g O O
( O O
x O O
) O O
( O O
cid:2 O O
) O O
0 O O
is O O
obtained O O
by O O
optimizing O O
the O O
lagrange O O
function O O
( O O
e.4 O O
) O O
with O O
respect O O
to O O
x O O
and O O
λ O O
subject O O
to O O
the O O
conditions O O
g O O
( O O
x O O
) O O
( O O
cid:2 O O
) O O
0 O O
λ O O
( O O
cid:2 O O
) O O
0 O O
λg O O
( O O
x O O
) O O
= O O
0 O O
( O O
e.9 O O
) O O
( O O
e.10 O O
) O O
( O O
e.11 O O
) O O
these O O
are O O
known O O
as O O
the O O
karush-kuhn-tucker O O
( O O
kkt O O
) O O
conditions O O
( O O
karush O O
, O O
1939 O O
; O O
kuhn O O
and O O
tucker O O
, O O
1951 O O
) O O
. O O
( O O
11.33 O O
) O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
( O O
cid:1 O O
) O O
) O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
( O O
τ O O
) O O
) O O
this O O
can O O
be O O
achieved O O
by O O
choosing O O
a O O
random O O
number O O
u O O
with O O
uniform B O
distribution I I
over O O
the O O
unit O O
interval O O
( O O
0 O O
, O O
1 O O
) O O
and O O
then O O
accepting O O
the O O
sample O O
if O O
a O O
( O O
z O O
( O O
cid:1 O O
) O O
, O O
z O O
( O O
τ O O
) O O
) O O
> O O
u. O O
note O O
that O O
if O O
the O O
step O O
from O O
z O O
( O O
τ O O
) O O
to O O
z O O
( O O
cid:1 O O
) O O
causes O O
an O O
increase O O
in O O
the O O
value O O
of O O
p O O
( O O
z O O
) O O
, O O
then O O
the O O
candidate O O
point O O
is O O
certain O O
to O O
be O O
kept O O
. O O
thus O O
to O O
classify O O
a O O
new O O
point O O
, O O
we O O
identify O O
the O O
k O O
nearest O O
points O O
from O O
the O O
training B B
data O O
set O O
and O O
then O O
assign O O
the O O
new O O
point O O
to O O
the O O
class O O
having O O
the O O
largest O O
number O O
of O O
representatives O O
amongst O O
this O O
set O O
. O O
7.1.3 O O
multiclass B B
svms O O
the O O
support B B
vector I I
machine I I
is O O
fundamentally O O
a O O
two-class O O
classiﬁer O O
. O O
4.4.1 O O
model B O
comparison I I
and O O
bic O O
as O O
well O O
as O O
approximating O O
the O O
distribution O O
p O O
( O O
z O O
) O O
we O O
can O O
also O O
obtain O O
an O O
approxi- O O
mation B B
to O O
the O O
normalization O O
constant O O
z. O O
using O O
the O O
approximation O O
( O O
4.133 O O
) O O
we O O
have O O
( O O
cid:6 O O
) O O
z O O
= O O
f O O
( O O
z O O
) O O
dz O O
( O O
cid:6 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:7 O O
) O O
f O O
( O O
z0 O O
) O O
= O O
f O O
( O O
z0 O O
) O O
exp O O
( O O
2π O O
) O O
m/2 O O
|a|1/2 O O
( O O
cid:13 O O
) O O
( O O
z O O
− O O
z0 O O
) O O
ta O O
( O O
z O O
− O O
z0 O O
) O O
−1 O O
2 O O
dz O O
( O O
4.135 O O
) O O
where O O
we O O
have O O
noted O O
that O O
the O O
integrand O O
is O O
gaussian O O
and O O
made O O
use O O
of O O
the O O
standard O O
result O O
( O O
2.43 O O
) O O
for O O
a O O
normalized O O
gaussian O O
distribution O O
. O O
13.2.6 O O
extensions O O
of O O
the O O
hidden O O
markov O O
model O O
the O O
basic O O
hidden O O
markov O O
model O O
, O O
along O O
with O O
the O O
standard O O
training O B
algorithm O O
based O O
on O O
maximum B B
likelihood I I
, O O
has O O
been O O
extended B B
in O O
numerous O O
ways O O
to O O
meet O O
the O O
requirements O O
of O O
particular O O
applications O O
. O O
thus O O
the O O
maximization O O
with O O
respect O O
to O O
a O O
mean B B
or O O
a O O
covariance B B
is O O
exactly O O
as O O
for O O
a O O
single O O
gaussian O O
, O O
except O O
that O O
it O O
involves O O
only O O
the O O
subset O O
of O O
data O O
points O O
that O O
are O O
‘ O O
assigned O O
’ O O
to O O
that O O
component O O
. O O
8.1.1 O O
example O O
: O O
polynomial O B
regression O I
as O O
an O O
illustration O O
of O O
the O O
use O O
of O O
directed B B
graphs O O
to O O
describe O O
probability B B
distri- O O
butions O O
, O O
we O O
consider O O
the O O
bayesian O O
polynomial O B
regression O I
model O O
introduced O O
in O O
sec- O O
exercise O O
8.1 O O
exercise O O
8.2 O O
8.1. O O
bayesian O O
networks O O
363 O O
figure O O
8.3 O O
directed B B
graphical O O
model O O
representing O O
the O O
joint O O
distribution O O
( O O
8.6 O O
) O O
corresponding O O
to O O
the O O
bayesian O O
polynomial O B
regression O I
model O O
introduced O O
in O O
sec- O O
tion O O
1.2.6. O O
w O O
t1 O O
tn O O
tion O O
1.2.6. O O
the O O
random O O
variables O O
in O O
this O O
model O O
are O O
the O O
vector O O
of O O
polynomial O B
coefﬁ- O O
cients O O
w O O
and O O
the O O
observed O O
data O O
t O O
= O O
( O O
t1 O O
, O O
. O O
we O O
therefore O O
obtain O O
the O O
minimum O O
value O O
of O O
j O O
by O O
selecting O O
these O O
eigenvectors O O
to O O
be O O
those O O
having O O
the O O
d O O
- O O
m O O
smallest O O
eigenvalues O O
, O O
and O O
hence O O
the O O
eigenvectors O O
defining O O
the O O
principal B B
subspace I I
are O O
those O O
corresponding O O
to O O
the O O
m O O
largest O O
eigenvalues O O
. O O
example-based O O
learning B B
for O O
view-based O O
human O O
face B O
detection I I
. O O
this O O
can O O
be O O
done O O
using O O
a O O
simple O O
variational B B
posterior O O
distribution O O
that O O
is O O
fully O O
factorized O O
with O O
respect O O
to O O
the O O
latent O O
variables O O
, O O
or O O
alternatively O O
by O O
using O O
a O O
more O O
powerful O O
approach O O
in O O
which O O
the O O
variational B B
distribution O O
is O O
described O O
by O O
independent B B
markov O O
chains O O
corresponding O O
to O O
the O O
chains O O
of O O
latent O B
variables O O
in O O
the O O
original O O
model O O
. O O
now O O
suppose O O
we O O
observe O O
the O O
value O O
of O O
y O O
, O O
as O O
indicated O O
by O O
the O O
shaded O O
node B O
in O O
figure O O
8.37 O O
( O O
b O O
) O O
. O O
( O O
8.91 O O
) O O
( O O
8.92 O O
) O O
thus O O
taking O O
the O O
logarithm O O
simply O O
has O O
the O O
effect O O
of O O
replacing O O
the O O
products O O
in O O
the O O
max-product O O
algorithm O O
with O O
sums O O
, O O
and O O
so O O
we O O
obtain O O
the O O
max-sum B B
algorithm I I
. O O
( O O
cid:28 O O
) O O
figure O O
2.24 O O
an O O
illustration O O
of O O
the O O
histogram O O
approach O O
to O O
density B B
estimation I I
, O O
in O O
which O O
a O O
data O O
set O O
of O O
50 O B
data O O
points O O
is O O
generated O O
from O O
the O O
distribution O O
shown O O
by O O
the O O
green O O
curve O O
. O O
n. O O
thus O O
the O O
solution O O
to O O
the O O
regression B B
problem O O
decouples O O
between O O
the O O
different O O
target O O
† O O
variables O O
, O O
and O O
we O O
need O O
only O O
compute O O
a O O
single O O
pseudo-inverse B O
matrix O O
φ O O
, O O
which O O
is O O
shared O O
by O O
all O O
of O O
the O O
vectors O O
wk O O
. O O
more O O
generally O O
, O O
it O O
would O O
also O O
apply O O
to O O
any O O
model O O
deﬁned O O
by O O
a O O
directed B B
probabilistic O O
graph O O
in O O
which O O
each O O
factor O O
is O O
a O O
conditional B B
distribution O O
corresponding O O
to O O
one O O
of O O
the O O
nodes O O
, O O
or O O
an O O
undirected B B
graph I I
in O O
which O O
each O O
factor O O
is O O
a O O
clique B B
potential O O
. O O
also O O
, O O
we O O
have O O
introduced O O
the O O
n O O
× O O
n O O
diagonal B B
matrix O O
r O O
with O O
elements O O
( O O
4.98 O O
) O O
we O O
see O O
that O O
the O O
hessian O O
is O O
no O O
longer O O
constant O O
but O O
depends O O
on O O
w O O
through O O
the O O
weight- O O
ing O O
matrix O O
r O O
, O O
corresponding O O
to O O
the O O
fact O O
that O O
the O O
error B B
function I I
is O O
no O O
longer O O
quadratic O O
. O O
in O O
the O O
limit O O
n O O
→ O O
∞ O O
, O O
the O O
second O O
term O O
in O O
( O O
3.59 O O
) O O
goes O O
to O O
zero O O
, O O
and O O
the O O
variance B B
σ2 O O
of O O
the O O
predictive B B
distribution I I
arises O O
solely O O
from O O
the O O
additive O O
noise O O
governed O O
by O O
the O O
parameter O O
β. O O
as O O
an O O
illustration O O
of O O
the O O
predictive B B
distribution I I
for O O
bayesian O O
linear B O
regression I I
models O O
, O O
let O O
us O O
return O O
to O O
the O O
synthetic O O
sinusoidal O O
data O O
set O O
of O O
section O O
1.1. O O
in O O
figure O O
3.8 O O
, O O
t O O
1 O O
0 O O
−1 O O
t O O
1 O O
0 O O
−1 O O
0 O O
0 O O
3.3. O O
bayesian O O
linear B O
regression I I
157 O O
t O O
1 O O
0 O O
−1 O O
x O O
1 O O
0 O O
x O O
1 O O
t O O
1 O O
0 O O
−1 O O
x O O
1 O O
0 O O
x O O
1 O O
figure O O
3.8 O O
examples O O
of O O
the O O
predictive B B
distribution I I
( O O
3.58 O O
) O O
for O O
a O O
model O O
consisting O O
of O O
9 O O
gaussian O O
basis O B
functions O O
of O O
the O O
form O O
( O O
3.4 O O
) O O
using O O
the O O
synthetic O O
sinusoidal O B
data O O
set O O
of O O
section O O
1.1. O O
see O O
the O O
text O O
for O O
a O O
detailed O O
discussion O O
. O O
one O O
approach O O
is O O
to O O
choose O O
a O O
feature B B
space I I
mapping O O
φ O O
( O O
x O O
) O O
and O O
then O O
use O O
this O O
to O O
ﬁnd O O
the O O
corresponding O O
kernel O O
, O O
as O O
is O O
illustrated O O
in O O
figure O O
6.1. O O
here O O
the O O
kernel B O
function I I
is O O
deﬁned O O
for O O
a O O
one-dimensional O O
input O O
space O O
by O O
m O O
( O O
cid:2 O O
) O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
2 O O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
φ O O
( O O
x O O
) O O
tφ O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
( O O
cid:4 O O
) O O
) O O
φi O O
( O O
x O O
) O O
φi O O
( O O
x O O
( O O
6.10 O O
) O O
where O O
φi O O
( O O
x O O
) O O
are O O
the O O
basis O B
functions O O
. O O
for O O
each O O
plot O O
, O O
the O O
red O O
curve O O
shows O O
the O O
mean B B
of O O
the O O
corresponding O O
gaussian O O
predictive B B
distribution I I
, O O
and O O
the O O
red O O
shaded O O
region O O
spans O O
one O O
standard B O
deviation I I
either O O
side O O
of O O
the O O
mean B B
. O O
a O O
function O O
f O O
( O O
x O O
) O O
is O O
said O O
to O O
be O O
convex O B
if O O
it O O
has O O
the O O
property O O
that O O
every O O
chord O O
lies O O
on O O
or O O
above O O
the O O
function O O
, O O
as O O
shown O O
in O O
figure O O
1.31. O O
any O O
value O O
of O O
x O O
in O O
the O O
interval O O
from O O
x O O
= O O
a O O
to O O
x O O
= O O
b O O
can O O
be O O
written O O
in O O
the O O
form O O
λa O O
+ O O
( O O
1 O O
− O O
λ O O
) O O
b O O
where O O
0 O O
( O O
cid:1 O O
) O O
λ O O
( O O
cid:1 O O
) O O
1. O O
the O O
corresponding O O
point O O
on O O
the O O
chord O O
is O O
given O O
by O O
λf O O
( O O
a O O
) O O
+ O O
( O O
1 O O
− O O
λ O O
) O O
f O O
( O O
b O O
) O O
, O O
claude O O
shannon O O
1916–2001 O O
after O O
graduating O O
from O O
michigan O O
and O O
mit O O
, O O
shannon O O
joined O O
the O O
at O O
& O O
t O O
bell O O
telephone O O
laboratories O O
in O O
1941. O B
his O O
paper O O
‘ O O
a O O
mathematical O O
theory B B
of O O
communication O O
’ O O
published O O
in O O
the O O
bell O O
system O O
technical O O
journal O O
in O O
1948 O B
laid O O
the O O
foundations O O
for O O
modern O O
information O O
the- O O
ory O O
. O O
n=3 O O
figure O O
13.4 O O
a O O
second-order O O
markov O O
chain O O
, O O
in O O
which O O
the O O
conditional B B
distribution O O
of O O
a O O
particular O O
observation O O
xn O O
depends O O
on O O
the O O
values O O
of O O
the O O
two O O
previous O O
observations O O
xn−1 O O
and O O
xn−2 O O
. O O
the O O
practical O O
applicability O O
of O O
gibbs O O
sampling O O
depends O O
on O O
the O O
ease O O
with O O
which O O
samples O O
can O O
be O O
drawn O O
from O O
the O O
conditional B B
distributions O O
p O O
( O O
zk|z\k O O
) O O
. O O
because O O
the O O
logarithm O O
function O O
is O O
monotonically O O
increasing O O
, O O
the O O
inequality O O
a O O
( O O
cid:2 O O
) O O
b O O
implies O O
ln O O
a O O
( O O
cid:2 O O
) O O
ln O O
b. O O
this O O
gives O O
a O O
lower B B
bound I I
on O O
the O O
log O O
of O O
the O O
joint O O
distribution O O
of O O
t O O
and O O
w O O
of O O
the O O
form O O
ln O O
{ O O
p O O
( O O
t|w O O
) O O
p O O
( O O
w O O
) O O
} O O
( O O
cid:2 O O
) O O
ln O O
p O O
( O O
w O O
) O O
+ O O
ln O O
σ O O
( O O
ξn O O
) O O
+ O O
wtφntn O O
− O O
( O O
wtφn O O
+ O O
ξn O O
) O O
/2 O O
− O O
λ O O
( O O
ξn O O
) O O
( O O
[ O O
wtφn O O
] O O
2 O O
− O O
ξ2 O O
n O O
) O O
. O O
10.1. O O
variational B B
inference I I
variational O O
methods O O
have O O
their O O
origins O O
in O O
the O O
18th O O
century O O
with O O
the O O
work O O
of O O
euler O O
, O O
lagrange O O
, O O
and O O
others O O
on O O
the O O
calculus B B
of I I
variations I I
. O O
this O O
can O O
be O O
done O O
, O O
for O O
instance O O
, O O
by O O
viewing O O
the O O
conditional B B
distribution O O
representing O O
the O O
linear B B
regression I I
model O O
as O O
a O O
node B B
in O O
a O O
directed B B
prob- O O
abilistic O O
graph O O
. O O
again O O
, O O
we O O
can O O
test O O
to O O
see O O
if O O
a O O
and O O
b O O
are O O
independent B B
by O O
marginalizing O O
over O O
c O O
to O O
give O O
( O O
cid:2 O O
) O O
p O O
( O O
a O O
, O O
b O O
) O O
= O O
p O O
( O O
a O O
) O O
p O O
( O O
c|a O O
) O O
p O O
( O O
b|c O O
) O O
= O O
p O O
( O O
a O O
) O O
p O O
( O O
b|a O O
) O O
. O O
in O O
section O O
7.1.2 O O
, O O
we O O
shall O O
consider O O
several O O
alternative O O
error B B
functions O O
for O O
classiﬁcation O O
and O O
we O O
shall O O
see O O
that O O
they O O
do O O
not O O
suffer O O
from O O
this O O
difﬁculty O O
. O O
we O O
see O O
that O O
any O O
ﬁnite O O
value O O
for O O
α O O
would O O
cause O O
the O O
distribution O O
to O O
be O O
elongated O O
in O O
a O O
direction O O
away O O
from O O
the O O
data O O
, O O
thereby O O
increasing O O
the O O
probability B B
mass O O
in O O
regions O O
away O O
from O O
the O O
observed O O
data O O
and O O
hence O O
reducing O O
the O O
value O O
of O O
the O O
density B B
at O O
the O O
target O O
data O O
vector O O
itself O O
. O O
from O O
( O O
12.42 O O
) O O
the O O
mean B B
is O O
given O O
by O O
where O O
m O O
is O O
given O O
by O O
( O O
12.41 O O
) O O
. O O
7.1.3 O O
multiclass B B
svms O O
. O O
taking O O
the O O
logarithm O O
of O O
the O O
likelihood B B
function I I
, O O
and O O
making O O
use O O
of O O
the O O
standard O O
form O O
( O O
1.46 O O
) O O
for O O
the O O
univariate O O
gaussian O O
, O O
we O O
have O O
ln O O
p O O
( O O
t|w O O
, O O
β O O
) O O
= O O
lnn O O
( O O
tn|wtφ O O
( O O
xn O O
) O O
, O O
β O O
−1 O O
) O O
= O O
n O O
2 O O
ln O O
β O O
− O O
n O O
2 O O
ln O O
( O O
2π O O
) O O
− O O
βed O O
( O O
w O O
) O O
( O O
3.11 O O
) O O
where O O
the O O
sum-of-squares B B
error I I
function O O
is O O
deﬁned O O
by O O
ed O O
( O O
w O O
) O O
= O O
1 O O
2 O O
{ O O
tn O O
− O O
wtφ O O
( O O
xn O O
) O O
} O O
2 O O
. O O
we O O
have O O
seen O O
that O O
the O O
conjugate B B
prior I I
for O O
λ O O
was O O
the O O
gamma B B
distribution I I
gam O O
( O O
λ|a0 O O
, O O
b0 O O
) O O
given O O
by O O
( O O
2.146 O O
) O O
. O O
( O O
3.69 O O
) O O
we O O
can O O
obtain O O
some O O
insight O O
into O O
the O O
model B O
evidence I I
by O O
making O O
a O O
simple O O
approx- O O
imation O O
to O O
the O O
integral O O
over O O
parameters O O
. O O
this O O
allows O O
us O O
to O O
impose O O
interesting O O
structure O O
on O O
the O O
distribution O O
, O O
with O O
the O O
general O O
gaussian O O
and O O
the O O
diagonal B B
covariance O B
gaussian O O
representing O O
opposite O O
ex- O O
tremes O O
. O O
section O O
13.2 O O
section O O
13.3 O O
13.2. O O
hidden O O
markov O O
models O O
the O O
hidden O O
markov O O
model O O
can O O
be O O
viewed O O
as O O
a O O
speciﬁc O O
instance O O
of O O
the O O
state B O
space I I
model I O
of O O
figure O O
13.5 O O
in O O
which O O
the O O
latent O B
variables O O
are O O
discrete O O
. O O
show O O
that O O
the O O
posterior O O
distribution O O
is O O
also O O
a O O
gaussian-gamma O O
distri- O O
bution O O
of O O
the O O
same O O
functional B B
form O O
as O O
the O O
prior B B
, O O
and O O
write O O
down O O
expressions O O
for O O
the O O
parameters O O
of O O
this O O
posterior O O
distribution O O
. O O
indeed O O
, O O
integrating O O
both O O
sides O O
of O O
( O O
1.43 O O
) O O
with O O
respect O O
to O O
w O O
, O O
we O O
can O O
express O O
the O O
denominator O O
in O O
bayes O O
’ O O
theorem O O
in O O
terms O O
of O O
the O O
prior B B
distribution O O
and O O
the O O
likelihood B B
function I I
( O O
cid:6 O O
) O O
p O O
( O O
d O O
) O O
= O O
p O O
( O O
d|w O O
) O O
p O O
( O O
w O O
) O O
dw O O
. O O
derive O O
the O O
em O O
algorithm O O
for O O
maximizing O O
the O O
posterior B O
probability I I
p O O
( O O
µ O O
, O O
π|x O O
) O O
. O O
( O O
4.136 O O
) O O
identifying O O
f O O
( O O
θ O O
) O O
= O O
p O O
( O O
d|θ O O
) O O
p O O
( O O
θ O O
) O O
and O O
z O O
= O O
p O O
( O O
d O O
) O O
, O O
and O O
applying O O
the O O
result O O
( O O
4.135 O O
) O O
, O O
we O O
obtain O O
ln O O
p O O
( O O
d O O
) O O
( O O
cid:7 O O
) O O
ln O O
p O O
( O O
d|θmap O O
) O O
+ O O
ln O O
p O O
( O O
θmap O O
) O O
+ O O
m O O
2 O O
ln O O
( O O
2π O O
) O O
− O O
1 O O
2 O O
ln|a| O O
( O O
4.137 O O
) O O
) O O
* O O
( O O
+ O O
occam O O
factor O O
exercise O O
4.22 O O
exercise O O
4.23 O O
section O O
3.5.3 O O
4.5. O O
bayesian O O
logistic B B
regression I I
217 O O
where O O
θmap O O
is O O
the O O
value O O
of O O
θ O O
at O O
the O O
mode O O
of O O
the O O
posterior O O
distribution O O
, O O
and O O
a O O
is O O
the O O
hessian O O
matrix O O
of O O
second O O
derivatives O O
of O O
the O O
negative O O
log O O
posterior O O
a O O
= O O
−∇∇ O O
ln O O
p O O
( O O
d|θmap O O
) O O
p O O
( O O
θmap O O
) O O
= O O
−∇∇ O O
ln O O
p O O
( O O
θmap|d O O
) O O
. O O
probabilistic O O
pea O O
pro O O
( O O
cid:173 O O
) O O
vides O O
an O O
elegant O O
compromise O O
in O O
which O O
the O O
m O O
most O O
significant O O
correlations O O
can O O
be O O
captured O O
while O O
still O O
ensuring O O
that O O
the O O
total O O
number O O
of O O
parameters O O
grows O O
only O O
linearly O O
with O O
d. O O
we O O
can O O
see O O
this O O
by O O
evaluating O O
the O O
number O O
of O O
degrees B O
of I I
freedom I I
in O O
the O O
ppca O O
model O O
as O O
follows O O
. O O
we O O
see O O
that O O
it O O
is O O
necessary O O
instead O O
to O O
keep O O
track O O
of O O
the O O
maximizing O O
states O O
during O O
the O O
forward O B
pass O O
using O O
the O O
functions O O
φ O O
( O O
xn O O
) O O
and O O
then O O
use O O
back-tracking B O
to O O
ﬁnd O O
a O O
consistent B B
solution O O
. O O
using O O
the O O
property O O
0 O O
< O O
yn O O
< O O
1 O O
, O O
which O O
follows O O
from O O
the O O
form O O
of O O
the O O
logistic B B
sigmoid I I
function O O
, O O
we O O
see O O
that O O
uthu O O
> O O
0 O O
for O O
an O O
arbitrary O O
vector O O
u O O
, O O
and O O
so O O
the O O
hessian O O
matrix O O
h O O
is O O
positive B B
deﬁnite I I
. O O
let O O
us O O
begin O O
by O O
considering O O
a O O
simple O O
example O O
, O O
namely O O
the O O
function O O
f O O
( O O
x O O
) O O
= O O
exp O O
( O O
−x O O
) O O
, O O
which O O
is O O
a O O
convex B B
function I I
of O O
x O O
, O O
and O O
which O O
is O O
shown O O
in O O
the O O
left-hand O O
plot O O
of O O
figure O O
10.10. O O
our O O
goal O O
is O O
to O O
approximate O O
f O O
( O O
x O O
) O O
by O O
a O O
simpler O O
function O O
, O O
in O O
particular O O
a O O
linear O O
function O O
of O O
x. O O
from O O
figure O O
10.10 O O
, O O
we O O
see O O
that O O
this O O
linear O O
function O O
will O O
be O O
a O O
lower B B
bound I I
on O O
f O O
( O O
x O O
) O O
if O O
it O O
corresponds O O
to O O
a O O
tangent O O
. O O
we O O
see O O
that O O
this O O
has O O
the O O
same O O
n O O
-1 O O
eigenvalues O O
as O O
the O O
original O O
covariance B B
matrix I I
( O O
which O O
itself O O
has O O
an O O
additional O O
d O O
- O O
n O O
+ O O
1 O O
eigenvalues O O
of O O
value O O
zero O O
) O O
. O O
show O O
the O O
marginal B B
of O O
this O O
distribution O O
for O O
one O O
of O O
the O O
test O O
observations O O
tj O O
where O O
n O O
+ O O
1 O O
( O O
cid:1 O O
) O O
j O O
( O O
cid:1 O O
) O O
n O O
+ O O
l O O
is O O
given O O
by O O
the O O
usual O O
gaussian O O
process O O
regression B B
result O O
( O O
6.66 O O
) O O
and O O
( O O
6.67 O O
) O O
. O O
we O O
see O O
that O O
this O O
leads O O
to O O
a O O
very O O
poor O O
model O O
for O O
the O O
highly O O
non-gaussian O O
inverse B B
problem I I
. O O
the O O
curves O O
show O O
the O O
prior B B
distribution O O
over O O
µ O O
( O O
the O O
curve O O
labelled O O
n O O
= O O
0 O O
) O O
, O O
which O O
in O O
this O O
case O O
is O O
itself O O
gaussian O O
, O O
along O O
with O O
the O O
posterior O O
distribution O O
given O O
by O O
( O O
2.140 O O
) O O
for O O
increasing O O
numbers O O
n O O
of O O
data O O
points O O
. O O
write O O
down O O
an O O
expression O O
for O O
the O O
difference O O
in O O
the O O
values O O
of O O
the O O
energy O B
associated O O
with O O
the O O
two O O
states O O
of O O
a O O
particular O O
variable O O
xj O O
, O O
with O O
all O O
other O O
variables O O
held O O
ﬁxed O O
, O O
and O O
show O O
that O O
it O O
depends O O
only O O
on O O
quantities O O
that O O
are O O
local B B
to O O
xj O O
in O O
the O O
graph O O
. O O
11.5.1 O O
dynamical O O
systems O O
the O O
dynamical O O
approach O O
to O O
stochastic B B
sampling O I
has O O
its O O
origins O O
in O O
algorithms O O
for O O
simulating O O
the O O
behaviour O O
of O O
physical O O
systems O O
evolving O O
under O O
hamiltonian O O
dynam- O O
ics O O
. O O
because O O
there O O
are O O
k O O
such O O
vectors O O
, O O
this O O
requires O O
log2 O O
k O O
bits B B
per O I
pixel O O
. O O
note O O
that O O
the O O
mean B B
of O O
the O O
conditional B B
distribution O O
p O O
( O O
xa|xb O O
) O O
, O O
given O O
by O O
( O O
2.81 O O
) O O
, O O
is O O
a O O
linear O O
function O O
of O O
xb O O
and O O
that O O
the O O
covariance B B
, O O
given O O
by O O
( O O
2.82 O O
) O O
, O O
is O O
independent B B
of O O
xa O O
. O O
4.13 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
by O O
making O O
use O O
of O O
the O O
result O O
( O O
4.88 O O
) O O
for O O
the O O
derivative B B
of O O
the O O
logistic O B
sig- O O
moid O O
, O O
show O O
that O O
the O O
derivative B B
of O O
the O O
error B B
function I I
( O O
4.90 O O
) O O
for O O
the O O
logistic B B
regression I I
model O O
is O O
given O O
by O O
( O O
4.91 O O
) O O
. O O
to O O
achieve O O
this O O
, O O
we O O
ﬁrst O O
remove O O
the O O
factor O B
( O O
cid:4 O O
) O O
fj O O
( O O
θ O O
) O O
from O O
the O O
approximation O O
is O O
most O O
accurate O O
in O O
the O O
regions O O
of O O
high O O
posterior B O
probability I I
as O O
deﬁned O O
by O O
the O O
remaining O O
factors O O
. O O
first O O
, O O
to O O
estimate O O
the O O
probability B B
density O O
at O O
a O O
particular O O
location O O
, O O
we O O
should O O
consider O O
the O O
data O O
points O O
that O O
lie O O
within O O
some O O
local B B
neighbourhood O O
of O O
that O O
point O O
. O O
common O O
sense O O
tells O O
us O O
that O O
this O O
is O O
unreasonable O O
, O O
and O O
in O O
fact O O
this O O
is O O
an O O
extreme O O
example O O
of O O
the O O
over-ﬁtting B B
associ- O I
ated O O
with O O
maximum B B
likelihood I I
. O O
the O O
prior B B
p O O
( O O
an O O
) O O
is O O
given O O
by O O
a O O
zero-mean O O
gaussian O O
process O O
with O O
covariance B B
ma- O O
trix O O
cn O O
, O O
and O O
the O O
data O O
term O O
( O O
assuming O O
independence O O
of O O
the O O
data O O
points O O
) O O
is O O
given O O
by O O
p O O
( O O
tn|an O O
) O O
= O O
σ O O
( O O
an O O
) O O
tn O O
( O O
1 O O
− O O
σ O O
( O O
an O O
) O O
) O O
1−tn O O
= O O
eantnσ O O
( O O
−an O O
) O O
. O O
1.2.5 O O
curve B O
ﬁtting I I
re-visited O O
we O O
have O O
seen O O
how O O
the O O
problem O O
of O O
polynomial B B
curve I I
ﬁtting I I
can O O
be O O
expressed O O
in O O
terms O O
of O O
error B B
minimization O O
. O O
we O O
shall O O
see O O
in O O
the O O
next O O
section O O
that O O
by O O
adopting O O
a O O
probabilistic O O
approach O O
, O O
we O O
obtain O O
‘ O O
soft B O
’ O O
assignments O O
of O O
data O O
points O O
to O O
clusters O O
in O O
a O O
way O O
that O O
reﬂects O O
the O O
level O O
of O O
uncertainty O O
over O O
the O O
most O O
appropriate O O
assignment O O
. O O
( O O
9.59 O O
) O O
we O O
see O O
that O O
this O O
sets O O
the O O
mean B B
of O O
component O O
k O O
equal O O
to O O
a O O
weighted O O
mean O B
of O O
the O O
data O O
, O O
with O O
weighting O O
coefﬁcients O O
given O O
by O O
the O O
responsibilities O O
that O O
component O O
k O O
takes O O
for O O
data O O
points O O
. O O
in O O
particular O O
, O O
quasi-newton O O
nonlinear O O
opti- O O
mization O O
algorithms O O
gradually O O
build O O
up O O
an O O
approximation O O
to O O
the O O
inverse B B
of O O
the O O
hes- O O
sian O O
during O O
training B B
. O O
the O O
ﬁnite O O
sum O O
approximation O I
to O O
the O O
expectation B B
, O O
given O O
by O O
( O O
11.2 O O
) O O
, O O
depends O O
on O O
being O O
able O O
to O O
draw O O
samples O O
from O O
the O O
distribution O O
p O O
( O O
z O O
) O O
. O O
then O O
, O O
for O O
each O O
conditional B B
distribution O O
we O O
add O O
directed B O
links O O
( O O
arrows O O
) O O
to O O
the O O
graph O O
from O O
the O O
nodes O O
corresponding O O
to O O
the O O
variables O O
on O O
which O O
the O O
distribution O O
is O O
conditioned O O
. O O
in O O
general O O
, O O
factor O O
graphs O O
can O O
therefore O O
always O O
be O O
drawn O O
as O O
two O O
rows O O
of O O
nodes O O
( O O
variable O O
nodes O O
at O O
the O O
top O O
and O O
factor O B
nodes O O
at O O
the O O
bottom O O
) O O
with O O
links O O
between O O
the O O
rows O O
, O O
as O O
shown O O
in O O
the O O
example O O
in O O
figure O O
8.40. O O
in O O
some O O
situations O O
, O O
however O O
, O O
other O O
ways O O
of O O
laying O O
out O O
the O O
graph O O
may O O
be O O
more O O
intuitive O O
, O O
for O O
example O O
when O O
the O O
factor B B
graph I I
is O O
derived O O
from O O
a O O
directed B B
or O O
undirected B B
graph I I
, O O
as O O
we O O
shall O O
see O O
. O O
in O O
this O O
example O O
, O O
the O O
ﬁrst O O
step O O
figure O O
14.5 O O
illustration O O
of O O
a O O
two-dimensional O O
in- O O
put O O
space O O
that O O
has O O
been O O
partitioned B B
into O O
ﬁve O O
regions O O
using O O
axis-aligned O O
boundaries O O
. O O
this O O
can O O
be O O
represented O O
by O O
the O O
directed B B
graph O O
shown O O
in O O
figure O O
8.37 O O
( O O
a O O
) O O
. O O
each O O
of O O
the O O
remaining O O
variables O O
is O O
then O O
sampled O O
independently O O
from O O
a O O
uniform B B
distribution I I
over O O
the O O
space O O
of O O
possible O O
instantiations O O
. O O
reinforcement B O
learning I O
: O O
an O O
introduction O O
. O O
provided O O
we O O
chose O O
one O O
of O O
these O O
values O O
when O O
we O O
do O O
the O O
back-tracking B O
, O O
we O O
are O O
assured O O
of O O
a O O
globally O O
consistent B O
maximizing O O
conﬁguration O O
. O O
we O O
now O O
explore O O
three O O
approaches O O
to O O
learning B B
the O O
parameters O O
of O O
linear O O
discrimi- O O
nant O O
functions O O
, O O
based O O
on O O
least O O
squares O O
, O O
fisher O O
’ O O
s O O
linear B B
discriminant I I
, O O
and O O
the O O
percep- O O
tron O O
algorithm O O
. O O
an O O
alternative O O
approach O O
is O O
to O O
ﬁx O O
the O O
number O O
of O O
basis O O
functions O O
in O O
advance O O
but O O
allow O O
them O O
to O O
be O O
adaptive O O
, O O
in O O
other O O
words O O
to O O
use O O
parametric O O
forms O O
for O O
the O O
basis O B
func- O O
tions O O
in O O
which O O
the O O
parameter O O
values O O
are O O
adapted O O
during O O
training B B
. O O
the O O
variance B B
of O O
f O O
( O O
x O O
) O O
is O O
deﬁned O O
by O O
var O O
[ O O
f O O
] O O
= O O
e O O
( O O
1.37 O O
) O O
( O O
1.38 O O
) O O
exercise O O
1.5 O O
exercise O O
1.6 O O
and O O
provides O O
a O O
measure O O
of O O
how O O
much O O
variability O O
there O O
is O O
in O O
f O O
( O O
x O O
) O O
around O O
its O O
mean O B
value O O
e O O
[ O O
f O O
( O O
x O O
) O O
] O O
. O O
the O O
coefﬁcient O O
k O O
is O O
determined O O
by O O
multiplying O O
both O O
10.7. O O
expectation B B
propagation I I
509 O O
( O O
cid:6 O O
) O O
( O O
cid:4 O O
) O O
fj O O
( O O
θ O O
) O O
q O O
( O O
cid:6 O O
) O O
sides O O
of O O
( O O
10.199 O O
) O O
by O O
q O O
\i O O
( O O
θ O O
) O O
and O O
integrating O O
to O O
give O O
k O O
= O O
\j O O
( O O
θ O O
) O O
dθ O O
( O O
10.200 O O
) O O
where O O
we O O
have O O
used O O
the O O
fact O O
that O O
qnew O O
( O O
θ O O
) O O
is O O
normalized O O
. O O
1.2. O O
probability B B
theory O O
21 O O
1.2.3 O O
bayesian O O
probabilities O O
so O O
far O O
in O O
this O O
chapter O O
, O O
we O O
have O O
viewed O O
probabilities O O
in O O
terms O O
of O O
the O O
frequencies O O
of O O
random O O
, O O
repeatable O O
events O O
. O O
to O O
see O O
this O O
, O O
consider O O
182 O O
4. O O
linear O O
models O O
for O O
classification O O
figure O O
4.1 O O
illustration O O
of O O
the O O
geometry O O
of O O
a O O
linear B O
discriminant I I
function O O
in O O
two O O
dimensions O O
. O O
in O O
particular O O
, O O
we O O
consider O O
a O O
linear-gaussian O O
state B O
space I O
model I O
so O O
that O O
the O O
latent O O
variables O O
{ O O
zn O O
} O O
, O O
as O O
well O O
as O O
the O O
observed O O
variables O O
{ O O
xn O O
} O O
, O O
are O O
multi- O O
variate O O
gaussian O O
distributions O O
whose O O
means O O
are O O
linear O O
functions O O
of O O
the O O
states O O
of O O
their O O
parents O O
in O O
the O O
graph O O
. O O
it O O
can O O
, O O
however O O
, O O
be O O
shown O O
that O O
the O O
expected O O
committee B O
error O B
will O O
not O O
exceed O O
the O O
expected O O
error B B
of O O
the O O
constituent O O
models O O
, O O
so O O
that O O
ecom O O
( O O
cid:1 O O
) O O
eav O O
. O O
422 O O
8. O O
graphical O O
models O O
8.25 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
in O O
( O O
8.86 O O
) O O
, O O
we O O
veriﬁed O O
that O O
the O O
sum-product B B
algorithm I I
run O O
on O O
the O O
graph O O
in O O
figure O O
8.51 O O
with O O
node B B
x3 O O
designated O O
as O O
the O O
root B B
node I I
gives O O
the O O
correct O O
marginal B B
for O O
x2 O O
. O O
14.2 O O
( O O
( O O
cid:12 O O
) O O
) O O
the O O
expected O O
sum-of-squares B B
error I O
eav O O
for O O
a O O
simple O O
committee B O
model O O
can O O
be O O
deﬁned O O
by O O
( O O
14.10 O O
) O O
, O O
and O O
the O O
expected O O
error B B
of O O
the O O
committee B B
itself O O
is O O
given O O
by O O
( O O
14.11 O O
) O O
. O O
today O O
, O O
there O O
are O O
many O O
hundreds O O
, O O
if O O
not O O
thousands O O
, O O
of O O
applications O O
of O O
neural O O
networks O O
in O O
widespread O O
use O O
, O O
with O O
examples O O
in O O
areas O B
such O O
as O O
handwriting B O
recognition I O
and O O
information O O
retrieval O I
be- O O
ing O O
used O O
routinely O O
by O O
millions O O
of O O
people O O
. O O
we O O
see O O
that O O
the O O
mean B B
is O O
correctly O O
captured O O
but O O
that O O
the O O
variance B B
of O O
q O O
( O O
z O O
) O O
is O O
controlled O O
by O O
the O O
direction O O
of O O
smallest O O
variance B B
of O O
p O O
( O O
z O O
) O O
, O O
and O O
that O O
the O O
variance B B
along O O
the O O
orthogonal O O
direction O O
is O O
signiﬁcantly O O
under-estimated O O
. O O
robust O O
linear O B
programming O O
discrimination O O
of O O
two O O
linearly B O
separable I O
sets O O
. O O
by O O
contrast O O
, O O
if O O
we O O
were O O
to O O
minimize O O
kl O O
( O O
p O O
( O O
cid:5 O O
) O O
q O O
) O O
, O O
the O O
resulting O O
approximations O O
would O O
average O O
across O O
all O O
of O O
the O O
modes O O
and O O
, O O
in O O
the O O
context O O
of O O
the O O
mixture B B
model I I
, O O
would O O
lead O O
to O O
poor O O
predictive O O
distributions O O
( O O
because O O
the O O
average O O
of O O
two O O
good O O
parameter O O
values O O
is O O
typically O O
itself O O
not O O
a O O
good O O
parameter O O
value O O
) O O
. O O
this O O
function O O
can O O
be O O
represented O O
in O O
the O O
form O O
of O O
a O O
network O O
diagram O O
as O O
shown O O
in O O
figure O O
5.1. O O
the O O
process O O
of O O
evaluating O O
( O O
5.7 O O
) O O
can O O
then O O
be O O
interpreted O O
as O O
a O O
forward B B
propagation I I
of O O
information O B
through O O
the O O
network O O
. O O
this O O
can O O
be O O
seen O O
by O O
ﬁrst O O
left O O
multiplying O B
( O O
c.29 O O
) O O
by O O
( O O
u O O
( O O
cid:3 O O
) O O
i O O
) O O
t O O
, O O
where O O
( O O
cid:11 O O
) O O
denotes O O
the O O
complex O O
conjugate B B
, O O
to O O
give O O
( O O
u O O
( O O
cid:3 O O
) O O
i O O
) O O
t O O
aui O O
= O O
λi O O
( O O
u O O
( O O
cid:3 O O
) O O
i O O
) O O
t O O
ui O O
. O O
similarly O O
, O O
in O O
section O O
2.5.2 O O
we O O
introduced O O
a O O
simple O O
technique O O
for O O
classiﬁcation O O
called O O
nearest O O
neighbours O O
, O O
which O O
involved O O
assigning O O
to O O
each O O
new O O
test O O
vector O I
the O O
same O O
label O O
as O O
the O O
chapter O O
5 O O
section O O
2.5.1 O O
291 O O
292 O O
6. O O
kernel O O
methods O O
closest O O
example O O
from O O
the O O
training B B
set I I
. O O
image O O
and O O
vi- O O
sion B O
computing O O
2 O O
, O O
13–29 O O
. O O
note O O
that O O
each O O
hidden O O
and O O
output O O
unit O O
has O O
an O O
associated O O
bias B B
parameter I I
( O O
omitted O O
for O O
clarity O O
) O O
. O O
by O O
writing O O
down O O
the O O
form O O
of O O
the O O
kl O O
divergence O O
kl O O
( O O
p O O
( O O
cid:5 O O
) O O
q O O
) O O
for O O
a O O
gaussian O O
q O O
( O O
x O O
) O O
and O O
then O O
differentiating O O
, O O
show O O
that O O
518 O O
10. O O
approximate O O
inference B B
minimization O O
of O O
kl O O
( O O
p O O
( O O
cid:5 O O
) O O
q O O
) O O
with O O
respect O O
to O O
µ O O
and O O
σ O O
leads O O
to O O
the O O
result O O
that O O
µ O O
is O O
given O O
by O O
the O O
expectation B B
of O O
x O O
under O O
p O O
( O O
x O O
) O O
and O O
that O O
σ O O
is O O
given O O
by O O
the O O
covariance B B
. O O
all O O
of O O
the O O
probabilistic O O
infer- O O
ence O O
and O O
learning B B
manipulations O O
discussed O O
in O O
this O O
book O O
, O O
no O O
matter O O
how O O
complex O O
, O O
amount O O
to O O
repeated O O
application O O
of O O
these O O
two O O
equations O O
. O O
because O O
β O O
is O O
a O O
variance B B
, O O
and O O
hence O O
nonnegative O O
, O O
we O O
use O O
the O O
gaussian O O
process O O
to O O
model O O
ln O O
β O O
( O O
x O O
) O O
. O O
first O O
of O O
all O O
we O O
note O O
that O O
, O O
without O O
loss O O
of O O
generality O O
, O O
we O O
can O O
use O O
the O O
product B O
rule I I
to O O
express O O
the O O
joint O O
distribution O O
for O O
a O O
sequence O O
of O O
observations O O
in O O
the O O
form O O
n O O
( O O
cid:14 O O
) O O
p O O
( O O
x1 O O
, O O
. O O
( O O
4.32 O O
) O O
( O O
4.33 O O
) O O
( O O
4.34 O O
) O O
( O O
4.35 O O
) O O
from O O
( O O
4.32 O O
) O O
, O O
and O O
making O O
use O O
of O O
our O O
choice O O
of O O
target O O
coding O O
scheme O O
for O O
the O O
tn O O
, O O
we O O
obtain O O
an O O
expression O O
for O O
the O O
bias B B
in O O
the O O
form O O
where O O
we O O
have O O
used O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
w0 O O
= O O
−wtm O O
tn O O
= O O
n1 O O
n O O
n1 O O
− O O
n2 O O
n O O
n2 O O
= O O
0 O O
and O O
where O O
m O O
is O O
the O O
mean B B
of O O
the O O
total O O
data O O
set O O
and O O
is O O
given O O
by O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
m O O
= O O
1 O O
n O O
xn O O
= O O
1 O O
n O O
( O O
n1m1 O O
+ O O
n2m2 O O
) O O
. O O
the O O
em O O
algorithm O O
starts O O
by O O
choosing O O
some O O
initial O O
values O O
for O O
the O O
parameters O O
in O O
the O O
e O O
step O O
of O O
the O O
em O O
algorithm O O
, O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:26 O O
) O O
( O O
cid:27 O O
) O O
10.6. O O
variational B B
logistic O O
regression B B
501 O O
we O O
then O O
use O O
these O O
parameter O O
values O O
to O O
ﬁnd O O
the O O
posterior O O
distribution O O
over O O
w O O
, O O
which O O
is O O
given O O
by O O
( O O
10.156 O O
) O O
. O O
we O O
can O O
evaluate O O
the O O
evidence B O
function I I
analytically O O
and O O
then O O
set O O
its O O
derivative B B
equal O O
to O O
zero O O
to O O
obtain O O
re-estimation O O
equations O O
for O O
α O O
and O O
β O O
, O O
which O O
we O O
shall O O
do O O
in O O
section O O
3.5.2. O O
alternatively O O
we O O
use O O
a O O
technique O O
called O O
the O O
expectation B B
maximization I I
( O O
em O O
) O O
algorithm O O
, O O
which O O
will O O
be O O
dis- O O
cussed O O
in O O
section O O
9.3.4 O O
where O O
we O O
shall O O
also O O
show O O
that O O
these O O
two O O
approaches O O
converge O O
to O O
the O O
same O O
solution O O
. O O
examples O O
of O O
undirected B B
and O O
directed B B
trees O O
are O O
shown O O
in O O
figure O O
8.39 O O
( O O
a O O
) O O
and O O
8.39 O O
( O O
b O O
) O O
. O O
( O O
13.64 O O
) O O
( O O
13.65 O O
) O O
section O O
13.3 O O
13.2. O O
hidden O O
markov O O
models O O
629 O O
finally O O
, O O
we O O
note O O
that O O
there O O
is O O
an O O
alternative O O
formulation O O
of O O
the O O
forward-backward B B
algorithm I I
( O O
jordan O O
, O O
2007 O O
) O O
in O O
which O O
the O O
backward O O
pass O O
is O O
deﬁned O O
by O O
a O O
recursion O O
based O O
the O O
quantities O O
γ O O
( O O
zn O O
) O O
= O O
( O O
cid:1 O O
) O O
α O O
( O O
zn O O
) O O
( O O
cid:1 O O
) O O
β O O
( O O
zn O O
) O O
instead O O
of O O
using O O
( O O
cid:1 O O
) O O
β O O
( O O
zn O O
) O O
. O O
this O O
leads O O
to O O
a O O
sequential O O
update O O
in O O
which O O
, O O
for O O
each O O
data O O
point O O
xn O O
in O O
turn O O
, O O
we O O
update O O
the O O
nearest O O
prototype O O
µk O O
using O O
( O O
9.5 O O
) O O
µnew O O
k O O
= O O
µold O O
k O O
+ O O
ηn O O
( O O
xn O O
− O O
µold O O
k O O
) O O
where O O
ηn O O
is O O
the O O
learning B B
rate I O
parameter I O
, O O
which O O
is O O
typically O O
made O O
to O O
decrease O O
mono- O O
tonically O O
as O O
more O O
data O O
points O O
are O O
considered O O
. O O
we O O
see O O
that O O
the O O
factor O B
involving O O
the O O
derivative B B
of O O
the O O
logistic B B
sigmoid I I
has O O
cancelled O O
, O O
leading O O
to O O
a O O
simpliﬁed O O
form O O
for O O
the O O
gradient O O
of O O
the O O
log O O
likelihood O O
. O O
, O O
xn−1 O O
) O O
= O O
n O O
( O O
µ|µn−1 O O
, O O
σ2 O O
n−1 O O
) O O
and O O
multiplying O O
by O O
the O O
likelihood O B
func- O O
tion O O
p O O
( O O
xn|µ O O
) O O
= O O
n O O
( O O
xn|µ O O
, O O
σ2 O O
) O O
and O O
then O O
completing B B
the I O
square I I
and O O
normalizing O O
to O O
obtain O O
the O O
posterior O O
distribution O O
after O O
n O O
observations O O
. O O
ln O O
p O O
( O O
d|wml O O
) O O
− O O
m O O
1.4. O O
the O O
curse B B
of I I
dimensionality I I
in O O
the O O
polynomial B O
curve I I
ﬁtting I I
example O O
we O O
had O O
just O O
one O O
input O O
variable O O
x. O O
for O O
prac- O O
tical O O
applications O O
of O O
pattern O O
recognition O O
, O O
however O O
, O O
we O O
will O O
have O O
to O O
deal O O
with O O
spaces O O
34 O O
1. O O
introduction O O
figure O O
1.19 O O
scatter O O
plot O O
of O O
the O O
oil B O
ﬂow I O
data I O
for O O
input O O
variables O O
x6 O O
and O O
x7 O O
, O O
in O O
which O O
red O O
denotes O O
the O O
‘ O O
homoge- O O
nous O O
’ O O
class O O
, O O
green O O
denotes O O
the O O
‘ O O
annular O O
’ O O
class O O
, O O
and O O
blue O O
denotes O O
the O O
‘ O O
laminar O O
’ O O
class O O
. O O
9.2.1 O O
maximum B B
likelihood I I
suppose O O
we O O
have O O
a O O
data O O
set O O
of O O
observations O O
{ O O
x1 O O
, O O
. O O
the O O
usual O O
justiﬁcation O O
for O O
a O O
gaussian O O
approximation O O
to O O
a O O
posterior O O
distribution O O
is O O
that O O
the O O
true O O
posterior O O
will O O
tend O O
to O O
a O O
gaussian O O
as O O
the O O
number O O
of O O
data O O
points O O
increases O O
as O O
a O O
consequence O O
of O O
the O O
central B O
limit I O
theorem I O
. O O
p O O
( O O
x O O
, O O
y O O
) O O
= O O
1 O O
z O O
xiyi O O
i O O
( O O
8.42 O O
) O O
( O O
8.43 O O
) O O
we O O
now O O
ﬁx O O
the O O
elements O O
of O O
y O O
to O O
the O O
observed O O
values O O
given O O
by O O
the O O
pixels O O
of O O
the O O
noisy O O
image O O
, O O
which O O
implicitly O O
deﬁnes O O
a O O
conditional B B
distribution O O
p O O
( O O
x|y O O
) O O
over O O
noise- O O
free O O
images O O
. O O
thus O O
the O O
predictive B B
distribution I I
can O O
be O O
carried O O
forward O O
indeﬁnitely O O
using O O
a O O
ﬁxed O O
amount O O
of O O
storage O O
, O O
as O O
may O O
be O O
required O O
for O O
real-time O O
applications O O
. O O
in O O
the O O
regression B B
case O O
, O O
we O O
can O O
view O O
the O O
network O O
as O O
having O O
an O O
output O O
activation B B
function I I
that O O
is O O
the O O
identity O O
, O O
so O O
that O O
yk O O
= O O
ak O O
. O O
( O O
14.54 O O
) O O
14.4 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
by O O
making O O
use O O
of O O
jensen O O
’ O O
s O O
in O O
equality O O
( O O
1.115 O O
) O O
, O O
show O O
that O O
the O O
result O O
( O O
14.54 O O
) O O
derived O O
in O O
the O O
previous O O
exercise O O
hods O O
for O O
any O O
error B B
function I I
e O O
( O O
y O O
) O O
, O O
not O O
just O O
sum-of- O O
squares O O
, O O
provided O O
it O O
is O O
a O O
convex B B
function I I
of O O
y O O
. O O
( O O
10.168 O O
) O O
we O O
are O O
now O O
faced O O
with O O
an O O
analytically O O
intractable O O
integration O O
over O O
w O O
and O O
α O O
, O O
which O O
we O O
shall O O
tackle O O
by O O
using O O
both O O
the O O
local B B
and O O
global O O
variational O B
approaches O O
in O O
the O O
same O O
model O O
to O O
begin O O
with O O
, O O
we O O
introduce O O
a O O
variational B B
distribution O O
q O O
( O O
w O O
, O O
α O O
) O O
, O O
and O O
then O O
apply O O
the O O
decomposition O O
( O O
10.2 O O
) O O
, O O
which O O
in O O
this O O
instance O O
takes O O
the O O
form O O
( O O
10.169 O O
) O O
where O O
the O O
lower B B
bound I I
l O O
( O O
q O O
) O O
and O O
the O O
kullback-leibler O O
divergence O O
kl O O
( O O
q O O
( O O
cid:5 O O
) O O
p O O
) O O
are O O
de- O O
ﬁned O O
by O O
ln O O
p O O
( O O
t O O
) O O
= O O
l O O
( O O
q O O
) O O
+ O O
kl O O
( O O
q O O
( O O
cid:5 O O
) O O
p O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
q O O
( O O
w O O
, O O
α O O
) O O
ln O O
( O O
cid:12 O O
) O O
( O O
cid:13 O O
) O O
p O O
( O O
w O O
, O O
α O O
, O O
t O O
) O O
q O O
( O O
w O O
, O O
α O O
) O O
p O O
( O O
w O O
, O O
α|t O O
) O O
) O O
q O O
( O O
w O O
, O O
α O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:13 O O
) O O
dw O O
dα O O
kl O O
( O O
q O O
( O O
cid:5 O O
) O O
p O O
) O O
= O O
− O O
q O O
( O O
w O O
, O O
α O O
) O O
ln O O
( O O
10.171 O O
) O O
at O O
this O O
point O O
, O O
the O O
lower B B
bound I I
l O O
( O O
q O O
) O O
is O O
still O O
intractable O O
due O O
to O O
the O O
form O O
of O O
the O O
likelihood O B
factor O O
p O O
( O O
t|w O O
) O O
. O O
5.13 O O
( O O
( O O
cid:12 O O
) O O
) O O
show O O
that O O
as O O
a O O
consequence O O
of O O
the O O
symmetry O O
of O O
the O O
hessian O O
matrix O O
h O O
, O O
the O O
number O O
of O O
independent B B
elements O O
in O O
the O O
quadratic O O
error O O
function O I
( O O
5.28 O O
) O O
is O O
given O O
by O O
w O O
( O O
w O O
+ O O
3 O O
) O O
/2 O O
. O O
because O O
the O O
vector O O
k O O
is O O
a O O
function O O
of O O
the O O
test O O
point O O
input O O
value O O
xn O O
+1 O O
, O O
we O O
see O O
that O O
the O O
predictive O B
distribu- O O
tion O O
is O O
a O O
gaussian O O
whose O O
mean B B
and O O
variance B B
both O O
depend O O
on O O
xn O O
+1 O O
. O O
given O O
the O O
latent B B
variable I I
z. O O
in O O
essence O B
. O O
here O O
we O O
sim- O O
ply O O
note O O
that O O
the O O
presence O O
of O O
the O O
fisher O O
information O O
matrix O I
causes O O
this O O
kernel O O
to O O
be O O
invariant O O
under O O
a O O
nonlinear O O
re-parameterization O O
of O O
the O O
density B B
model O O
θ O O
→ O O
ψ O O
( O O
θ O O
) O O
. O O
this O O
property O O
also O O
holds O O
for O O
the O O
case O O
of O O
the O O
conditional B B
gaussian O O
dis- O O
tribution O O
p O O
( O O
t|x O O
, O O
w O O
, O O
β O O
) O O
of O O
the O O
linear B B
regression I I
model O O
. O O
we O O
see O O
that O O
once O O
again O O
the O O
mean B B
of O O
the O O
approximation O O
is O O
correct O O
, O O
but O O
that O O
it O O
places O O
signiﬁcant O O
probability B B
mass O O
in O O
regions O O
of O O
variable O O
space O O
that O O
have O O
very O O
low O O
probabil- O O
ity O O
. O O
, O O
xd O O
) O O
such O O
1.2. O O
probability B B
theory O O
19 O O
that O O
the O O
probability B B
of O O
x O O
falling O O
in O O
an O O
inﬁnitesimal O O
volume O O
δx O O
containing O O
the O O
point O O
x O O
is O O
given O O
by O O
p O O
( O O
x O O
) O O
δx O O
. O O
13.2.1 O O
maximum B B
likelihood I I
for O O
the O O
hmm O O
. O O
from O O
these O O
two O O
relationships O O
, O O
it O O
is O O
easily O O
shown O O
that O O
h O O
( O O
x O O
) O O
must O O
be O O
given O O
by O O
the O O
logarithm O O
of O O
p O O
( O O
x O O
) O O
and O O
so O O
we O O
have O O
exercise O O
1.28 O O
q O O
| O O
t O O
− O O
y O O
| O O
q O O
| O O
t O O
− O O
y O O
| O O
2 O O
1 O O
0 O O
−2 O O
2 O O
1 O O
0 O O
−2 O O
q O O
= O O
0.3 O O
−1 O O
0 O O
y O O
− O O
t O O
1 O O
2 O O
q O O
= O O
2 O O
−1 O O
0 O O
y O O
− O O
t O O
1 O O
2 O O
q O O
| O O
t O O
− O O
y O O
| O O
q O O
| O O
t O O
− O O
y O O
| O O
2 O O
1 O O
0 O O
−2 O O
2 O O
1 O O
0 O O
−2 O O
1.6. O O
information B O
theory I I
49 O O
q O O
= O O
1 O O
−1 O O
0 O O
y O O
− O O
t O O
1 O O
2 O O
q O O
= O O
10 O O
−1 O O
0 O O
y O O
− O O
t O O
1 O O
2 O O
figure O O
1.29 O O
plots O O
of O O
the O O
quantity O O
lq O O
= O O
|y O O
− O O
t|q O O
for O O
various O O
values O O
of O O
q. O O
h O O
( O O
x O O
) O O
= O O
− O O
log2 O O
p O O
( O O
x O O
) O O
( O O
1.92 O O
) O O
where O O
the O O
negative O O
sign O O
ensures O O
that O O
information O O
is O O
positive O B
or O O
zero O O
. O O
by O O
convention O O
the O O
( O O
inverse B B
) O O
regularization B B
parameter O O
, O O
denoted O O
c O O
, O O
appears O O
in O O
front O O
of O O
the O O
error B B
term O O
. O O
these O O
represent O O
error B B
signals O O
δ O O
for O O
each O O
pattern O O
and O O
for O O
each O O
output O O
unit O O
, O O
and O O
can O O
be O O
back- O O
propagated O O
to O O
the O O
hidden O O
units O O
and O O
the O O
error B B
function I I
derivatives O O
evaluated O O
in O O
the O O
usual O O
way O O
. O O
2.58 O O
( O O
( O O
cid:12 O O
) O O
) O O
the O O
result O O
( O O
2.226 O O
) O O
showed O O
that O O
the O O
negative O O
gradient O O
of O O
ln O O
g O O
( O O
η O O
) O O
for O O
the O O
exponen- O O
tial O O
family O O
is O O
given O O
by O O
the O O
expectation B B
of O O
u O O
( O O
x O O
) O O
. O O
nevertheless O O
, O O
these O O
nonparametric B O
methods I O
are O O
still O O
severely O O
limited O O
. O O
we O O
are O O
also O O
interested O O
in O O
the O O
gaussian O O
distribution O O
deﬁned O O
over O O
a O O
d-dimensional O O
vector O O
x O O
of O O
continuous O O
variables O O
, O O
which O O
is O O
given O O
by O O
n O O
( O O
x|µ O O
, O O
σ O O
) O O
= O O
1 O O
1 O O
( O O
x O O
− O O
µ O O
) O O
tς O O
−1 O O
( O O
x O O
− O O
µ O O
) O O
|σ|1/2 O O
exp O O
( O O
2π O O
) O O
d/2 O O
( O O
1.52 O O
) O O
where O O
the O O
d-dimensional O O
vector O O
µ O O
is O O
called O O
the O O
mean B B
, O O
the O O
d O O
× O O
d O O
matrix O O
σ O O
is O O
called O O
the O O
covariance B B
, O O
and O O
|σ| O O
denotes O O
the O O
determinant O O
of O O
σ. O O
we O O
shall O O
make O O
use O O
of O O
the O O
multivariate O O
gaussian O O
distribution O O
brieﬂy O O
in O O
this O O
chapter O O
, O O
although O O
its O O
properties O O
will O O
be O O
studied O O
in O O
detail O O
in O O
section O O
2.3 O O
. O O
for O O
such O O
models O O
, O O
we O O
can O O
therefore O O
obtain O O
the O O
predictive B B
distribution I I
either O O
by O O
taking O O
a O O
parameter O O
space O O
viewpoint O O
and O O
using O O
the O O
linear B B
regression I I
result O O
or O O
by O O
taking O O
a O O
function O O
space O O
viewpoint O O
and O O
using O O
the O O
gaussian O O
process O O
result O O
. O O
thus O O
we O O
can O O
evaluate O O
k O O
using O O
only O O
the O O
kernel B B
function I I
and O O
then O O
use O O
k O O
to O O
determine O O
the O O
eigenvalues O O
and O O
eigenvectors O O
. O O
we O O
can O O
now O O
interpret O O
p O O
( O O
ck O O
) O O
as O O
the O O
prior B B
probability O O
for O O
the O O
class O O
ck O O
, O O
and O O
p O O
( O O
ck|x O O
) O O
as O O
the O O
corresponding O O
posterior B O
probability I I
. O O
the O O
right-hand O O
plot O O
shows O O
the O O
corresponding O O
feature B B
space I I
( O O
φ1 O O
, O O
φ2 O O
) O O
together O O
with O O
the O O
linear O B
decision O I
boundary O I
obtained O O
given O O
by O O
a O O
logistic B B
regression I I
model O O
of O O
the O O
form O O
discussed O O
in O O
section O O
4.3.2. O O
this O O
corresponds O O
to O O
a O O
nonlinear O O
decision B B
boundary I I
in O O
the O O
original O O
input O O
space O O
, O O
shown O O
by O O
the O O
black O O
curve O O
in O O
the O O
left-hand O O
plot O O
. O O
( O O
3.39 O O
) O O
we O O
now O O
take O O
the O O
expectation B B
of O O
this O O
expression O O
with O O
respect O O
to O O
d O O
and O O
note O O
that O O
the O O
ﬁnal O O
term O O
will O O
vanish O O
, O O
giving O O
( O O
cid:8 O O
) O O
{ O O
y O O
( O O
x O O
; O O
d O O
) O O
− O O
h O O
( O O
x O O
) O O
} O O
2 O O
( O O
) O O
* O O
ed O O
= O O
{ O O
ed O O
[ O O
y O O
( O O
x O O
; O O
d O O
) O O
] O O
− O O
h O O
( O O
x O O
) O O
} O O
2 O O
( O O
cid:9 O O
) O O
( O O
cid:8 O O
) O O
{ O O
y O O
( O O
x O O
; O O
d O O
) O O
− O O
ed O O
[ O O
y O O
( O O
x O O
; O O
d O O
) O O
] O O
} O O
2 O O
) O O
* O O
( O O
cid:9 O O
) O O
+ O O
+ O O
( O O
+ O O
ed O O
. O O
, O O
xn O O
) O O
( O O
13.55 O O
) O O
which O O
we O O
expect O O
to O O
be O O
well O O
behaved O O
numerically O O
because O O
it O O
is O O
a O O
probability B B
distribu- O O
tion O O
over O O
k O O
variables O O
for O O
any O O
value O O
of O O
n. O O
in O O
order O O
to O O
relate O O
the O O
scaled O O
and O O
original O O
al- O O
pha O O
variables O O
, O O
we O O
introduce O O
scaling O O
factors O O
deﬁned O O
by O O
conditional B B
distributions O O
over O O
the O O
observed O O
variables O O
cn O O
= O O
p O O
( O O
xn|x1 O O
, O O
. O O
a O O
second O O
approach O O
uses O O
expectation B B
propagation I I
( O O
opper O O
and O O
winther O O
, O O
2000b O O
; O O
minka O O
, O O
2001b O O
; O O
seeger O O
, O O
2003 O O
) O O
. O O
similarly O O
, O O
arma O O
( O O
autoregressive O O
moving O O
aver- O O
age O O
) O O
models O O
, O O
kalman O O
ﬁlters O O
, O O
and O O
radial B O
basis I B
function I I
networks O O
can O O
all O O
be O O
viewed O O
as O O
forms O O
of O O
gaussian O O
process O O
models O O
. O O
consider O O
the O O
fragment O O
of O O
graph O O
shown O O
in O O
figure O O
8.46 O O
in O O
which O O
we O O
see O O
that O O
the O O
tree B B
structure O O
of O O
the O O
graph O O
allows O O
us O O
to O O
partition O O
the O O
factors O O
in O O
the O O
joint O O
distribution O O
into O O
groups O O
, O O
with O O
one O O
group O O
associated O O
with O O
each O O
of O O
the O O
factor O B
nodes O O
that O O
is O O
a O O
neighbour O O
of O O
the O O
variable O O
node B B
x. O O
we O O
see O O
that O O
the O O
joint O O
distribution O O
can O O
be O O
written O O
as O O
a O O
product O O
of O O
the O O
form O O
p O O
( O O
x O O
) O O
= O O
fs O O
( O O
x O O
, O O
xs O O
) O O
( O O
8.62 O O
) O O
( O O
cid:14 O O
) O O
s∈ne O O
( O O
x O O
) O O
ne O O
( O O
x O O
) O O
denotes O O
the O O
set O O
of O O
factor O B
nodes O O
that O O
are O O
neighbours O O
of O O
x O O
, O O
and O O
xs O O
denotes O O
the O O
set O O
of O O
all O O
variables O O
in O O
the O O
subtree O O
connected O O
to O O
the O O
variable O O
node B B
x O O
via O O
the O O
factor O B
node O O
404 O O
8. O O
graphical O O
models O O
figure O O
8.46 O O
a O O
fragment O O
of O O
a O O
factor B B
graph I I
illustrating O O
the O O
evaluation O O
of O O
the O O
marginal B B
p O O
( O O
x O O
) O O
. O O
for O O
more O O
practical O O
exam- O O
ples O O
, O O
where O O
the O O
desired O O
distribution O O
may O O
be O O
multimodal O O
and O O
sharply O O
peaked O O
, O O
it O O
will O O
be O O
extremely O O
difﬁcult O O
to O O
ﬁnd O O
a O O
good O O
proposal B B
distribution I I
and O O
comparison O O
function O O
. O O
by O O
taking O O
general O O
parametric O O
forms O O
for O O
these O O
distributions O O
we O O
can O O
derive O O
the O O
form O O
of O O
the O O
lower B B
bound I I
as O O
a O O
function O O
of O O
the O O
parameters O O
of O O
the O O
distributions O O
. O O
however O O
, O O
the O O
integrand O O
as O O
a O O
function O O
of O O
w O O
typically O O
has O O
a O O
strongly O O
skewed O O
mode O O
so O O
that O O
the O O
laplace O O
approximation O O
fails O O
to O O
capture O O
the O O
bulk O O
of O O
the O O
probability B B
mass O O
, O O
leading O O
to O O
poorer O O
re- O O
sults O O
than O O
those O O
obtained O O
by O O
maximizing O O
the O O
evidence O B
( O O
mackay O O
, O O
1999 O O
) O O
. O O
such O O
transformations O O
produce O O
signiﬁcant O O
changes O O
in O O
the O O
raw O O
data O O
, O O
expressed O O
in O O
terms O O
of O O
the O O
intensities O O
at O O
each O O
of O O
the O O
pixels O O
in O O
the O O
image O O
, O O
and O O
yet O O
should O O
give O O
rise O O
to O O
the O O
same O O
output O O
from O O
the O O
classiﬁcation B B
system O O
. O O
in O O
the O O
case O O
of O O
directed B B
graphs O O
, O O
a O O
tree B B
is O O
deﬁned O O
such O O
that O O
there O O
is O O
a O O
single O O
node B B
, O O
called O O
the O O
root O B
, O O
which O O
has O O
no O O
parents O O
, O O
and O O
all O O
other O O
nodes O O
have O O
one O O
parent O O
. O O
this O O
is O O
an O O
example O O
of O O
an O O
induced B O
factorization I B
. O O
4.3.1 O O
fixed O O
basis O B
functions O O
so O O
far O O
in O O
this O O
chapter O O
, O O
we O O
have O O
considered O O
classiﬁcation B B
models O O
that O O
work O O
di- O O
rectly O O
with O O
the O O
original O O
input O O
vector O O
x. O O
however O O
, O O
all O O
of O O
the O O
algorithms O O
are O O
equally O O
applicable O O
if O O
we O O
ﬁrst O O
make O O
a O O
ﬁxed O O
nonlinear O O
transformation O O
of O O
the O O
inputs O O
using O O
a O O
vector O O
of O O
basis O O
functions O O
φ O O
( O O
x O O
) O O
. O O
( O O
xb O O
− O O
µb O O
) O O
tλba O O
( O O
xa O O
− O O
µa O O
) O O
− O O
1 O O
2 O O
( O O
2.70 O O
) O O
we O O
see O O
that O O
as O O
a O O
function O O
of O O
xa O O
, O O
this O O
is O O
again O O
a O O
quadratic O O
form O O
, O O
and O O
hence O O
the O O
cor- O O
responding O O
conditional B B
distribution O O
p O O
( O O
xa|xb O O
) O O
will O O
be O O
gaussian O O
. O O
let O O
us O O
start O O
with O O
a O O
discussion O O
of O O
histogram O O
methods O O
for O O
density O O
estimation O I
, O O
which O O
we O O
have O O
already O O
encountered O O
in O O
the O O
context O O
of O O
marginal B B
and O O
conditional B B
distributions O O
in O O
figure O O
1.11 O O
and O O
in O O
the O O
context O O
of O O
the O O
central B O
limit I O
theorem I O
in O O
figure O O
2.6. O O
here O O
we O O
explore O O
the O O
properties O O
of O O
histogram O O
density O B
models O O
in O O
more O O
detail O O
, O O
focussing O O
on O O
the O O
case O O
of O O
a O O
single O O
continuous O O
variable O O
x. O O
standard O O
histograms O O
simply O O
partition O O
x O O
into O O
distinct O O
bins O O
of O O
width O O
∆i O O
and O O
then O O
count O O
the O O
number O O
ni O O
of O O
observations O O
of O O
x O O
falling O O
in O O
bin O O
i. O O
in O O
order O O
to O O
turn O O
this O O
count O O
into O O
a O O
normalized O O
probability O B
density B O
, O O
we O O
simply O O
divide O O
by O O
the O O
total O O
number O O
n O O
of O O
observations O O
and O O
by O O
the O O
width O O
∆i O O
of O O
the O O
bins O O
to O O
obtain O O
probability B B
values O O
for O O
each O O
bin O O
given O O
by O O
pi O O
= O O
ni O O
n∆i O O
( O O
2.241 O O
) O O
p O O
( O O
x O O
) O O
dx O O
= O O
1. O O
this O O
gives O O
a O O
model O O
for O O
the O O
density B B
for O O
which O O
it O O
is O O
easily O O
seen O O
that O O
p O O
( O O
x O O
) O O
that O O
is O O
constant O O
over O O
the O O
width O O
of O O
each O O
bin O O
, O O
and O O
often O O
the O O
bins O O
are O O
chosen O O
to O O
have O O
the O O
same O O
width O O
∆i O O
= O O
∆ O O
. O O
the O O
exploration B O
of O O
state O O
space O I
then O O
proceeds O O
by O O
a O O
random O O
walk O O
and O O
takes O O
of O O
order O O
( O O
σmax/σmin O O
) O O
2 O O
steps O O
to O O
arrive O O
at O O
a O O
roughly O O
independent B B
state O I
. O O
if O O
all O O
of O O
the O O
eigenvalues O O
are O O
nonnegative O O
, O O
then O O
the O O
covariance B B
matrix I I
is O O
said O O
to O O
be O O
positive O B
semideﬁnite O O
. O O
show O O
that O O
the O O
number O O
of O O
independent B B
parameters O O
in O O
the O O
matrix O O
ws O O
ij O O
is O O
given O O
by O O
d O O
( O O
d O O
+ O O
1 O O
) O O
/2 O O
. O O
vari- O O
ational O O
bayesian O O
learning B B
of O O
ica O O
with O O
missing B O
data I I
. O O
we O O
shall O O
assume O O
for O O
the O O
moment O O
that O O
the O O
training B B
data O O
set O O
is O O
linearly B B
separable I I
in O O
feature B O
space I I
, O O
so O O
that O O
by O O
deﬁnition O O
there O O
exists O O
at O O
least O O
one O O
choice O O
of O O
the O O
parameters O O
w O O
and O O
b O O
such O O
that O O
a O O
function O O
of O O
the O O
form O O
( O O
7.1 O O
) O O
satisﬁes O O
y O O
( O O
xn O O
) O O
> O O
0 O O
for O O
points O O
having O O
tn O O
= O O
+1 O O
and O O
y O O
( O O
xn O O
) O O
< O O
0 O O
for O O
points O O
having O O
tn O O
= O O
−1 O O
, O O
so O O
that O O
tny O O
( O O
xn O O
) O O
> O O
0 O O
for O O
all O O
training B B
data O O
points O O
. O O
an O O
alternative O O
approach O O
to O O
backpropagation B B
for O O
computing O O
the O O
derivatives O O
of O O
the O O
error B B
function I I
is O O
to O O
use O O
ﬁnite B O
differences I O
. O O
how- O O
ever O O
, O O
for O O
numerical O O
reasons O O
it O O
is O O
convenient O O
to O O
introduce O O
a O O
noise-like O O
term O O
governed O O
by O O
a O O
parameter O O
ν O O
that O O
ensures O O
that O O
the O O
covariance B B
matrix I I
is O O
positive B B
deﬁnite I I
. O O
10.7.2 O O
expectation B B
propagation I I
on O O
graphs O O
. O O
the O O
problem O O
can O O
be O O
resolved O O
by O O
adopting O O
a O O
rather O O
different O O
kind O O
of O O
message B B
passing I I
from O O
the O O
root B O
node I I
to O O
the O O
leaves O O
. O O
exercise O O
5.28 O O
270 O O
5. O O
neural O O
networks O O
section O O
2.3.9 O O
exercise O O
5.29 O O
recall O O
that O O
the O O
simple O O
weight B B
decay I O
regularizer O O
, O O
given O O
in O O
( O O
5.112 O O
) O O
, O O
can O O
be O O
viewed O O
as O O
the O O
negative O O
log O O
of O O
a O O
gaussian O O
prior B B
distribution O O
over O O
the O O
weights O O
. O O
such O O
models O O
have O O
particularly O O
nice O O
properties O O
if O O
we O O
choose O O
the O O
relationship O O
be- O O
tween O O
each O O
parent-child O O
pair O O
in O O
a O O
directed B B
graph O O
to O O
be O O
conjugate B B
, O O
and O O
we O O
shall O O
ex- O O
plore O O
several O O
examples O O
of O O
this O O
shortly O O
. O O
9.3.4 O O
em O O
for O O
bayesian O O
linear B O
regression I I
as O O
a O O
third O O
example O O
of O O
the O O
application O O
of O O
em O O
, O O
we O O
return O O
to O O
the O O
evidence O B
ap- O O
proximation B B
for O O
bayesian O O
linear B O
regression I I
. O O
this O O
simple O O
classiﬁer O O
corresponds O O
to O O
a O O
form O O
of O O
decision B B
tree I I
known O O
as O O
a O O
‘ O O
decision O B
stumps O I
’ O O
, O O
i.e. O O
, O O
a O O
deci- O O
sion B B
tree O O
with O O
a O O
single O O
node B B
. O O
and O O
ﬁt O O
the O O
emission O O
density O O
by O O
maximum B B
likelihood I I
, O O
and O O
then O O
use O O
the O O
resulting O O
values O O
to O O
ini- O O
tialize O O
the O O
parameters O O
for O O
em O O
. O O
w O O
( O O
τ O O
) O O
j O O
|w O O
( O O
5.199 O O
) O O
( O O
5.200 O O
) O O
compare O O
this O O
result O O
with O O
the O O
discussion O O
in O O
section O O
3.5.3 O O
of O O
regularization B B
with O O
simple O O
weight B B
decay I O
, O O
and O O
hence O O
show O O
that O O
( O O
ρτ O O
) O O
−1 O O
is O O
analogous O O
to O O
the O O
regularization B B
param- O O
eter O O
λ. O O
the O O
above O O
results O O
also O O
show O O
that O O
the O O
effective B O
number I I
of I I
parameters I I
in O O
the O O
network O O
, O O
as O O
deﬁned O O
by O O
( O O
3.91 O O
) O O
, O O
grows O O
as O O
the O O
training B B
progresses O O
. O O
this O O
gives O O
a O O
discrete O O
distribution O O
for O O
which O O
the O O
entropy B B
takes O O
the O O
form O O
h∆ O O
= O O
− O O
p O O
( O O
xi O O
) O O
∆ O O
ln O O
( O O
p O O
( O O
xi O O
) O O
∆ O O
) O O
= O O
− O O
p O O
( O O
xi O O
) O O
∆ O O
ln O O
p O O
( O O
xi O O
) O O
− O O
ln O O
∆ O O
( O O
1.102 O O
) O O
( O O
cid:2 O O
) O O
i O O
( O O
cid:2 O O
) O O
i O O
( O O
cid:5 O O
) O O
i O O
p O O
( O O
xi O O
) O O
∆ O O
= O O
1 O O
, O O
which O O
follows O O
from O O
( O O
1.101 O O
) O O
. O O
also O O
, O O
the O O
determinant O O
of O O
an O O
inverse B B
matrix O I
is O O
given O O
by O O
|ab| O O
= O O
|a||b| O O
1 O O
|a| O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
= O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
a−1 O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
im O O
+ O O
atb O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
= O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
in O O
+ O O
abt O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
= O O
1 O O
+ O O
atb O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
in O O
+ O O
abt O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
. O O
in O O
many O O
signal O O
processing O O
applications O O
, O O
it O O
is O O
of O O
interest O O
to O O
consider O O
ba- O O
sis O O
functions O O
that O O
are O O
localized O O
in O O
both O O
space O O
and O O
frequency O O
, O O
leading O O
to O O
a O O
class O O
of O O
functions O O
known O O
as O O
wavelets B B
. O O
making O O
use O O
of O O
the O O
result O O
( O O
4.106 O O
) O O
for O O
the O O
derivatives O O
of O O
the O O
softmax B O
function I I
, O O
we O O
obtain O O
∇wj O O
e O O
( O O
w1 O O
, O O
. O O
if O O
we O O
choose O O
an O O
initial O O
point O O
from O O
the O O
distribution O O
( O O
11.63 O O
) O O
and O O
then O O
update O O
it O O
using O O
l O O
leapfrog O O
interactions O O
, O O
the O O
probability B B
of O O
the O O
transition O O
going O O
from O O
r O O
to O O
r O O
( O O
cid:4 O O
) O O
is O O
given O O
by O O
1 O O
zh O O
exp O O
( O O
−h O O
( O O
r O O
) O O
) O O
δv O O
1 O O
2 O O
min O O
{ O O
1 O O
, O O
exp O O
( O O
−h O O
( O O
r O O
) O O
+ O O
h O O
( O O
r O O
( O O
cid:4 O O
) O O
) O O
) O O
} O O
. O O
each O O
image O O
is O O
a O O
point O O
figure O O
1.23 O O
plot O O
of O O
the O O
probability B B
density O O
with O O
to O O
radius O O
r O O
of O O
a O O
gaus- O O
respect O O
sian O O
distribution O O
for O O
various O O
values O O
of O O
in O O
a O O
high-dimensional O O
space O O
, O O
most O O
of O O
the O O
probability B B
mass O O
of O O
a O O
gaussian O O
is O O
lo- O O
cated O O
within O O
a O O
thin O O
shell O O
at O O
a O O
speciﬁc O O
radius O O
. O O
this O O
result O O
is O O
illustrated O O
for O O
two O O
classes O O
, O O
and O O
a O O
single O O
input O O
variable O O
x O O
, O O
in O O
figure O O
1.24. O O
for O O
the O O
more O O
general O O
case O O
of O O
k O O
classes O O
, O O
it O O
is O O
slightly O O
easier O O
to O O
maximize O O
the O O
probability B B
of O O
being O O
correct O O
, O O
which O O
is O O
given O O
by O O
k O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
k=1 O O
rk O O
k=1 O O
p O O
( O O
x O O
∈ O O
rk O O
, O O
ck O O
) O O
( O O
cid:6 O O
) O O
p O O
( O O
correct O O
) O O
= O O
= O O
p O O
( O O
x O O
, O O
ck O O
) O O
dx O O
( O O
1.79 O O
) O O
which O O
is O O
maximized O O
when O O
the O O
regions O O
rk O O
are O O
chosen O O
such O O
that O O
each O O
x O O
is O O
assigned O O
to O O
the O O
class O O
for O O
which O O
p O O
( O O
x O O
, O O
ck O O
) O O
is O O
largest O O
. O O
duality O O
will O O
also O O
play O O
an O O
important O O
role O O
when O O
we O O
discuss O O
support B B
vector I I
machines O O
in O O
chapter O O
7. O O
exercise O O
6.1 O O
exercise O O
6.2 O O
6.2. O O
constructing O O
kernels O O
in O O
order O O
to O O
exploit O O
kernel B O
substitution I O
, O O
we O O
need O O
to O O
be O O
able O O
to O O
construct O O
valid O O
kernel O O
functions O O
. O O
if O O
we O O
wish O O
to O O
obtain O O
independent B B
samples O O
, O O
then O O
we O O
can O O
discard O O
most O O
of O O
the O O
sequence O O
and O O
just O O
re- O O
tain O B
every O O
m O O
th O O
sample O O
. O O
maximizing O O
the O O
log O O
likelihood O O
function O O
( O O
9.14 O O
) O O
for O O
a O O
gaussian O O
mixture B B
model I I
turns O O
out O O
to O O
be O O
a O O
more O O
complex O O
problem O O
than O O
for O O
the O O
case O O
of O O
a O O
single O O
gaussian O O
. O O
conditional B B
independence I I
properties O O
play O O
an O O
important O O
role O O
in O O
using O O
probabilis- O O
tic O O
models O O
for O O
pattern O O
recognition O O
by O O
simplifying O O
both O O
the O O
structure O O
of O O
a O O
model O O
and O O
the O O
computations O O
needed O O
to O O
perform O O
inference B B
and O O
learning B B
under O O
that O O
model O O
. O O
−∞ O O
the O O
result O O
( O O
6.45 O O
) O O
is O O
known O O
as O O
the O O
nadaraya-watson O B
model O O
, O O
or O O
kernel B O
regression I I
( O O
nadaraya O O
, O O
1964 O O
; O O
watson O O
, O O
1964 O O
) O O
. O O
( O O
τ O O
( O O
cid:4 O O
) O O
) O O
t O O
∇y O O
( O O
x O O
) O O
+ O O
τ O O
t∇∇y O O
( O O
x O O
) O O
τ O O
because O O
the O O
distribution O O
of O O
transformations O O
has O O
zero O O
mean B B
we O O
have O O
e O O
[ O O
ξ O O
] O O
= O O
0. O O
also O O
, O O
we O O
shall O O
denote O O
e O O
[ O O
ξ2 O O
] O O
by O O
λ. O O
omitting O O
terms O O
of O O
o O O
( O O
ξ3 O O
) O O
, O O
the O O
average O O
error B B
function I I
then O O
becomes O O
( O O
5.131 O O
) O O
where O O
e O O
is O O
the O O
original O O
sum-of-squares B O
error I O
, O O
and O O
the O O
regularization B B
term O O
ω O O
takes O O
the O O
form O O
( O O
cid:4 O O
) O O
e O O
= O O
e O O
+ O O
λω O O
ω O O
= O O
( O O
τ O O
( O O
cid:4 O O
) O O
) O O
t O O
∇y O O
( O O
x O O
) O O
+ O O
τ O O
t∇∇y O O
( O O
x O O
) O O
τ O O
( O O
cid:19 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:29 O O
) O O
( O O
cid:10 O O
) O O
+ O O
{ O O
y O O
( O O
x O O
) O O
− O O
e O O
[ O O
t|x O O
] O O
} O O
1 O O
2 O O
( O O
cid:30 O O
) O O
( O O
cid:11 O O
) O O
2 O O
τ O O
t∇y O O
( O O
x O O
) O O
p O O
( O O
x O O
) O O
dx O O
( O O
5.132 O O
) O O
in O O
which O O
we O O
have O O
performed O O
the O O
integration O O
over O O
t. O O
5.5. O O
regularization B B
in O O
neural O O
networks O O
267 O O
we O O
can O O
further O O
simplify O O
this O O
regularization B B
term O O
as O O
follows O O
. O O
in O O
fact O O
, O O
it O O
is O O
deﬁned O O
to O O
be O O
as O O
one O O
less O O
than O O
the O O
size O O
of O O
the O O
largest O O
clique B B
, O O
to O O
ensure O O
that O O
a O O
tree B B
has O O
a O O
treewidth B O
of O O
1. O O
because O O
there O O
in O O
general O O
there O O
can O O
be O O
multiple O O
different O O
junction O O
trees O O
that O O
can O O
be O O
constructed O O
from O O
a O O
given O O
starting O O
graph O O
, O O
the O O
treewidth B B
is O O
deﬁned O O
by O O
the O O
junction O B
tree O I
for O O
which O O
the O O
largest O O
clique B B
has O O
the O O
fewest O O
variables O O
. O O
for O O
large O O
training B B
data O O
sets O O
, O O
however O O
, O O
the O O
direct O O
application O O
of O O
gaussian O O
process O O
methods O O
can O O
become O O
infeasible O O
, O O
and O O
so O O
a O O
range O O
of O O
approximation O O
schemes O O
have O O
been O O
developed O O
that O O
have O O
better O O
scaling O O
with O O
training B B
set I I
size O O
than O O
the O O
exact O O
approach O O
( O O
gibbs O O
, O O
1997 O O
; O O
tresp O O
, O O
2001 O O
; O O
smola O O
and O O
bartlett O O
, O O
2001 O O
; O O
williams O O
and O O
seeger O O
, O O
2001 O O
; O O
csat´o O O
and O O
opper O O
, O O
2002 O O
; O O
seeger O O
et O O
al. O O
, O O
2003 O O
) O O
. O O
( O O
2003 O O
) O O
found O O
that O O
using O O
the O O
cross-entropy B B
error I I
function I I
instead O O
of O O
the O O
sum-of-squares O B
for O O
a O O
classiﬁcation B B
problem O O
leads O O
to O O
faster O O
training B B
as O O
well O O
as O O
improved O O
generalization B B
. O O
similarly O O
, O O
the O O
linear B B
dynamical I I
system I I
, O O
used O O
to O O
model O O
time O O
series O O
data O O
for O O
applications O O
such O O
as O O
tracking O O
, O O
is O O
also O O
a O O
joint O O
gaussian O O
distribution O O
over O O
a O O
potentially O O
large O O
number O O
of O O
observed O O
and O O
latent O O
variables O O
and O O
again O O
is O O
tractable O O
due O O
to O O
the O O
structure O O
imposed O O
on O O
the O O
distribution O O
. O O
however O O
, O O
numerical O O
differentiation O O
plays O O
an O O
important O O
role O O
in O O
practice O O
, O O
because O O
a O O
comparison O O
of O O
the O O
derivatives O O
calculated O O
by O O
backpropagation B B
with O O
those O O
obtained O O
us- O O
ing O O
central O O
differences O O
provides O O
a O O
powerful O O
check O O
on O O
the O O
correctness O O
of O O
any O O
software O O
implementation O O
of O O
the O O
backpropagation B B
algorithm O O
. O O
which O O
is O O
easily O O
visualized O O
for O O
d O O
= O O
2 O O
and O O
m O O
= O O
1. O O
coo O O
, O O
ider O O
a O O
collectioo O O
nf O O
data O O
point' O O
, O O
n O O
twi O O
) O O
dimension O O
' O O
, O O
aod O O
let O O
tile O O
u O O
'' O O
'' O O
'-dimensiunal O O
principal B O
subspace I O
be O O
represented O O
by O O
a O O
< O O
ohd O O
rod O O
. O O
normal O O
the O O
normal B O
distribution I O
is O O
simply O O
another O O
name O O
for O O
the O O
gaussian O O
. O O
12.2.4 O O
factor B O
analysis I I
factor O O
analysis O I
is O O
a O O
linear-gaussian O O
latent B B
variable I I
model O O
that O O
is O O
closely O O
related O O
to O O
probabilistic O O
pca O O
. O O
it O O
may O O
also O O
lead O O
to O O
improved O O
predictive O O
performance O O
, O O
particularly O O
when O O
the O O
class-conditional O O
density B B
assumptions O O
give O O
a O O
poor O O
approximation O O
to O O
the O O
true O O
dis- O O
tributions O O
. O O
the O O
ﬁrst O O
stage O O
in O O
applying O O
the O O
laplace O O
framework O O
to O O
this O O
model O O
is O O
to O O
initialize O O
the O O
hyperparameter B B
α O O
, O O
and O O
then O O
to O O
determine O O
the O O
parameter O O
vector O O
w O O
by O O
maximizing O O
the O O
log O O
posterior O O
distribution O O
. O O
substituting O O
for O O
the O O
gaussian O O
distribution O O
, O O
we O O
see O O
that O O
the O O
function O O
q O O
( O O
θ O O
, O O
θold O O
) O O
, O O
as O O
a O O
function O O
of O O
the O O
parameter O O
vector O O
wk O O
, O O
takes O O
the O O
form O O
( O O
cid:12 O O
) O O
n O O
( O O
cid:2 O O
) O O
γnk O O
n=1 O O
( O O
cid:10 O O
) O O
( O O
cid:13 O O
) O O
( O O
cid:11 O O
) O O
2 O O
q O O
( O O
θ O O
, O O
θold O O
) O O
= O O
− O O
β O O
2 O O
tn O O
− O O
wt O O
k O O
φn O O
+ O O
const O O
( O O
14.39 O O
) O O
where O O
the O O
constant O O
term O O
includes O O
the O O
contributions O O
from O O
other O O
weight O O
vectors O O
wj O O
for O O
j O O
( O O
cid:9 O O
) O O
= O O
k. O O
note O O
that O O
the O O
quantity O O
we O O
are O O
maximizing O O
is O O
similar O O
to O O
the O O
( O O
negative O O
of O O
the O O
) O O
standard O O
sum-of-squares O O
error B O
( O O
3.12 O O
) O O
for O O
a O O
single O O
linear B O
regression I I
model O O
, O O
but O O
with O O
the O O
inclusion O O
of O O
the O O
responsibilities O O
γnk O O
. O O
( O O
5.179 O O
) O O
note O O
that O O
this O O
result O O
was O O
exact O O
for O O
the O O
linear B B
regression I I
case O O
. O O
here O O
we O O
shall O O
consider O O
only O O
the O O
sum-product B B
algorithm I I
because O O
it O O
is O O
simpler O O
to O O
derive O O
and O O
to O O
apply O O
, O O
as O O
well O O
as O O
being O O
more O O
general O O
. O O
( O O
7.2 O O
) O O
the O O
margin B B
is O O
given O O
by O O
the O O
perpendicular O O
distance O O
to O O
the O O
closest O O
point O O
xn O O
from O O
the O O
data O O
set O O
, O O
and O O
we O O
wish O O
to O O
optimize O O
the O O
parameters O O
w O O
and O O
b O O
in O O
order O O
to O O
maximize O O
this O O
distance O O
. O O
we O O
can O O
also O O
use O O
the O O
em O O
algorithm O O
to O O
maximize O O
the O O
posterior O O
distribution O O
p O O
( O O
θ|x O O
) O O
for O O
models O O
in O O
which O O
we O O
have O O
introduced O O
a O O
prior B B
p O O
( O O
θ O O
) O O
over O O
the O O
parameters O O
. O O
in O O
many O O
applications O O
, O O
however O O
, O O
the O O
supply O O
of O O
data O O
for O O
training B O
and O O
testing O O
will O O
be O O
limited O O
, O O
and O O
in O O
order O O
to O O
build O O
good O O
models O O
, O O
we O O
wish O O
to O O
use O O
as O O
much O O
of O O
the O O
available O O
data O O
as O O
possible O O
for O O
training O O
. O O
if O O
we O O
consider O O
only O O
those O O
instances O O
for O O
which O O
x O O
= O O
xi O O
, O O
then O O
the O O
fraction O O
of O O
such O O
instances O O
for O O
which O O
y O O
= O O
yj O O
is O O
written O O
p O O
( O O
y O O
= O O
yj|x O O
= O O
xi O O
) O O
and O O
is O O
called O O
the O O
conditional B B
probability I I
of O O
y O O
= O O
yj O O
given O O
x O O
= O O
xi O O
. O O
it O O
is O O
said O O
that O O
von O O
neumann O O
recommended O O
to O O
shannon O O
that O O
he O O
use O O
the O O
term O O
entropy B B
, O O
not O O
only O O
be- O O
cause O O
of O O
its O O
similarity O O
to O O
the O O
quantity O O
used O O
in O O
physics O O
, O O
but O O
also O O
because O O
“ O O
nobody O O
knows O O
what O O
entropy B B
really O O
is O O
, O O
so O O
in O O
any O O
discussion O O
you O O
will O O
always O O
have O O
an O O
advan- O O
tage O O
” O O
. O O
while O O
at O O
the O O
root B O
node I I
the O O
maximum O B
probability O I
can O O
then O O
be O O
computed O O
, O O
by O O
analogy O O
with O O
( O O
8.63 O O
) O O
, O O
using O O
so O O
far O O
, O O
we O O
have O O
seen O O
how O O
to O O
ﬁnd O O
the O O
maximum O B
of O O
the O O
joint O O
distribution O O
by O O
prop- O O
agating O O
messages O O
from O O
the O O
leaves O O
to O O
an O O
arbitrarily O O
chosen O O
root B O
node I O
. O O
in O O
the O O
case O O
of O O
a O O
single O O
variable O O
x O O
, O O
the O O
gaussian O O
distribution O O
can O O
be O O
written O O
in O O
the O O
form O O
1 O O
( O O
cid:1 O O
) O O
( O O
cid:2 O O
) O O
n O O
( O O
x|µ O O
, O O
σ2 O O
) O O
= O O
( O O
2.42 O O
) O O
− O O
1 O O
2σ2 O O
( O O
x O O
− O O
µ O O
) O O
2 O O
( O O
2πσ2 O O
) O O
1/2 O O
exp O O
( O O
cid:1 O O
) O O
n O O
( O O
x|µ O O
, O O
σ O O
) O O
= O O
where O O
µ O O
is O O
the O O
mean B B
and O O
σ2 O O
is O O
the O O
variance B B
. O O
the O O
concept O O
of O O
a O O
kernel O O
formulated O O
as O O
an O O
inner O O
product O O
in O O
a O O
feature B O
space I I
allows O O
us O O
to O O
build O O
interesting O O
extensions O O
of O O
many O O
well-known O O
algorithms O O
by O O
making O O
use O O
of O O
the O O
kernel B O
trick I O
, O O
also O O
known O O
as O O
kernel B B
substitution I I
. O O
first O O
, O O
however O O
, O O
we O O
need O O
to O O
determine O O
the O O
variational B B
parameters O O
{ O O
ξn O O
} O O
by O O
maximizing O O
the O O
lower B B
bound I I
on O O
the O O
marginal B B
likelihood I I
. O O
to O O
do O O
this O O
we O O
note O O
that O O
if O O
we O O
make O O
the O O
rescaling O O
w O O
→ O O
κw O O
and O O
b O O
→ O O
κb O O
, O O
then O O
the O O
distance O O
from O O
any O O
point O O
xn O O
to O O
the O O
decision B B
surface I I
, O O
given O O
by O O
tny O O
( O O
xn O O
) O O
/ O O
( O O
cid:5 O O
) O O
w O O
( O O
cid:5 O O
) O O
, O O
is O O
unchanged O O
. O O
( O O
7.39 O O
) O O
( O O
7.40 O O
) O O
( O O
7.41 O O
) O O
n=1 O O
this O O
approach O O
has O O
the O O
advantage O O
that O O
the O O
parameter O O
ν O O
, O O
which O O
replaces O O
c O O
, O O
can O O
be O O
interpreted O O
as O O
both O O
an O O
upper O O
bound O O
on O O
the O O
fraction O O
of O O
margin B B
errors O O
( O O
points O O
for O O
which O O
ξn O O
> O O
0 O O
and O O
hence O O
which O O
lie O O
on O O
the O O
wrong O O
side O O
of O O
the O O
margin B B
boundary O O
and O O
which O O
may O O
or O O
may O O
not O O
be O O
misclassiﬁed O O
) O O
and O O
a O O
lower B B
bound I I
on O O
the O O
fraction O O
of O O
support O B
vectors O O
. O O
expectation B B
propagation I I
for O O
ap- O O
proximate O O
bayesian O O
inference B B
. O O
finally O O
, O O
show O O
that O O
, O O
if O O
the O O
kernel O O
takes O O
the O O
form O O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
φ O O
( O O
x O O
) O O
tφ O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
, O O
that O O
the O O
classiﬁcation B B
is O O
based O O
on O O
the O O
closest O O
mean B B
in O O
the O O
feature B O
space I I
φ O O
( O O
x O O
) O O
. O O
these O O
are O O
directions O O
in O O
which O O
the O O
likelihood B B
function I I
is O O
relatively O O
insensitive O O
to O O
the O O
parameter O O
value O O
and O O
so O O
the O O
parameter O O
has O O
been O O
set O O
to O O
a O O
small O O
value O O
by O O
the O O
prior B B
. O O
137 O O
138 O O
3. O O
linear O O
models O O
for B O
regression I I
given O O
a O O
training B B
data O O
set O O
comprising O O
n O O
observations O O
{ O O
xn O O
} O O
, O O
where O O
n O O
= O O
1 O O
, O O
. O O
at O O
each O O
step O O
the O O
weight B B
vector I I
is O O
moved O O
in O O
the O O
direction O O
of O O
the O O
greatest O O
rate O O
of O O
decrease O O
of O O
the O O
error B B
function I I
, O O
and O O
so O O
this O O
approach O O
is O O
known O O
as O O
gradient B B
descent I I
or O O
steepest B O
descent I I
. O O
weighting O O
and O O
integrating O O
evidence O O
for O O
stochastic B B
simulation O O
in O O
bayesian O O
networks O O
. O O
error B B
functions O O
based O O
on O O
maximum B B
likelihood I I
for O O
a O O
set O O
of O O
independent B B
observations O O
comprise O O
a O O
sum O O
of O O
terms O O
, O O
one O O
for O O
each O O
data O O
point O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
e O O
( O O
w O O
) O O
= O O
en O O
( O O
w O O
) O O
. O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
k O O
( O O
cid:2 O O
) O O
( O O
10.79 O O
) O O
( O O
cid:11 O O
) O O
−1 O O
k O O
k=1 O O
because O O
the O O
remaining O O
integrations O O
are O O
intractable O O
, O O
we O O
approximate O O
the O O
predictive O B
density O O
by O O
replacing O O
the O O
true O O
posterior O O
distribution O O
p O O
( O O
π O O
, O O
µ O O
, O O
λ|x O O
) O O
with O O
its O O
variational B B
approximation O O
q O O
( O O
π O O
) O O
q O O
( O O
µ O O
, O O
λ O O
) O O
to O O
give O O
p O O
( O O
( O O
cid:1 O O
) O O
x|x O O
) O O
= O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
k O O
( O O
cid:2 O O
) O O
k=1 O O
πkn O O
( O O
cid:10 O O
) O O
( O O
cid:1 O O
) O O
x|µk O O
, O O
λ O O
−1 O O
k O O
( O O
cid:11 O O
) O O
q O O
( O O
π O O
) O O
q O O
( O O
µk O O
, O O
λk O O
) O O
dπ O O
dµk O O
dλk O O
( O O
10.80 O O
) O O
exercise O O
10.19 O O
where O O
we O O
have O O
made O O
use O O
of O O
the O O
factorization B B
( O O
10.55 O O
) O O
and O O
in O O
each O O
term O O
we O O
have O O
im- O O
plicitly O O
integrated O O
out O O
all O O
variables O O
{ O O
µj O O
, O O
λj O O
} O O
for O O
j O O
( O O
cid:9 O O
) O O
= O O
k O O
the O O
remaining O O
integrations O O
can O O
now O O
be O O
evaluated O O
analytically O O
giving O O
a O O
mixture O B
of O O
student O O
’ O O
s O O
t-distributions O O
p O O
( O O
( O O
cid:1 O O
) O O
x|x O O
) O O
= O O
k O O
( O O
cid:2 O O
) O O
k=1 O O
1 O O
( O O
cid:1 O O
) O O
α O O
αkst O O
( O O
( O O
cid:1 O O
) O O
x|mk O O
, O O
lk O O
, O O
νk O O
+ O O
1 O O
− O O
d O O
) O O
( O O
10.81 O O
) O O
in O O
which O O
the O O
kth O O
component O O
has O O
mean B B
mk O O
, O O
and O O
the O O
precision O O
is O O
given O O
by O O
lk O O
= O O
( O O
νk O O
+ O O
1 O O
− O O
d O O
) O O
βk O O
( O O
1 O O
+ O O
βk O O
) O O
wk O O
( O O
10.82 O O
) O O
exercise O O
10.20 O O
in O O
which O O
νk O O
is O O
given O O
by O O
( O O
10.63 O O
) O O
. O O
then O O
we O O
introduce O O
the O O
latent B O
variable I I
423 O O
section O O
9.1 O O
424 O O
9. O O
mixture B B
models O O
and O O
em O O
section O O
9.2 O O
section O O
9.3 O O
section O O
9.4 O O
view O O
of O O
mixture B B
distributions O O
in O O
which O O
the O O
discrete O O
latent O O
variables O O
can O O
be O O
interpreted O O
as O O
deﬁning O O
assignments O O
of O O
data O O
points O O
to O O
speciﬁc O O
components O O
of O O
the O O
mixture B B
. O O
boosting B I
can O O
give O O
good O O
results O O
even O O
if O O
the O O
base O O
classiﬁers O O
have O O
a O O
performance O O
that O O
is O O
only O O
slightly O O
better O O
than O O
random O O
, O O
and O O
hence O O
sometimes O O
the O O
base O O
classiﬁers O O
are O O
known O O
as O O
weak O B
learners O O
. O O
consider O O
a O O
multivariate O O
distribution O O
p O O
( O O
z O O
) O O
having O O
strong O O
correlations O O
between O O
the O O
components O O
of O O
z O O
, O O
as O O
illustrated O O
in O O
fig- O O
ure O O
11.10. O O
the O O
scale O O
ρ O O
of O O
the O O
proposal B B
distribution I I
should O O
be O O
as O O
large O O
as O O
possible O O
without O O
incurring O O
high O O
rejection O B
rates O O
. O O
8.20 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
consider O O
the O O
message B B
passing I I
protocol O O
for O O
the O O
sum-product B B
algorithm I I
on O O
a O O
tree-structured O O
factor B B
graph I I
in O O
which O O
messages O O
are O O
ﬁrst O O
propagated O O
from O O
the O O
leaves O O
to O O
an O O
arbitrarily O O
chosen O O
root B O
node I O
and O O
then O O
from O O
the O O
root B B
node I I
out O O
to O O
the O O
leaves O O
. O O
note O O
that O O
there O O
is O O
no O O
section O O
8.1.4 O O
section O O
8.2.2 O O
572 O O
11. O O
continuous O O
lat O O
! O O
: O O
:nt O O
vanim1li O O
: O O
:s O O
/.- O O
, O O
, O O
, O O
, O O
, O O
, O O
, O O
, O O
, O O
flgu.. O O
12.9 O O
i\n O O
~i O O
'' O O
'tfat O O
'' O O
'' O O
oilt O O
> O O
e O O
ii O O
'' O O
'' O O
'' O O
fative O O
vi O O
& O O
w O O
oi1t O O
> O O
e O O
p O O
< O O
ot O O
> O O
abi O O
! O O
; O O
st O O
'' O O
, O O
pea O O
modeifof O O
'' O O
two-dimensiooal O O
< O O
! O O
ala O O
space O O
and O O
a O O
on O O
& O O
- O O
< O O
lirnent.ionallat/l O O
< O O
1t O O
space O O
, O O
an O O
ob O O
& O O
erved O O
< O O
! O O
ala O O
point O O
x O O
is O O
generated O O
by O O
first O O
drawing O O
a O O
value O O
i O O
fof O O
1t O O
> O O
e O O
iat O O
& O O
n1 O O
vafiatlle O O
/f O O
( O O
lm O O
~s O O
prior B B
dist O O
, O O
~t O O
'' O O
'' O O
p O O
( O O
~ O O
) O O
and O O
itlen O O
drawing O O
a O O
val O O
'' O O
'' O O
fof O O
x O O
lrom O O
an O O
iso/fopk O O
: O O
gaussian O O
distr~t O O
'' O O
'' O O
( O O
iijust O O
, O O
al O O
& O O
( O O
l O O
by O O
the O O
red O O
cir O O
< O O
: O O
ie O O
's O O
) O O
having O O
mean B B
wi O O
+ O O
'' O O
and O O
coy8r1.once O O
, O O
,'1 O O
the O O
l/f O O
& O O
er\ O O
ellips. O O
& O O
$ O O
show O O
l O O
! O O
le O O
density B B
`` O O
'' O O
'' O O
toors O O
! O O
of O O
the O O
marg O O
'' O O
' O O
'' O O
1 O O
dis1r1bulion O O
pix O O
) O O
. O O
9 O O
0 O O
1 O O
0.0 O O
−2 O O
0 O O
2 O O
4 O O
figure O O
10.13 O O
illustration O O
of O O
the O O
bayesian O O
approach O O
to O O
logistic B B
regression I I
for O O
a O O
simple O O
linearly B B
separable I I
data O O
set O O
. O O
4. O O
the O O
ﬁnal O O
option O O
is O O
to O O
build O O
the O O
invariance B B
properties O O
into O O
the O O
structure O O
of O O
a O O
neu- O O
ral O O
network O O
( O O
or O O
into O O
the O O
deﬁnition O O
of O O
a O O
kernel B O
function I O
in O O
the O O
case O O
of O O
techniques O O
such O O
as O O
the O O
relevance B B
vector I I
machine I I
) O O
. O O
( O O
7.46 O O
) O O
exercise O O
7.6 O O
from O O
this O O
we O O
can O O
construct O O
an O O
error B B
function I I
by O O
taking O O
the O O
negative O O
logarithm O O
of O O
the O O
likelihood B B
function I I
that O O
, O O
with O O
a O O
quadratic O O
regularizer O O
, O O
takes O O
the O O
form O O
n O O
( O O
cid:2 O O
) O O
elr O O
( O O
yntn O O
) O O
+ O O
λ O O
( O O
cid:5 O O
) O O
w O O
( O O
cid:5 O O
) O O
2. O O
where O O
n=1 O O
elr O O
( O O
yt O O
) O O
= O O
ln O O
( O O
1 O O
+ O O
exp O O
( O O
−yt O O
) O O
) O O
. O O
( O O
11.24 O O
) O O
( O O
cid:14 O O
) O O
zi O O
( O O
cid:9 O O
) O O
∈e O O
r O O
( O O
z O O
) O O
= O O
( O O
cid:14 O O
) O O
p O O
( O O
zi|pai O O
) O O
p O O
( O O
zi|pai O O
) O O
zi∈e O O
1 O O
( O O
cid:14 O O
) O O
zi∈e O O
this O O
method O O
can O O
be O O
further O O
extended B B
using O O
self-importance O O
sampling O O
( O O
shachter O O
and O O
peot O O
, O O
1990 O O
) O O
in O O
which O O
the O O
importance B B
sampling I I
distribution O I
is O O
continually O O
updated O O
to O O
reﬂect O O
the O O
current O O
estimated O O
posterior O O
distribution O O
. O O
sep- O O
arating O O
off O O
the O O
contribution O O
from O O
base O O
classiﬁer O O
ym O O
( O O
x O O
) O O
, O O
we O O
can O O
then O O
write O O
the O O
error B B
function I I
in O O
the O O
form O O
( O O
cid:13 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
n O O
( O O
cid:2 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
n=1 O O
e O O
= O O
= O O
exp O O
−tnfm−1 O O
( O O
xn O O
) O O
− O O
1 O O
2 O O
tnαmym O O
( O O
xn O O
) O O
( O O
cid:13 O O
) O O
w O O
( O O
m O O
) O O
n O O
exp O O
−1 O O
2 O O
tnαmym O O
( O O
xn O O
) O O
( O O
14.22 O O
) O O
( O O
m O O
) O O
n O O
= O O
exp O O
{ O O
−tnfm−1 O O
( O O
xn O O
) O O
} O O
can O O
be O O
viewed O O
as O O
constants O O
where O O
the O O
coefﬁcients O O
w O O
if O O
we O O
denote O O
by O O
tm O O
the O O
set O O
of O O
because O O
we O O
are O O
optimizing O O
only O O
αm O O
and O O
ym O O
( O O
x O O
) O O
. O O
consider O O
two O O
points O O
xa O O
and O O
xb O O
both O O
of O O
which O O
lie O O
on O O
the O O
decision B B
surface I I
. O O
one O O
technique O O
that O O
is O O
often O O
used O O
to O O
control O O
the O O
over-ﬁtting B B
phenomenon O O
in O O
such O O
cases O O
is O O
that O O
of O O
regularization B B
, O O
which O O
involves O O
adding O O
a O O
penalty O O
term O O
to O O
the O O
error B B
function I I
( O O
1.2 O O
) O O
in O O
order O O
to O O
discourage O O
the O O
coefﬁcients O O
from O O
reaching O O
large O O
values O O
. O O
10.3.1 O O
variational B B
distribution O O
our O O
ﬁrst O O
goal O O
is O O
to O O
ﬁnd O O
an O O
approximation O O
to O O
the O O
posterior O O
distribution O O
p O O
( O O
w O O
, O O
α|t O O
) O O
. O O
c O O
figure O O
8.17 O O
the O O
second O O
of O O
our O O
three O O
examples O O
of O O
3-node O O
graphs O O
used O O
to O O
motivate O O
the O O
conditional B B
indepen- O O
dence O O
framework O O
for O O
directed O O
graphical O O
models O O
. O O
a O O
variant O O
of O O
this O O
quantity O O
, O O
called O O
the O O
bayesian O O
information B O
criterion I I
, O O
or O O
bic O O
, O O
will O O
be O O
discussed O O
in O O
section O O
4.4.1. O O
such O O
criteria O O
do O O
not O O
take O O
account O O
of O O
the O O
uncertainty O O
in O O
the O O
model O O
parameters O O
, O O
however O O
, O O
and O O
in O O
practice O O
they O O
tend O O
to O O
favour O O
overly O O
simple O O
models O O
. O O
note O O
that O O
this O O
distribution O O
fails O O
to O O
capture O O
the O O
two O O
clumps O O
in O O
the O O
data O O
and O O
indeed O O
places O O
much O O
of O O
its O O
probability B B
mass O O
in O O
the O O
central O B
region O O
between O O
the O O
clumps O O
where O O
the O O
data O O
are O O
relatively O O
sparse O O
. O O
we O O
therefore O O
return O O
to O O
our O O
general O O
result O O
( O O
2.246 O O
) O O
for O O
local O O
density B O
estimation I O
, O O
and O O
instead O O
of O O
ﬁxing O O
v O O
and O O
determining O O
the O O
value O O
of O O
k O O
from O O
the O O
data O O
, O O
we O O
consider O O
a O O
ﬁxed O O
value O O
of O O
k O O
and O O
use O O
the O O
data O O
to O O
ﬁnd O O
an O O
appropriate O O
value O O
for O O
v O O
. O O
using O O
the O O
sum B O
rule I I
, O O
the O O
denominator O O
in O O
bayes O O
’ O O
theorem O O
can O O
be O O
expressed O O
in O O
terms O O
of O O
the O O
quantities O O
appearing O O
in O O
the O O
numerator O O
( O O
cid:2 O O
) O O
p O O
( O O
x O O
) O O
= O O
p O O
( O O
x|y O O
) O O
p O O
( O O
y O O
) O O
. O O
its O O
definition O O
differs O O
from O O
that O O
of O O
probabilistic O O
pca O O
only O O
in O O
that O O
the O O
conditional B B
distribution O O
of O O
the O O
observed B O
variable I O
x O O
given O O
the O O
latent B B
variable I I
z O O
is O O
584 O O
12. O O
continuous O O
latent O B
variables O O
• O O
•• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
•• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
•• O O
• O O
• O O
•• O O
• O O
• O O
•• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
• O O
••• O O
•• O O
• O O
• O O
• O O
• O O
• O O
• O O
figure O O
12.14 O O
'hinloo O O
' O O
diagrams O O
of O O
the O O
matrix O O
w O O
in O O
which O O
each O O
element O O
01 O O
the O O
matrix O O
is O O
depicted O O
as O O
a O O
square O O
( O O
white O O
lor O O
positive O O
and O O
black O O
lor O O
negative O O
values O O
) O O
whose O O
area O O
is O O
proportional O O
to O O
the O O
magnitude O O
of O O
that O O
element O O
. O O
this O O
will O O
provide O O
a O O
good O O
illustration O O
of O O
the O O
application O O
of O O
variational B B
methods O O
and O O
will O O
also O O
demonstrate O O
how O O
a O O
bayesian O O
treatment O O
elegantly O O
resolves O O
many O O
of O O
the O O
difﬁculties O O
associated O O
with O O
the O O
maximum B B
likelihood I I
approach O O
( O O
attias O O
, O O
1999b O O
) O O
. O O
11.4. O O
slice B B
sampling I I
we O O
have O O
seen O O
that O O
one O O
of O O
the O O
difﬁculties O O
with O O
the O O
metropolis O O
algorithm O O
is O O
the O O
sensi- O O
tivity O O
to O O
step O O
size O O
. O O
novelty B O
detection I O
and O O
neu- O O
ral O O
network O O
validation O O
. O O
if O O
we O O
also O O
give O O
the O O
leaf O O
models O O
a O O
probabilistic O O
inter- O O
pretation O O
, O O
we O O
arrive O O
at O O
a O O
fully O O
probabilistic O O
tree-based O O
model O O
called O O
the O O
hierarchical B B
mixture I O
of I I
experts I I
, O O
which O O
we O O
consider O O
in O O
section O O
14.5.3. O O
an O O
alternative O O
way O O
to O O
motivate O O
the O O
hierarchical B B
mixture I O
of I I
experts I I
model O O
is O O
to O O
start O O
with O O
a O O
standard O O
probabilistic O O
mixtures O O
of O O
unconditional O O
density B B
models O O
such O O
as O O
gaussians O O
and O O
replace O O
the O O
component O O
densities O O
with O O
conditional B B
distributions O O
. O O
we O O
now O O
write O O
down O O
a O O
recursive O O
backpropagation B B
formula O O
to O O
determine O O
the O O
derivatives O O
∂yk/∂aj O O
( O O
cid:2 O O
) O O
∂yk O O
∂aj O O
= O O
∂al O O
∂aj O O
( O O
cid:2 O O
) O O
∂yk O O
∂al O O
( O O
cid:4 O O
) O O
( O O
aj O O
) O O
l O O
l O O
= O O
h O O
wlj O O
∂yk O O
∂al O O
( O O
5.74 O O
) O O
where O O
the O O
sum O O
runs O O
over O O
all O O
units O O
l O O
to O O
which O O
unit O O
j O O
sends O O
connections O O
( O O
corresponding O O
to O O
the O O
ﬁrst O O
index O O
of O O
wlj O O
) O O
. O O
a O O
matrix O O
whose O O
eigenvalues O O
are O O
strictly O O
positive O O
is O O
said O O
to O O
be O O
positive B B
deﬁnite I I
. O O
it O O
is O O
again O O
easy O O
to O O
show O O
, O O
through O O
the O O
use O O
of O O
the O O
d-separation B B
criterion O O
, O O
that O O
the O O
markov O O
property O O
( O O
13.5 O O
) O O
for O O
the O O
chain O O
of O O
latent O B
variables O O
still O O
holds O O
. O O
because O O
we O O
now O O
have O O
a O O
fully O B
probabilistic O O
model O O
for O O
pca O O
, O O
we O O
can O O
deal O O
with O O
missing B O
data I O
, O O
provided O O
that O O
it O O
is O O
missing B O
at I O
random I O
, O O
by O O
marginalizing O O
over O O
the O O
dis O O
( O O
cid:173 O O
) O O
tribution O O
of O O
the O O
unobserved O O
variables O O
. O O
we O O
therefore O O
begin O O
by O O
writing O O
down O O
the O O
following O O
conditional B B
independence I I
properties O O
( O O
jordan O O
, O O
2007 O O
) O O
p O O
( O O
x|zn O O
) O O
= O O
p O O
( O O
x1 O O
, O O
. O O
( O O
13.11 O O
) O O
z O O
because O O
the O O
joint O O
distribution O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
does O O
not O O
factorize O O
over O O
n O O
( O O
in O O
contrast O O
to O O
the O O
mixture B B
distribution I I
considered O O
in O O
chapter O O
9 O O
) O O
, O O
we O O
can O O
not O O
simply O O
treat O O
each O O
of O O
the O O
summations O O
over O O
zn O O
independently O O
. O O
for O O
convenience O O
, O O
we O O
shall O O
denote O O
the O O
s O O
400 O O
8. O O
graphical O O
models O O
figure O O
8.40 O O
example O O
of O O
a O O
factor B B
graph I I
, O O
which O O
corresponds O O
to O O
the O O
factorization B B
( O O
8.60 O O
) O O
. O O
6.4. O O
gaussian O O
processes O O
303 O O
figure O O
6.3 O O
illustration O O
of O O
the O O
nadaraya-watson O O
kernel B O
regression I I
model O O
using O O
isotropic B B
gaussian O O
kernels O O
, O O
for O O
the O O
sinusoidal B O
data I O
set O O
. O O
an O O
introduction O O
to O O
mcmc O O
for O O
ma- O O
chine O O
learning B B
. O O
additional O O
background O O
on O O
neural B O
network I I
models O O
can O O
be O O
found O O
in O O
bishop O O
( O O
1995a O O
) O O
. O O
the O O
step O O
size O O
is O O
governed O O
by O O
the O O
stan- O O
dard O O
deviation O I
of O O
the O O
conditional B B
distri- O O
bution O O
( O O
green O O
curve O O
) O O
, O O
and O O
is O O
o O O
( O O
l O O
) O O
, O O
lead- O O
ing O O
to O O
slow O O
progress O O
in O O
the O O
direction O O
of O O
elongation O O
of O O
the O O
joint O O
distribution O O
( O O
red O O
ellipse O O
) O O
. O O
to O O
see O O
the O O
problem O O
, O O
note O O
that O O
the O O
probability B B
that O O
a O O
sequence O O
sampled O O
from O O
a O O
given O O
hidden O O
markov O O
model O O
will O O
spend O O
precisely O O
t O O
steps O O
in O O
state O B
k O O
and O O
then O O
make O O
a O O
transition O O
to O O
a O O
different O O
state O O
is O O
given O O
by O O
p O O
( O O
t O O
) O O
= O O
( O O
akk O O
) O O
t O O
( O O
1 O O
− O O
akk O O
) O O
∝ O O
exp O O
( O O
−t O O
ln O O
akk O B
) O O
( O O
13.74 O O
) O O
and O O
so O O
is O O
an O O
exponentially O O
decaying O O
function O O
of O O
t O O
. O O
if O O
, O O
at O O
each O O
stage O O
of O O
the O O
gibbs O O
sampling O O
algorithm O O
, O O
instead O O
of O O
drawing O O
a O O
sample O O
from O O
the O O
corresponding O O
conditional B B
distribution O O
, O O
we O O
make O O
a O O
point O O
estimate O O
of O O
the O O
variable O O
given O O
by O O
the O O
maximum O B
of O O
the O O
conditional B B
distribution O O
, O O
then O O
we O O
obtain O O
the O O
iterated B B
conditional I I
modes I O
( O O
icm O O
) O O
algorithm O O
discussed O O
in O O
section O O
8.3.3. O O
thus O O
icm O O
can O O
be O O
seen O O
as O O
a O O
greedy O O
approximation O O
to O O
gibbs O O
sampling O O
. O O
9.2.1 O O
maximum B B
likelihood I I
. O O
learning B O
with O O
kernels O O
. O O
( O O
5.79 O O
) O O
using O O
( O O
5.48 O O
) O O
and O O
( O O
5.49 O O
) O O
, O O
the O O
second O O
derivatives O O
on O O
the O O
right-hand O O
side O O
of O O
( O O
5.79 O O
) O O
can O O
be O O
found O O
recursively O O
using O O
the O O
chain O O
rule O O
of O O
differential B O
calculus O O
to O O
give O O
a O O
backprop- O O
agation O O
equation O O
of O O
the O O
form O O
∂2en O O
∂a2 O O
j O O
( O O
cid:4 O O
) O O
( O O
aj O O
) O O
2 O O
= O O
h O O
wkjwk O O
( O O
cid:1 O O
) O O
j O O
∂2en O O
∂ak∂ak O O
( O O
cid:1 O O
) O O
( O O
cid:4 O O
) O O
( O O
cid:4 O O
) O O
( O O
aj O O
) O O
+ O O
h O O
wkj O O
∂en O O
∂ak O O
. O O
it O O
can O O
be O O
shown O O
that O O
a O O
homogeneous B O
markov O O
chain O O
will O O
be O O
ergodic O O
, O O
subject O O
only O O
to O O
weak O O
restrictions O O
on O O
the O O
invariant O O
distribution O O
and O O
the O O
transition O O
probabilities O O
( O O
neal O O
, O O
1993 O O
) O O
. O O
also O O
, O O
each O O
step O O
by O O
deﬁnition O O
samples O O
from O O
the O O
correct O O
conditional B B
distribution O O
p O O
( O O
zi|z\i O O
) O O
. O O
( O O
d.2 O O
) O O
the O O
analogous O O
deﬁnition O O
of O O
a O O
functional B B
derivative O O
arises O O
when O O
we O O
consider O O
how O O
much O O
a O O
functional B B
f O O
[ O O
y O O
] O O
changes O O
when O O
we O O
make O O
a O O
small O O
change O O
η O O
( O O
x O O
) O O
to O O
the O O
function O O
703 O O
704 O O
d. O O
calculus B O
of I O
variations I I
figure O O
d.1 O O
a O O
functional B B
derivative O O
can O O
be O O
deﬁned O O
by O O
considering O O
how O O
the O O
value O O
of O O
a O O
functional B B
f O O
[ O O
y O O
] O O
changes O O
when O O
the O O
function O O
y O O
( O O
x O O
) O O
is O O
changed O O
to O O
y O O
( O O
x O O
) O O
+ O O
η O O
( O O
x O O
) O O
where O O
η O O
( O O
x O O
) O O
is O O
an O O
arbitrary O O
function O O
of O O
x. O O
y O O
( O O
x O O
) O O
y O O
( O O
x O O
) O O
+ O O
η O O
( O O
x O O
) O O
x O O
y O O
( O O
x O O
) O O
, O O
where O O
η O O
( O O
x O O
) O O
is O O
an O O
arbitrary O O
function O O
of O O
x O O
, O O
as O O
illustrated O O
in O O
figure O O
d.1 O O
. O O
to O O
do O O
this O O
, O O
we O O
employ O O
the O O
variational B B
framework O O
of O O
section O O
10.1 O O
, O O
with O O
a O O
variational B B
10.3. O O
variational B B
linear O O
regression B B
487 O O
figure O O
10.8 O O
probabilistic B O
graphical I O
model I O
representing O O
the O O
joint O O
dis- O O
regression B B
the O O
bayesian O O
linear O B
for O O
tribution O O
( O O
10.90 O O
) O O
model O O
. O O
an O O
important O O
example O O
, O O
which O O
arises O O
frequently O O
in O O
pattern O O
recognition O O
, O O
is O O
the O O
logistic B B
sigmoid I I
function O O
deﬁned O O
by O O
σ O O
( O O
x O O
) O O
= O O
1 O O
1 O O
+ O O
e−x O O
. O O
determination O O
of O O
p O O
( O O
x O O
, O O
t O O
) O O
from O O
a O O
set O O
of O O
training B B
data O O
is O O
an O O
example O O
of O O
inference B B
and O O
is O O
typically O O
a O O
very O O
difﬁcult O O
problem O O
whose O O
solution O O
forms O O
the O O
subject O O
of O O
much O O
of O O
this O O
book O O
. O O
the O O
corresponding O O
likelihood B B
function I I
for O O
this O O
second O O
data O O
point O O
alone O O
is O O
shown O O
in O O
the O O
left O O
plot O O
. O O
however O O
, O O
it O O
is O O
easily O O
generalized B B
, O O
for O O
instance O O
by O O
considering O O
additional O O
layers O O
of O O
processing O O
each O O
consisting O O
of O O
a O O
weighted O O
linear O O
combination O O
of O O
the O O
form O O
( O O
5.4 O O
) O O
followed O O
by O O
an O O
element-wise O O
transformation O O
using O O
a O O
nonlinear O O
activation O B
func- O O
tion O O
. O O
( O O
3.88 O O
) O O
1 O O
λi O O
+ O O
α O O
i O O
( O O
cid:2 O O
) O O
thus O O
the O O
stationary B B
points O O
of O O
( O O
3.86 O O
) O O
with O O
respect O O
to O O
α O O
satisfy O O
0 O O
= O O
m O O
2α O O
− O O
1 O O
2 O O
n O O
mn O O
− O O
1 O O
mt O O
2 O O
1 O O
λi O O
+ O O
α O O
i O O
. O O
the O O
least-squares O O
regression B I
function I I
is O O
obtained O O
by O O
ﬁnding O O
the O O
or- O O
thogonal O O
projection O O
of O O
the O O
data O O
vector O O
t O O
onto O O
the O O
subspace O O
spanned O O
by O O
the O O
basis O B
functions O O
φj O O
( O O
x O O
) O O
in O O
which O O
each O O
basis B B
function I I
is O O
viewed O O
as O O
a O O
vec- O O
tor O O
ϕj O O
of O O
length O O
n O O
with O O
elements O O
φj O O
( O O
xn O O
) O O
. O O
the O O
various O O
terms O O
in O O
the O O
bound O O
are O O
easily O O
evaluated O O
to O O
give O O
the O O
following O O
results O O
exercise O O
10.16 O O
k O O
( O O
cid:2 O O
) O O
( O O
cid:12 O O
) O O
ln O O
( O O
cid:4 O O
) O O
λk O O
− O O
dβ O O
nk O O
1 O O
2 O O
−νk O O
( O O
xk O O
− O O
mk O O
) O O
twk O O
( O O
xk O O
− O O
mk O O
) O O
− O O
d O O
ln O O
( O O
2π O O
) O O
k O O
− O O
νktr O O
( O O
skwk O O
) O O
−1 O O
k=1 O O
( O O
cid:13 O O
) O O
n O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
rnk O O
ln O O
( O O
cid:4 O O
) O O
πk O O
e O O
[ O O
ln O O
p O O
( O O
x|z O O
, O O
µ O O
, O O
λ O O
) O O
] O O
= O O
e O O
[ O O
ln O O
p O O
( O O
z|π O O
) O O
] O O
= O O
n=1 O O
k=1 O O
e O O
[ O O
ln O O
p O O
( O O
π O O
) O O
] O O
= O O
ln O O
c O O
( O O
α0 O O
) O O
+ O O
( O O
α0 O O
− O O
1 O O
) O O
k O O
( O O
cid:2 O O
) O O
k=1 O O
ln O O
( O O
cid:4 O O
) O O
πk O O
( O O
10.71 O O
) O O
( O O
10.72 O O
) O O
( O O
10.73 O O
) O O
482 O O
10. O O
approximate O O
inference B B
e O O
[ O O
ln O O
p O O
( O O
µ O O
, O O
λ O O
) O O
] O O
= O O
1 O O
2 O O
k O O
( O O
cid:2 O O
) O O
k=1 O O
( O O
cid:12 O O
) O O
d O O
ln O O
( O O
β0/2π O O
) O O
+ O O
ln O O
( O O
cid:4 O O
) O O
λk O O
− O O
dβ0 O O
βk O O
( O O
cid:13 O O
) O O
+ O O
k O O
ln O O
b O O
( O O
w0 O O
, O O
ν0 O O
) O O
νktr O O
( O O
w O O
−1 O O
0 O O
wk O O
) O O
−β0νk O O
( O O
mk O O
− O O
m0 O O
) O O
twk O O
( O O
mk O O
− O O
m0 O O
) O O
( O O
ν0 O O
− O O
d O O
− O O
1 O O
) O O
k O O
( O O
cid:2 O O
) O O
+ O O
2 O O
k=1 O O
k=1 O O
ln O O
( O O
cid:4 O O
) O O
λk O O
− O O
1 O O
k O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
( O O
αk O O
− O O
1 O O
) O O
ln O O
( O O
cid:4 O O
) O O
πk O O
+ O O
ln O O
c O O
( O O
α O O
) O O
( O O
cid:16 O O
) O O
( O O
cid:12 O O
) O O
ln O O
( O O
cid:4 O O
) O O
λk O O
+ O O
d O O
rnk O O
ln O O
rnk O O
( O O
cid:15 O O
) O O
ln O O
k=1 O O
1 O O
2 O O
βk O O
2π O O
2 O O
2 O O
n O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
n=1 O O
k=1 O O
k=1 O O
e O O
[ O O
ln O O
q O O
( O O
z O O
) O O
] O O
= O O
e O O
[ O O
ln O O
q O O
( O O
π O O
) O O
] O O
= O O
e O O
[ O O
ln O O
q O O
( O O
µ O O
, O O
λ O O
) O O
] O O
= O O
( O O
10.74 O O
) O O
( O O
10.75 O O
) O O
( O O
10.76 O O
) O O
( O O
10.77 O O
) O O
( O O
cid:13 O O
) O O
− O O
h O O
[ O O
q O O
( O O
λk O O
) O O
] O O
− O O
d O O
2 O O
where O O
d O O
is O O
the O O
dimensionality O O
of O O
x O O
, O O
h O O
[ O O
q O O
( O O
λk O O
) O O
] O O
is O O
the O O
entropy B B
of O O
the O O
wishart O O
distribu- O O
tion O O
given O O
by O O
( O O
b.82 O O
) O O
, O O
and O O
the O O
coefﬁcients O O
c O O
( O O
α O O
) O O
and O O
b O O
( O O
w O O
, O O
ν O O
) O O
are O O
deﬁned O O
by O O
( O O
b.23 O O
) O O
and O O
( O O
b.79 O O
) O O
, O O
respectively O O
. O O
we O O
there- O O
fore O O
need O O
to O O
ﬁnd O O
a O O
measure O O
of O O
performance O O
which O O
depends O O
only O O
on O O
the O O
training B B
data O O
and O O
which O O
does O O
not O O
suffer O O
from O O
bias B B
due O O
to O O
over-ﬁtting B O
. O O
suppose O O
we O O
have O O
a O O
training B B
set I I
{ O O
xn O O
, O O
tn O O
} O O
and O O
we O O
use O O
a O O
parzen O O
density B B
estimator O O
to O O
model O O
the O O
joint O O
distribution O O
p O O
( O O
x O O
, O O
t O O
) O O
, O O
so O O
that O O
section O O
9.1 O O
section O O
2.5.1 O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
p O O
( O O
x O O
, O O
t O O
) O O
= O O
1 O O
n O O
f O O
( O O
x O O
− O O
xn O O
, O O
t O O
− O O
tn O O
) O O
( O O
6.42 O O
) O O
where O O
f O O
( O O
x O O
, O O
t O O
) O O
is O O
the O O
component O O
density B B
function O O
, O O
and O O
there O O
is O O
one O O
such O O
component O O
centred O O
on O O
each O O
data O O
point O O
. O O
xviii O O
contents O O
10.2.1 O O
variational B B
distribution O O
. O O
the O O
extension O O
to O O
networks O O
with O O
multiclass B B
softmax O O
outputs O O
is O O
straightforward O O
. O O
because O O
the O O
inverse B B
of O O
a O O
symmetric O O
matrix O O
is O O
also O O
symmetric O O
( O O
see O O
exercise O O
2.22 O O
) O O
, O O
it O O
follows O O
that O O
the O O
covariance B B
matrix I I
may O O
also O O
be O O
chosen O O
to O O
be O O
symmetric O O
without O O
loss O O
of O O
generality O O
. O O
2.3.1 O O
conditional B B
gaussian O O
distributions O O
. O O
ensemble O O
learning B B
for O O
hid- O O
den O O
markov O O
models O O
. O O
the O O
central B B
limit I O
theorem I O
( O O
due O O
to O O
laplace O O
) O O
tells O O
us O O
that O O
, O O
subject O O
to O O
certain O O
mild O O
conditions O O
, O O
the O O
sum O O
of O O
a O O
set O O
of O O
random O O
variables O O
, O O
which O O
is O O
of O O
course O O
itself O O
a O O
random O O
variable O O
, O O
has O O
a O O
distribution O O
that O O
becomes O O
increas- O O
ingly O O
gaussian O O
as O O
the O O
number O O
of O O
terms O O
in O O
the O O
sum O O
increases O O
( O O
walker O O
, O O
1969 O O
) O O
. O O
this O O
transformation O O
corresponds O O
to O O
a O O
change O O
of O O
p O O
( O O
( O O
cid:1 O O
) O O
x| O O
( O O
cid:1 O O
) O O
σ O O
) O O
= O O
( O O
cid:15 O O
) O O
( O O
cid:1 O O
) O O
x O O
( O O
cid:1 O O
) O O
σ O O
( O O
2.237 O O
) O O
( O O
cid:16 O O
) O O
1 O O
( O O
cid:1 O O
) O O
σ O O
f O O
scale O O
, O O
for O O
example O O
from O O
meters O O
to O O
kilometers O O
if O O
x O O
is O O
a O O
length O O
, O O
and O O
we O O
would O O
like O O
to O O
choose O O
a O O
prior B B
distribution O O
that O O
reﬂects O O
this O O
scale B O
invariance I O
. O O
3.1.3 O O
sequential B O
learning I I
batch O O
techniques O O
, O O
such O O
as O O
the O O
maximum B B
likelihood I I
solution O O
( O O
3.15 O O
) O O
, O O
which O O
in- O O
volve O O
processing O O
the O O
entire O O
training B B
set I I
in O O
one O O
go O O
, O O
can O O
be O O
computationally O O
costly O O
for O O
large O O
data O O
sets O O
. O O
the O O
bottom O O
row O O
shows O O
the O O
restored O O
images O O
obtained O O
using O O
iterated O B
conditional O B
models O O
( O O
icm O O
) O O
on O O
the O O
left O O
and O O
using O O
the O O
graph-cut B O
algorithm I O
on O O
the O O
right O O
. O O
note O O
that O O
p O O
( O O
x O O
= O O
xi O O
) O O
is O O
sometimes O O
called O O
the O O
marginal B B
probability I I
, O O
because O O
it O O
is O O
obtained O O
by O O
marginalizing O O
, O O
or O O
summing O O
out O O
, O O
the O O
other O O
variables O O
( O O
in O O
this O O
case O O
y O O
) O O
. O O
the O O
conjugate B B
prior I I
for O O
µ O O
is O O
the O O
beta B B
distribution I I
. O O
by O O
con- O O
trast O O
, O O
the O O
advantage O O
of O O
the O O
mixture B B
density I I
network I I
approach O O
is O O
that O O
the O O
component O O
exercise O O
14.17 O O
section O O
4.3.3 O O
674 O O
14. O O
combining B B
models I O
exercises O O
densities O O
and O O
the O O
mixing O O
coefﬁcients O O
share O O
the O O
hidden O O
units O O
of O O
the O O
neural B B
network I I
. O O
having O O
solved O O
the O O
quadratic O O
programming O O
problem O O
and O O
found O O
a O O
value O O
for O O
a O O
, O O
we O O
can O O
then O O
determine O O
the O O
value O O
of O O
the O O
threshold B O
parameter I O
b O O
by O O
noting O O
that O O
any O O
support B O
vector I I
xn O O
satisﬁes O O
tny O O
( O O
xn O O
) O O
= O O
1. O O
using O O
( O O
7.13 O O
) O O
this O O
gives O O
( O O
cid:23 O O
) O O
amtmk O O
( O O
xn O O
, O O
xm O O
) O O
+ O O
b O O
= O O
1 O O
( O O
7.17 O O
) O O
( O O
cid:22 O O
) O O
( O O
cid:2 O O
) O O
tn O O
m∈s O O
where O O
s O O
denotes O O
the O O
set O O
of O O
indices O O
of O O
the O O
support O B
vectors O O
. O O
the O O
ﬁnal O O
maximization O O
is O O
performed O O
over O O
the O O
product O B
of O O
all O O
messages O O
arriving O O
at O O
the O O
root B B
node I I
, O O
and O O
gives O O
the O O
maximum O B
value O O
for O O
p O O
( O O
x O O
) O O
. O O
an O O
important O O
class O O
of O O
such O O
approximations O O
, O O
that O O
can O O
broadly O O
be O O
called O O
variational B B
methods O O
, O O
will O O
be O O
discussed O O
in O O
detail O O
in O O
chapter O O
10. O O
complementing O O
these O O
deterministic O O
approaches O O
is O O
a O O
wide O O
range O O
of O O
sampling B O
methods I O
, O O
also O O
called O O
monte O O
carlo O O
methods O O
, O O
that O O
are O O
based O O
on O O
stochastic B B
numerical O O
sampling O I
from O O
distributions O O
and O O
that O O
will O O
be O O
discussed O O
at O O
length O O
in O O
chapter O O
11. O O
here O O
we O O
consider O O
one O O
simple O O
approach O O
to O O
approximate O O
inference B B
in O O
graphs O O
with O O
loops O O
, O O
which O O
builds O O
directly O O
on O O
the O O
previous O O
discussion O O
of O O
exact O O
inference O O
in O O
trees O O
. O O
for O O
continuous O O
state O O
spaces O O
, O O
a O O
common O O
choice O O
is O O
a O O
gaussian O O
centred O O
on O O
the O O
current O O
state O O
, O O
leading O O
to O O
an O O
important O O
trade-off O O
in O O
determin- O O
if O O
the O O
variance B B
is O O
small O O
, O O
then O O
the O O
ing O B
the O O
variance B B
parameter O I
of O O
this O O
distribution O O
. O O
1000 O O
j O O
500 O O
0 O O
1 O O
2 O O
3 O O
4 O O
section O O
9.2.2 O O
section O O
2.3.5 O O
exercise O O
9.2 O O
case O O
, O O
the O O
assignment O O
of O O
each O O
data O O
point O O
to O O
the O O
nearest O O
cluster O O
centre O O
is O O
equivalent O O
to O O
a O O
classiﬁcation B B
of O O
the O O
data O O
points O O
according O O
to O O
which O O
side O O
they O O
lie O O
of O O
the O O
perpendicular O O
bisector O O
of O O
the O O
two O O
cluster O O
centres O O
. O O
the O O
two O O
paths O O
through O O
the O O
lattice O O
correspond O O
to O O
conﬁgurations O O
that O O
give O O
the O O
global O B
maximum O O
of O O
the O O
joint O O
probability B B
distribution O O
, O O
and O O
either O O
of O O
these O O
can O O
be O O
found O O
by O O
tracing O O
back O O
along O O
the O O
black O O
lines O O
in O O
the O O
opposite O O
direction O O
to O O
the O O
arrow O O
. O O
in O O
contrast O O
, O O
a O O
fully O B
con- O O
nected O O
graph O O
of O O
m O O
nodes O O
would O O
have O O
km O O
− O O
1 O O
param- O O
eters O O
, O O
which O O
grows O O
exponentially O O
with O O
m. O O
x1 O O
x2 O O
xm O O
a O O
separate O O
multinomial B O
distribution I I
, O O
and O O
the O O
total O O
number O O
of O O
parameters O O
would O O
be O O
2 O O
( O O
k O O
− O O
1 O O
) O O
. O O
however O O
, O O
as O O
we O O
shall O O
see O O
there O O
are O O
some O O
signiﬁcant O O
limitations O O
to O O
the O O
maximum B B
likelihood I I
ap- O O
proach O O
, O O
and O O
in O O
chapter O O
10 O O
we O O
shall O O
show O O
that O O
an O O
elegant O O
bayesian O O
treatment O O
can O O
be O O
given O O
using O O
the O O
framework O O
of O O
variational B B
inference I I
. O O
( O O
13.89 O O
) O O
( O O
13.90 O O
) O O
( O O
13.91 O O
) O O
here O O
we O O
have O O
made O O
use O O
of O O
the O O
matrix O O
inverse B B
identities O O
( O O
c.5 O O
) O O
and O O
( O O
c.7 O O
) O O
and O O
also O O
deﬁned O O
the O O
kalman O O
gain O O
matrix O O
( O O
cid:11 O O
) O O
−1 O O
kn O O
= O O
pn−1ct O O
cpn−1ct O O
+ O O
σ O O
. O O
the O O
motivation O O
for O O
considering O O
factorial B O
hmm O O
can O O
be O O
seen O O
by O O
noting O O
that O O
in O O
order O O
to O O
represent O O
, O O
say O O
, O O
10 O O
bits B B
of O O
information O O
at O O
a O O
given O O
time O O
step O O
, O O
a O O
standard O O
hmm O O
would O O
need O O
k O O
= O O
210 O O
= O O
1024 O O
latent O O
states O O
, O O
whereas O O
a O O
factorial B O
hmm O O
could O O
make O O
use O O
of O O
10 O O
binary O O
latent O O
chains O O
. O O
testing O O
for O O
conditional O B
independence O I
in O O
undirected B B
graphs O O
is O O
therefore O O
simpler O O
than O O
in O O
directed B B
graphs O O
. O O
sequential O O
sparse O O
bayesian O O
learning B B
algorithm O O
1. O O
if O O
solving O O
a O O
regression B B
problem O O
, O O
initialize O O
β O O
. O O
( O O
4.83 O O
) O O
we O O
now O O
restrict O O
attention O O
to O O
the O O
subclass O O
of O O
such O O
distributions O O
for O O
which O O
u O O
( O O
x O O
) O O
= O O
x. O O
then O O
we O O
make O O
use O O
of O O
( O O
2.236 O O
) O O
to O O
introduce O O
a O O
scaling O O
parameter O O
s O O
, O O
so O O
that O O
we O O
obtain O O
the O O
restricted O O
set O O
of O O
exponential B O
family I I
class-conditional O O
densities O O
of O O
the O O
form O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
( O O
cid:13 O O
) O O
( O O
cid:12 O O
) O O
1 O O
s O O
p O O
( O O
x|λk O O
, O O
s O O
) O O
= O O
1 O O
s O O
h O O
1 O O
s O O
x O O
g O O
( O O
λk O O
) O O
exp O O
λt O O
k O O
x O O
. O O
note O O
that O O
this O O
is O O
exactly O O
the O O
same O O
posterior O O
distribution O O
as O O
would O O
be O O
obtained O O
by O O
combining O B
the O O
original O O
prior B B
with O O
the O O
likelihood B B
function I I
for O O
the O O
two O O
data O O
points O O
. O O
aside O O
from O O
difﬁculties O O
with O O
the O O
learning B B
algorithm O O
, O O
the O O
perceptron B B
does O O
not O O
pro- O O
vide O O
probabilistic O O
outputs O O
, O O
nor O O
does O O
it O O
generalize O O
readily O O
to O O
k O O
> O O
2 O O
classes O O
. O O
8.2 O O
conditional B B
independence I I
. O O
by O O
convention O O
, O O
we O O
shall O O
propagate O O
messages O O
that O O
are O O
nor- O O
malized O O
marginal B B
distributions O O
corresponding O O
to O O
p O O
( O O
zn|x1 O O
, O O
. O O
in O O
the O O
left-hand O O
column O O
is O O
a O O
plot O O
of O O
the O O
likelihood B B
function I I
p O O
( O O
t|x O O
, O O
w O O
) O O
for O O
this O O
data O O
point O O
as O O
a O O
function O O
of O O
w. O O
note O O
that O O
the O O
likelihood B B
function I I
provides O O
a O O
soft B O
constraint O O
that O O
the O O
line O O
must O O
pass O O
close O O
to O O
the O O
data O O
point O O
, O O
where O O
close O O
is O O
determined O O
by O O
the O O
noise O O
precision O O
β. O O
for O O
comparison O O
, O O
the O O
true O O
parameter O O
values O O
a0 O O
= O O
−0.3 O O
and O O
a1 O O
= O O
0.5 O O
used O O
to O O
generate O O
the O O
data O O
set O O
are O O
shown O O
by O O
a O O
white O O
cross O O
in O O
the O O
plots O O
in O O
the O O
left O O
column O O
of O O
figure O O
3.7. O O
when O O
we O O
multiply O O
this O O
likelihood B B
function I I
by O O
the O O
prior B B
from O O
the O O
top O O
row O O
, O O
and O O
normalize O O
, O O
we O O
obtain O O
the O O
posterior O O
distribution O O
shown O O
in O O
the O O
middle O O
plot O O
on O O
the O O
second O O
row O O
. O O
if O O
we O O
denote O O
the O O
mixing O O
coefﬁcients O O
by O O
πk O O
, O O
then O O
the O O
mixture B B
distribution I I
can O O
be O O
written O O
p O O
( O O
t|θ O O
) O O
= O O
πkn O O
( O O
t|wt O O
k O O
φ O O
, O O
β O O
−1 O O
) O O
( O O
14.34 O O
) O O
where O O
θ O O
denotes O O
the O O
set O O
of O O
all O O
adaptive O O
parameters O O
in O O
the O O
model O O
, O O
namely O O
w O O
= O O
{ O O
wk O O
} O O
, O O
( O O
cid:22 O O
) O O
π O O
= O O
{ O O
πk O O
} O O
, O O
and O O
β. O O
the O O
log O O
likelihood O O
function O O
for O O
this O O
model O O
, O O
given O O
a O O
data O O
set O O
of O O
k O O
( O O
cid:2 O O
) O O
observations O O
{ O O
φn O O
, O O
tn O O
} O O
, O O
then O O
takes O O
the O O
form O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:23 O O
) O O
k=1 O O
ln O O
p O O
( O O
t|θ O O
) O O
= O O
ln O O
πkn O O
( O O
tn|wt O O
k O O
φn O O
, O O
β O O
−1 O O
) O O
( O O
14.35 O O
) O O
k O O
( O O
cid:2 O O
) O O
where O O
t O O
= O O
( O O
t1 O O
, O O
. O O
the O O
process O O
of O O
selecting O O
a O O
speciﬁc O O
model O O
, O O
given O O
a O O
new O O
input O O
x O O
, O O
can O O
be O O
described O O
by O O
a O O
sequential O B
decision O B
making O O
process O O
corresponding O O
to O O
the O O
traversal O O
of O O
a O O
binary O O
tree O O
( O O
one O O
that O O
splits O O
into O O
two O O
branches O O
at O O
each O O
node B B
) O O
. O O
5.5 O O
regularization B B
in O O
neural O O
networks O O
. O O
taken O O
to O O
have O O
a O O
diagonal B B
rather O O
than O O
an O O
isotropic B B
covariance O O
so O O
that O O
p O O
( O O
xlz O O
) O O
= O O
n O O
( O O
xlwz O O
+ O O
1 O O
' O O
. O O
the O O
dirichlet O O
forms O O
the O O
conjugate B B
prior I I
for O O
the O O
multinomial B B
distribution I I
and O O
rep- O O
resents O O
a O O
generalization B B
of O O
the O O
beta B B
distribution I I
. O O
to O O
see O O
this O O
, O O
we O O
ﬁrst O O
note O O
that O O
minimizing O O
( O O
3.29 O O
) O O
is O O
equivalent O O
to O O
minimizing O O
the O O
unregularized O O
sum-of-squares B B
error I I
( O O
3.12 O O
) O O
subject O O
to O O
the O O
constraint O O
|wj|q O O
( O O
cid:1 O O
) O O
η O O
( O O
3.30 O O
) O O
j=1 O O
for O O
an O O
appropriate O O
value O O
of O O
the O O
parameter O O
η O O
, O O
where O O
the O O
two O O
approaches O O
can O O
be O O
related O O
using O O
lagrange O O
multipliers O O
. O O
again O O
, O O
there O O
can O O
be O O
multiple O O
factor O O
graphs O O
all O O
of O O
which O O
correspond O O
to O O
the O O
same O O
directed B B
graph O O
. O O
if O O
x O O
and O O
y O O
are O O
indepen- O O
dent O O
, O O
then O O
their O O
covariance B B
vanishes O O
. O O
( O O
14.49 O O
) O O
672 O O
14. O O
combining B B
models I I
( O O
cid:5 O O
) O O
n O O
( O O
cid:2 O O
) O O
1 O O
n O O
the O O
m O O
step O O
involves O O
maximization O O
of O O
this O O
function O O
with O O
respect O O
to O O
θ O O
, O O
keeping O O
θold O O
, O O
and O O
hence O O
γnk O O
, O O
ﬁxed O O
. O O
we O O
ﬁrst O O
apply O O
the O O
general O O
result O O
( O O
10.9 O O
) O O
to O O
ﬁnd O O
an O O
expression O O
for O O
the O O
10.1. O O
variational B B
inference I I
467 O O
optimal O O
factor O O
q O O
( O O
cid:1 O O
) O O
1 O O
( O O
z1 O O
) O O
. O O
the O O
rows O O
correspond O O
to O O
the O O
true O O
class O O
, O O
whereas O O
the O O
columns O O
cor- O O
respond O O
to O O
the O O
assignment O O
of O O
class O O
made O O
by O O
our O O
deci- O O
sion B B
criterion O O
. O O
the O O
conjugate B B
prior I I
for O O
µ O O
is O O
the O O
gaussian O O
, O O
the O O
conjugate B B
prior I I
for O O
λ O O
is O O
the O O
wishart O O
, O O
and O O
the O O
conjugate B B
prior I I
for O O
( O O
µ O O
, O O
λ O O
) O O
is O O
the O O
gaussian-wishart O O
. O O
because O O
each O O
summation O O
effectively O O
removes O O
a O O
variable O O
from O O
the O O
distribution O O
, O O
this O O
can O O
be O O
viewed O O
as O O
the O O
removal O O
of O O
a O O
node B B
from O O
the O O
graph O O
. O O
this O O
can O O
produce O O
substantial O O
improvements O O
in O O
performance O O
compared O O
to O O
the O O
use O O
of O O
a O O
single O O
model O O
and O O
is O O
discussed O O
in O O
section O O
14.3. O O
instead O O
of O O
averaging O O
the O O
predictions O O
of O O
a O O
set O O
of O O
models O O
, O O
an O O
alternative O O
form O O
of O O
653 O O
654 O O
14. O O
combining B B
models I O
model O O
combination O O
is O O
to O O
select O O
one O O
of O O
the O O
models O O
to O O
make O O
the O O
prediction O O
, O O
in O O
which O O
the O O
choice O O
of O O
model O O
is O O
a O O
function O O
of O O
the O O
input O O
variables O O
. O O
( O O
1.10 O O
) O O
( O O
1.11 O O
) O O
here O O
p O O
( O O
x O O
, O O
y O O
) O O
is O O
a O O
joint O O
probability B B
and O O
is O O
verbalized O O
as O O
“ O O
the O O
probability B B
of O O
x O O
and O O
y O O
” O O
. O O
2. O O
the O O
hessian O O
forms O O
the O O
basis O O
of O O
a O O
fast O O
procedure O O
for O O
re-training O O
a O O
feed-forward O O
network O O
following O O
a O O
small O O
change O O
in O O
the O O
training B B
data O O
( O O
bishop O O
, O O
1991 O O
) O O
. O O
later O O
in O O
this O O
chapter O O
, O O
and O O
also O O
in O O
subsequent O O
chapters O O
, O O
we O O
shall O O
highlight O O
the O O
sig- O O
niﬁcant O O
limitations O O
of O O
the O O
maximum B B
likelihood I I
approach O O
. O O
by O O
deﬁnition O O
, O O
these O O
conditional B B
distributions O O
correspond O O
to O O
the O O
children O O
of O O
node B B
j O O
, O O
and O O
they O O
therefore O O
also O O
depend O O
on O O
the O O
co-parents B O
of O O
the O O
child O O
nodes O O
, O O
i.e. O O
, O O
the O O
other O O
parents O O
of O O
the O O
child O O
nodes O O
besides O O
node B B
xj O O
itself O O
. O O
the O O
marginal B B
density O O
, O O
however O O
, O O
is O O
given O O
by O O
a O O
superposition O O
of O O
diagonal B B
gaussians O O
( O O
with O O
weighting O O
coefﬁcients O O
given O O
by O O
the O O
class O O
priors O O
) O O
and O O
so O O
will O O
no O O
longer O O
factorize O O
with O O
respect O O
to O O
its O O
components O O
. O O
2.3. O O
the O O
gaussian O O
distribution O O
the O O
gaussian O O
, O O
also O O
known O O
as O O
the O O
normal B B
distribution I I
, O O
is O O
a O O
widely O O
used O O
model O O
for O O
the O O
distribution O O
of O O
continuous O O
variables O O
. O O
as O O
discussed O O
in O O
sec- O O
tion O O
1.5.5 O O
, O O
a O O
common O O
choice O O
of O O
loss B B
function I I
for O O
real-valued O O
variables O O
is O O
the O O
squared O O
loss O B
, O O
for O O
which O O
the O O
optimal O O
solution O O
is O O
given O O
by O O
the O O
conditional B B
expectation I I
of O O
t. O O
although O O
linear O O
models O O
have O O
signiﬁcant O O
limitations O O
as O O
practical O O
techniques O O
for O O
pattern O O
recognition O O
, O O
particularly O O
for O O
problems O O
involving O O
input O O
spaces O O
of O O
high O O
dimen- O O
sionality O O
, O O
they O O
have O O
nice O O
analytical O O
properties O O
and O O
form O O
the O O
foundation O B
for O O
more O O
so- O O
phisticated O O
models O O
to O O
be O O
discussed O O
in O O
later O O
chapters O O
. O O
in O O
section O O
4.4 O O
, O O
we O O
discussed O O
the O O
laplace O O
approximation O O
, O O
which O O
is O O
based O O
on O O
a O O
local B B
gaussian O O
approximation O O
to O O
a O O
mode O O
( O O
i.e. O O
, O O
a O O
maximum O B
) O O
of O O
the O O
distribution O O
. O O
links O O
representing O O
bias B O
parameters O O
have O O
been O O
omitted O O
for O O
clarity O O
. O O
for O O
a O O
continuous O O
data O O
density O O
p O O
( O O
x O O
) O O
, O O
a O O
principal B B
curve I O
is O O
defined O O
as O O
one O O
for O O
which O O
every O O
point O O
on O O
the O O
curve O O
is O O
the O O
mean B B
of O O
all O O
those O O
points O O
in O O
data O O
space O O
that O O
project O O
to O O
it O O
, O O
so O O
that O O
je O O
[ O O
xlgf O O
( O O
x O O
) O O
= O O
> O O
.. O O
] O O
= O O
f O O
( O O
> O O
'' O O
) O O
. O O
international O O
journal O O
of O O
computer O O
vi- O O
sion B O
57 O O
( O O
2 O O
) O O
, O O
137–154 O O
. O O
( O O
7.11 O O
) O O
( O O
7.12 O O
) O O
n=1 O O
here O O
the O O
kernel B O
function I I
is O O
deﬁned O O
by O O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
φ O O
( O O
x O O
) O O
tφ O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
. O O
the O O
noiseless B O
coding I O
theorem I O
( O O
shannon O O
, O O
1948 O O
) O O
states O O
that O O
the O O
entropy B B
is O O
a O O
lower B B
bound I I
on O O
the O O
number O O
of O O
bits B B
needed O I
to O O
transmit O O
the O O
state O O
of O O
a O O
random O O
variable O O
. O O
in O O
practice O O
, O O
sampling B O
methods I O
can O O
be O O
compu- O O
tationally O O
demanding O O
, O O
often O O
limiting O O
their O O
use O O
to O O
small-scale O O
problems O O
. O O
alternatively O O
, O O
we O O
can O O
adopt O O
a O O
generative O O
approach O O
in O O
which O O
we O O
model O O
the O O
class-conditional O O
densities O O
given O O
by O O
p O O
( O O
x|ck O O
) O O
, O O
together O O
with O O
the O O
prior B B
probabilities O O
p O O
( O O
ck O O
) O O
for O O
the O O
classes O O
, O O
and O O
then O O
we O O
compute O O
the O O
required O O
posterior O O
probabilities O O
using O O
bayes O O
’ O O
theorem O O
p O O
( O O
ck|x O O
) O O
= O O
p O O
( O O
x|ck O O
) O O
p O O
( O O
ck O O
) O O
p O O
( O O
x O O
) O O
. O O
with O O
these O O
two O O
approximations O O
, O O
we O O
will O O
obtain O O
models O O
that O O
are O O
analogous O O
to O O
the O O
linear B B
regression I I
and O O
classiﬁcation B B
models O O
discussed O O
in O O
earlier O O
chapters O O
and O O
so O O
we O O
can O O
exploit O O
the O O
results O O
obtained O O
there O O
. O O
the O O
corresponding O O
contours O O
of O O
constant O O
density B B
are O O
given O O
by O O
axis-aligned O O
ellipsoids O O
. O O
finally O O
, O O
we O O
derive O O
an O O
expression O O
for O O
evaluating O O
the O O
messages O O
from O O
variable O O
nodes O O
to O O
factor O O
nodes O O
, O O
again O O
by O O
making O O
use O O
of O O
the O O
( O O
sub- O O
) O O
graph O O
factorization B B
. O O
) O O
, O O
learning B B
in O O
graphical O O
models O O
, O O
pp O O
. O O
appendix O O
b. O O
probability B B
distributions O O
in O O
this O O
appendix O O
, O O
we O O
summarize O O
the O O
main O O
properties O O
of O O
some O O
of O O
the O O
most O O
widely O O
used O O
probability B B
distributions O O
, O O
and O O
for O O
each O O
distribution O O
we O O
list O O
some O O
key O O
statistics O O
such O O
as O O
the O O
expectation B B
e O O
[ O O
x O O
] O O
, O O
the O O
variance B B
( O O
or O O
covariance B B
) O O
, O O
the O O
mode O O
, O O
and O O
the O O
entropy B B
h O O
[ O O
x O O
] O O
. O O
to O O
do O O
this O O
, O O
we O O
apply O O
the O O
k-nearest-neighbour O O
density B B
estimation I I
technique O O
to O O
each O O
class O O
separately O O
and O O
then O O
make O O
use O O
of O O
bayes O O
’ O O
theorem O O
. O O
t O O
1 O O
( O O
12.43 O O
) O O
n=l O O
setting O O
the O O
derivative B B
of O O
the O O
log O O
likelihood O O
with O O
respect O O
to O O
jl O O
equal O O
to O O
zero O O
gives O O
the O O
expected O O
result O O
jl O O
= O O
x O O
where O O
x O O
is O O
the O O
data O O
mean O O
defined O O
by O O
( O O
12.1 O O
) O O
. O O
the O O
ability O O
to O O
categorize O O
correctly O O
new O O
examples O O
that O O
differ O O
from O O
those O O
used O O
for O O
train- O O
ing O O
is O O
known O O
as O O
generalization B B
. O O
we O O
begin O O
by O O
considering O O
the O O
problem O O
of O O
ﬁnding O O
the O O
marginal B B
p O O
( O O
x O O
) O O
for O O
partic- O O
ular O O
variable O O
node B B
x. O O
for O O
the O O
moment O O
, O O
we O O
shall O O
suppose O O
that O O
all O O
of O O
the O O
variables O O
are O O
hidden O O
. O O
having O O
found O O
values O O
α O O
( O O
cid:1 O O
) O O
and O O
β O O
( O O
cid:1 O O
) O O
for O O
the O O
hyperparameters O O
that O O
maximize O O
the O O
marginal B B
likelihood I I
, O O
we O O
can O O
evaluate O O
the O O
predictive B B
distribution I I
over O O
t O O
for O O
a O O
new O O
input O O
x. O O
using O O
( O O
7.76 O O
) O O
and O O
( O O
7.81 O O
) O O
, O O
this O O
is O O
given O O
by O O
p O O
( O O
t|x O O
, O O
x O O
, O O
t O O
, O O
α O O
( O O
cid:1 O O
) O O
, O O
β O O
( O O
cid:1 O O
) O O
) O O
= O O
p O O
( O O
t|x O O
, O O
w O O
, O O
β O O
( O O
cid:1 O O
) O O
) O O
p O O
( O O
w|x O O
, O O
t O O
, O O
α O O
( O O
cid:1 O O
) O O
, O O
β O O
( O O
cid:1 O O
) O O
) O O
dw O O
t|mtφ O O
( O O
x O O
) O O
, O O
σ2 O O
( O O
x O O
) O O
. O O
data O O
set O O
using O O
the O O
graphical O B
representation O O
shown O O
in O O
figure O O
9.6. O O
from O O
( O O
9.7 O O
) O O
the O O
log O O
of O O
the O O
likelihood B B
function I I
is O O
given O O
by O O
n O O
( O O
cid:2 O O
) O O
ln O O
p O O
( O O
x|π O O
, O O
µ O O
, O O
σ O O
) O O
= O O
ln O O
( O O
cid:24 O O
) O O
k O O
( O O
cid:2 O O
) O O
( O O
cid:25 O O
) O O
πkn O O
( O O
xn|µk O O
, O O
σk O O
) O O
. O O
in O O
order O O
to O O
achieve O O
more O O
signiﬁcant O O
improvements O O
, O O
we O O
turn O O
to O O
a O O
more O O
sophisticated O O
technique O O
for O O
building O O
committees O O
, O O
known O O
as O O
boosting B B
. O O
however O O
, O O
this O O
can O O
result O O
in O O
much O O
slower O O
training B O
because O O
, O O
instead O O
of O O
solving O O
k O O
separate O O
optimization O O
problems O O
each O O
over O O
n O O
data O O
points O O
with O O
an O O
overall O O
cost O O
of O O
o O O
( O O
kn O O
2 O O
) O O
, O O
a O O
single O O
optimization O O
problem O O
of O O
size O O
( O O
k O O
− O O
1 O O
) O O
n O O
must O O
be O O
solved O O
giving O O
an O O
overall O O
cost O O
of O O
o O O
( O O
k O O
2n O O
2 O O
) O O
. O O
( O O
b O O
) O O
a O O
factor B B
graph I I
representing O O
the O O
same O O
distribution O O
as O O
the O O
directed B B
graph O O
, O O
whose O O
factor O O
satisﬁes O O
f O O
( O O
x1 O O
, O O
x2 O O
, O O
x3 O O
) O O
= O O
p O O
( O O
x1 O O
) O O
p O O
( O O
x2 O O
) O O
p O O
( O O
x3|x1 O O
, O O
x2 O O
) O O
. O O
a O O
fast O O
procedure O O
for O O
retraining O O
the O O
multilayer B B
perceptron I I
. O O
12.7 O O
( O O
* O O
* O O
) O O
by O O
making O O
use O O
of O O
the O O
results O O
( O O
2.270 O O
) O O
and O O
( O O
2.271 O O
) O O
for O O
the O O
mean B B
and O O
covariance B B
of O O
a O O
general O O
distribution O O
, O O
derive O O
the O O
result O O
( O O
12.35 O O
) O O
for O O
the O O
marginal B B
distribution O O
p O O
( O O
x O O
) O O
in O O
the O O
probabilistic O O
pca O O
model O O
. O O
this O O
issue O O
does O O
not O O
arise O O
when O O
we O O
use O O
maximum B B
likelihood I I
, O O
because O O
the O O
likelihood B B
function I I
p O O
( O O
x|λ O O
) O O
is O O
a O O
simple O O
function O O
of O O
λ O O
and O O
so O O
we O O
are O O
free O O
to O O
use O O
any O O
convenient O O
parameterization O O
. O O
the O O
whole O O
feature B O
map I I
therefore O O
has O O
25 O O
adjustable O O
weight O B
parameters O O
plus O O
one O O
adjustable O O
bias B B
parameter I I
. O O
one O O
dif- O O
ﬁculty O O
they O O
saw O O
arose O O
from O O
the O O
second O O
law O O
of O O
thermo- O O
dynamics O O
, O O
which O O
states O O
that O O
the O O
entropy B B
of O O
a O O
closed O O
system O O
tends O O
to O O
increase O O
with O O
time O O
. O O
w O O
( O O
λ|w O O
, O O
ν O O
) O O
= O O
b O O
( O O
w O O
, O O
ν O O
) O O
|λ| O O
( O O
ν−d−1 O O
) O O
/2 O O
exp O O
where O O
( O O
cid:22 O O
) O O
b O O
( O O
w O O
, O O
ν O O
) O O
≡ O O
|w|−ν/2 O O
2νd/2 O O
πd O O
( O O
d−1 O O
) O O
/4 O O
γ O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
−1 O O
2 O O
tr O O
( O O
w−1λ O O
) O O
( O O
cid:16 O O
) O O
( O O
cid:23 O O
) O O
−1 O O
( O O
cid:15 O O
) O O
d O O
( O O
cid:14 O O
) O O
ν O O
+ O O
1 O O
− O O
i O O
( O O
b.78 O O
) O O
( O O
b.79 O O
) O O
( O O
b.80 O O
) O O
( O O
b.81 O O
) O O
e O O
[ O O
λ O O
] O O
= O O
νw O O
e O O
[ O O
ln|λ| O O
] O O
= O O
( O O
cid:15 O O
) O O
d O O
( O O
cid:2 O O
) O O
i=1 O O
ψ O O
ν O O
+ O O
1 O O
− O O
i O O
2 O O
2 O O
( O O
cid:16 O O
) O O
i=1 O O
+ O O
d O O
ln O O
2 O O
+ O O
ln|w| O O
h O O
[ O O
λ O O
] O O
= O O
− O O
ln O O
b O O
( O O
w O O
, O O
ν O O
) O O
− O O
( O O
ν O O
− O O
d O O
− O O
1 O O
) O O
2 O O
( O O
b.82 O O
) O O
where O O
w O O
is O O
a O O
d O O
× O O
d O O
symmetric O O
, O O
positive B O
deﬁnite I I
matrix I O
, O O
and O O
ψ O O
( O O
· O O
) O O
is O O
the O O
digamma B O
function I I
deﬁned O O
by O O
( O O
b.25 O O
) O O
. O O
using O O
the O O
sum O O
and O O
product O O
rules O O
of O O
probability B B
, O O
we O O
see O O
that O O
the O O
mutual B O
information I O
is O O
related O O
to O O
the O O
conditional B B
entropy I O
through O O
i O O
[ O O
x O O
, O O
y O O
] O O
= O O
h O O
[ O O
x O O
] O O
− O O
h O O
[ O O
x|y O O
] O O
= O O
h O O
[ O O
y O O
] O O
− O O
h O O
[ O O
y|x O O
] O O
. O O
this O O
is O O
an O O
example O O
of O O
a O O
more O O
general O O
result O O
, O O
as O O
we O O
shall O O
see O O
in O O
section O O
4.3.6. O O
to O O
ﬁnd O O
a O O
batch O O
algorithm O O
, O O
we O O
again O O
appeal O O
to O O
the O O
newton-raphson O O
update O O
to O O
obtain O O
the O O
corresponding O O
irls O B
algorithm O O
for O O
the O O
multiclass B B
problem O O
. O O
to O O
find O O
the O O
value O O
of O O
j O O
at O O
the O O
minimum O O
, O O
we O O
back-substitute O O
the O O
solution O O
for O O
u2 O O
into O O
the O O
distortion B B
measure I I
to O O
give O O
j O O
= O O
a2 O O
. O O
let O O
us O O
consider O O
the O O
relative B B
merits O O
of O O
these O O
three O O
alternatives O O
. O O
( O O
8.35 O O
) O O
( O O
cid:6 O O
) O O
∞ O O
n O O
( O O
cid:14 O O
) O O
section O O
3.3 O O
independence O O
property O O
here O O
µ O O
is O O
a O O
latent B O
variable I I
, O O
because O O
its O O
value O O
is O O
not O O
observed O O
. O O
note O O
that O O
some O O
authors O O
consider O O
instead O O
a O O
utility B O
function I O
, O O
whose O O
value O O
they O O
aim O O
to O O
maximize O O
. O O
show O O
that O O
the O O
marginal B B
p O O
( O O
xi O O
) O O
can O O
also O O
be O O
written O O
as O O
the O O
product O O
of O O
the O O
incoming O O
message O O
along O O
any O O
one O O
of O O
the O O
links O O
with O O
the O O
outgoing O O
message O O
along O O
the O O
same O O
link B B
. O O
these O O
messages O O
can O O
themselves O O
be O O
evaluated O O
recursively O O
by O O
passing O O
mes- O O
sages O O
from O O
both O O
ends O O
of O O
the O O
chain O O
to- O O
wards O O
node B B
xn O O
. O O
suppose O O
we O O
have O O
a O O
fully O B
bayesian O O
model O O
in O O
which O O
all O O
parameters O O
are O O
given O O
prior B B
distributions O O
. O O
consider O O
the O O
functional B B
dependence O O
of O O
( O O
2.70 O O
) O O
on O O
xa O O
in O O
which O O
xb O O
is O O
regarded O O
as O O
a O O
constant O O
. O O
from O O
the O O
deﬁnition O O
of O O
probability B B
, O O
these O O
fractions O O
would O O
equal O O
the O O
corresponding O O
probabilities O O
p O O
( O O
y O O
) O O
in O O
the O O
limit O O
n O O
→ O O
∞ O O
. O O
13.24 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
consider O O
a O O
generalization B B
of O O
( O O
13.75 O O
) O O
and O O
( O O
13.76 O O
) O O
in O O
which O O
we O O
include O O
constant O O
terms O O
a O O
and O O
c O O
in O O
the O O
gaussian O O
means O O
, O O
so O O
that O O
p O O
( O O
zn|zn−1 O O
) O O
= O O
n O O
( O O
zn|azn−1 O O
+ O O
a O O
, O O
γ O O
) O O
p O O
( O O
xn|zn O O
) O O
= O O
n O O
( O O
xn|czn O O
+ O O
c O O
, O O
σ O O
) O O
. O O
show O O
that O O
the O O
con- O O
tours O O
of O O
constant O O
error B B
are O O
ellipses O O
whose O O
axes O O
are O O
aligned O O
with O O
the O O
eigenvectors O O
ui O O
, O O
with O O
lengths O O
that O O
are O O
inversely O O
proportional O O
to O O
the O O
square O O
root O O
of O O
the O O
corresponding O O
eigenvalues O O
λi O O
. O O
it O O
therefore O O
sends O O
a O O
message O O
to O O
the O O
node B B
to O O
which O O
it O O
is O O
linked O O
, O O
which O O
in O O
turn O O
will O O
therefore O O
receive O O
all O O
the O O
messages O O
it O O
requires O O
in O O
order O O
to O O
send O O
its O O
own O O
message O O
towards O O
the O O
root O O
, O O
and O O
so O O
again O O
we O O
have O O
a O O
valid O O
algorithm O O
, O O
thereby O O
completing O B
the O O
proof O O
. O O
for O O
m O O
hidden O O
units O O
, O O
any O O
given O O
weight B B
vector I I
will O O
belong O O
to O O
a O O
set O O
of O O
m O O
! O O
equivalent O O
weight O O
vectors O O
associated O O
with O O
this O O
inter- O O
change O O
symmetry O O
, O O
corresponding O O
to O O
the O O
m O O
! O O
different O O
orderings O O
of O O
the O O
hidden O O
units O O
. O O
when O O
combined O O
with O O
decision B B
theory I I
, O O
discussed O O
in O O
section O O
1.5 O O
, O O
it O O
allows O O
us O O
to O O
make O O
optimal O O
predictions O O
given O O
all O O
the O O
information O O
available O O
to O O
us O O
, O O
even O O
though O O
that O O
infor- O O
mation B B
may O O
be O O
incomplete O O
or O O
ambiguous O O
. O O
this O O
approach O O
is O O
known O O
as O O
loopy O B
belief O I
propa- O O
gation O O
( O O
frey O O
and O O
mackay O O
, O O
1998 O O
) O O
and O O
is O O
possible O O
because O O
the O O
message B B
passing I I
rules O O
( O O
8.66 O O
) O O
and O O
( O O
8.69 O O
) O O
for O O
the O O
sum-product B B
algorithm I I
are O O
purely O O
local B B
. O O
in O O
summary O O
then O O
, O O
the O O
hamiltonian O O
dynamical O O
approach O O
involves O O
alternating O O
be- O O
tween O O
a O O
series O O
of O O
leapfrog O O
updates O O
and O O
a O O
resampling O O
of O O
the O O
momentum O O
variables O O
from O O
their O O
marginal B B
distribution O O
. O O
4.3.6 O O
canonical O O
link O B
functions O O
for O O
the O O
linear B B
regression I I
model O O
with O O
a O O
gaussian O O
noise O O
distribution O O
, O O
the O O
error B B
function I I
, O O
corresponding O O
to O O
the O O
negative O O
log O O
likelihood O O
, O O
is O O
given O O
by O O
( O O
3.12 O O
) O O
. O O
show O O
that O O
the O O
corresponding O O
entropy B B
is O O
given O O
by O O
h O O
[ O O
y O O
] O O
= O O
h O O
[ O O
x O O
] O O
+ O O
ln|a| O O
where O O
|a| O O
denotes O O
the O O
determinant O O
of O O
a O O
. O O
another O O
continuous O O
error B B
function I I
that O O
has O O
sometimes O O
been O O
used O O
to O O
solve O O
classiﬁcation B B
problems O O
is O O
the O O
squared O O
error B B
, O O
which O O
is O O
again O O
plotted O O
in O O
figure O O
7.5. O O
it O O
has O O
the O O
property O O
, O O
however O O
, O O
of O O
placing O O
increasing O O
emphasis O O
on O O
data O O
points O O
that O O
are O O
correctly O O
classiﬁed O O
but O O
that O O
are O O
a O O
long O O
way O O
from O O
the O O
decision B B
boundary I I
on O O
the O O
correct O O
side O O
. O O
( O O
2.227 O O
) O O
p O O
( O O
x|η O O
) O O
= O O
setting O O
the O O
gradient O O
of O O
ln O O
p O O
( O O
x|η O O
) O O
with O O
respect O O
to O O
η O O
to O O
zero O O
, O O
we O O
get O O
the O O
following O O
condition O O
to O O
be O O
satisﬁed O O
by O O
the O O
maximum B B
likelihood I I
estimator O O
ηml O O
−∇ O O
ln O O
g O O
( O O
ηml O O
) O O
= O O
1 O O
n O O
u O O
( O O
xn O O
) O O
( O O
2.228 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
which O O
can O O
in O O
principle O O
be O O
solved O O
to O O
obtain O O
ηml O O
. O O
( O O
5.191 O O
) O O
show O O
that O O
there O O
exists O O
an O O
equivalent O O
network O O
, O O
which O O
computes O O
exactly O O
the O O
same O O
func- O O
tion O O
, O O
but O O
with O O
hidden B O
unit I I
activation O O
functions O O
given O O
by O O
tanh O O
( O O
a O O
) O O
where O O
the O O
tanh O O
func- O O
tion O O
is O O
deﬁned O O
by O O
( O O
5.59 O O
) O O
. O O
suppose O O
we O O
have O O
a O O
training B B
set I I
of O O
r O O
observation O O
sequences O O
xr O O
, O O
where O O
r O O
= O O
1 O O
, O O
. O O
5.5.2 O O
early B O
stopping I O
an O O
alternative O O
to O O
regularization B B
as O O
a O O
way O O
of O O
controlling O O
the O O
effective O O
complexity O O
of O O
a O O
network O O
is O O
the O O
procedure O O
of O O
early B O
stopping I O
. O O
as O O
well O O
as O O
being O O
of O O
practical O O
importance O O
in O O
its O O
own O O
right O O
, O O
our O O
discus- O O
sion B O
of O O
bernoulli O O
mixtures O O
will O O
also O O
lay O O
the O O
foundation O B
for O O
a O O
consideration O O
of O O
hidden O O
markov O O
models O O
over O O
discrete O O
variables O O
. O O
we O O
can O O
obtain O O
the O O
tangent O O
line O O
y O O
( O O
x O O
) O O
at O O
a O O
speciﬁc O O
value O O
of O O
x O O
, O O
say O O
x O O
= O O
ξ O O
, O O
by O O
making O O
a O O
ﬁrst B O
order I O
taylor O O
expansion O O
section O O
1.6.1 O O
( O O
10.125 O O
) O O
so O O
that O O
y O O
( O O
x O O
) O O
( O O
cid:1 O O
) O O
f O O
( O O
x O O
) O O
with O O
equality O O
when O O
x O O
= O O
ξ. O O
for O O
our O O
example O O
function O O
f O O
( O O
x O O
) O O
= O O
y O O
( O O
x O O
) O O
= O O
f O O
( O O
ξ O O
) O O
+ O O
f O O
( O O
cid:4 O O
) O O
( O O
ξ O O
) O O
( O O
x O O
− O O
ξ O O
) O O
figure O O
10.10 O O
in O O
the O O
left-hand O O
ﬁg- O O
ure O O
the O O
red O O
curve O O
shows O O
the O O
function O O
exp O O
( O O
−x O O
) O O
, O O
and O O
the O O
blue O O
line O O
shows O O
the O O
tangent O B
at O O
x O O
= O O
ξ O O
deﬁned O O
by O O
( O O
10.125 O O
) O O
with O O
ξ O O
= O O
1. O O
this O O
line O O
has O O
slope O B
λ O O
= O O
f O O
( O O
cid:1 O O
) O O
( O O
ξ O O
) O O
= O O
− O O
exp O O
( O O
−ξ O O
) O O
. O O
the O O
increase O O
in O O
the O O
log O O
likelihood O O
function O O
is O O
therefore O O
greater O O
than O O
the O O
increase O O
in O O
the O O
lower B B
bound I I
, O O
as O O
452 O O
9. O O
mixture B B
models O O
and O O
em O O
kl O O
( O O
q||p O O
) O O
= O O
0 O O
figure O O
9.12 O O
illustration O O
of O O
the O O
e O O
step O O
of O O
the O O
em O O
algorithm O O
. O O
( O O
1.39 O O
) O O
in O O
particular O O
, O O
we O O
can O O
consider O O
the O O
variance B B
of O O
the O O
variable O O
x O O
itself O O
, O O
which O O
is O O
given O O
by O O
var O O
[ O O
x O O
] O O
= O O
e O O
[ O O
x2 O O
] O O
− O O
e O O
[ O O
x O O
] O O
2. O O
for O O
two O O
random O O
variables O O
x O O
and O O
y O O
, O O
the O O
covariance B B
is O O
deﬁned O O
by O O
cov O O
[ O O
x O O
, O O
y O O
] O O
= O O
ex O O
, O O
y O O
[ O O
{ O O
x O O
− O O
e O O
[ O O
x O O
] O O
} O O
{ O O
y O O
− O O
e O O
[ O O
y O O
] O O
} O O
] O O
= O O
ex O O
, O O
y O O
[ O O
xy O O
] O O
− O O
e O O
[ O O
x O O
] O O
e O O
[ O O
y O O
] O O
( O O
1.40 O O
) O O
( O O
1.41 O O
) O O
which O O
expresses O O
the O O
extent O O
to O O
which O O
x O O
and O O
y O O
vary O O
together O O
. O O
we O O
can O O
then O O
write O O
down O O
the O O
standard O O
forward-propagation O O
and O O
backpropagation B B
equations O O
for O O
the O O
evaluation O O
of O O
∇e O O
and O O
apply O O
( O O
5.96 O O
) O O
to O O
these O O
equations O O
to O O
give O O
a O O
set O O
of O O
forward-propagation O O
and O O
backpropagation B B
equations O O
for O O
the O O
evaluation O O
of O O
vth O O
( O O
møller O O
, O O
1993 O O
; O O
pearlmutter O O
, O O
1994 O O
) O O
. O O
in O O
order O O
to O O
communicate O O
the O O
value O O
of O O
x O O
to O O
a O O
receiver O O
, O O
we O O
would O O
need O O
to O O
transmit O O
a O O
message O O
of O O
length O O
3 O O
bits B B
. O O
by O O
contrast O O
, O O
when O O
we O O
combine O O
multiple O O
models O O
, O O
as O O
in O O
( O O
14.5 O O
) O O
, O O
we O O
see O O
that O O
different O O
data O O
points O O
within O O
the O O
data O O
set O O
can O O
potentially O O
be O O
generated O O
from O O
different O O
values O O
of O O
the O O
latent B B
variable I I
z O O
and O O
hence O O
by O O
different O O
components O O
. O O
this O O
sequential O O
view O O
of O O
bayesian O O
inference B B
is O O
very O O
general O O
and O O
applies O O
to O O
any O O
problem O O
in O O
which O O
the O O
observed O O
data O O
are O O
assumed O O
to O O
be O O
independent B B
and O O
identically O O
distributed O O
. O O
this O O
is O O
vation O O
xn+1 O O
to O O
evaluate O O
the O O
corresponding O O
weights O O
w O O
illustrated O O
, O O
for O O
the O O
case O O
of O O
a O O
single O O
variable O O
z O O
, O O
in O O
figure O O
13.23. O O
n O O
} O O
with O O
corresponding O O
weights O O
{ O O
w O O
( O O
l O O
) O O
( O O
l O O
) O O
n+1 O O
∝ O O
p O O
( O O
xn+1|z O O
( O O
l O O
) O O
the O O
particle O B
ﬁltering O I
, O O
or O O
sequential O B
monte O O
carlo O O
, O O
approach O O
has O O
appeared O O
in O O
the O O
literature O O
under O O
various O O
names O O
including O O
the O O
bootstrap B O
ﬁlter I O
( O O
gordon O O
et O O
al. O O
, O O
1993 O O
) O O
, O O
survival B O
of I O
the I O
ﬁttest I O
( O O
kanazawa O O
et O O
al. O O
, O O
1995 O O
) O O
, O O
and O O
the O O
condensation B O
algorithm I O
( O O
isard O O
and O O
blake O O
, O O
1998 O O
) O O
. O O
this O O
two- O O
stage O O
approach O O
is O O
equivalent O O
to O O
assuming O O
that O O
the O O
output O O
y O O
( O O
x O O
) O O
of O O
the O O
support B B
vector I I
machine I I
represents O O
the O O
log-odds O O
of O O
x O O
belonging O O
to O O
class O O
t O O
= O O
1. O O
because O O
the O O
svm O O
training B B
procedure O O
is O O
not O O
speciﬁcally O O
intended O O
to O O
encourage O O
this O O
, O O
the O O
svm O O
can O O
give O O
a O O
poor O O
approximation O O
to O O
the O O
posterior O O
probabilities O O
( O O
tipping O O
, O O
2001 O O
) O O
. O O
xn O O
α O O
w O O
tn O O
n O O
the O O
required O O
predictive B O
distribution I I
for O O
( O O
cid:1 O O
) O O
t O O
is O O
then O O
obtained O O
, O O
from O O
the O O
sum O O
rule O I
of O O
ˆt O O
σ2 O O
ˆx O O
probability B B
, O O
by O O
integrating O O
out O O
the O O
model O O
parameters O O
w O O
so O O
that O O
p O O
( O O
( O O
cid:1 O O
) O O
t| O O
( O O
cid:1 O O
) O O
x O O
, O O
x O O
, O O
t O O
, O O
α O O
, O O
σ2 O O
) O O
∝ O O
p O O
( O O
( O O
cid:1 O O
) O O
t O O
, O O
t O O
, O O
w| O O
( O O
cid:1 O O
) O O
x O O
, O O
x O O
, O O
α O O
, O O
σ2 O O
) O O
dw O O
( O O
cid:6 O O
) O O
where O O
we O O
are O O
implicitly O O
setting O O
the O O
random O O
variables O O
in O O
t O O
to O O
the O O
speciﬁc O O
values O O
ob- O O
served O O
in O O
the O O
data O O
set O O
. O O
it O O
is O O
governed O O
by O O
a O O
single O O
continuous O O
parameter O O
µ O O
∈ O O
[ O O
0 O O
, O O
1 O O
] O O
that O O
represents O O
the O O
probability B B
of O O
x O O
= O O
1. O O
bern O O
( O O
x|µ O O
) O O
= O O
µx O O
( O O
1 O O
− O O
µ O O
) O O
1−x O O
( O O
b.1 O O
) O O
( O O
b.2 O O
) O O
( O O
b.3 O O
) O O
( O O
b.4 O O
) O O
( O O
b.5 O O
) O O
e O O
[ O O
x O O
] O O
= O O
µ O O
var O O
[ O O
x O O
] O O
= O O
µ O O
( O O
1 O O
− O O
µ O O
) O O
( O O
cid:12 O O
) O O
h O O
[ O O
x O O
] O O
= O O
−µ O O
ln O O
µ O O
− O O
( O O
1 O O
− O O
µ O O
) O O
ln O O
( O O
1 O O
− O O
µ O O
) O O
. O O
the O O
predictive B B
distribution I I
shown O O
in O O
figure O O
3.8 O O
allows O O
us O O
to O O
visualize O O
the O O
point- O O
wise O O
uncertainty O O
in O O
the O O
predictions O O
, O O
governed O O
by O O
( O O
3.59 O O
) O O
. O O
thus O O
11.5. O O
the O O
hybrid O O
monte O O
carlo O O
algorithm O O
549 O O
for O O
each O O
position B O
variable I O
there O O
is O O
a O O
corresponding O O
momentum B O
variable I O
, O O
and O O
the O O
joint O O
space O O
of O O
position O O
and O O
momentum O O
variables O O
is O O
called O O
phase B B
space I I
. O O
5.7.2 O O
hyperparameter B B
optimization O O
so O O
far O O
, O O
we O O
have O O
assumed O O
that O O
the O O
hyperparameters O O
α O O
and O O
β O O
are O O
ﬁxed O O
and O O
known O O
. O O
slice B B
sampling I I
can O O
be O O
applied O O
to O O
multivariate O O
distributions O O
by O O
repeatedly O O
sam- O O
pling O O
each O O
variable O O
in O O
turn O O
, O O
in O O
the O O
manner O O
of O I
gibbs O O
sampling O O
. O O
a O O
are O O
deﬁned O O
by O O
( O O
4.149 O O
) O O
and O O
( O O
4.150 O O
) O O
, O O
respectively O O
, O O
and O O
κ O O
( O O
σ2 O O
( O O
4.155 O O
) O O
a O O
) O O
is O O
de- O O
note O O
that O O
the O O
decision B B
boundary I I
corresponding O O
to O O
p O O
( O O
c1|φ O O
, O O
t O O
) O O
= O O
0.5 O O
is O O
given O O
by O O
µa O O
= O O
0 O O
, O O
which O O
is O O
the O O
same O O
as O O
the O O
decision B B
boundary I I
obtained O O
by O O
using O O
the O O
map O O
value O O
for O O
w. O O
thus O O
if O O
the O O
decision O B
criterion O O
is O O
based O O
on O O
minimizing O O
misclassiﬁca- O O
tion O O
rate O O
, O O
with O O
equal O O
prior B B
probabilities O O
, O O
then O O
the O O
marginalization O O
over O O
w O O
has O O
no O O
ef- O O
fect O O
. O O
e O O
[ O O
f O O
( O O
zn O O
) O O
] O O
= O O
f O O
( O O
zn O O
) O O
p O O
( O O
zn|xn O O
) O O
dzn O O
f O O
( O O
zn O O
) O O
p O O
( O O
zn|xn O O
, O O
xn−1 O O
) O O
dzn O O
f O O
( O O
zn O O
) O O
p O O
( O O
xn|zn O O
) O O
p O O
( O O
zn|xn−1 O O
) O O
dzn O O
p O O
( O O
xn|zn O O
) O O
p O O
( O O
zn|xn−1 O O
) O O
dzn O O
= O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:7 O O
) O O
l O O
( O O
cid:2 O O
) O O
= O O
l=1 O O
13.3. O O
linear O O
dynamical O O
systems O O
645 O O
chapter O O
11 O O
13.3.4 O O
particle O O
ﬁlters O O
for O O
dynamical O O
systems O O
which O O
do O O
not O O
have O O
a O O
linear-gaussian O O
, O O
for O O
example O O
, O O
if O O
they O O
use O O
a O O
non-gaussian O O
emission O O
density O O
, O O
we O O
can O O
turn O O
to O O
sampling B O
methods I I
in O O
order O O
to O O
ﬁnd O O
a O O
tractable O O
inference B B
algorithm O O
. O O
1/2 O O
i O O
for O O
the O O
gaussian O O
distribution O O
to O O
be O O
well O O
deﬁned O O
, O O
it O O
is O O
necessary O O
for O O
all O O
of O O
the O O
eigenvalues O O
λi O O
of O O
the O O
covariance B B
matrix I I
to O O
be O O
strictly O O
positive O O
, O O
otherwise O O
the O O
dis- O O
tribution O O
can O O
not O O
be O O
properly O O
normalized O O
. O O
slice B B
sampling I I
involves O O
augmenting O O
z O O
with O O
an O O
additional O O
variable O O
u O O
and O O
then O O
drawing O O
samples O O
from O O
the O O
joint O O
( O O
z O O
, O O
u O O
) O O
space O O
. O O
120 O O
2. O O
probability B B
distributions O O
( O O
cid:26 O O
) O O
− O O
( O O
( O O
cid:4 O O
) O O
x/σ O O
) O O
2 O O
( O O
cid:27 O O
) O O
an O O
example O O
of O O
a O O
scale B O
parameter I I
would O O
be O O
the O O
standard B O
deviation I I
σ O O
of O O
a O O
gaussian O O
distribution O O
, O O
after O O
we O O
have O O
taken O O
account O O
of O O
the O O
location B O
parameter I O
µ O O
, O O
because O O
where O O
( O O
cid:4 O O
) O O
x O O
= O O
x O O
− O O
µ. O O
as O O
discussed O O
earlier O O
, O O
it O O
is O O
often O O
more O O
convenient O O
to O O
work O O
in O O
terms O O
n O O
( O O
x|µ O O
, O O
σ2 O O
) O O
∝ O O
σ O O
−1 O O
exp O O
of O O
the O O
precision O O
λ O O
= O O
1/σ2 O O
rather O O
than O O
σ O O
itself O O
. O O
( O O
2.81 O O
) O O
( O O
2.82 O O
) O O
comparing O O
( O O
2.73 O O
) O O
and O O
( O O
2.82 O O
) O O
, O O
we O O
see O O
that O O
the O O
conditional B B
distribution O O
p O O
( O O
xa|xb O O
) O O
takes O O
a O O
simpler O O
form O O
when O O
expressed O O
in O O
terms O O
of O O
the O O
partitioned B B
precision O O
matrix O O
than O O
when O O
it O O
is O O
expressed O O
in O O
terms O O
of O O
the O O
partitioned B B
covariance O I
matrix O O
. O O
contours O O
of O O
con- O O
stant O O
error B B
are O O
then O O
ellipses O O
whose O O
axes O O
are O O
aligned O O
with O O
the O O
eigenvectors O O
ui O O
of O O
the O O
hes- O O
sian O O
matrix O O
, O O
with O O
lengths O O
that O O
are O O
inversely O O
proportional O O
to O O
the O O
square O O
roots O O
of O O
the O O
correspond- O O
ing O O
eigenvectors O O
λi O O
. O O
from O O
a O O
bayesian O O
perspective O O
, O O
we O O
can O O
view O O
p O O
( O O
x O O
) O O
as O O
the O O
prior B B
distribution O O
for O O
x O O
and O O
p O O
( O O
x|y O O
) O O
as O O
the O O
posterior O O
distribu- O O
tion O O
after O O
we O O
have O O
observed O O
new O O
data O O
y. O O
the O O
mutual B B
information I I
therefore O O
represents O O
the O O
reduction O O
in O O
uncertainty O O
about O O
x O O
as O O
a O O
consequence O O
of O O
the O O
new O O
observation O O
y. O O
exercises O O
1.1 O O
( O O
( O O
cid:1 O O
) O O
) O O
www O O
consider O O
the O O
sum-of-squares B B
error I I
function O O
given O O
by O O
( O O
1.2 O O
) O O
in O O
which O O
the O O
function O O
y O O
( O O
x O O
, O O
w O O
) O O
is O O
given O O
by O O
the O O
polynomial O O
( O O
1.1 O O
) O O
. O O
similarly O O
, O O
for O O
the O O
combination O O
of O O
logistic B B
sigmoid I I
activation O O
function O O
and O O
cross-entropy B O
error I I
function I I
( O O
4.90 O O
) O O
, O O
and O O
for O O
the O O
softmax O O
activation O O
function O I
with O O
the O O
multiclass B B
cross-entropy O O
error B B
function I I
( O O
4.108 O O
) O O
, O O
we O O
again O O
obtain O O
this O O
same O O
simple O O
form O O
. O O
in O O
the O O
case O O
of O O
a O O
single O O
variable O O
x O O
∈ O O
( O O
−∞ O O
, O O
∞ O O
) O O
it O O
is O O
governed O O
by O O
two O O
parameters O O
, O O
the O O
mean B B
µ O O
∈ O O
( O O
−∞ O O
, O O
∞ O O
) O O
and O O
the O O
variance B B
σ2 O O
> O O
0 O O
. O O
note O O
that O O
if O O
we O O
wish O O
to O O
minimize O O
( O O
rather O O
than O O
maximize O O
) O O
the O O
function O O
f O O
( O O
x O O
) O O
sub- O O
ject O O
to O O
an O O
inequality B O
constraint I O
g O O
( O O
x O O
) O O
( O O
cid:2 O O
) O O
0 O O
, O O
then O O
we O O
minimize O O
the O O
lagrangian O O
function O O
l O O
( O O
x O O
, O O
λ O O
) O O
= O O
f O O
( O O
x O O
) O O
− O O
λg O O
( O O
x O O
) O O
with O O
respect O O
to O O
x O O
, O O
again O O
subject O O
to O O
λ O O
( O O
cid:2 O O
) O O
0. O O
finally O O
, O O
it O O
is O O
straightforward O O
to O O
extend O O
the O O
technique O O
of O O
lagrange O O
multipliers O O
to O O
the O O
case O O
of O O
multiple O O
equality O O
and O O
inequality O O
constraints O O
. O O
these O O
are O O
particular O O
cases O O
of O O
a O O
more O O
general O O
result O O
obtained O O
by O O
assuming O O
that O O
the O O
class-conditional O O
densities O O
p O O
( O O
x|ck O O
) O O
are O O
members O O
of O O
the O O
exponential B B
family I I
of O O
distributions O O
. O O
the O O
integration O O
over O O
w O O
can O O
be O O
performed O O
analytically O O
by O O
noting O O
that O O
p O O
( O O
w O O
) O O
is O O
gaussian O O
and O O
h O O
( O O
w O O
, O O
ξ O O
) O O
is O O
the O O
exponential O B
of O O
a O O
quadratic O O
function O O
of O O
w. O O
thus O O
, O O
by O O
completing B B
the I O
square I I
and O O
making O O
use O O
of O O
the O O
standard O O
result O O
for O O
the O O
normalization O O
coefﬁcient O O
of O O
a O O
gaussian O O
distribution O O
, O O
we O O
can O O
obtain O O
a O O
closed O O
form O O
solution O O
which O O
takes O O
the O O
form O O
exercise O O
10.33 O O
exercise O O
10.34 O O
exercise O O
10.35 O O
502 O O
10. O O
approximate O O
inference B B
6 O O
4 O O
2 O O
0 O O
−2 O O
−4 O O
−6 O O
−4 O O
6 O O
4 O O
2 O O
0 O O
−2 O O
−4 O O
−6 O O
−4 O O
−2 O O
0 O O
2 O O
4 O O
5 O O
2 O O
. O O
reject B O
option I O
. O O
they O O
can O O
not O O
be O O
expressed O O
as O O
a O O
function O O
of O O
the O O
difference O O
x O O
− O O
x O O
( O O
cid:4 O O
) O O
, O O
as O O
a O O
consequence O O
of O O
the O O
gaussian O O
weight O O
prior O I
being O O
centred O O
on O O
zero O O
which O O
breaks O O
translation B O
invariance I B
in O O
weight O B
space O O
. O O
biological B O
sequence I O
analysis O O
. O O
for O O
classiﬁcation O O
problems O O
involving O O
two O O
classes O O
, O O
we O O
can O O
use O O
a O O
single O O
logistic B B
sigmoid I I
output O O
, O O
or O O
alternatively O O
we O O
can O O
use O O
a O O
network O O
with O O
two O O
outputs O O
having O O
a O O
softmax O O
output O O
activation B B
function I I
. O O
4.5. O O
bayesian O O
logistic B B
regression I I
we O O
now O O
turn O O
to O O
a O O
bayesian O O
treatment O O
of O O
logistic B B
regression I I
. O O
we O O
see O O
how O O
the O O
bayesian O O
model O O
is O O
able O O
to O O
discover O O
the O O
appropriate O O
dimensionality O O
by O O
suppressing O O
the O O
6 O O
surplus O O
degrees B O
of I O
freedom I I
. O O
thus O O
the O O
predictive B B
distribution I I
p O O
( O O
xn+1|x1 O O
, O O
. O O
to O O
do O O
so O O
, O O
we O O
return O O
to O O
the O O
view O O
of O O
a O O
speciﬁc O O
( O O
directed B O
or O O
undirected B B
) O O
graph O O
as O O
a O O
ﬁlter O O
, O O
so O O
that O O
the O O
set O O
of O O
all O O
possible O O
distributions O O
over O O
the O O
given O O
variables O O
could O O
be O O
reduced O O
to O O
a O O
subset O O
that O O
respects O O
the O O
conditional B B
independencies O O
implied O O
by O O
the O O
graph O O
. O O
10.2 O O
illustration O O
: O O
variational B B
mixture O O
of O O
gaussians O O
. O O
vari- O O
ational O O
bayesian O O
model B O
selection I O
for O O
mixture B O
distributions O O
. O O
this O O
involves O O
including O O
within O O
the O O
training B B
set I I
a O O
sufﬁciently O O
large O O
number O O
of O O
examples O O
of O O
the O O
effects O O
of O O
the O O
various O O
transformations O O
. O O
10.1.2 O O
properties O O
of O O
factorized O O
approximations O O
our O O
approach O O
to O O
variational B B
inference I I
is O O
based O O
on O O
a O O
factorized O O
approximation O O
to O O
the O O
true O O
posterior O O
distribution O O
. O O
( O O
2.169 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:5 O O
) O O
( O O
cid:5 O O
) O O
( O O
cid:13 O O
) O O
shortly O O
, O O
we O O
shall O O
see O O
how O O
this O O
result O O
arises O O
naturally O O
as O O
the O O
maximum B B
likelihood I I
estimator O O
for O O
an O O
appropriately O O
deﬁned O O
distribution O O
over O O
a O O
periodic B B
variable I I
. O O
principles O O
of O O
neurodynam- O O
ics O O
: O O
perceptrons O O
and O O
the O O
theory B B
of O O
brain O O
mech- O O
anisms O O
. O O
m=0 O O
2.1.1 O O
the O O
beta B B
distribution I I
we O O
have O O
seen O O
in O O
( O O
2.8 O O
) O O
that O O
the O O
maximum B B
likelihood I I
setting O O
for O O
the O O
parameter O O
µ O O
in O O
the O O
bernoulli O O
distribution O O
, O O
and O O
hence O O
in O O
the O O
binomial B B
distribution I I
, O O
is O O
given O O
by O O
the O O
fraction O O
of O O
the O O
observations O O
in O O
the O O
data O O
set O O
having O O
x O O
= O O
1. O O
as O O
we O O
have O O
already O O
noted O O
, O O
this O O
can O O
give O O
severely O O
over-ﬁtted O O
results O O
for O O
small O O
data O O
sets O O
. O O
it O O
also O O
has O O
the O O
effect O O
of O O
discouraging O O
pathological O O
solutions O O
in O O
which O O
one O O
or O O
more O O
of O O
the O O
σj O O
goes O O
to O O
zero O O
, O O
corresponding O O
to O O
a O O
gaussian O O
component O O
collapsing O O
onto O O
one O O
of O O
the O O
weight B B
parameter I I
values O O
. O O
to O O
do O O
so O O
, O O
we O O
consider O O
all O O
possible O O
paths O O
from O O
any O O
node B B
in O O
a O O
to O O
any O O
node B B
in O O
b. O O
any O O
such O O
path O O
is O O
said O O
to O O
be O O
blocked O O
if O O
it O O
includes O O
a O O
node B B
such O O
that O O
either O O
( O O
a O O
) O O
the O O
arrows O O
on O O
the O O
path O O
meet O O
either O O
head-to-tail O O
or O O
tail-to-tail O O
at O O
the O O
node B B
, O O
and O O
the O O
node B B
is O O
in O O
the O O
set O O
c O O
, O O
or O O
( O O
b O O
) O O
the O O
arrows O O
meet O O
head-to-head O I
at O O
the O O
node B B
, O O
and O O
neither O O
the O O
node B B
, O O
nor O O
any O O
of O O
its O O
descendants O O
, O O
is O O
in O O
the O O
set O O
c. O O
if O O
all O O
paths O O
are O O
blocked O O
, O O
then O O
a O O
is O O
said O O
to O O
be O O
d-separated O O
from O O
b O O
by O O
c O O
, O O
and O O
the O O
joint O O
distribution O O
over O O
all O O
of O O
the O O
variables O O
in O O
the O O
graph O O
will O O
satisfy O O
a O O
⊥⊥ O O
b O O
| O O
c. O O
the O O
concept O O
of O O
d-separation B B
is O O
illustrated O O
in O O
figure O O
8.22. O O
in O O
graph O O
( O O
a O O
) O O
, O O
the O O
path O O
from O O
a O O
to O O
b O O
is O O
not O O
blocked O O
by O O
node B B
f O O
because O O
it O O
is O O
a O O
tail-to-tail O O
node O B
for O O
this O O
path O O
and O O
is O O
not O O
observed O O
, O O
nor O O
is O O
it O O
blocked O O
by O O
node B B
e O O
because O O
, O O
although O O
the O O
latter O O
is O O
a O O
head-to-head O O
node O O
, O O
it O O
has O O
a O O
descendant O O
c O O
because O O
is O O
in O O
the O O
conditioning O O
set O O
. O O
suppose O O
, O O
however O O
, O O
that O O
we O O
observe O O
values O O
of O O
z O O
one O O
at O O
a O O
time O O
and O O
we O O
wish O O
to O O
ﬁnd O O
a O O
corresponding O O
sequential B B
estimation I I
scheme O O
for O O
θ O O
( O O
cid:1 O O
) O O
. O O
making O O
use O O
of O O
the O O
expressions O O
( O O
12.31 O O
) O O
and O O
( O O
12.32 O O
) O O
for O O
the O O
latent O O
and O O
conditional B B
distributions O O
, O O
respectively O O
, O O
and O O
tak O O
( O O
cid:173 O O
) O O
ing O O
the O O
expectation B B
with O O
respect O O
to O O
the O O
posterior O O
distribution O O
over O O
the O O
latent O O
variables O O
, O O
we O O
obtain O O
note O O
that O O
this O O
depends O O
on O O
the O O
posterior O O
distribution O O
only O O
through O O
the O O
sufficient O O
statis O O
( O O
cid:173 O O
) O O
tics O O
of O O
the O O
gaussian O O
. O O
for O O
any O O
given O O
value O O
of O O
w O O
, O O
the O O
deﬁnition O O
( O O
6.49 O O
) O O
deﬁnes O O
a O O
partic- O O
ular O O
function O O
of O O
x. O O
the O O
probability B B
distribution O O
over O O
w O O
deﬁned O O
by O O
( O O
6.50 O O
) O O
therefore O O
induces O O
a O O
probability B B
distribution O O
over O O
functions O O
y O O
( O O
x O O
) O O
. O O
in O O
transactions O O
of O O
the O O
7th O O
prague O O
conference O O
on O O
information B O
theory I I
, O O
statistical O O
decision O O
functions O O
and O O
random O O
processes O O
, O O
pp O O
. O O
( O O
3.93 O O
) O O
the O O
stationary B B
point O O
of O O
the O O
marginal B B
likelihood I I
therefore O O
satisﬁes O O
0 O O
= O O
n O O
2β O O
exercise O O
3.22 O O
and O O
rearranging O O
we O O
obtain O O
1 O O
β O O
= O O
− O O
1 O O
2 O O
tn O O
− O O
mt O O
n O O
φ O O
( O O
xn O O
) O O
n O O
− O O
γ O O
n=1 O O
tn O O
− O O
mt O O
n O O
φ O O
( O O
xn O O
) O O
. O O
section O O
2.3.5 O O
we O O
see O O
that O O
this O O
sequential O O
approach O O
to O O
learning B B
arises O I
naturally O O
when O O
we O O
adopt O O
a O O
bayesian O O
viewpoint O O
. O O
one O O
solution O O
to O O
this O O
dilemma O O
is O O
to O O
use O O
cross-validation B O
, O O
which O O
is O O
illustrated O O
in O O
figure O O
1.18. O O
this O O
allows O O
a O O
proportion O O
( O O
s O O
− O O
1 O O
) O O
/s O O
of O O
the O O
available O O
data O O
to O O
be O O
used O O
for O O
training O O
while O O
making O O
use O O
of O O
all O O
of O O
the O O
1.4. O B
the O O
curse B B
of I I
dimensionality I I
33 O O
figure O O
1.18 O O
the O O
technique O O
of O O
s-fold O O
cross-validation B O
, O O
illus- O O
trated O O
here O O
for O O
the O O
case O O
of O O
s O O
= O O
4 O O
, O O
involves O O
tak- O O
ing O O
the O O
available O O
data O O
and O O
partitioning O O
it O O
into O O
s O O
groups O O
( O O
in O O
the O O
simplest O O
case O O
these O O
are O O
of O O
equal O O
size O O
) O O
. O O
( O O
b O O
) O O
x1 O O
x1 O O
( O O
a O O
) O O
( O O
b O O
) O O
if O O
we O O
wish O O
to O O
minimize O O
the O O
probability B B
of O O
misclassiﬁcation O O
, O O
this O O
is O O
done O O
by O O
assigning O O
the O O
test O O
point O O
x O O
to O O
the O O
class O O
having O O
the O O
largest O O
posterior B B
probability I I
, O O
corresponding O O
to O O
the O O
largest O O
value O O
of O O
kk/k O O
. O O
for O O
instance O O
, O O
in O O
the O O
graph O O
in O O
figure O O
8.36 O O
, O O
the O O
cycle O O
a–c–b–d–a O B
is O O
chord-less O O
a O O
link B B
could O I
be O O
added O O
between O O
a O O
and O O
b O O
or O O
alternatively O O
between O O
c O O
and O O
d. O O
note O O
that O O
the O O
joint O O
dis- O O
tribution O O
for O O
the O O
resulting O O
triangulated B O
graph I I
is O O
still O O
deﬁned O O
by O O
a O O
product O O
of O O
the O O
same O O
potential O B
functions O O
, O O
but O O
these O O
are O O
now O O
considered O O
to O O
be O O
functions O O
over O O
expanded O O
sets O O
of O O
variables O O
. O O
( O O
1.95 O O
) O O
we O O
now O O
consider O O
the O O
limit O O
n O O
→ O O
∞ O O
, O O
in O O
which O O
the O O
fractions O O
ni/n O O
are O O
held O O
ﬁxed O O
, O O
and O O
apply O O
stirling O O
’ O O
s O O
approximation O O
( O O
cid:2 O O
) O O
i O O
which O O
gives O O
h O O
= O O
− O O
lim O O
n→∞ O O
( O O
cid:5 O O
) O O
i O O
ln O O
n O O
! O O
( O O
cid:8 O O
) O O
n O O
ln O O
n O O
− O O
n O O
( O O
cid:18 O O
) O O
( O O
cid:17 O O
) O O
( O O
cid:18 O O
) O O
( O O
cid:2 O O
) O O
( O O
cid:17 O O
) O O
ni O O
n O O
ln O O
ni O O
n O O
= O O
− O O
( O O
cid:2 O O
) O O
i O O
pi O O
ln O O
pi O O
( O O
1.96 O O
) O O
( O O
1.97 O O
) O O
i O O
ni O O
= O O
n. O O
here O O
pi O O
= O O
limn→∞ O O
( O O
ni/n O O
) O O
is O O
the O O
probability B B
where O O
we O O
have O O
used O O
of O O
an O O
object O O
being O O
assigned O O
to O O
the O O
ith O O
bin O O
. O O
as O O
before O O
, O O
we O O
can O O
use O O
implies O O
that O O
for O O
each O O
x O O
we O O
should O O
minimize O O
the O O
product B O
rule I I
p O O
( O O
x O O
, O O
ck O O
) O O
= O O
p O O
( O O
ck|x O O
) O O
p O O
( O O
x O O
) O O
to O O
eliminate O O
the O O
common O O
factor O O
of O O
p O O
( O O
x O O
) O O
. O O
this O O
leaves O O
the O O
following O O
contribution O O
to O O
the O O
derivative B B
with O O
respect O O
to O O
a O O
component O O
θj O O
of O O
θ O O
n O O
| O O
∂ O O
ln|wn O O
+ O O
c O O
−1 O O
n O O
( O O
cid:2 O O
) O O
−1 O O
2 O O
∂a O O
( O O
cid:1 O O
) O O
n O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:8 O O
) O O
n=1 O O
n=1 O O
= O O
−1 O O
2 O O
∂a O O
( O O
cid:1 O O
) O O
n O O
∂θj O O
( O O
cid:9 O O
) O O
( O O
i O O
+ O O
cn O O
wn O O
) O O
−1cn O O
n O O
( O O
1 O O
− O O
σ O O
( O O
cid:1 O O
) O O
n O O
) O O
( O O
1 O O
− O O
2σ O O
( O O
cid:1 O O
) O O
nn O O
σ O O
( O O
cid:1 O O
) O O
n O O
) O O
∂a O O
( O O
cid:1 O O
) O O
n O O
∂θj O O
( O O
6.92 O O
) O O
n O O
= O O
σ O O
( O O
a O O
( O O
cid:1 O O
) O O
where O O
σ O O
( O O
cid:1 O O
) O O
deﬁnition O O
of O O
wn O O
. O O
for O O
the O O
moment O O
, O O
however O O
, O O
we O O
shall O O
proceed O O
rather O O
informally O O
and O O
consider O O
a O O
simple O O
approach O O
based O O
on O O
curve B O
ﬁtting I I
. O O
note O O
that O O
the O O
equivalent B O
kernel I I
depends O O
on O O
the O O
input O O
values O O
xn O O
from O O
the O O
data O O
set O O
because O O
these O O
appear O O
in O O
the O O
deﬁnition O O
of O O
sn O O
. O O
14.3.1 O O
minimizing O O
exponential O O
error O O
14.3.2 O O
error B B
functions O O
for O O
boosting O O
. O O
substituting O O
( O O
8.68 O O
) O O
into O O
( O O
8.67 O O
) O O
, O O
we O O
406 O O
8. O O
graphical O O
models O O
figure O O
8.48 O O
illustration O O
of O O
the O O
evaluation O O
of O O
the O O
message O B
sent O O
by O O
a O O
variable O O
node B B
to O O
an O O
adjacent O O
factor O O
node O O
. O O
sparse O O
bayesian O O
learning B B
for O O
efﬁcient O O
visual O O
tracking O O
. O O
( O O
b O O
) O O
contours O O
of O O
the O O
marginal B B
probability I I
density O O
p O O
( O O
x O O
) O O
of O O
the O O
mixture B B
distribution I I
. O O
for O O
the O O
case O O
when O O
the O O
eigenvalues O O
are O O
equal O O
, O O
any O O
choice O O
of O O
principal O B
direction O O
will O O
give O O
rise O O
to O O
the O O
same O O
value O O
of O O
j. O O
the O O
general O O
solution O O
to O O
the O O
minimization O O
of O O
j O O
for O O
arbitrary O O
d O O
and O O
arbitrary O O
m O O
< O O
d O O
is O O
obtained O O
by O O
choosing O O
the O O
{ O O
ui O O
} O O
to O O
be O O
eigenvectors O O
of O O
the O O
covariance B B
matrix I I
given O O
by O O
( O O
12.17 O O
) O O
where O O
i O O
= O O
1 O O
, O O
... O O
, O O
d O O
, O O
and O O
as O O
usual O O
the O O
eigenvectors O O
{ O O
ui O O
} O O
are O O
chosen O O
to O O
be O O
orthonor O O
( O O
cid:173 O O
) O O
mal O O
. O O
is O O
a O O
latent B O
variable I I
model O O
thi O O
' O O
can O O
be O O
don O O
. O O
suppose O O
, O O
however O O
, O O
that O O
we O O
have O O
observed O O
a O O
ﬁnite O O
set O O
of O O
training B B
points O O
xn O O
, O O
for O O
n O O
= O O
1 O O
, O O
. O O
when O O
we O O
evaluate O O
the O O
predictive B B
distribution I I
, O O
we O O
require O O
c- O O
1 O O
, O O
which O O
involves O O
the O O
inversion O O
of O O
a O O
d O O
x O O
d O O
matrix O O
. O O
at O O
the O O
same O O
time O O
that O O
the O O
perceptron B B
was O O
being O O
developed O O
, O O
a O O
closely O O
related O O
system O O
called O O
the O O
adaline B O
, O O
which O O
is O O
short O O
for O O
‘ O O
adaptive O O
linear O B
element O O
’ O O
, O O
was O O
being O O
explored O O
by O O
widrow O O
and O O
co-workers O O
. O O
for O O
known O O
mean B B
and O O
unknown O O
precision B O
matrix I I
λ O O
, O O
the O O
conjugate B B
prior I I
is O O
the O O
wishart O O
distribution O O
given O O
by O O
w O O
( O O
λ|w O O
, O O
ν O O
) O O
= O O
b|λ| O O
( O O
ν−d−1 O O
) O O
/2 O O
exp O O
( O O
2.155 O O
) O O
where O O
ν O O
is O O
called O O
the O O
number O O
of O O
degrees B O
of I I
freedom I I
of O O
the O O
distribution O O
, O O
w O O
is O O
a O O
d×d O O
scale O O
matrix O O
, O O
and O O
tr O O
( O O
· O O
) O O
denotes O O
the O O
trace O O
. O O
11.2. O O
markov O O
chain O O
monte O O
carlo O O
in O O
the O O
previous O O
section O O
, O O
we O O
discussed O O
the O O
rejection B B
sampling I I
and O O
importance O B
sam- O O
pling O O
strategies O O
for O O
evaluating O O
expectations O O
of O O
functions O O
, O O
and O O
we O O
saw O O
that O O
they O O
suffer O O
from O O
severe O O
limitations O O
particularly O O
in O O
spaces O O
of O O
high O O
dimensionality O O
. O O
be O O
of O O
low O O
accuracy O O
, O O
which O O
is O O
known O O
as O O
outlier B B
detection O I
or O O
novelty B O
detection I I
( O O
bishop O O
, O O
1994 O O
; O O
tarassenko O O
, O O
1995 O O
) O O
. O O
direct O O
maximization O O
of O O
the O O
likelihood B B
function I I
will O O
therefore O O
lead O O
to O O
complex O O
ex- O O
pressions O O
with O O
no O O
closed-form O O
solutions O O
, O O
as O O
was O O
the O O
case O O
for O O
simple O O
mixture B B
models O O
( O O
recall O O
that O O
a O O
mixture B B
model I I
for O O
i.i.d O O
. O O
224 O O
4. O O
linear O O
models O O
for O O
classification O O
4.26 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
in O O
this O O
exercise O O
, O O
we O O
prove O O
the O O
relation O O
( O O
4.152 O O
) O O
for O O
the O O
convolution O O
of O O
a O O
probit B O
function I O
with O O
a O O
gaussian O O
distribution O O
. O O
these O O
inference B B
problems O O
can O O
be O O
solved O O
efﬁciently O O
using O O
the O O
sum-product B B
algorithm I I
, O O
which O O
in O O
the O O
context O O
of O O
the O O
linear B B
dynamical I I
system I I
gives O O
rise O O
to O O
the O O
kalman O O
ﬁlter O O
and O O
kalman O O
smoother O O
equations O O
. O O
we O O
therefore O O
see O O
that O O
the O O
maximum B B
likelihood I I
result O O
of O O
a O O
point O O
estimate O O
for O O
µ O O
given O O
by O O
( O O
2.143 O O
) O O
is O O
recovered O O
precisely O O
from O O
the O O
bayesian O O
formalism O O
in O O
the O O
limit O O
of O O
an O O
inﬁnite O O
number O O
0 O O
→ O O
∞ O O
in O O
which O O
the O O
of O O
observations O O
. O O
( O O
9.51 O O
) O O
n=1 O O
k=1 O O
again O O
we O O
see O O
the O O
appearance O O
of O O
the O O
summation O O
inside O O
the O O
logarithm O O
, O O
so O O
that O O
the O O
maximum B B
likelihood I I
solution O O
no O O
longer O O
has O O
closed O O
form O O
. O O
the O O
existence O O
of O O
a O O
dual B O
representation I O
based O O
on O O
the O O
gram O O
matrix O O
is O O
a O O
property O O
of O O
many O O
linear O O
models O O
, O O
including O O
the O O
perceptron B B
. O O
these O O
are O O
equivalent O O
concepts O O
if O O
we O O
take O O
the O O
utility O O
to O O
be O O
simply O O
the O O
negative O O
of O O
the O O
loss O B
, O O
and O O
throughout O O
this O O
text O O
we O O
shall O O
use O O
the O O
loss B B
function I I
convention O O
. O O
thus O O
we O O
see O O
that O O
the O O
dual O O
formulation O O
allows O O
the O O
solution O O
to O O
the O O
least-squares O O
problem O O
to O O
be O O
expressed O O
entirely O O
in O O
terms O O
of O O
the O O
kernel B O
function I I
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
. O O
however O O
, O O
if O O
the O O
maximum B B
likelihood I I
solution O O
is O O
found O O
by O O
numerical O O
optimization O O
of O O
the O O
likelihood B B
function I I
, O O
for O O
instance O O
using O O
an O O
algorithm O O
such O O
as O O
conjugate B B
gradients O O
( O O
fletcher O O
, O O
1987 O O
; O O
nocedal O O
and O O
wright O O
, O O
1999 O O
; O O
bishop O O
and O O
nabney O O
, O O
2008 O O
) O O
or O O
through O O
the O O
em O O
algorithm O O
, O O
then O O
the O O
resulting O O
value O O
of O O
r O O
is O O
es O O
( O O
cid:173 O O
) O O
sentially O O
arbitrary O O
. O O
then O O
in O O
the O O
subsequent O O
variational B B
m O O
step O O
, O O
we O O
use O O
this O O
revised O O
pa- O O
rameter O O
posterior O O
distribution O O
to O O
ﬁnd O O
the O O
expected O O
natural B O
parameters I O
e O O
[ O O
ηt O O
] O O
, O O
which O O
gives O O
rise O O
to O O
a O O
revised O O
variational B B
distribution O O
over O O
the O O
latent O O
variables O O
. O O
10.3.1 O O
variational B B
distribution O O
. O O
in O O
so O O
doing O O
, O O
we O O
incur O O
some O O
level O O
of O O
loss O O
that O O
we O O
denote O O
by O O
lkj O O
, O O
which O O
we O O
can O O
view O O
as O O
the O O
k O O
, O O
j O O
element O O
of O O
a O O
loss B B
matrix I I
. O O
it O O
is O O
worth O O
emphasizing O O
, O O
however O O
, O O
that O O
this O O
mechanism O O
for O O
achieving O O
sparsity B O
in O O
probabilistic O O
models O O
through O O
automatic O O
rele- O O
vance O O
determination O O
is O O
quite O O
general O O
and O O
can O O
be O O
applied O O
to O O
any O O
model O O
expressed O O
as O O
an O O
adaptive O O
linear O O
combination O O
of O O
basis O O
functions O O
. O O
( O O
a O O
) O O
contours O O
of O O
constant O O
density B B
for O O
each O O
of O O
the O O
mixture B B
components O O
, O O
in O O
which O O
the O O
3 O O
components O O
are O O
denoted O O
red O O
, O O
blue O O
and O O
green O O
, O O
and O O
the O O
values O O
of O O
the O O
mixing O O
coefﬁcients O O
are O O
shown O O
below O O
each O O
component O O
. O O
models O O
have O O
therefore O O
been O O
proposed O O
( O O
broomhead O O
and O O
lowe O O
, O O
1988 O O
; O O
moody O O
and O O
darken O O
, O O
1989 O O
; O O
poggio O O
and O O
girosi O O
, O O
1990 O O
) O O
, O O
which O O
retain O O
the O O
expan- O O
sion B B
in O O
radial O B
basis O I
functions O O
but O O
where O O
the O O
number O O
m O O
of O O
basis O O
functions O O
is O O
smaller O O
than O O
the O O
number O O
n O O
of O O
data O O
points O O
. O O
we O O
now O O
show O O
that O O
this O O
is O O
a O O
general O O
result O O
of O O
assuming O O
a O O
conditional B B
distribution O O
for O O
the O O
target O O
variable O O
from O O
the O O
exponential B B
family I I
, O O
along O O
with O O
a O O
corresponding O O
choice O O
for O O
the O O
activation B B
function I I
known O O
as O O
the O O
canonical B O
link I O
function I I
. O O
now O O
consider O O
a O O
variational B B
10.4. O O
exponential B O
family I O
distributions O O
491 O O
distribution O O
that O O
factorizes O O
between O O
the O O
latent O O
variables O O
and O O
the O O
parameters O O
, O O
so O O
that O O
q O O
( O O
z O O
, O O
η O O
) O O
= O O
q O O
( O O
z O O
) O O
q O O
( O O
η O O
) O O
. O O
..-+ O O
-- O O
w O O
n O O
12.2.1 O O
maximum B B
likelihood I I
pea O O
we O O
next O O
consider O O
the O O
determination O O
of O O
the O O
model O O
parameters O O
using O O
maximum B B
likelihood I I
. O O
8.29 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
show O O
that O O
if O O
the O O
sum-product B B
algorithm I I
is O O
run O O
on O O
a O O
factor B B
graph I I
with O O
a O O
tree B B
structure O O
( O O
no O O
loops O O
) O O
, O O
then O O
after O O
a O O
ﬁnite O O
number O O
of O O
messages O O
have O O
been O O
sent O O
, O O
there O O
will O O
be O O
no O O
pending O O
messages O O
. O O
a O O
( O O
1.24 O O
) O O
18 O O
1. O O
introduction O O
figure O O
1.12 O O
the O O
concept O O
of O O
probability B B
for O O
discrete O O
variables O O
can O O
be O O
ex- O O
tended O O
to O O
that O O
of O O
a O O
probability B B
density O O
p O O
( O O
x O O
) O O
over O O
a O O
continuous O O
variable O O
x O O
and O O
is O O
such O O
that O O
the O O
probability B B
of O O
x O O
lying O O
in O O
the O O
inter- O O
val O O
( O O
x O O
, O O
x O O
+ O O
δx O O
) O O
is O O
given O O
by O O
p O O
( O O
x O O
) O O
δx O O
for O O
δx O O
→ O O
0. O O
the O O
probability B B
density O O
can O O
be O O
expressed O O
as O O
the O O
derivative B B
of O O
a O O
cumulative O O
distri- O O
bution O O
function O O
p O O
( O O
x O O
) O O
. O O
if O O
we O O
take O O
the O O
graph O O
of O O
figure O O
8.4 O O
and O O
include O O
the O O
deterministic O O
parameters O O
, O O
we O O
obtain O O
the O O
graph O O
shown O O
in O O
figure O O
8.5. O O
when O O
we O O
apply O O
a O O
graphical B B
model I I
to O O
a O O
problem O O
in O O
machine O O
learning O O
or O O
pattern O O
recognition O O
, O O
we O O
will O O
typically O O
set O O
some O O
of O O
the O O
random O O
variables O O
to O O
speciﬁc O O
observed O O
figure O O
8.4 O O
an O O
alternative O O
, O O
more O O
compact O O
, O O
representation O O
of O O
the O O
graph O O
shown O O
in O O
figure O O
8.3 O O
in O O
which O O
we O O
have O O
introduced O O
a O O
plate B O
( O O
the O O
box O O
labelled O O
n O O
) O O
that O O
represents O O
n O O
nodes O O
of O O
which O O
only O O
a O O
single O O
example O O
tn O O
is O O
shown O O
explicitly O O
. O O
the O O
development O O
of O O
sampling B O
methods I I
, O O
such O O
as O O
markov O O
chain O O
monte O O
carlo O O
( O O
discussed O O
in O O
chapter O O
11 O O
) O O
along O O
with O O
dramatic O O
improvements O O
in O O
the O O
speed O O
and O O
memory O O
capacity O O
of O O
computers O O
, O O
opened O O
the O O
door O O
to O O
the O O
practical O O
use O O
of O O
bayesian O O
techniques O O
in O O
an O O
im- O O
pressive O O
range O O
of O O
problem O O
domains O O
. O O
4.1. O O
discriminant O O
functions O O
191 O O
( O O
cid:4 O O
) O O
4.1.6 O O
fisher O O
’ O O
s O O
discriminant O O
for O O
multiple O O
classes O O
we O O
now O O
consider O O
the O O
generalization B B
of O O
the O O
fisher O O
discriminant O O
to O O
k O O
> O O
2 O O
classes O O
, O O
and O O
we O O
shall O O
assume O O
that O O
the O O
dimensionality O O
d O O
of O O
the O O
input O O
space O O
is O O
greater O O
than O O
the O O
k O O
x O O
, O O
where O O
number O O
k O O
of O O
classes O O
. O O
, O O
n O O
, O O
and O O
the O O
scalar O O
cn O O
k O O
kt O O
c O O
308 O O
6. O O
kernel O O
methods O O
( O O
1.00 O O
, O O
4.00 O O
, O O
0.00 O O
, O O
0.00 O O
) O O
( O O
9.00 O O
, O O
4.00 O O
, O O
0.00 O O
, O O
0.00 O O
) O O
( O O
1.00 O O
, O O
64.00 O O
, O O
0.00 O O
, O O
0.00 O O
) O O
3 O O
1.5 O O
0 O O
−1.5 O O
−3 O O
−1 O O
3 O O
1.5 O O
0 O O
−1.5 O O
−3 O O
−1 O O
9 O O
4.5 O O
0 O O
−4.5 O O
0 O O
−0.5 O O
0.5 O O
( O O
1.00 O O
, O O
0.25 O O
, O O
0.00 O O
, O O
0.00 O O
) O O
1 O O
−9 O O
−1 O O
9 O O
4.5 O O
0 O O
−4.5 O O
−9 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
( O O
1.00 O O
, O O
4.00 O O
, O O
10.00 O O
, O O
0.00 O O
) O O
−0.5 O O
0 O O
0.5 O O
1 O O
3 O O
1.5 O O
0 O O
−1.5 O O
−3 O O
−1 O O
4 O O
2 O O
0 O O
−2 O O
−4 O O
−1 O O
0 O O
−0.5 O O
0.5 O O
( O O
1.00 O O
, O O
4.00 O O
, O O
0.00 O O
, O O
5.00 O O
) O O
1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
figure O O
6.5 O O
samples O O
from O O
a O O
gaussian O O
process O O
prior B B
deﬁned O O
by O O
the O O
covariance B B
function O I
( O O
6.63 O O
) O O
. O O
1.14 O O
( O O
( O O
cid:1 O O
) O O
( O O
cid:1 O O
) O O
) O O
show O O
that O O
an O O
arbitrary O O
square O O
matrix O O
with O O
elements O O
wij O O
can O O
be O O
written O O
in O O
the O O
form O O
wij O O
= O O
ws O O
ij O O
are O O
symmetric O O
and O O
anti-symmetric O O
matrices O O
, O O
respectively O O
, O O
satisfying O O
ws O O
ji O O
for O O
all O O
i O O
and O O
j. O O
now O O
consider O O
the O O
second B O
order I I
term O O
in O O
a O O
higher O O
order O O
polynomial O O
in O O
d O O
dimensions O O
, O O
given O O
by O O
ij O O
= O O
−wa O O
ij O O
and O O
wa O O
ij O O
+ O O
wa O O
ij O O
where O O
ws O O
ij O O
= O O
ws O O
ji O O
and O O
wa O O
d O O
( O O
cid:2 O O
) O O
d O O
( O O
cid:2 O O
) O O
show O O
that O O
d O O
( O O
cid:2 O O
) O O
d O O
( O O
cid:2 O O
) O O
wijxixj O O
. O O
if O O
we O O
are O O
given O O
a O O
distribution O O
that O O
is O O
expressed O O
in O O
terms O O
of O O
an O O
undirected B B
graph I I
, O O
then O O
we O O
can O O
readily O O
convert O O
it O O
to O O
a O O
factor B B
graph I I
. O O
2.41 O O
( O O
( O O
cid:12 O O
) O O
) O O
use O O
the O O
deﬁnition O O
of O O
the O O
gamma B B
function I I
( O O
1.141 O O
) O O
to O O
show O O
that O O
the O O
gamma O B
dis- O O
tribution O O
( O O
2.146 O O
) O O
is O O
normalized O O
. O O
5.7.3 O O
bayesian O O
neural O O
networks O O
for O O
classiﬁcation O O
so O O
far O O
, O O
we O O
have O O
used O O
the O O
laplace O O
approximation O O
to O O
develop O O
a O O
bayesian O O
treat- O O
ment O O
of O O
neural B O
network I I
regression O B
models O O
. O O
also O O
, O O
we O O
note O O
that O O
a O O
variable O O
node B B
can O O
send O O
a O O
message O O
to O O
a O O
factor O B
node O O
once O O
it O O
has O O
received O O
incoming O O
messages O O
from O O
all O O
other O O
neighbouring O O
factor O O
nodes O O
. O O
the O O
other O O
parameter O O
in O O
this O O
model O O
is O O
the O O
scalar O O
a O O
2 O O
governing O O
the O O
variance B B
of O O
the O O
conditional B B
distribution O O
. O O
generalized B O
in- O O
verse O O
of O O
matrices O O
and O O
its O O
applications O O
. O O
these O O
are O O
given O O
, O O
in O O
decreasing O O
order O O
of O O
complexity O O
, O O
by O O
: O O
( O O
a O O
) O O
first O O
solve O O
the O O
inference B B
problem O O
of O O
determining O O
the O O
class-conditional O O
densities O O
p O O
( O O
x|ck O O
) O O
for O O
each O O
class O O
ck O O
individually O O
. O O
learning B B
kernel O O
classiﬁers O O
. O O
for O O
a O O
discrete O O
distribution O O
, O O
it O O
is O O
given O O
by O O
( O O
cid:2 O O
) O O
e O O
[ O O
f O O
] O O
= O O
p O O
( O O
x O O
) O O
f O O
( O O
x O O
) O O
( O O
1.33 O O
) O O
x O O
so O O
that O O
the O O
average O O
is O O
weighted O O
by O O
the O O
relative B B
probabilities O O
of O O
the O O
different O O
values O O
of O O
x. O O
in O O
the O O
case O O
of O O
continuous O O
variables O O
, O O
expectations O O
are O O
expressed O O
in O O
terms O O
of O O
an O O
integration O O
with O O
respect O O
to O O
the O O
corresponding O O
probability B B
density O O
( O O
cid:6 O O
) O O
e O O
[ O O
f O O
] O O
= O O
p O O
( O O
x O O
) O O
f O O
( O O
x O O
) O O
dx O O
. O O
which O O
h3.\ O O
pm O O
'' O O
en O O
problematic O O
due O O
to O O
lr.e O O
`` O O
'' O O
'' O O
i O O
< O O
lcmifiabilily O O
of O O
factof O O
analysis O O
associmed O O
with O O
mation B B
' O O
in O O
this O O
'pace O O
. O O
condensation O O
– O O
conditional B B
density O O
propagation O O
for O O
visual O O
tracking O O
. O O
recall O O
that O O
for O O
a O O
discrete O O
multinomial O B
ran- O O
dom O O
variable O O
the O O
expected O O
value O O
of O O
one O O
of O O
its O O
components O O
is O O
just O O
the O O
probability B B
of O O
that O O
component O O
having O O
the O O
value O O
1. O O
thus O O
we O O
are O O
interested O O
in O O
ﬁnding O O
the O O
posterior O O
distribution O O
p O O
( O O
zn|x1 O O
, O O
. O O
the O O
blue O O
arrow O O
denotes O O
the O O
direction O O
of O O
information O O
ﬂow O O
during O O
forward B B
propagation I I
, O O
and O O
the O O
red O O
arrows O O
indicate O O
the O O
backward O O
propagation O O
of O O
error B B
information O I
. O O
in O O
fact O O
, O O
we O O
can O O
write O O
out O O
all O O
four O O
conditional B B
probabilities O O
for O O
the O O
type O O
of O O
fruit O O
, O O
given O O
the O O
selected O O
box O O
p O O
( O O
f O O
= O O
a|b O O
= O O
r O O
) O O
= O O
1/4 O O
p O O
( O O
f O O
= O O
o|b O O
= O O
r O O
) O O
= O O
3/4 O O
p O O
( O O
f O O
= O O
a|b O O
= O O
b O O
) O O
= O O
3/4 O O
p O O
( O O
f O O
= O O
o|b O O
= O O
b O O
) O O
= O O
1/4 O O
. O O
it O O
should O O
be O O
emphasized O O
that O O
these O O
diagrams O O
do O O
not O O
represent O O
probabilistic O O
graphical O O
models O O
of O O
the O O
kind O O
to O O
be O O
consid- O O
ered O O
in O O
chapter O O
8 O O
because O O
the O O
internal O O
nodes O O
represent O O
deterministic O O
variables O O
rather O O
than O O
stochastic B B
ones O O
. O O
4. O O
select O O
a O O
candidate O O
basis B O
function I I
ϕi O O
. O O
support B O
vector I O
machines O O
and O O
other O O
kernel-based O O
learning B B
methods O O
. O O
additive O O
logistic B O
regression I B
: O O
a O O
statistical O O
view O O
of O O
boosting B B
. O O
international O O
journal O O
of O O
computer O O
vi- O O
sion B O
29 O O
( O O
1 O O
) O O
, O O
5–18 O O
. O O
we O O
see O O
that O O
the O O
mixture B B
model I O
gives O O
a O O
much O O
better O O
representation O O
of O O
the O O
data O O
distribution O O
, O O
and O O
this O O
is O O
reﬂected O O
in O O
the O O
higher O O
likelihood O O
value O O
. O O
similarly O O
, O O
if O O
n O O
= O O
0 O O
, O O
then O O
the O O
posterior O O
distribution O O
reverts O O
to O O
the O O
prior B B
. O O
chunking B O
can O O
be O O
implemented O O
using O O
protected B O
conjugate I B
gradients I O
( O O
burges O O
, O O
1998 O O
) O O
. O O
, O O
xn−1 O O
) O O
= O O
p O O
( O O
xn|xn−1 O O
) O O
( O O
13.3 O O
) O O
which O O
is O O
easily O O
veriﬁed O O
by O O
direct O O
evaluation O O
starting O O
from O O
( O O
13.2 O O
) O O
and O O
using O O
the O O
prod- O O
uct O O
rule O O
of O O
probability B B
. O O
7.2 O O
( O O
( O O
cid:12 O O
) O O
) O O
show O O
that O O
, O O
if O O
the O O
1 O O
on O O
the O O
right-hand O O
side O O
of O O
the O O
constraint O O
( O O
7.5 O O
) O O
is O O
replaced O O
by O O
some O O
arbitrary O O
constant O O
γ O O
> O O
0 O O
, O O
the O O
solution O O
for O O
the O O
maximum B B
margin I I
hyperplane O O
is O O
unchanged O O
. O O
466 O O
10. O O
approximate O O
inference B B
divergence O O
, O O
and O O
the O O
minimum O O
occurs O O
when O O
qj O O
( O O
zj O O
) O O
= O O
( O O
cid:4 O O
) O O
p O O
( O O
x O O
, O O
zj O O
) O O
. O O
every O O
node B B
is O O
then O O
in O O
a O O
position O O
to O O
send O O
a O O
message O O
. O O
at O O
each O O
stage O O
, O O
the O O
posterior O O
is O O
a O O
beta B B
distribution I I
with O O
some O O
total O O
number O O
of O O
( O O
prior B B
and O O
actual O O
) O O
observed O O
values O O
for O O
x O O
= O O
1 O O
and O O
x O O
= O O
0 O O
given O O
by O O
the O O
parameters O O
a O O
and O O
b. O O
incorporation O O
of O O
an O O
additional O O
observation O O
of O O
x O O
= O O
1 O O
simply O O
corresponds O O
to O O
incrementing O O
the O O
value O O
of O O
a O O
by O O
1 O O
, O O
whereas O O
for O O
an O O
observation O O
of O O
x O O
= O O
0 O O
we O O
increment O O
b O O
by O O
1. O O
figure O O
2.3 O O
illustrates O O
one O O
step O O
in O O
this O O
process O O
. O O
670 O O
14. O O
combining B B
models I I
1.5 O O
1 O O
0.5 O O
0 O O
−0.5 O O
−1 O O
−1.5 O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
1.5 O O
1 O O
0.5 O O
0 O O
−0.5 O O
−1 O O
−1.5 O O
1.5 O O
1 O O
0.5 O O
0 O O
−0.5 O O
−1 O O
−1.5 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
0 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
0 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
0 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
figure O O
14.8 O O
example O O
of O O
a O O
synthetic O O
data O O
set O O
, O O
shown O O
by O O
the O O
green O O
points O O
, O O
having O O
one O O
input O O
variable O O
x O O
and O O
one O O
target O O
variable O O
t O O
, O O
together O O
with O O
a O O
mixture O B
of O O
two O O
linear B O
regression I I
models O O
whose O O
mean B B
functions O O
y O O
( O O
x O O
, O O
wk O O
) O O
, O O
where O O
k O O
∈ O O
{ O O
1 O O
, O O
2 O O
} O O
, O O
are O O
shown O O
by O O
the O O
blue O O
and O O
red O O
lines O O
. O O
the O O
dashed O O
blue O O
lines O O
show O O
the O O
z O O
= O O
0.5 O O
contours O O
for O O
each O O
of O O
the O O
hidden O O
units O O
, O O
and O O
the O O
red O O
line O O
shows O O
the O O
y O O
= O O
0.5 O O
decision B O
surface I I
for O O
the O O
net- O O
work O O
. O O
to O O
do O O
this O O
, O O
simply O O
note O O
that O O
the O O
joint O O
distribution O O
of O O
all O O
latent O O
and O O
observed O O
variables O O
in O O
a O O
linear B O
dynamical I I
system I I
is O O
gaussian O O
, O O
and O O
hence O O
all O O
conditionals O O
and O O
marginals O O
will O O
also O O
be O O
gaussian O O
, O O
and O O
then O O
make O O
use O O
of O O
the O O
result O O
( O O
2.98 O O
) O O
. O O
anachronistically O O
, O O
this O O
process O O
of O O
‘ O O
marrying O O
the O O
parents O O
’ O O
has O O
become O O
known O O
as O O
moralization B B
, O O
and O O
the O O
resulting O O
undirected B B
graph I I
, O O
after O O
dropping O O
the O O
arrows O O
, O O
is O O
called O O
the O O
moral O O
graph O O
. O O
8.6 O O
( O O
( O O
cid:12 O O
) O O
) O O
for O O
the O O
model O O
shown O O
in O O
figure O O
8.13 O O
, O O
we O O
have O O
seen O O
that O O
the O O
number O O
of O O
parameters O O
required O O
to O O
specify O O
the O O
conditional B B
distribution O O
p O O
( O O
y|x1 O O
, O O
. O O
7.1.1 O O
overlapping O O
class O O
distributions O O
so O O
far O O
, O O
we O O
have O O
assumed O O
that O O
the O O
training B B
data O O
points O O
are O O
linearly B B
separable I I
in O O
the O O
feature B O
space I I
φ O O
( O O
x O O
) O O
. O O
to O O
see O O
this O O
, O O
imagine O O
choosing O O
a O O
perturbation O O
η O O
( O O
x O O
) O O
that O O
is O O
zero O O
everywhere O O
except O O
in O O
the O O
neighbourhood O O
of O O
a O O
point O O
( O O
cid:1 O O
) O O
x O O
, O O
in O O
which O O
case O O
the O O
functional B B
derivative O O
must O O
be O O
zero O O
at O O
x O O
= O O
( O O
cid:1 O O
) O O
x. O O
however O O
, O O
because O O
this O O
must O O
be O O
true O O
for O O
every O O
choice O O
of O O
( O O
cid:1 O O
) O O
x O O
, O O
the O O
functional B B
derivative O O
must O O
vanish O O
for O O
all O O
values O O
of O O
x. O O
consider O O
a O O
functional B B
that O O
is O O
deﬁned O O
by O O
an O O
integral O O
over O O
a O O
function O O
g O O
( O O
y O O
, O O
y O O
( O O
cid:1 O O
) O O
that O O
depends O O
on O O
both O O
y O O
( O O
x O O
) O O
and O O
its O O
derivative B B
y O O
dence O O
on O O
x O O
f O O
[ O O
y O O
] O O
= O O
g O O
( O O
y O O
( O O
x O O
) O O
, O O
y O O
, O O
x O O
) O O
( O O
cid:1 O O
) O O
( O O
x O O
) O O
as O O
well O O
as O O
having O O
a O O
direct O O
depen- O O
( O O
cid:1 O O
) O O
( O O
x O O
) O O
, O O
x O O
) O O
dx O O
( O O
d.5 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:13 O O
) O O
where O O
the O O
value O O
of O O
y O O
( O O
x O O
) O O
is O O
assumed O O
to O O
be O O
ﬁxed O O
at O O
the O O
boundary O O
of O O
the O O
region O O
of O O
integration O O
( O O
which O O
might O O
be O O
at O O
inﬁnity O O
) O O
. O O
if O O
we O O
change O O
the O O
sign O O
of O O
all O O
of O O
the O O
weights O O
and O O
the O O
bias B B
feeding O O
into O O
a O O
particular O O
hidden B O
unit I O
, O O
then O O
, O O
for O O
a O O
given O O
input O O
pattern O O
, O O
the O O
sign O O
of O O
the O O
activation O B
of O O
the O O
hidden B O
unit I O
will O O
be O O
reversed O O
, O O
because O O
‘ O O
tanh O O
’ O O
is O O
an O O
odd O O
function O O
, O O
so O O
that O O
tanh O O
( O O
−a O O
) O O
= O O
− O O
tanh O O
( O O
a O O
) O O
. O O
the O O
likelihood B B
function I I
, O O
that O O
is O O
the O O
probability B B
of O O
the O O
observed O O
data O O
given O O
µ O O
, O O
viewed O O
as O O
a O O
function O O
of O O
µ O O
, O O
is O O
given O O
by O O
1 O O
p O O
( O O
x|µ O O
) O O
= O O
p O O
( O O
xn|µ O O
) O O
= O O
( O O
2.137 O O
) O O
again O O
we O O
emphasize O O
that O O
the O O
likelihood B B
function I I
p O O
( O O
x|µ O O
) O O
is O O
not O O
a O O
probability B B
distri- O O
bution O O
over O O
µ O O
and O O
is O O
not O O
normalized O O
. O O
in O O
the O O
subsequent O O
m O O
step O O
, O O
we O O
maximize O O
this O O
expectation B B
. O O
, O O
n. O O
from O O
( O O
6.49 O O
) O O
, O O
this O O
vector O O
is O O
given O O
by O O
y O O
= O O
φw O O
( O O
6.51 O O
) O O
where O O
φ O O
is O O
the O O
design B B
matrix I I
with O O
elements O O
φnk O O
= O O
φk O O
( O O
xn O O
) O O
. O O
information B O
theory I B
, O O
infer- O O
ence O O
and O O
learning B B
algorithms O O
. O O
all O O
of O O
these O O
distributions O O
are O O
members O O
of O O
the O O
exponential B B
family I I
and O O
are O O
widely O O
used O O
as O O
building O O
blocks O O
for O O
more O O
sophisticated O O
probabilistic O O
models O O
. O O
the O O
model O O
is O O
then O O
known O O
as O O
a O O
homogeneous B B
markov O O
chain O O
. O O
similarly O O
, O O
from O O
( O O
2.151 O O
) O O
we O O
see O O
that O O
the O O
n O O
data O O
points O O
contribute O O
n O O
σ2 O O
ml O O
is O O
the O O
variance B B
, O O
and O O
so O O
we O O
can O O
interpret O O
the O O
parameter O O
b0 O O
in O O
the O O
prior B B
as O O
arising O O
from O O
the O O
2a0 O O
‘ O O
effective O O
’ O O
prior B B
observations O O
having O O
variance B B
2b0/ O O
( O O
2a0 O O
) O O
= O O
b0/a0 O O
. O O
if O O
we O O
allow O O
any O O
possible O O
choice O O
for O O
q O O
( O O
z O O
) O O
, O O
then O O
the O O
maximum O B
of O O
the O O
lower B B
bound I I
occurs O O
when O O
the O O
kl O O
diver- O O
gence O O
vanishes O O
, O O
which O O
occurs O O
when O O
q O O
( O O
z O O
) O O
equals O O
the O O
posterior O O
distribution O O
p O O
( O O
z|x O O
) O O
. O O
the O O
corresponding O O
transformation O O
for O O
a O O
multiclass B B
distribution O O
is O O
given O O
by O O
the O O
softmax B O
function I I
. O O
5.5.2 O O
early B O
stopping I O
. O O
the O O
hmm O O
is O O
widely O O
used O O
in O O
speech B B
recognition I I
( O O
jelinek O O
, O O
1997 O O
; O O
rabiner O O
and O O
juang O O
, O O
1993 O O
) O O
, O O
natural B O
language I O
modelling I O
( O O
manning O O
and O O
sch¨utze O O
, O O
1999 O O
) O O
, O O
on-line O O
handwriting O O
recognition O O
( O O
nag O O
et O O
al. O O
, O O
1986 O O
) O O
, O O
and O O
for O O
the O O
analysis O O
of O O
biological O O
sequences O O
such O O
as O O
proteins O O
and O O
dna O O
( O O
krogh O O
et O O
al. O O
, O O
1994 O O
; O O
durbin O O
et O O
al. O O
, O O
1998 O O
; O O
baldi O O
and O O
brunak O O
, O O
2001 O O
) O O
. O O
we O O
can O O
evaluate O O
the O O
mean B B
and O O
covariance B B
of O O
this O O
distribution O O
by O O
taking O O
moments O O
, O O
and O O
interchanging O O
the O O
order O O
of O O
integration O O
over O O
a O O
and O O
w O O
, O O
so O O
that O O
µa O O
= O O
e O O
[ O O
a O O
] O O
= O O
p O O
( O O
a O O
) O O
a O O
da O O
= O O
q O O
( O O
w O O
) O O
wtφ O O
dw O O
= O O
wt O O
mapφ O O
( O O
4.149 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:26 O O
) O O
where O O
we O O
have O O
used O O
the O O
result O O
( O O
4.144 O O
) O O
for O O
the O O
variational B B
posterior O O
distribution O O
q O O
( O O
w O O
) O O
. O O
to O O
be O O
precise O O
, O O
the O O
error B B
function I I
is O O
concave O O
, O O
as O O
we O O
shall O O
see O O
shortly O O
, O O
and O O
hence O O
has O O
a O O
unique O O
minimum O O
. O O
3.3.1 O O
parameter O O
distribution O O
we O O
begin O O
our O O
discussion O O
of O O
the O O
bayesian O O
treatment O O
of O O
linear B O
regression I I
by O O
in- O O
troducing O O
a O O
prior B B
probability O O
distribution O O
over O O
the O O
model O O
parameters O O
w. O O
for O O
the O O
mo- O O
ment O O
, O O
we O O
shall O O
treat O O
the O O
noise O O
precision B O
parameter I I
β O O
as O O
a O O
known O O
constant O O
. O O
because O O
the O O
covariance B B
matrix I I
cov O O
[ O O
x O O
] O O
is O O
no O O
longer O O
diagonal B B
, O O
the O O
mixture B B
distribution I I
can O O
capture O O
correlations O O
between O O
the O O
vari- O O
ables O O
, O O
unlike O O
a O O
single O O
bernoulli O O
distribution O O
. O O
the O O
corresponding O O
inverse B B
problem I I
, O O
shown O O
on O O
the O O
right O O
, O O
is O O
obtained O O
by O O
exchanging O O
the O O
roles O O
of O O
x O O
and O O
t. O O
here O O
the O O
same O O
net- O O
work O O
trained O O
again O O
by O O
minimizing O O
the O O
sum-of-squares B B
error I I
function O O
gives O O
a O O
very O O
poor O O
ﬁt O O
to O O
the O O
data O O
due O O
to O O
the O O
multimodality B B
of O O
the O O
data O O
set O O
. O O
because O O
the O O
manifold B B
in O O
gtm O O
is O O
defined O O
as O O
a O O
continuous O O
surface O O
, O O
not O O
just O O
at O O
the O O
prototype O O
vectors O O
as O O
in O O
the O O
som O O
, O O
it O O
is O O
possible O O
to O O
compute O O
the O O
magnification O O
factors O O
corresponding O O
to O O
the O O
local B B
expansions O O
and O O
compressions O O
of O O
the O O
manifold B B
needed O O
to O O
fit O O
the O O
data O O
set O O
( O O
bishop O O
et O O
al. O O
, O O
1997b O O
) O O
as O O
well O O
as O O
the O O
directional O O
curvatures O O
of O O
the O O
manifold B B
( O O
tino O O
et O O
al. O O
, O O
2001 O O
) O O
. O O
here O O
β O O
was O O
initialized O O
to O O
the O O
reciprocal O O
of O O
the O O
true O O
variance B B
of O O
the O O
set O O
of O O
target O O
values O O
. O O
we O O
will O O
introduce O O
the O O
basic O O
concepts O O
of O O
probability B B
theory O O
by O O
considering O O
a O O
sim- O O
ple O O
example O O
. O O
in O O
section O O
1.5.4 O O
, O O
we O O
discussed O O
the O O
distinction O O
between O O
the O O
discriminative O B
and O O
the O O
generative O B
approaches O O
to O O
classiﬁcation B B
. O O
the O O
primary O O
disadvantage O O
of O O
factorial B O
hmms O O
, O O
however O O
, O O
lies O O
in O O
the O O
additional O O
complexity O O
of O O
training B B
them O O
. O O
let O O
us O O
consider O O
for O O
a O O
moment O O
the O O
problem O O
of O O
approx- O O
imating O O
a O O
general O O
distribution O O
by O O
a O O
factorized B O
distribution I O
. O O
because O O
the O O
error B B
e O O
( O O
w O O
) O O
is O O
a O O
smooth O O
continuous O O
function O O
of O O
w O O
, O O
its O O
smallest O O
value O O
will O O
occur O O
at O O
a O O
section O O
5.1.1 O O
point O O
in O O
weight O B
space O O
such O O
that O O
the O O
gradient O O
of O O
the O O
error B B
function I I
vanishes O O
, O O
so O O
that O O
5.2. O O
network O O
training B B
237 O O
∇e O O
( O O
w O O
) O O
= O O
0 O O
( O O
5.26 O O
) O O
as O O
otherwise O O
we O O
could O O
make O O
a O O
small O O
step O O
in O O
the O O
direction O O
of O O
−∇e O O
( O O
w O O
) O O
and O O
thereby O O
further O O
reduce O O
the O O
error B B
. O O
the O O
terms O O
involving O O
different O O
n O O
are O O
independent B B
and O O
so O O
we O O
can O O
optimize O O
for O O
each O O
n O O
separately O O
by O O
choosing O O
rnk O O
to O O
be O O
1 O O
for O O
whichever O O
value O O
of O O
k O O
gives O O
the O O
minimum O O
value O O
of O O
( O O
cid:5 O O
) O O
xn O O
− O O
µk O O
( O O
cid:5 O O
) O O
2. O O
in O O
other O O
words O O
, O O
we O O
simply O O
assign O O
the O O
nth O O
data O O
point O O
to O O
the O O
closest O O
cluster O O
centre O O
. O O
the O O
adaptive O B
re- O O
jection O O
sampling B O
methods I I
discussed O O
in O O
section O O
11.1.3 O O
therefore O O
provide O O
a O O
framework O O
for O O
monte O O
carlo O O
sampling O O
from O O
directed B B
graphs O O
with O O
broad O O
applicability O O
. O O
1.2.6 O O
bayesian O O
curve B O
ﬁtting I I
. O O
, O O
xn O O
) O O
t O O
, O O
the O O
noise O O
variance B B
σ2 O O
, O O
and O O
the O O
hyperparameter B B
α O O
representing O O
the O O
precision O O
of O O
the O O
gaussian O O
prior B B
over O O
w O O
, O O
all O O
of O O
which O O
are O O
parameters O O
of O O
the O O
model O O
rather O O
than O O
random O O
variables O O
. O O
( O O
14.57 O O
) O O
exercises O O
675 O O
14.6 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
by O O
differentiating O O
the O O
error B B
function I I
( O O
14.23 O O
) O O
with O O
respect O O
to O O
αm O O
, O O
show O O
that O O
the O O
parameters O O
αm O O
in O O
the O O
adaboost O O
algorithm O O
are O O
updated O O
using O O
( O O
14.17 O O
) O O
in O O
which O O
m O O
is O O
deﬁned O O
by O O
( O O
14.16 O O
) O O
. O O
recall O O
that O O
to O O
compute O O
the O O
likelihood O B
we O O
should O O
take O O
the O O
joint O O
distribution O O
p O O
( O O
x O O
, O O
z O O
) O O
and O O
sum O O
over O O
all O O
possible O O
values O O
of O O
z. O O
each O O
such O O
value O O
represents O O
a O O
particular O O
choice O O
of O O
hidden O O
state O O
for O O
every O O
time O O
step O O
, O O
in O O
other O O
words O O
every O O
term O O
in O O
the O O
summation O O
is O O
a O O
path O O
through O O
the O O
lattice B B
diagram I I
, O O
and O O
recall O O
that O O
there O O
are O O
exponentially O O
many O O
such O O
paths O O
. O O
( O O
4.152 O O
) O O
exercise O O
4.24 O O
exercise O O
4.25 O O
exercise O O
4.26 O O
220 O O
4. O O
linear O O
models O O
for O O
classification O O
we O O
now O O
apply O O
the O O
approximation O O
σ O O
( O O
a O O
) O O
( O O
cid:7 O O
) O O
φ O O
( O O
λa O O
) O O
to O O
the O O
probit O B
functions O O
appearing O O
on O O
both O O
sides O O
of O O
this O O
equation O O
, O O
leading O O
to O O
the O O
following O O
approximation O O
for O O
the O O
convo- O O
lution O O
of O O
a O O
logistic B B
sigmoid I I
with O O
a O O
gaussian O O
( O O
cid:6 O O
) O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
σ O O
( O O
a O O
) O O
n O O
( O O
a|µ O O
, O O
σ2 O O
) O O
da O O
( O O
cid:7 O O
) O O
σ O O
κ O O
( O O
σ2 O O
) O O
µ O O
( O O
4.153 O O
) O O
exercises O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
where O O
we O O
have O O
deﬁned O O
( O O
4.154 O O
) O O
applying O O
this O O
result O O
to O O
( O O
4.151 O O
) O O
we O O
obtain O O
the O O
approximate O O
predictive B O
distribution I I
κ O O
( O O
σ2 O O
) O O
= O O
( O O
1 O O
+ O O
πσ2/8 O O
) O O
−1/2 O O
. O O
1.5. O O
decision B B
theory I I
we O O
have O O
seen O O
in O O
section O O
1.2 O O
how O O
probability B B
theory O O
provides O O
us O O
with O O
a O O
consistent B B
mathematical O O
framework O O
for O O
quantifying O O
and O O
manipulating O O
uncertainty O O
. O O
( O O
10.42 O O
) O O
it O O
is O O
remarkable O O
that O O
this O O
is O O
the O O
only O O
assumption O O
that O O
we O O
need O O
to O O
make O O
in O O
order O O
to O O
obtain O O
a O O
tractable O O
practical O O
solution O O
to O O
our O O
bayesian O O
mixture B B
model I I
. O O
3 O O
linear O O
models O O
for B B
regression I I
3.1 O O
. O O
for O O
the O O
purposes O O
of O O
d-separation B B
, O O
parameters O O
such O O
as O O
α O O
and O O
σ2 O O
in O O
figure O O
8.5 O O
, O O
indicated O O
by O O
small O O
ﬁlled O O
circles O O
, O O
behave O O
in O O
the O O
same O O
was O O
as O O
observed O O
nodes O O
. O O
thus O O
in O O
general O O
the O O
number O O
of O O
parameters O O
required O O
to O O
specify O O
this O O
conditional B B
distribution O O
will O O
grow O O
exponentially O O
with O O
m. O O
we O O
can O O
ob- O O
tain O O
a O O
more O O
parsimonious O O
form O O
for O O
the O O
conditional B B
distribution O O
by O O
using O O
a O O
logistic B B
sigmoid I I
function O O
acting O O
on O O
a O O
linear O O
combination O O
of O O
the O O
parent O O
variables O O
, O O
giving O O
( O O
cid:22 O O
) O O
( O O
cid:23 O O
) O O
m O O
( O O
cid:2 O O
) O O
section O O
2.4 O O
p O O
( O O
y O O
= O O
1|x1 O O
, O O
. O O
we O O
now O O
take O O
the O O
various O O
parameters O O
of O O
the O O
mixture B B
model I I
, O O
namely O O
the O O
mixing O O
k O O
( O O
x O O
) O O
, O O
to O O
be O O
governed O O
by O O
coefﬁcients O O
πk O O
( O O
x O O
) O O
, O O
the O O
means O O
µk O O
( O O
x O O
) O O
, O O
and O O
the O O
variances O O
σ2 O O
274 O O
5. O O
neural O O
networks O O
xd O O
x1 O O
p O O
( O O
t|x O O
) O O
θm O O
θ1 O O
θ O O
figure O O
5.20 O O
the O O
mixturedensitynetwork O O
can O O
represent O O
general O O
conditional B B
probability I I
densities O O
p O O
( O O
t|x O O
) O O
by O O
considering O O
a O O
parametric O O
mixture B B
model I O
for O O
the O O
distribution O O
of O O
t O O
whose O O
parameters O O
are O O
determined O O
by O O
the O O
outputs O O
of O O
a O O
neural B B
network I I
that O O
takes O O
x O O
as O O
its O O
input O O
vector O O
. O O
any O O
point O O
x O O
in O O
data O O
space O O
can O O
then O O
be O O
summarized O O
by O O
its O O
posterior O O
mean O O
and O O
covariance B B
in O O
latent O B
space O O
. O O
however O O
, O O
even O O
as O O
a O O
discriminant B B
function I I
( O O
where O O
we O O
use O O
it O O
to O O
make O O
decisions O O
directly O O
and O O
dispense O O
with O O
any O O
probabilistic O O
interpretation O O
) O O
it O O
suf- O O
fers O O
from O O
some O O
severe O O
problems O O
. O O
it O O
is O O
easily O O
seen O O
that O O
the O O
distribution O O
is O O
normalized O O
p O O
( O O
x|µ O O
) O O
= O O
µk O O
= O O
1 O O
( O O
2.27 O O
) O O
and O O
that O O
e O O
[ O O
x|µ O O
] O O
= O O
( O O
2.28 O O
) O O
now O O
consider O O
a O O
data O O
set O O
d O O
of O O
n O O
independent B B
observations O O
x1 O O
, O O
. O O
learning B B
therefore O O
proceeds O O
by O O
choosing O O
initial O O
values O O
for O O
α O O
and O O
β O O
, O O
evalu- O O
ating O O
the O O
mean B B
and O O
covariance B B
of O O
the O O
posterior O O
using O O
( O O
7.82 O O
) O O
and O O
( O O
7.83 O O
) O O
, O O
respectively O O
, O O
and O O
then O O
alternately O O
re-estimating O O
the O O
hyperparameters O O
, O O
using O O
( O O
7.87 O O
) O O
and O O
( O O
7.88 O O
) O O
, O O
and O O
re-estimating O O
the O O
posterior O O
mean O O
and O O
covariance B B
, O O
using O O
( O O
7.82 O O
) O O
and O O
( O O
7.83 O O
) O O
, O O
until O O
a O O
suit- O O
able O O
convergence O O
criterion O O
is O O
satisﬁed O O
. O O
we O O
now O O
assume O O
that O O
it O O
is O O
the O O
latent O B
variables O O
that O O
form O O
a O O
markov O O
chain O O
, O O
giving O O
rise O O
to O O
the O O
graphical O B
structure O O
known O O
as O O
a O O
state B O
space I I
model I O
, O O
which O O
is O O
shown O O
in O O
figure O O
13.5. O O
it O O
satisﬁes O O
the O O
key O O
conditional B B
independence I I
property O O
that O O
zn−1 O O
and O O
zn+1 O O
are O O
indepen- O O
dent O O
given O O
zn O O
, O O
so O O
that O O
zn+1 O O
⊥⊥ O O
zn−1 O O
| O O
zn O O
. O O
8.26 O O
( O O
( O O
cid:12 O O
) O O
) O O
consider O O
a O O
tree-structured O O
factor B B
graph I I
over O O
discrete O O
variables O O
, O O
and O O
suppose O O
we O O
wish O O
to O O
evaluate O O
the O O
joint O O
distribution O O
p O O
( O O
xa O O
, O O
xb O O
) O O
associated O O
with O O
two O O
variables O O
xa O O
and O O
xb O O
that O O
do O O
not O O
belong O O
to O O
a O O
common O O
factor O O
. O O
its O O
inverse B B
is O O
used O O
to O O
determine O O
the O O
predic- O O
tive O O
distribution O O
for O O
a O O
trained O O
network O O
, O O
its O O
eigenvalues O O
determine O O
the O O
values O O
of O O
hyperparameters O O
, O O
and O O
its O O
determinant O O
is O O
used O O
to O O
evaluate O O
the O O
model B O
evidence I I
. O O
the O O
model O O
now O O
takes O O
the O O
form O O
of O O
a O O
linear O O
combination O O
of O O
basis O O
functions O O
transformed O O
by O O
a O O
logistic B B
sigmoid I I
function O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
y O O
( O O
x O O
, O O
w O O
) O O
= O O
σ O O
wtφ O O
( O O
x O O
) O O
( O O
7.108 O O
) O O
exercise O O
7.17 O O
354 O O
7. O O
sparse O O
kernel O O
machines O O
section O O
4.4 O O
exercise O O
7.18 O O
where O O
σ O O
( O O
· O O
) O O
is O O
the O O
logistic B B
sigmoid I I
function O O
deﬁned O O
by O O
( O O
4.59 O O
) O O
. O O
the O O
perceptron B B
learning O O
algorithm O O
has O O
a O O
simple O O
interpretation O O
, O O
as O O
follows O O
. O O
show O O
how O O
the O O
sum-product B B
algorithm I I
can O O
be O O
used O O
to O O
compute O O
the O O
marginal B B
distribution O O
over O O
that O O
subset O O
. O O
mean O B
ﬁeld O I
approaches O O
to O O
independent B B
component I O
analysis I I
. O O
it O O
should O O
be O O
noted O O
, O O
however O O
, O O
that O O
in O O
this O O
limit O O
the O O
output O O
variables O O
of O O
the O O
neural B B
network I I
become O O
independent B B
. O O
figure O O
6.4 O O
shows O O
samples O O
of O O
functions O O
drawn O O
from O O
gaus- O O
sian O O
processes O O
for O O
two O O
different O O
choices O O
of O O
kernel B O
function I O
. O O
( O O
5.171 O O
) O O
p O O
( O O
t|x O O
, O O
w O O
, O O
β O O
) O O
( O O
cid:7 O O
) O O
n O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
( O O
cid:11 O O
) O O
exercise O O
5.38 O O
we O O
can O O
therefore O O
make O O
use O O
of O O
the O O
general O O
result O O
( O O
2.115 O O
) O O
for O O
the O O
marginal B B
p O O
( O O
t O O
) O O
to O O
give O O
p O O
( O O
t|x O O
, O O
d O O
, O O
α O O
, O O
β O O
) O O
= O O
n O O
( O O
cid:10 O O
) O O
t|y O O
( O O
x O O
, O O
wmap O O
) O O
, O O
σ2 O O
( O O
x O O
) O O
( O O
5.172 O O
) O O
280 O O
5. O O
neural O O
networks O O
where O O
the O O
input-dependent O O
variance B B
is O O
given O O
by O O
−1 O O
+ O O
gta−1g O O
. O O
if O O
the O O
partitioning O O
of O O
the O O
input O O
space O O
is O O
given O O
, O O
and O O
we O O
minimize O O
the O O
sum-of-squares B B
error I I
function O O
, O O
then O O
the O O
optimal O O
value O O
of O O
the O O
predictive O B
variable O O
within O O
any O O
given O O
region O O
is O O
just O O
given O O
by O O
the O O
average O O
of O O
the O O
values O O
of O O
tn O O
for O O
those O O
data O O
points O O
that O O
fall O O
in O O
that O O
region O O
. O O
consider O O
the O O
case O O
of O O
independent B B
identically I O
distributed I O
data O O
. O O
in O O
a O O
space O O
of O O
high O O
dimensional- O O
ity O O
, O O
the O O
quantity O O
of O O
data O O
needed O O
to O O
provide O O
meaningful O O
estimates O O
of O O
local B B
probability O O
density B O
would O O
be O O
prohibitive O O
. O O
7.2.1 O O
rvm O O
for B O
regression I I
. O O
note O O
that O O
a O O
distribution O O
represented O O
as O O
a O O
directed B B
tree O I
can O O
easily O O
be O O
converted O O
into O O
one O O
represented O O
by O O
an O O
undirected B B
tree O O
, O O
and O O
vice O O
versa O O
. O O
similarly O O
, O O
in O O
the O O
multivariate O O
case O O
it O O
is O O
again O O
convenient O O
to O O
subtract O O
off O O
the O O
mean B B
, O O
giving O O
rise O O
to O O
the O O
covariance B B
of O O
a O O
random O O
vector O O
x O O
deﬁned O O
by O O
cov O O
[ O O
x O O
] O O
= O O
e O O
( O O
2.63 O O
) O O
for O O
the O O
speciﬁc O O
case O O
of O O
a O O
gaussian O O
distribution O O
, O O
we O O
can O O
make O O
use O O
of O O
e O O
[ O O
x O O
] O O
= O O
µ O O
, O O
together O O
with O O
the O O
result O O
( O O
2.62 O O
) O O
, O O
to O O
give O O
( O O
x O O
− O O
e O O
[ O O
x O O
] O O
) O O
( O O
x O O
− O O
e O O
[ O O
x O O
] O O
) O O
t O O
( O O
cid:8 O O
) O O
( O O
cid:9 O O
) O O
. O O
it O O
is O O
also O O
known O O
as O O
the O O
normal B B
distribution I I
. O O
the O O
goal O O
of O O
regression B B
is O O
to O O
predict O O
the O O
value O O
of O O
one O O
or O O
more O O
continuous O O
target O O
variables O O
t O O
given O O
the O O
value O O
of O O
a O O
d-dimensional O O
vec- O O
tor O O
x O O
of O O
input O O
variables O O
. O O
exercises O O
133 O O
2.29 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
using O O
the O O
partitioned B B
matrix O O
inversion O O
formula O O
( O O
2.76 O O
) O O
, O O
show O O
that O O
the O O
inverse B B
of O O
the O O
precision B O
matrix I I
( O O
2.104 O O
) O O
is O O
given O O
by O O
the O O
covariance B B
matrix I I
( O O
2.105 O O
) O O
. O O
such O O
a O O
model O O
has O O
both O O
discrete O O
latent O O
vari O O
( O O
cid:173 O O
) O O
ables O O
, O O
corresponding O O
to O O
the O O
discrete O O
mixture B B
, O O
as O O
well O O
as O O
continuous O O
latent O O
variables O O
, O O
and O O
the O O
likelihood B B
function I I
can O O
be O O
maximized O O
using O O
the O O
em O O
algorithm O O
. O O
this O O
is O O
illustrated O O
as O O
a O O
venn O O
diagram O O
in O O
figure O O
8.34. O O
figure O O
8.35 O O
shows O O
an O O
example O O
of O O
a O O
directed B B
graph O O
that O O
is O O
a O O
perfect B B
map I I
for O O
a O O
distribution O O
satisfying O O
the O O
conditional B B
independence I I
properties O O
a O O
⊥⊥ O O
b O O
| O O
∅ O O
and O O
a O O
( O O
cid:9 O O
) O O
⊥⊥ O O
b O O
| O O
c. O O
there O O
is O O
no O O
corresponding O O
undirected B B
graph I I
over O O
the O O
same O O
three O O
vari- O O
ables O O
that O O
is O O
a O O
perfect B B
map I I
. O O
however O O
, O O
once O O
we O O
have O O
( O O
at O O
least O O
) O O
two O O
components O O
in O O
the O O
mixture B B
, O O
one O O
of O O
the O O
components O O
can O O
have O O
a O O
ﬁnite O O
variance O O
and O O
therefore O O
assign O O
ﬁnite O O
probability O I
to O O
all O O
of O O
the O O
data O O
points O O
while O O
the O O
other O O
component O O
can O O
shrink O O
onto O O
one O O
speciﬁc O O
data O O
point O O
and O O
thereby O O
contribute O O
an O O
ever O O
increasing O O
additive O O
value O O
to O O
the O O
log O O
likelihood O O
. O O
k=1 O O
k=1 O O
setting O O
the O O
derivative B B
of O O
( O O
2.31 O O
) O O
with O O
respect O O
to O O
µk O O
to O O
zero O O
, O O
we O O
obtain O O
µk O O
= O O
−mk/λ O O
. O O
as O O
we O O
shall O O
see O O
, O O
by O O
using O O
error B B
backpropagation I I
, O O
each O O
such O O
evaluation O O
takes O O
only O O
o O O
( O O
w O O
) O O
steps O O
and O O
so O O
the O O
minimum O O
can O O
now O O
be O O
found O O
in O O
o O O
( O O
w O O
2 O O
) O O
steps O O
. O O
in O O
section O O
4.1.7 O O
, O O
we O O
described O O
the O O
perceptron B B
algorithm O O
that O O
is O O
guaranteed O O
to O O
ﬁnd O O
a O O
solution O O
in O O
a O O
ﬁnite O O
number O O
of O O
steps O O
. O O
appendix O O
a O O
data O O
sets O O
appendix O O
b O O
probability B B
distributions O O
appendix O O
c O O
properties O O
of O O
matrices O O
677 O O
685 O O
695 O O
xx O O
contents O O
appendix O O
d O O
calculus B O
of I O
variations I I
appendix O O
e O O
lagrange O O
multipliers O O
references O O
index O O
703 O O
707 O O
711 O O
729 O O
1 O O
introduction O O
the O O
problem O O
of O O
searching O O
for O O
patterns O O
in O O
data O O
is O O
a O O
fundamental O O
one O O
and O O
has O O
a O O
long O O
and O O
successful O O
history O O
. O O
each O O
measurement O O
comprises O O
the O O
duration O O
of O O
appendix O O
a O O
2.3. O O
the O O
gaussian O O
distribution O O
111 O O
figure O O
2.22 O O
example O O
of O O
a O O
gaussian O O
mixture B B
distribution I I
in O O
one O O
dimension O O
showing O O
three O O
gaussians O O
( O O
each O O
scaled O O
by O O
a O O
coefﬁcient O O
) O O
in O O
blue O O
and O O
their O O
sum O O
in O O
red O O
. O O
discuss O O
qualitatively O O
the O O
effect O O
this O O
will O O
have O O
on O O
the O O
variational B B
approximation O O
to O O
the O O
model B O
evidence I I
, O O
and O O
how O O
this O O
effect O O
will O O
vary O O
with O O
the O O
number O O
of O O
components O O
in O O
the O O
mixture B B
. O O
198 O O
4. O O
linear O O
models O O
for O O
classification O O
note O O
that O O
in O O
( O O
4.57 O O
) O O
we O O
have O O
simply O O
rewritten O O
the O O
posterior O O
probabilities O O
in O O
an O O
equivalent O O
form O O
, O O
and O O
so O O
the O O
appearance O O
of O O
the O O
logistic B B
sigmoid I I
may O O
seem O O
rather O O
vac- O O
uous O O
. O O
maximizing O O
the O O
likelihood O B
in- O O
volves O O
adjusting O O
the O O
mean B B
and O O
vari- O O
ance O O
of O O
the O O
gaussian O O
so O O
as O O
to O O
maxi- O O
mize O O
this O O
product O O
. O O
for O O
8.4. O O
inference B B
in O O
graphical O O
models O O
411 O O
table O O
8.1 O O
example O O
of O O
a O O
joint O O
distribution O O
over O O
two O O
binary O O
variables O O
for O O
which O O
the O O
maximum O B
of O O
the O O
joint O O
distribution O O
occurs O O
for O O
dif- O O
ferent O O
variable O O
values O O
compared O O
to O O
the O O
maxima O O
of O O
the O O
two O O
marginals O O
. O O
show O O
that O O
the O O
sum O O
of O O
two O O
positive B O
deﬁnite I I
matrices O O
is O O
itself O O
positive B O
deﬁnite I O
. O O
this O O
corresponds O O
to O O
deleting O O
the O O
horizontal O O
links O O
in O O
the O O
graphical B O
model I I
shown O O
in O O
figure O O
13.5. O O
the O O
joint O O
probability B B
distribution O O
over O O
both O O
latent O O
and O O
observed O O
variables O O
is O O
then O O
given O O
by O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
= O O
p O O
( O O
z1|π O O
) O O
( O O
cid:31 O O
) O O
n O O
( O O
cid:14 O O
) O O
n O O
( O O
cid:14 O O
) O O
p O O
( O O
zn|zn−1 O O
, O O
a O O
) O O
p O O
( O O
xm|zm O O
, O O
φ O O
) O O
( O O
13.10 O O
) O O
exercise O O
13.4 O O
n=2 O O
m=1 O O
where O O
x O O
= O O
{ O O
x1 O O
, O O
. O O
bagging B O
predictors O O
. O O
such O O
a O O
function O O
is O O
called O O
a O O
discriminant B B
function I I
. O O
however O O
, O O
it O O
is O O
worth O O
pointing O O
out O O
the O O
close O O
connection O O
with O O
the O O
mixture B B
density I I
network I I
discussed O O
in O O
section O O
5.6. O O
the O O
principal O B
advantage O O
of O O
the O O
mixtures O O
of O O
experts O O
model O O
is O O
that O O
it O O
can O O
be O O
optimized O O
by O O
em O O
in O O
which O O
the O O
m O O
step O O
for O O
each O O
mixture B B
component I I
and O O
gating O O
model O O
involves O O
a O O
convex O B
optimization O O
( O O
although O O
the O O
overall O O
optimization O O
is O O
nonconvex O O
) O O
. O O
( O O
5.43 O O
) O O
5.3. O O
error B B
backpropagation I I
241 O O
this O O
update O O
is O O
repeated O O
by O O
cycling O O
through O O
the O O
data O O
either O O
in O O
sequence O O
or O O
by O O
selecting O O
points O O
at O O
random O O
with O O
replacement O O
. O O
we O O
ean O O
gh'e O O
a O O
, O O
imple O O
physical B O
analogy I O
for O O
this O O
em O O
algorithm O O
. O O
thus O O
the O O
weight B B
vector I I
coincides O O
with O O
that O O
found O O
from O O
the O O
fisher O O
criterion O O
. O O
after O O
each O O
such O O
update O O
, O O
the O O
gradient O O
is O O
re-evaluated O O
for O O
the O O
new O O
weight B B
vector I I
and O O
the O O
process O O
repeated O O
. O O
this O O
inspires O O
the O O
data B O
augmentation I O
algorithm O O
, O O
which O O
alter- O O
nates O O
between O O
two O O
steps O O
known O O
as O O
the O O
i-step O O
( O O
imputation B O
step I O
, O O
analogous O O
to O O
an O O
e O O
step O O
) O O
and O O
the O O
p-step O O
( O O
posterior B O
step I O
, O O
analogous O O
to O O
an O O
m O O
step O O
) O O
. O O
here O O
we O O
shall O O
motivate O O
the O O
concept O O
of O O
d-separation B B
and O O
give O O
a O O
general O O
state- O O
ment O O
of O O
the O O
d-separation B B
criterion O O
. O O
2.3.2 O O
marginal B B
gaussian O O
distributions O O
. O O
µx→f O O
( O O
x O O
) O O
= O O
1 O O
µf→x O O
( O O
x O O
) O O
= O O
f O O
( O O
x O O
) O O
x O O
f O O
f O O
x O O
( O O
a O O
) O O
( O O
b O O
) O O
8.4. O O
inference B B
in O O
graphical O O
models O O
407 O O
as O O
illustrated O O
in O O
figure O O
8.49 O O
( O O
b O O
) O O
. O O
a O O
key O O
difference O O
compared O O
to O O
the O O
perceptron B B
, O O
however O O
, O O
is O O
that O O
the O O
neural O B
net- O O
work O O
uses O O
continuous O O
sigmoidal O O
nonlinearities O O
in O O
the O O
hidden O O
units O O
, O O
whereas O O
the O O
per- O O
ceptron O O
uses O O
step-function O O
nonlinearities O O
. O O
, O O
m O O
represent O O
a O O
set O O
of O O
con- O O
sistency O O
conditions O O
for O O
the O O
maximum O B
of O O
the O O
lower B B
bound I I
subject O O
to O O
the O O
factorization B B
constraint O O
. O O
, O O
z O O
( O O
m O O
) O O
such O O
that O O
the O O
following O O
conditional B B
independence I I
property O O
holds O O
for O O
m O O
∈ O O
{ O O
1 O O
, O O
. O O
the O O
algorithms O O
discussed O O
in O O
this O O
chapter O O
will O O
be O O
equally O O
applicable O O
if O O
we O O
ﬁrst O O
make O O
a O O
ﬁxed O O
nonlinear O O
transformation O O
of O O
the O O
input O O
variables O O
using O O
a O O
vector O O
of O O
basis O O
functions O O
φ O O
( O O
x O O
) O O
as O O
we O O
did O O
for B O
regression I I
models O O
in O O
chapter O O
3. O O
we O O
begin O O
by O O
consider- O O
ing O O
classiﬁcation O B
directly O O
in O O
the O O
original O O
input O O
space O O
x O O
, O O
while O O
in O O
section O O
4.3 O O
we O O
shall O O
ﬁnd O O
it O O
convenient O O
to O O
switch O O
to O O
a O O
notation O O
involving O O
basis O B
functions O O
for O O
consistency O O
with O O
later O O
chapters O O
. O O
show O O
that O O
the O O
variable O O
y O O
= O O
µ O O
+ O O
lz O O
has O O
a O O
gaussian O O
distribution O O
with O O
mean B B
µ O O
and O O
covariance B B
σ. O O
this O O
provides O O
a O O
technique O O
for O O
generating O O
samples O O
from O O
a O O
general O O
multivariate O O
gaus- O O
sian O O
using O O
samples O O
from O O
a O O
univariate O O
gaussian O O
having O O
zero O O
mean B B
and O O
unit O O
variance B B
. O O
for O O
a O O
sum-of-squares B B
error I I
function O O
with O O
a O O
regularizer O O
deﬁned O O
in O O
terms O O
of O O
a O O
differential B B
operator O O
, O O
the O O
optimal O O
solution O O
is O O
given O O
by O O
an O O
expansion O O
in O O
the O O
green O O
’ O O
s O O
functions O O
of O O
the O O
operator O O
( O O
which O O
are O O
analogous O O
to O O
the O O
eigenvectors O O
of O O
a O O
discrete O O
matrix O O
) O O
, O O
again O O
with O O
one O O
basis B B
function I I
centred O O
on O O
each O O
data O O
n O O
( O O
cid:2 O O
) O O
300 O O
6. O O
kernel O O
methods O O
point O O
. O O
this O O
gives O O
ie O O
[ O O
x O O
] O O
cov O O
[ O O
x O O
] O O
ie O O
[ O O
wz O O
+ O O
jl O O
+ O O
e O O
] O O
= O O
jl O O
ie O O
[ O O
( O O
wz O O
+ O O
e O O
) O O
( O O
wz O O
+ O O
e O O
) O O
t O O
] O O
ie O O
[ O O
wzztwt O O
] O O
+ O O
ie O O
[ O O
eet O O
] O O
= O O
wwt O O
+ O O
0- O O
21 O O
( O O
12.37 O O
) O O
( O O
12.38 O O
) O O
where O O
we O O
have O O
used O O
the O O
fact O O
that O O
z O O
and O O
e O O
are O O
independent B B
random O O
variables O O
and O O
hence O O
are O O
uncorrelated O O
. O O
( O O
d O O
) O O
plot O O
of O O
1 O O
0 O O
1 O O
0 O O
0 O O
0 O O
1 O O
( O O
a O O
) O O
0 O O
( O O
b O O
) O O
1 O O
0 O O
1 O O
0 O O
1 O O
0 O O
( O O
c O O
) O O
( O O
d O O
) O O
1 O O
1 O O
we O O
illustrate O O
the O O
use O O
of O O
a O O
mixture B B
density I I
network I I
by O O
returning O O
to O O
the O O
toy O O
ex- O O
ample O O
of O O
an O O
inverse B B
problem I O
shown O O
in O O
figure O O
5.19. O O
plots O O
of O O
the O O
mixing O O
coefﬁ- O O
cients O O
πk O O
( O O
x O O
) O O
, O O
the O O
means O O
µk O O
( O O
x O O
) O O
, O O
and O O
the O O
conditional B B
density O O
contours O O
corresponding O O
to O O
p O O
( O O
t|x O O
) O O
, O O
are O O
shown O O
in O O
figure O O
5.21. O O
the O O
outputs O O
of O O
the O O
neural B B
network I I
, O O
and O O
hence O O
the O O
parameters O O
in O O
the O O
mixture B B
model I I
, O O
are O O
necessarily O O
continuous O O
single-valued O O
functions O O
of O O
the O O
input O O
variables O O
. O O
due O O
to O O
the O O
weight B B
sharing I I
, O O
the O O
evaluation O O
of O O
the O O
activations O O
of O O
these O O
units O O
is O O
equivalent O O
to O O
a O O
convolution O O
of O O
the O O
image O O
pixel O O
intensities O O
with O O
a O O
‘ O O
kernel O O
’ O O
comprising O O
the O O
weight O B
parameters O O
. O O
here O O
we O O
describe O O
the O O
most O O
widely O O
used O O
form O O
of O O
boosting B B
algorithm O O
called O O
adaboost O O
, O O
short O O
for O O
‘ O O
adaptive O O
boosting O B
’ O O
, O O
developed O O
by O O
freund O O
and O O
schapire O O
( O O
1996 O O
) O O
. O O
8.4.8 O O
learning B O
the O O
graph O O
structure O O
. O O
note O O
that O O
this O O
proof O O
ignores O O
any O O
overlap O O
between O O
the O O
regions O O
r O O
and O O
r O O
( O O
cid:4 O O
) O O
but O O
is O O
easily O O
generalized B I
to O O
allow O O
for O O
such O O
overlap O O
. O O
2 O O
λ O O
1 O O
exercise O O
2.45 O O
( O O
cid:22 O O
) O O
0 O O
−2 O O
in O O
the O O
case O O
of O O
the O O
multivariate O O
gaussian O O
distribution O O
n O O
( O O
cid:10 O O
) O O
0 O O
µ O O
( O O
cid:11 O O
) O O
x|µ O O
, O O
λ O O
−1 O O
2 O O
for O O
a O O
d- O O
dimensional O O
variable O O
x O O
, O O
the O O
conjugate B B
prior I I
distribution O O
for O O
the O O
mean B B
µ O O
, O O
assuming O O
the O O
precision O O
is O O
known O O
, O O
is O O
again O O
a O O
gaussian O O
. O O
further- O O
more O O
, O O
the O O
terms O O
involving O O
µ O O
and O O
λ O O
themselves O O
comprise O O
a O O
sum O O
over O O
k O O
of O O
terms O O
involving O O
µk O O
and O O
λk O O
leading O O
to O O
the O O
further O O
factorization B B
q O O
( O O
π O O
, O O
µ O O
, O O
λ O O
) O O
= O O
q O O
( O O
π O O
) O O
q O O
( O O
µk O O
, O O
λk O O
) O O
. O O
it O O
is O O
worth O O
emphasizing O O
that O O
because O O
the O O
linear B B
dynamical I I
system I I
is O O
a O O
linear- O O
gaussian O O
model O O
, O O
the O O
joint O O
distribution O O
over O O
all O O
latent O O
and O O
observed O O
variables O O
is O O
simply O O
a O O
gaussian O O
, O O
and O O
so O O
in O O
principle O O
we O O
could O O
solve O O
inference B B
problems O O
by O O
using O O
the O O
standard O O
results O O
derived O O
in O O
previous O O
chapters O O
for O O
the O O
marginals O O
and O O
conditionals O O
of O O
a O O
multivariate O O
gaussian O O
. O O
however O O
, O O
there O O
is O O
nothing O O
speciﬁc O O
to O O
discrete O O
variables O O
either O O
in O O
the O O
graphical O O
framework O O
or O O
in O O
the O O
probabilistic O O
construction O O
of O O
the O O
sum-product B B
algorithm I I
. O O
tractable O O
inference B B
for O O
complex O O
stochastic B B
processes O O
. O O
here O O
we O O
shall O O
evaluate O O
the O O
integral O O
instead O O
by O O
completing B B
the I O
square I I
in O O
the O O
exponent O O
and O O
making O O
use O O
of O O
the O O
standard O O
form O O
for O O
the O O
normalization O O
coefﬁcient O O
of O O
a O O
gaussian O O
. O O
we O O
shall O O
build O O
extensively O O
on O O
the O O
analogous O O
results O O
for O O
linear O O
classiﬁcation B O
models O O
discussed O O
in O O
section O O
4.5 O O
, O O
and O O
so O O
we O O
encourage O O
the O O
reader O O
to O O
familiarize O O
themselves O O
with O O
that O O
material O O
before O O
studying O O
this O O
section O O
. O O
in O O
this O O
case O O
, O O
we O O
can O O
write O O
the O O
conditional B B
distribution O O
for O O
node O O
i O O
in O O
the O O
form O O
p O O
( O O
xi|pai O O
) O O
= O O
n O O
wijxj O O
+ O O
bi O O
, O O
σi O O
( O O
8.19 O O
) O O
⎛⎝xi O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:2 O O
) O O
j∈pai O O
⎞⎠ O O
section O O
2.3.6 O O
where O O
now O O
wij O O
is O O
a O O
matrix O O
( O O
which O O
is O O
nonsquare O O
if O O
xi O O
and O O
xj O O
have O O
different O O
dimen- O O
sionalities O O
) O O
. O O
this O O
distributed O O
message B O
passing I O
formulation O O
has O O
good O O
scaling O O
properties O O
and O O
is O O
well O O
suited O B
to O O
large O O
networks O O
. O O
together O O
with O O
the O O
class O O
priors O O
, O O
this O O
deﬁnes O O
an O O
opti- O O
mal O O
misclassiﬁcation-rate O O
decision B O
boundary I I
. O O
5.4 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
consider O O
a O O
binary O O
classiﬁcation O B
problem O O
in O O
which O O
the O O
target O O
values O O
are O O
t O O
∈ O O
{ O O
0 O O
, O O
1 O O
} O O
, O O
with O O
a O O
network O O
output O O
y O O
( O O
x O O
, O O
w O O
) O O
that O O
represents O O
p O O
( O O
t O O
= O O
1|x O O
) O O
, O O
and O O
suppose O O
that O O
there O O
is O O
a O O
probability B B
 O O
that O O
the O O
class O O
label O O
on O O
a O O
training B B
data O O
point O O
has O O
been O O
incorrectly O O
set O O
. O O
10.3 O O
variational B B
linear O O
regression B B
. O O
as O O
a O O
simple O O
example O O
, O O
consider O O
a O O
kernel B O
function I I
given O O
by O O
k O O
( O O
x O O
, O O
z O O
) O O
= O O
xtz O O
. O O
the O O
gaussian O O
is O O
the O O
distribution O O
that O O
maximizes O O
the O O
entropy B B
for O O
a O O
given O O
variance B B
( O O
or O O
covariance B B
) O O
. O O
then O O
make O O
use O O
of O O
the O O
orthonormality O O
properties O O
of O O
the O O
vectors O O
u1 O O
, O O
'' O O
'' O O
um O O
to O O
show O O
that O O
the O O
new O O
vector O O
um+1 O O
is O O
an O O
eigenvector O O
of O O
s. O O
finally O O
, O O
show O O
that O O
the O O
variance B B
is O O
maximized O O
if O O
the O O
eigenvector O O
is O O
chosen O O
to O O
be O O
the O O
one O O
corresponding O O
to O O
eigenvector O O
am+1 O O
where O O
the O O
eigenvalues O O
have O O
been O O
ordered O O
in O O
decreasing O O
value O O
. O O
these O O
equations O O
deﬁne O O
the O O
‘ O O
forward B B
kinematics I I
’ O O
of O O
the O O
robot B O
arm I O
. O O
the O O
orthogonal O O
matrix O I
r O O
that O O
expresses O O
these O O
rotations O O
has O O
size O O
m O O
x O O
m. O O
in O O
the O O
first O O
column O O
of O O
this O O
matrix O O
there O O
are O O
m O O
- O O
1 O O
independent B B
parameters O O
, O O
because O O
the O O
column O O
vector O O
must O O
be O O
normalized O O
to O O
unit O O
length O O
. O O
the O O
results O O
( O O
2.73 O O
) O O
and O O
( O O
2.75 O O
) O O
are O O
expressed O O
in O O
terms O O
of O O
the O O
partitioned B B
precision O O
matrix O O
of O O
the O O
original O O
joint O O
distribution O O
p O O
( O O
xa O O
, O O
xb O O
) O O
. O O
( O O
8.52 O O
) O O
the O O
reader O O
is O O
encouraged O O
to O O
study O O
this O O
re-ordering O O
carefully O O
as O O
the O O
underlying O O
idea O O
forms O O
the O O
basis O B
for O O
the O O
later O O
discussion O O
of O O
the O O
general O O
sum-product B B
algorithm I I
. O O
we O O
see O O
that O O
this O O
is O O
most O O
naturally O O
expressed O O
in O O
terms O O
of O O
the O O
inverse B B
variance O O
, O O
which O O
is O O
called O O
the O O
precision O O
. O O
to O O
evaluate O O
the O O
predictive B B
distribution I I
, O O
we O O
ﬁrst O O
note O O
that O O
the O O
function O O
σ O O
( O O
wtφ O O
) O O
de- O O
pends O O
on O O
w O O
only O O
through O O
its O O
projection O O
onto O O
φ. O O
denoting O O
a O O
= O O
wtφ O O
, O O
we O O
have O O
σ O O
( O O
wtφ O O
) O O
q O O
( O O
w O O
) O O
dw O O
( O O
cid:6 O O
) O O
δ O O
( O O
a O O
− O O
wtφ O O
) O O
σ O O
( O O
a O O
) O O
da O O
where O O
δ O O
( O O
· O O
) O O
is O O
the O O
dirac O O
delta O O
function O O
. O O
one O O
approach O O
is O O
to O O
use O O
bootstrap B O
data O O
sets O O
, O O
discussed O O
in O O
section O O
1.2.3. O O
consider O O
a O O
regression B B
problem O O
in O O
which O O
we O O
are O O
trying O O
to O O
predict O O
the O O
value O O
of O O
a O O
single O O
continuous O O
variable O O
, O O
and O O
suppose O O
we O O
generate O O
m O O
bootstrap B O
data O O
sets O O
and O O
then O O
use O O
each O O
to O O
train O O
a O O
separate O O
copy O O
ym O O
( O O
x O O
) O O
of O O
a O O
predictive O B
model O O
where O O
m O O
= O O
1 O O
, O O
. O O
we O O
can O O
deﬁne O O
ui O O
to O O
be O O
the O O
set O O
of O O
such O O
distributions O O
that O O
are O O
consistent B B
with O O
the O O
set O O
of O O
conditional B B
independence I I
statements O O
that O O
can O O
be O O
read O O
from O O
the O O
graph O O
using O O
graph O O
separation O O
. O O
the O O
generalization B B
error O I
, O O
however O O
, O O
is O O
not O O
a O O
simple O O
function O O
of O O
m O O
due O O
to O O
the O O
presence O O
of O O
local B B
minima O O
in O O
the O O
error B B
function I I
, O O
as O O
illustrated O O
in O O
figure O O
5.10. O O
here O O
we O O
see O O
the O O
effect O O
of O O
choosing O O
multiple O O
random O O
initializations O O
for O O
the O O
weight B B
vector I I
for O O
a O O
range O O
of O O
values O O
of O O
m. O O
the O O
overall O O
best O O
validation B O
set I O
performance O O
in O O
this O O
case O O
occurred O O
for O O
a O O
particular O O
solution O O
having O O
m O O
= O O
8. O O
in O O
practice O O
, O O
one O O
approach O O
to O O
choosing O O
m O O
is O O
in O O
fact O O
to O O
plot O O
a O O
graph O O
of O O
the O O
kind O O
shown O O
in O O
figure O O
5.10 O O
and O O
then O O
to O O
choose O O
the O O
speciﬁc O O
solution O O
having O O
the O O
smallest O O
validation B O
set I O
error O B
. O O
sup- O O
pose O O
that O O
we O O
have O O
initialized O O
all O O
of O O
the O O
factors O O
and O O
that O O
we O O
choose O O
to O O
reﬁne O O
factor O O
x1 O O
x2 O O
x3 O O
x1 O O
x2 O O
x3 O O
10.7. O O
expectation B B
propagation I I
515 O O
fa O O
fb O O
˜fa1 O O
˜fa2 O O
fc O O
x4 O O
˜fb2 O O
˜fb3 O O
˜fc2 O O
˜fc4 O O
x4 O O
figure O O
10.18 O O
on O O
the O O
left O O
is O O
a O O
simple O O
factor B B
graph I I
from O O
figure O O
8.51 O O
and O O
reproduced O O
here O O
for O O
convenience O O
. O O
12.19 O O
( O O
** O O
) O O
iiii O O
! O O
i O O
show O O
that O O
the O O
factor B B
analysis I I
model O O
described O O
in O O
section O O
12.2.4 O O
is O O
invariant O O
under O O
rotations O O
of O O
the O O
latent O B
space O O
coordinates O O
. O O
on O O
the O O
right O O
is O O
the O O
result O O
of O O
using O O
logistic O B
regressions O O
as O O
described O O
in O O
section O O
4.3.2 O O
showing O O
correct O O
classiﬁcation B B
of O O
the O O
training B B
data O O
. O O
2.3.6 O O
bayesian O O
inference B B
for O O
the O O
gaussian O O
the O O
maximum B B
likelihood I I
framework O O
gave O O
point O O
estimates O O
for O O
the O O
parameters O O
µ O O
and O O
σ. O O
now O O
we O O
develop O O
a O O
bayesian O O
treatment O O
by O O
introducing O O
prior B B
distributions O O
over O O
these O O
parameters O O
. O O
the O O
left-hand O O
plol O O
shows O O
the O O
result O O
irom O O
maximum B B
likelihood I I
probabilistic O O
pca O O
, O O
and O O
the O O
left·hand O O
plot O O
shows O O
the O O
corresponding O O
resuft O O
from O O
bayesian O O
pea O O
. O O
it O O
is O O
easily O O
seen O O
that O O
a O O
transition B O
probability I I
that O O
satisﬁes O O
detailed O O
balance O O
with O O
respect O O
to O O
a O O
particular O O
distribution O O
will O O
leave O O
that O O
distribution O O
invariant O O
, O O
because O O
p O O
( O O
cid:1 O O
) O O
( O O
z O O
( O O
cid:4 O O
) O O
) O O
t O O
( O O
z O O
( O O
cid:4 O O
) O O
, O O
z O O
) O O
= O O
p O O
( O O
cid:1 O O
) O O
( O O
z O O
) O O
t O O
( O O
z O O
, O O
z O O
( O O
cid:4 O O
) O O
) O O
= O O
p O O
( O O
cid:1 O O
) O O
( O O
z O O
) O O
p O O
( O O
z O O
( O O
cid:4 O O
) O O
|z O O
) O O
= O O
p O O
( O O
cid:1 O O
) O O
( O O
z O O
) O O
. O O
each O O
point O O
in O O
the O O
data O O
set O O
is O O
generated O O
independently O O
using O O
the O O
following O O
steps O O
: O O
1. O O
choose O O
one O O
of O O
the O O
three O O
phase O B
conﬁgurations O O
at O O
random O O
with O O
equal O O
probability B B
. O O
expectation B B
propagation I I
we O O
are O O
given O O
a O O
joint O O
distribution O O
over O O
observed O O
data O O
d O O
and O O
stochastic B B
variables O O
θ O O
in O O
the O O
form O O
of O O
a O O
product O O
of O O
factors O O
p O O
( O O
d O O
, O O
θ O O
) O O
= O O
fi O O
( O O
θ O O
) O O
( O O
10.202 O O
) O O
and O O
we O O
wish O O
to O O
approximate O O
the O O
posterior O O
distribution O O
p O O
( O O
θ|d O O
) O O
by O O
a O O
distribution O O
of O O
the O O
form O O
( O O
cid:14 O O
) O O
( O O
cid:14 O O
) O O
i O O
1 O O
z O O
q O O
( O O
θ O O
) O O
= O O
we O O
also O O
wish O O
to O O
approximate O O
the O O
model B O
evidence I I
p O O
( O O
d O O
) O O
. O O
( O O
3.14 O O
) O O
φ O O
= O O
the O O
quantity O O
wml O O
= O O
( O O
3.15 O O
) O O
which O O
are O O
known O O
as O O
the O O
normal B B
equations I O
for O O
the O O
least O O
squares O O
problem O O
. O O
we O O
have O O
seen O O
that O O
both O O
the O O
e O O
and O O
the O O
m O O
steps O O
of O O
the O O
em O O
algorithm O O
are O O
increas- O O
ing O O
the O O
value O O
of O O
a O O
well-deﬁned O O
bound O O
on O O
the O O
log O O
likelihood O O
function O O
and O O
that O O
the O O
454 O O
9. O O
mixture B B
models O O
and O O
em O O
complete O O
em O O
cycle O O
will O O
change O O
the O O
model O O
parameters O O
in O O
such O O
a O O
way O O
as O O
to O O
cause O O
the O O
log O O
likelihood O O
to O O
increase O O
( O O
unless O O
it O O
is O O
already O O
at O O
a O O
maximum O B
, O O
in O O
which O O
case O O
the O O
parameters O O
remain O O
unchanged O O
) O O
. O O
this O O
invariance B B
can O O
be O O
understood O O
in O O
terms O O
of O O
rotations O O
within O O
the O O
latent O O
space O O
. O O
for O O
in- O O
stance O O
, O O
if O O
we O O
have O O
two O O
models O O
that O O
are O O
a-posteriori O O
equally O O
likely O O
and O O
one O O
predicts O O
a O O
narrow O O
distribution O O
around O O
t O O
= O O
a O O
while O O
the O O
other O O
predicts O O
a O O
narrow O O
distribution O O
around O O
t O O
= O O
b O O
, O O
the O O
overall O O
predictive B O
distribution I O
will O O
be O O
a O O
bimodal O O
distribution O O
with O O
modes O O
at O O
t O O
= O O
a O O
and O O
t O O
= O O
b O O
, O O
not O O
a O O
single O O
model O O
at O O
t O O
= O O
( O O
a O O
+ O O
b O O
) O O
/2 O O
. O O
this O O
model O O
is O O
also O O
known O O
as O O
latent B B
class I O
analysis I O
( O O
lazarsfeld O O
and O O
henry O O
, O O
1968 O O
; O O
mclachlan O O
and O O
peel O O
, O O
2000 O O
) O O
. O O
to O O
do O O
this O O
, O O
we O O
make O O
use O O
of O O
the O O
following O O
identity O O
for O O
the O O
inverse B B
of O O
a O O
partitioned B B
matrix O O
−mbd−1 O O
( O O
cid:16 O O
) O O
−1 O O
( O O
cid:15 O O
) O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
m O O
= O O
−d−1cm O O
d−1 O O
+ O O
d−1cmbd−1 O O
a O O
b O O
c O O
d O O
( O O
2.76 O O
) O O
exercise O O
2.24 O O
where O O
we O O
have O O
deﬁned O O
( O O
2.77 O O
) O O
the O O
quantity O O
m−1 O O
is O O
known O O
as O O
the O O
schur O O
complement O O
of O O
the O O
matrix O O
on O O
the O O
left-hand O O
side O O
of O O
( O O
2.76 O O
) O O
with O O
respect O O
to O O
the O O
submatrix O O
d. O O
using O O
the O O
deﬁnition O O
m O O
= O O
( O O
a O O
− O O
bd−1c O O
) O O
−1 O O
. O O
furthermore O O
, O O
in O O
the O O
next O O
section O O
we O O
shall O O
derive O O
an O O
alternative O O
procedure O O
for O O
training O O
the O O
relevance B B
vector I I
machine I I
that O O
improves O O
training B B
speed O O
signiﬁcantly O O
. O O
the O O
use O O
of O O
such O O
augmented O O
data O O
can O O
lead O O
to O O
signiﬁcant O O
improvements O O
in O O
generalization B B
( O O
simard O O
et O O
al. O O
, O O
2003 O O
) O O
, O O
although O O
it O O
can O O
also O O
be O O
computationally O O
costly O O
. O O
healing O O
the O O
relevance B B
vector I I
machine I I
by O O
aug- O O
mentation O O
. O O
this O O
effectively O O
makes O O
a O O
hard O O
assignment O O
of O O
each O O
data O O
point O O
to O O
one O O
of O O
the O O
components O O
in O O
the O O
mixture B B
. O O
consider O O
the O O
example O O
of O O
polynomial B B
curve I I
ﬁtting I I
discussed O O
in O O
section O O
1.1. O O
it O O
seems O O
reasonable O O
to O O
apply O O
the O O
frequentist B B
notion O O
of O O
probability B B
to O O
the O O
random O O
values O O
of O O
the O O
observed O O
variables O O
tn O O
. O O
k=1 O O
( O O
2.39 O O
) O O
2.2. O O
multinomial O B
variables O O
77 O O
figure O O
2.4 O O
the O O
dirichlet O O
distribution O O
over O O
three O O
variables O O
µ1 O O
, O O
µ2 O O
, O O
µ3 O O
is O O
conﬁned O O
to O O
a O O
simplex B O
( O O
a O O
bounded O O
linear O O
manifold O O
) O O
of O O
the O O
form O O
shown O O
, O O
as O O
a O O
consequence O O
of O O
the O O
constraints O O
0 O O
( O O
cid:1 O O
) O O
µk O O
( O O
cid:1 O O
) O O
1 O O
and O O
k O O
µk O O
= O O
1. O O
p O O
µ2 O O
µ3 O O
µ1 O O
plots O O
of O O
the O O
dirichlet O O
distribution O O
over O O
the O O
simplex B O
, O O
for O O
various O O
settings O O
of O O
the O O
param- O O
eters O O
αk O O
, O O
are O O
shown O O
in O O
figure O O
2.5. O O
posterior O O
distribution O O
for O O
the O O
parameters O O
{ O O
µk O O
} O O
in O O
the O O
form O O
multiplying O O
the O O
prior B B
( O O
2.38 O O
) O O
by O O
the O O
likelihood B B
function I I
( O O
2.34 O O
) O O
, O O
we O O
obtain O O
the O O
p O O
( O O
µ|d O O
, O O
α O O
) O O
∝ O O
p O O
( O O
d|µ O O
) O O
p O O
( O O
µ|α O O
) O O
∝ O O
k O O
( O O
cid:14 O O
) O O
k=1 O O
µαk+mk−1 O O
k O O
. O O
) O O
, O O
proceed- O O
ings O O
of O O
the O O
fifth O O
berkeley O O
symposium O O
on O O
mathe- O O
matical O O
statistics O O
and O O
probability B B
, O O
volume O O
i O O
, O O
pp O O
. O O
given O O
these O O
limitations O O
, O O
we O O
turn O O
in O O
the O O
next O O
section O O
to O O
a O O
bayesian O O
treatment O O
of O O
linear O O
basis O O
function O O
models O O
, O O
which O O
not O O
only O O
provides O O
powerful O O
insights O O
into O O
the O O
issues O B
of O O
over-ﬁtting B B
but O O
which O O
also O O
leads O O
to O O
practical O O
techniques O O
for O O
addressing O O
the O O
question O O
model O O
complexity O O
. O O
we O O
can O O
motivate O O
the O O
kernel B O
regression I I
model O O
( O O
3.61 O O
) O O
from O O
a O O
different O O
perspective O O
, O O
starting O O
with O O
kernel O O
density O O
estimation O I
. O O
similarly O O
, O O
imagine O O
that O O
we O O
interchange O O
the O O
values O O
of O O
all O O
of O O
the O O
weights O O
( O O
and O O
the O O
bias B B
) O O
leading O O
both O O
into O O
and O O
out O O
of O O
a O O
particular O O
hidden B O
unit I O
with O O
the O O
corresponding O O
values O O
of O O
the O O
weights O O
( O O
and O O
bias B B
) O O
associated O O
with O O
a O O
different O O
hidden B O
unit I O
. O O
the O O
right-hand O O
plot O O
shows O O
the O O
true O O
conditional B B
distribution O O
p O O
( O O
t|x O O
) O O
from O O
which O O
the O O
labels O O
are O O
generated O O
, O O
in O O
which O O
the O O
green O O
curve O O
denotes O O
the O O
mean B B
, O O
and O O
the O O
shaded O O
region O O
spans O O
one O O
standard B O
deviation I I
on O O
each O O
side O O
of O O
the O O
mean B B
. O O
2.12 O O
( O O
( O O
cid:12 O O
) O O
) O O
the O O
uniform B B
distribution I I
for O O
a O O
continuous O O
variable O O
x O O
is O O
deﬁned O O
by O O
u O O
( O O
x|a O O
, O O
b O O
) O O
= O O
1 O O
b O O
− O O
a O O
, O O
a O O
( O O
cid:1 O O
) O O
x O O
( O O
cid:1 O O
) O O
b O O
. O O
we O O
call O O
this O O
the O O
prior B B
probability O O
because O O
it O O
is O O
the O O
probability B B
available O O
before O O
we O O
observe O O
the O O
identity O O
of O O
the O O
fruit O O
. O O
it O O
also O O
motivates O O
the O O
extension O O
to O O
regression B B
problems O O
( O O
friedman O O
, O O
2001 O O
) O O
. O O
let O O
us O O
begin O O
by O O
writing O O
down O O
the O O
conditions O O
that O O
must O O
be O O
satisﬁed O O
at O O
a O O
maximum O B
of O O
the O O
likelihood B B
function I I
. O O
we O O
can O O
gain O O
some O O
insight O O
into O O
the O O
resulting O O
model O O
by O O
running O O
it O O
generatively O O
, O O
as O O
shown O O
in O O
figure O O
13.11. O O
figure O O
13.10 O O
lattice B O
diagram I O
for O O
a O O
3-state O O
left- O O
to-right O O
hmm O O
in O O
which O O
the O O
state O O
index O O
k O O
is O O
allowed O O
to O O
increase O O
by O O
at O O
most O O
1 O O
at O O
each O O
transition O O
. O O
similarly O O
, O O
using O O
the O O
identity O O
cos O O
( O O
a O O
− O O
b O O
) O O
= O O
( O O
cid:10 O O
) O O
exp O O
{ O O
i O O
( O O
a O O
− O O
b O O
) O O
} O O
( O O
2.296 O O
) O O
( O O
2.297 O O
) O O
( O O
2.298 O O
) O O
136 O O
2. O O
probability B B
distributions O O
where O O
( O O
cid:10 O O
) O O
denotes O O
the O O
real O O
part O O
, O O
prove O O
( O O
2.178 O O
) O O
. O O
this O O
corresponds O O
to O O
the O O
case O O
of O O
factor B B
analysis I I
. O O
we O O
can O O
now O O
ask O O
questions O O
such O O
as O O
: O O
“ O O
what O O
is O O
the O O
overall O O
probability B B
that O O
the O O
se- O O
lection O O
procedure O O
will O O
pick O O
an O O
apple O O
? O O
” O O
, O O
or O O
“ O O
given O O
that O O
we O O
have O O
chosen O O
an O O
orange O O
, O O
what O O
is O O
the O O
probability B B
that O O
the O O
box O O
we O O
chose O O
was O O
the O O
blue O O
one O O
? O O
” O O
. O O
if O O
we O O
assume O O
that O O
the O O
posterior O O
distribution O O
is O O
sharply O O
peaked O O
, O O
as O O
will O O
occur O O
for O O
sufficiently O O
large O O
data O O
sets O O
, O O
then O O
the O O
re-estimation O O
equations O O
obtained O O
by O O
maximizing O O
the O O
marginal B B
likelihood I I
with O O
respect O O
to O O
ai O O
take O O
the O O
simple O O
form O O
which O O
follows O O
from O O
( O O
3.98 O O
) O O
, O O
noting O O
that O O
the O O
dimensionality O O
of O O
wi O O
is O O
d. O O
these O O
re O O
( O O
cid:173 O O
) O O
estimations O O
are O O
interleaved O O
with O O
the O O
em O O
algorithm O O
updates O O
for O O
determining O O
wand O O
a O O
2 O O
• O O
the O O
e-step O O
equations O O
are O O
again O O
given O O
by O O
( O O
12.54 O O
) O O
and O O
( O O
12.55 O O
) O O
. O O
from O O
( O O
3.11 O O
) O O
, O O
( O O
3.12 O O
) O O
, O O
and O O
( O O
3.52 O O
) O O
, O O
we O O
can O O
write O O
the O O
evidence B B
function I I
in O O
the O O
form O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
n/2 O O
( O O
cid:17 O O
) O O
( O O
cid:18 O O
) O O
m/2 O O
( O O
cid:6 O O
) O O
p O O
( O O
t|α O O
, O O
β O O
) O O
= O O
β O O
2π O O
α O O
2π O O
exp O O
{ O O
−e O O
( O O
w O O
) O O
} O O
dw O O
( O O
3.78 O O
) O O
exercise O O
3.18 O O
exercise O O
3.19 O O
3.5. O O
the O O
evidence B B
approximation I I
167 O O
where O O
m O O
is O O
the O O
dimensionality O O
of O O
w O O
, O O
and O O
we O O
have O O
deﬁned O O
e O O
( O O
w O O
) O O
= O O
βed O O
( O O
w O O
) O O
+ O O
αew O O
( O O
w O O
) O O
= O O
β O O
2 O O
( O O
cid:5 O O
) O O
t O O
− O O
φw O O
( O O
cid:5 O O
) O O
2 O O
+ O O
α O O
2 O O
wtw O O
. O O
( O O
6.71 O O
) O O
( O O
cid:24 O O
) O O
2 O O
( O O
cid:2 O O
) O O
i=1 O O
( O O
cid:25 O O
) O O
samples O O
from O O
the O O
resulting O O
prior B B
over O O
functions O O
y O O
( O O
x O O
) O O
are O O
shown O O
for O O
two O O
different O O
settings O O
of O O
the O O
precision O O
parameters O O
ηi O O
in O O
figure O O
6.9. O O
we O O
see O O
that O O
, O O
as O O
a O O
particu- O O
lar O O
parameter O O
ηi O O
becomes O O
small O O
, O O
the O O
function O O
becomes O O
relatively O O
insensitive O O
to O O
the O O
corresponding O O
input O O
variable O O
xi O O
. O O
3.5.2 O O
maximizing O O
the O O
evidence B B
function I I
let O O
us O O
ﬁrst O O
consider O O
the O O
maximization O O
of O O
p O O
( O O
t|α O O
, O O
β O O
) O O
with O O
respect O O
to O O
α. O O
this O O
can O O
be O O
done O O
by O O
ﬁrst O O
deﬁning O O
the O O
following O O
eigenvector O O
equation O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
( O O
3.87 O O
) O O
( O O
cid:2 O O
) O O
from O O
( O O
3.81 O O
) O O
, O O
it O O
then O O
follows O O
that O O
a O O
has O O
eigenvalues O O
α O O
+ O O
λi O O
. O O
we O O
already O O
know O O
that O O
the O O
exact O O
maximum O O
likelihood O I
solution O O
for O O
jl O O
is O O
given O O
by O O
the O O
sample B O
mean I I
x O O
defined O O
by O O
( O O
12.1 O O
) O O
, O O
and O O
it O O
is O O
convenient O O
to O O
substitute O O
for O O
jl O O
at O O
this O O
stage O O
. O O
these O O
are O O
nodes O O
that O O
have O O
‘ O O
head-to-head O O
’ O O
paths O O
encountered O O
in O O
our O O
discussion O O
of O O
conditional B B
independence I I
. O O
the O O
max-sum B O
algorithm I I
, O O
with O O
back-tracking B O
, O O
gives O O
an O O
exact O O
maximizing O O
conﬁguration O O
for O O
the O O
variables O O
provided O O
the O O
factor B B
graph I I
is O O
a O O
tree B B
. O O
here O O
we O O
focus O O
on O O
a O O
particular O O
tree-based O O
framework O O
called O O
classiﬁcation B B
and I I
regression I I
trees I O
, O O
or O O
cart O O
( O O
breiman O O
et O O
al. O O
, O O
1984 O O
) O O
, O O
although O O
there O O
are O O
many O O
other O O
variants O O
going O O
by O O
such O O
names O O
as O O
id3 O O
and O O
c4.5 O O
( O O
quinlan O O
, O O
1986 O O
; O O
quinlan O O
, O O
1993 O O
) O O
. O O
for O O
every O O
data O O
point O O
xn O O
, O O
instead O O
of O O
having O O
a O O
value O O
t O O
for O O
the O O
class O O
label O O
, O O
we O O
have O O
instead O O
a O O
value O O
πn O O
representing O O
the O O
probability B B
that O O
tn O O
= O O
1. O O
given O O
a O O
probabilistic O O
model O O
p O O
( O O
t O O
= O O
1|φ O O
) O O
, O O
write O O
down O O
the O O
log O O
likelihood O O
function O O
appropriate O O
to O O
such O O
a O O
data O O
set O O
. O O
5.37 O O
( O O
( O O
cid:12 O O
) O O
) O O
verify O O
the O O
results O O
( O O
5.158 O O
) O O
and O O
( O O
5.160 O O
) O O
for O O
the O O
conditional B B
mean O O
and O O
variance B B
of O O
the O O
mixture B B
density I I
network I I
model O O
. O O
5.4.5 O O
exact B O
evaluation I O
of O O
the O O
hessian O O
so O O
far O O
, O O
we O O
have O O
considered O O
various O O
approximation O O
schemes O O
for O O
evaluating O O
the O O
hessian O O
matrix O O
or O O
its O O
inverse B B
. O O
3.4 O O
bayesian O O
model B O
comparison I I
. O O
note O O
that O O
the O O
error B B
func- O O
tion O O
in O O
this O O
case O O
is O O
quadratic O O
and O O
hence O O
the O O
newton-raphson O O
formula O O
gives O O
the O O
exact O O
solution O O
in O O
one O O
step O O
. O O
modelled O O
using O O
the O O
binomial B B
distribution I I
( O O
2.9 O O
) O O
or O O
as O O
1-of-2 O O
variables O O
and O O
modelled O O
using O O
the O O
multinomial B B
distribution I I
( O O
2.34 O O
) O O
with O O
k O O
= O O
2 O O
. O O
partitioned B O
gaussians O O
given O O
a O O
joint O O
gaussian O O
distribution O O
n O O
( O O
x|µ O O
, O O
σ O O
) O O
with O O
λ O O
≡ O O
σ O O
−1 O O
and O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
xa O O
xb O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
µa O O
µb O O
x O O
= O O
, O O
µ O O
= O O
( O O
2.94 O O
) O O
90 O O
2. O O
probability B B
distributions O O
1 O O
xb O O
0.5 O O
xb O O
= O O
0.7 O O
p O O
( O O
xa|xb O O
= O O
0.7 O O
) O O
10 O O
5 O O
p O O
( O O
xa O O
, O O
xb O O
) O O
p O O
( O O
xa O O
) O O
0 O O
0 O O
0.5 O O
xa O O
1 O O
0 O O
0 O O
0.5 O O
xa O O
1 O O
figure O O
2.9 O O
the O O
plot O O
on O O
the O O
left O O
shows O O
the O O
contours O O
of O O
a O O
gaussian O O
distribution O O
p O O
( O O
xa O O
, O O
xb O O
) O O
over O O
two O O
variables O O
, O O
and O O
the O O
plot O O
on O O
the O O
right O O
shows O O
the O O
marginal B B
distribution O O
p O O
( O O
xa O O
) O O
( O O
blue O O
curve O O
) O O
and O O
the O O
conditional B B
distribution O O
p O O
( O O
xa|xb O O
) O O
for O O
xb O O
= O O
0.7 O O
( O O
red O O
curve O O
) O O
. O O
one O O
widely O O
used O O
framework O O
of O O
this O O
kind O O
is O O
known O O
as O O
a O O
decision B B
tree I I
in O O
which O O
the O O
selec- O O
tion O O
process O O
can O O
be O O
described O O
as O O
a O O
sequence O O
of O O
binary O O
selections O O
corresponding O O
to O O
the O O
traversal O O
of O O
a O O
tree B B
structure O O
and O O
is O O
discussed O O
in O O
section O O
14.4. O O
in O O
this O O
case O O
, O O
the O O
individual O O
models O O
are O O
generally O O
chosen O O
to O O
be O O
very O O
simple O O
, O O
and O O
the O O
overall O O
ﬂexibility O O
of O O
the O O
model O O
arises O O
from O O
the O O
input-dependent O O
selection O O
process O O
. O O
second O O
, O O
real O O
data O O
will O O
typically O O
exhibit O O
some O O
smoothness O O
properties O O
( O O
at O O
least O O
locally O O
) O O
so O O
that O O
for O O
the O O
most O O
part O O
small O O
changes O O
in O O
the O O
input O O
variables O O
will O O
produce O O
small O O
changes O O
in O O
the O O
target O O
variables O O
, O O
and O O
so O O
we O O
can O O
ex- O O
ploit O O
local B B
interpolation-like O O
techniques O O
to O O
allow O O
us O O
to O O
make O O
predictions O O
of O O
the O O
target O O
variables O O
for O O
new O O
values O O
of O O
the O O
input O O
variables O O
. O O
the O O
receptive O O
ﬁelds O O
are O O
chosen O O
to O O
be O O
contiguous O O
and O O
nonoverlapping O O
so O O
that O O
there O O
are O O
half O O
the O O
number O O
of O O
rows O O
and O O
columns O O
in O O
the O O
subsampling B B
layer O O
compared O O
with O O
the O O
convolutional B B
layer O O
. O O
plot O O
( O O
a O O
) O O
shows O O
the O O
data O O
points O O
in O O
green O O
, O O
together O O
with O O
the O O
initial O O
conﬁgura- O O
tion O O
of O O
the O O
mixture B B
model I I
in O O
which O O
the O O
one O O
standard-deviation O O
contours O O
for O O
the O O
two O O
438 O O
9. O O
mixture B B
models O O
and O O
em O O
gaussian O O
components O O
are O O
shown O O
as O O
blue O O
and O O
red O O
circles O O
. O O
as O O
the O O
number O O
of O O
grid O O
points O O
increases O O
, O O
so O O
the O O
envelope O O
function O O
becomes O O
a O O
better O O
approximation O O
of O O
the O O
desired O O
distribution O O
p O O
( O O
z O O
) O O
and O O
the O O
probability B B
of O O
rejection O B
decreases O O
. O O
this O O
difference O O
is O O
highlighted O O
by O O
the O O
example O O
in O O
figure O O
12.7. O O
another O O
common O O
application O O
of O O
principal B B
component I I
analysis I I
is O O
to O O
data O O
visual O O
( O O
cid:173 O O
) O O
ization O O
. O O
thus O O
for O O
the O O
factor O B
p O O
( O O
c|a O O
, O O
b O O
) O O
, O O
there O O
will O O
be O O
links O O
from O O
nodes O O
a O O
and O O
b O O
to O O
node B B
c O O
, O O
whereas O O
for O O
the O O
factor O B
p O O
( O O
a O O
) O O
there O O
will O O
be O O
no O O
incoming O O
links O O
. O O
for O O
convenience O O
, O O
consider O O
a O O
gaussian O O
distribution O O
p O O
( O O
z O O
) O O
with O O
independent B B
components O O
, O O
for O O
which O O
the O O
hamiltonian O O
is O O
given O O
by O O
( O O
cid:2 O O
) O O
i O O
( O O
cid:2 O O
) O O
i O O
h O O
( O O
z O O
, O O
r O O
) O O
= O O
1 O O
2 O O
1 O O
σ2 O O
i O O
z2 O O
i O O
+ O O
1 O O
2 O O
r2 O O
i O O
. O O
it O O
takes O O
the O O
concept O O
of O O
chunking B O
to O O
the O O
extreme O O
limit O O
and O O
considers O O
just O O
two O O
lagrange O O
multipliers O O
at O O
a O O
time O O
. O O
5.5.7 O O
soft B O
weight I B
sharing I I
one O O
way O O
to O O
reduce O O
the O O
effective O O
complexity O O
of O O
a O O
network O O
with O O
a O O
large O O
number O O
of O O
weights O O
is O O
to O O
constrain O O
weights O O
within O O
certain O O
groups O O
to O O
be O O
equal O O
. O O
the O O
precise O O
form O O
of O O
the O O
function O O
y O O
( O O
x O O
) O O
is O O
determined O O
during O O
the O O
training B B
phase O O
, O O
also O O
known O O
as O O
the O O
learning B B
phase O O
, O O
on O O
the O O
basis O B
of O O
the O O
training B B
data O O
. O O
from O O
these O O
we O O
obtain O O
the O O
following O O
expressions O O
for O O
the O O
mean B B
and O O
covariance B B
of O O
the O O
conditional B B
distribution O O
p O O
( O O
xa|xb O O
) O O
bb O O
( O O
xb O O
− O O
µb O O
) O O
−1 O O
µa|b O O
= O O
µa O O
+ O O
σabς O O
σa|b O O
= O O
σaa O O
− O O
σabς O O
−1 O O
bb O O
σba O O
. O O
by O O
repeated O O
application O O
of O O
the O O
product B O
rule I I
of I I
probability I I
, O O
this O O
joint O O
distribution O O
can O O
be O O
written O O
as O O
a O O
product O O
of O O
conditional B B
distributions O O
, O O
one O O
for O O
each O O
of O O
the O O
variables O O
p O O
( O O
x1 O O
, O O
. O O
they O O
didn O O
’ O O
t O O
fully O O
appreciate O O
boltzmann O O
’ O O
s O O
argu- O O
ments O O
, O O
which O O
were O O
statistical O O
in O O
nature O O
and O O
which O O
con- O O
cluded O O
not O O
that O O
entropy B B
could O O
never O O
decrease O O
over O O
time O O
but O O
simply O O
that O O
with O O
overwhelming O O
probability B B
it O O
would O O
generally O O
increase O O
. O O
show O O
that O O
this O O
two-level O O
hierarchical O B
mixture O O
is O O
equivalent O O
to O O
a O O
conventional O O
single-level O O
mixture B B
model I I
. O O
figure O O
1.22 O O
plot O O
of O O
the O O
fraction O O
of O O
the O O
volume O O
of O O
a O O
sphere O O
lying O O
in O O
the O O
range O O
r O O
= O O
1− O O
to O O
r O O
= O O
1 O O
for O O
various O O
values O O
of O O
the O O
dimensionality O O
d. O O
1.4. O O
the O O
curse B B
of I I
dimensionality I I
37 O O
n O O
o O O
i O O
t O O
c O O
a O O
r O O
f O O
e O O
m O O
u O O
o O O
v O O
l O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O I
0 O O
0 O O
d O O
= O O
20 O O
d O O
= O O
5 O O
d O O
= O O
2 O O
d O O
= O O
1 O O
0.2 O O
0.4 O O
0.6 O O
0.8 O O
1 O O
 O O
although O O
the O O
curse B B
of I I
dimensionality I I
certainly O O
raises O O
important O O
issues O O
for O O
pat- O O
tern O O
recognition O O
applications O O
, O O
it O O
does O O
not O O
prevent O O
us O O
from O O
ﬁnding O O
effective O O
techniques O O
applicable O O
to O O
high-dimensional O O
spaces O O
. O O
4.3. O O
probabilistic O O
discriminative O O
models O O
for O O
the O O
two-class O O
classiﬁcation B B
problem O O
, O O
we O O
have O O
seen O O
that O O
the O O
posterior B O
probability I I
of O O
class O O
c1 O O
can O O
be O O
written O O
as O O
a O O
logistic B B
sigmoid I I
acting O O
on O O
a O O
linear O O
function O O
of O O
x O O
, O O
for O O
a O O
wide O O
choice O O
of O O
class-conditional O O
distributions O O
p O O
( O O
x|ck O O
) O O
. O O
( O O
b.72 O O
) O O
in O O
the O O
limit O O
ν O O
→ O O
∞ O O
, O O
the O O
t-distribution O O
reduces O O
to O O
a O O
gaussian O O
with O O
mean B B
µ O O
and O O
pre- O O
cision O O
λ. O O
student O O
’ O O
s O O
t-distribution O O
provides O O
a O O
generalization B B
of O O
the O O
gaussian O O
whose O O
maximum B B
likelihood I I
parameter O O
values O O
are O O
robust O O
to O O
outliers B B
. O O
this O O
allows O O
the O O
product O O
of O O
sigmoid B O
functions O O
to O O
be O O
approximated O O
by O O
a O O
product O O
of O O
gaussians O O
thereby O O
allowing O O
the O O
marginalization O O
over O O
an O O
to O O
be O O
performed O O
analyti- O O
cally O O
. O O
on O O
infor- O O
mation B B
and O O
sufﬁciency O O
. O O
2.43 O O
( O O
( O O
cid:12 O O
) O O
) O O
the O O
following O O
distribution O O
p O O
( O O
x|σ2 O O
, O O
q O O
) O O
= O O
( O O
cid:16 O O
) O O
( O O
cid:15 O O
) O O
−|x|q O O
2σ2 O O
( O O
2.293 O O
) O O
( O O
2.294 O O
) O O
q O O
2 O O
( O O
2σ2 O O
) O O
1/qγ O O
( O O
1/q O O
) O O
exp O O
p O O
( O O
x|σ2 O O
, O O
q O O
) O O
dx O O
= O O
1 O O
( O O
cid:6 O O
) O O
∞ O O
−∞ O O
is O O
a O O
generalization B B
of O O
the O O
univariate O O
gaussian O O
distribution O O
. O O
if O O
the O O
distribution O O
of O O
x O O
is O O
conditioned O O
on O O
another O O
variable O O
z O O
, O O
then O O
the O O
corre- O O
sponding O O
conditional B B
expectation I I
will O O
be O O
written O O
ex O O
[ O O
f O O
( O O
x O O
) O O
|z O O
] O O
. O O
8.3.1 O O
conditional B B
independence I I
properties O O
. O O
now O O
let O O
us O O
exploit O O
this O O
result O O
to O O
obtain O O
a O O
practical O O
algorithm O O
for O O
approximate O O
inference B B
. O O
we O O
shall O O
discuss O O
the O O
mo- O O
tivation O O
for O O
this O O
choice O O
of O O
error B B
function I I
later O O
in O O
this O O
chapter O O
. O O
6.4 O O
( O O
( O O
cid:12 O O
) O O
) O O
in O O
appendix O O
c O O
, O O
we O O
give O O
an O O
example O O
of O O
a O O
matrix O O
that O O
has O O
positive O O
elements O O
but O O
that O O
has O O
a O O
negative O O
eigenvalue O O
and O O
hence O O
that O O
is O O
not O O
positive B O
deﬁnite I O
. O O
what O O
is O O
the O O
interpretation O O
of O O
this O O
form O O
of O O
loss B B
matrix I I
? O O
1.23 O O
( O O
( O O
cid:1 O O
) O O
) O O
derive O O
the O O
criterion O O
for O O
minimizing O O
the O O
expected O O
loss O O
when O O
there O O
is O O
a O O
general O O
loss B B
matrix I I
and O O
general O O
prior B B
probabilities O O
for O O
the O O
classes O O
. O O
the O O
parameter O O
σ O O
is O O
known O O
as O O
a O O
scale B O
parameter I I
, O O
and O O
the O O
density B B
exhibits O O
scale B O
invariance I B
because O O
if O O
we O O
scale O O
x O O
by O O
a O O
constant O O
to O O
give O O
( O O
cid:1 O O
) O O
x O O
= O O
cx O O
, O O
then O O
where O O
we O O
have O O
deﬁned O O
( O O
cid:1 O O
) O O
σ O O
= O O
cσ O O
. O O
in O O
many O O
cases O O
, O O
however O O
, O O
this O O
will O O
prove O O
to O O
be O O
too O O
wasteful O O
of O O
valuable O O
training B B
data O O
, O O
and O O
we O O
have O O
to O O
seek O O
more O O
sophisticated O O
approaches O O
. O O
using O O
the O O
perceptron B B
learning O O
rule O O
( O O
4.55 O O
) O O
, O O
show O O
that O O
the O O
learned O O
weight B O
vector I I
w O O
can O O
be O O
written O O
as O O
a O O
linear O O
combination O O
of O O
the O O
vectors O O
tnφ O O
( O O
xn O O
) O O
where O O
tn O O
∈ O O
{ O O
−1 O O
, O O
+1 O O
} O O
. O O
it O O
has O O
the O O
advantage O O
that O O
the O O
error B B
function I I
remains O O
a O O
quadratic O O
function O O
of O O
w O O
, O O
and O O
so O O
its O O
exact O O
minimizer O O
can O O
be O O
found O O
in O O
closed O O
form O O
. O O
here O O
we O O
discuss O O
an O O
approximation O O
in O O
which O O
we O O
set O O
the O O
hyperparameters O O
to O O
speciﬁc O O
values O O
determined O O
by O O
maximizing O O
the O O
marginal B B
likeli- O O
hood O O
function O O
obtained O O
by O O
ﬁrst O O
integrating O O
over O O
the O O
parameters O O
w. O O
this O O
framework O O
is O O
known O O
in O O
the O O
statistics O O
literature O O
as O O
empirical O O
bayes O O
( O O
bernardo O O
and O O
smith O O
, O O
1994 O O
; O O
gelman O O
et O O
al. O O
, O O
2004 O O
) O O
, O O
or O O
type B O
2 I O
maximum I B
likelihood I I
( O O
berger O O
, O O
1985 O O
) O O
, O O
or O O
generalized B B
maximum I B
likelihood I I
( O O
wahba O O
, O O
1975 O O
) O O
, O O
and O O
in O O
the O O
machine O O
learning O O
literature O O
is O O
also O O
called O O
the O O
evidence B B
approximation I I
( O O
gull O O
, O O
1989 O O
; O O
mackay O O
, O O
1992a O O
) O O
. O O
1 O O
+ O O
( O O
cid:22 O O
) O O
m−1 O O
( O O
cid:2 O O
) O O
( O O
cid:23 O O
) O O
−1 O O
2.4. O O
the O O
exponential B B
family I I
115 O O
making O O
use O O
of O O
the O O
constraint O O
( O O
2.209 O O
) O O
, O O
the O O
multinomial B B
distribution I I
in O O
this O O
representa- O O
tion O O
then O O
becomes O O
which O O
we O O
can O O
solve O O
for O O
µk O O
by O O
ﬁrst O O
summing O O
both O O
sides O O
over O O
k O O
and O O
then O O
rearranging O O
and O O
back-substituting O O
to O O
give O O
( O O
2.213 O O
) O O
this O O
is O O
called O O
the O O
softmax B O
function I I
, O O
or O O
the O O
normalized B O
exponential I O
. O O
thus O O
the O O
probability B B
that O O
the O O
tank O O
is O O
empty O O
has O O
decreased O O
( O O
from O O
0.257 O O
to O O
0.111 O O
) O O
as O O
a O O
result O O
of O O
the O O
observation O O
of O O
the O O
state O O
of O O
the O O
battery O O
. O O
this O O
is O O
known O O
as O O
a O O
one-versus-one B O
classiﬁer I O
. O O
we O O
assume O O
here O O
that O O
the O O
noise O O
variance B B
is O O
known O O
and O O
hence O O
we O O
set O O
the O O
precision B O
parameter I I
to O O
its O O
true O O
value O O
β O O
= O O
( O O
1/0.2 O O
) O O
2 O O
= O O
25. O O
similarly O O
, O O
we O O
ﬁx O O
the O O
parameter O O
α O O
to O O
2.0. O O
we O O
shall O O
shortly O O
discuss O O
strategies O O
for O O
determining O O
α O O
and O O
β O O
from O O
the O O
training B B
data O O
. O O
if O O
there O O
are O O
l O O
components O O
in O O
the O O
mixture B B
model I I
( O O
5.148 O O
) O O
, O O
and O O
if O O
t O O
has O O
k O O
components O O
, O O
then O O
the O O
network O O
will O O
have O O
l O O
output O O
unit O O
activations O O
denoted O O
by O O
aπ O O
k O O
that O O
determine O O
the O O
mixing O O
coefﬁcients O O
πk O O
( O O
x O O
) O O
, O O
k O O
outputs O O
k O O
that O O
determine O O
the O O
kernel O O
widths O O
σk O O
( O O
x O O
) O O
, O O
and O O
l O O
× O O
k O O
outputs O O
denoted O O
denoted O O
by O O
aσ O O
µ O O
kj O O
that O O
determine O O
the O O
components O O
µkj O O
( O O
x O O
) O O
of O O
the O O
kernel O O
centres O O
µk O O
( O O
x O O
) O O
. O O
in O O
a O O
practical O O
implementation O O
, O O
the O O
tangent O O
vector O I
τ O O
n O O
can O O
be O O
approximated O O
us- O O
ing O O
ﬁnite O O
differences O O
, O O
by O O
subtracting O O
the O O
original O O
vector O O
xn O O
from O O
the O O
corresponding O O
vector O O
after O O
transformation O O
using O O
a O O
small O O
value O O
of O O
ξ O O
, O O
and O O
then O O
dividing O O
by O O
ξ. O O
this O O
is O O
illustrated O O
in O O
figure O O
5.16. O O
the O O
regularization B B
function O I
depends O O
on O O
the O O
network O O
weights O O
through O O
the O O
jaco- O O
bian O O
j. O O
a O O
backpropagation B B
formalism O O
for O O
computing O O
the O O
derivatives O O
of O O
the O O
regu- O O
larizer O O
with O O
respect O O
to O O
the O O
network O O
weights O O
is O O
easily O O
obtained O O
by O O
extension O O
of O O
the O O
techniques O O
introduced O O
in O O
section O O
5.3. O O
if O O
the O O
transformation O O
is O O
governed O O
by O O
l O O
parameters O O
( O O
e.g. O O
, O O
l O O
= O O
3 O O
for O O
the O O
case O O
of O O
translations O O
combined O O
with O O
in-plane O O
rotations O O
in O O
a O O
two-dimensional O O
image O O
) O O
, O O
then O O
the O O
manifold B B
m O O
will O O
have O O
dimensionality O O
l O O
, O O
and O O
the O O
corresponding O O
regularizer O O
is O O
given O O
by O O
the O O
sum O O
of O O
terms O O
of O O
the O O
form O O
( O O
5.128 O O
) O O
, O O
one O O
for O O
each O O
transformation O O
. O O
exact O O
bayesian O O
infer- O O
ence O O
for O O
logistic O O
regression B I
is O O
intractable O O
. O O
in O O
particular O O
, O O
we O O
see O O
from O O
figure O O
3.12 O O
that O O
the O O
model B O
evidence I I
can O O
be O O
sensitive O O
to O O
many O O
aspects O O
of O O
the O O
prior B B
, O O
such O O
as O O
the O O
behaviour O O
in O O
the O O
tails O O
. O O
because O O
p O O
( O O
xk O O
= O O
1 O O
) O O
= O O
µk O O
, O O
the O O
parameters O O
must O O
satisfy O O
0 O O
( O O
cid:1 O O
) O O
µk O O
( O O
cid:1 O O
) O O
1 O O
and O O
k O O
µk O O
= O O
1. O O
the O O
multinomial B B
distribution I I
is O O
a O O
multivariate O O
generalization B B
of O O
the O O
binomial O B
and O O
gives O O
the O O
distribution O O
over O O
counts O O
mk O O
for O O
a O O
k-state O O
discrete O O
variable O O
to O O
be O O
in O O
state O O
k O O
given O O
a O O
total O O
number O O
of O O
observations O O
n. O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
m O O
( O O
cid:14 O O
) O O
mult O O
( O O
m1 O O
, O O
m2 O O
, O O
. O O
602 O O
12. O O
continuous O O
latent O B
variables O O
12.22 O O
( O O
* O O
* O O
) O O
write O O
down O O
an O O
expression O O
for O O
the O O
expected O O
complete-data O O
log O O
likelihood O O
func O O
( O O
cid:173 O O
) O O
tion O O
for O O
the O O
factor B B
analysis I I
model O O
, O O
and O O
hence O O
derive O O
the O O
corresponding O O
m O O
step O O
equa O O
( O O
cid:173 O O
) O O
tions O O
( O O
12.69 O O
) O O
and O O
( O O
12.70 O O
) O O
. O O
show O O
that O O
the O O
differential B B
entropy I I
of O O
this O O
pair O O
of O O
variables O O
satisﬁes O O
h O O
[ O O
x O O
, O O
y O O
] O O
( O O
cid:1 O O
) O O
h O O
[ O O
x O O
] O O
+ O O
h O O
[ O O
y O O
] O O
( O O
1.152 O O
) O O
with O O
equality O O
if O O
, O O
and O O
only O O
if O O
, O O
x O O
and O O
y O O
are O O
statistically O O
independent B B
. O O
how O O
can O O
we O O
turn O O
this O O
intuition O O
into O O
a O O
learning B B
algorithm O O
? O O
one O O
very O O
simple O O
ap- O O
proach O O
would O O
be O O
to O O
divide O O
the O O
input O O
space O O
into O O
regular O O
cells O O
, O O
as O O
indicated O O
in O O
fig- O O
ure O O
1.20. O O
when O O
we O O
are O O
given O O
a O O
test O O
point O O
and O O
we O O
wish O O
to O O
predict O O
its O O
class O O
, O O
we O O
ﬁrst O O
decide O O
which O O
cell O O
it O O
belongs O O
to O O
, O O
and O O
we O O
then O O
ﬁnd O O
all O O
of O O
the O O
training B B
data O O
points O O
that O O
1.4. O O
the O O
curse B B
of I I
dimensionality I I
35 O O
figure O O
1.20 O O
illustration O O
of O O
a O O
simple O O
approach O O
to O O
the O O
solution O O
of O O
a O O
classiﬁcation B B
problem O O
in O O
which O O
the O O
input O O
space O O
is O O
divided O O
into O O
cells O O
and O O
any O O
new O O
test O O
point O O
is O O
assigned O O
to O O
the O O
class O O
that O O
has O O
a O O
majority O O
number O O
of O O
rep- O O
resentatives O O
in O O
the O O
same O O
cell O O
as O O
the O O
test O O
point O O
. O O
we O O
have O O
seen O O
that O O
a O O
gaussian O O
process O O
is O O
determined O O
by O O
its O O
covariance B B
( O O
kernel O O
) O O
function O O
. O O
directed B O
graphs O O
, O O
whose O O
factorization B B
is O O
deﬁned O O
by O O
( O O
8.5 O O
) O O
, O O
represent O O
special O O
cases O O
of O O
( O O
8.59 O O
) O O
in O O
which O O
the O O
factors O O
fs O O
( O O
xs O O
) O O
are O O
local B B
conditional O O
distributions O O
. O O
in O O
such O O
cases O O
, O O
the O O
conditional B B
mode O O
may O O
be O O
of O O
more O O
value O O
. O O
the O O
covariance B B
is O O
then O O
given O O
by O O
the O O
inverse B B
of O O
the O O
matrix O O
of O O
second O O
derivatives O O
of O O
the O O
negative O O
log O O
likelihood O O
, O O
which O O
takes O O
the O O
form O O
sn O O
= O O
−∇∇ O O
ln O O
p O O
( O O
w|t O O
) O O
= O O
s O O
−1 O O
0 O O
+ O O
yn O O
( O O
1 O O
− O O
yn O O
) O O
φnφt O O
n. O O
( O O
4.143 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
the O O
gaussian O O
approximation O O
to O O
the O O
posterior O O
distribution O O
therefore O O
takes O O
the O O
form O O
q O O
( O O
w O O
) O O
= O O
n O O
( O O
w|wmap O O
, O O
sn O O
) O O
. O O
it O O
should O O
be O O
emphasized O O
that O O
the O O
subsequent O O
analysis O O
is O O
valid O O
for O O
arbitrary O O
choices O O
of O O
basis B O
function I I
, O O
and O O
for O O
generality O O
we O O
shall O O
work O O
with O O
the O O
form O O
( O O
7.77 O O
) O O
. O O
we O O
see O O
from O O
figure O O
6.10 O O
that O O
η1 O O
converges O O
to O O
a O O
relatively O O
large O O
value O O
, O O
η2 O O
converges O O
to O O
a O O
much O O
smaller O O
value O O
, O O
and O O
η3 O O
becomes O O
very O O
small O O
indicating O O
that O O
x3 O O
is O O
irrelevant O O
for O O
predicting O O
t. O O
the O O
ard O O
framework O O
is O O
easily O O
incorporated O O
into O O
the O O
exponential-quadratic O O
kernel O O
( O O
6.63 O O
) O O
to O O
give O O
the O O
following O O
form O O
of O O
kernel B O
function I O
, O O
which O O
has O O
been O O
found O O
useful O O
for O O
applications O O
of O O
gaussian O O
processes O O
to O O
a O O
range O O
of O O
regression B B
problems O O
( O O
cid:24 O O
) O O
d O O
( O O
cid:2 O O
) O O
( O O
cid:25 O O
) O O
d O O
( O O
cid:2 O O
) O O
k O O
( O O
xn O O
, O O
xm O O
) O O
= O O
θ0 O O
exp O O
−1 O O
2 O O
ηi O O
( O O
xni O O
− O O
xmi O O
) O O
2 O O
+ O O
θ2 O O
+ O O
θ3 O O
xnixmi O O
( O O
6.72 O O
) O O
i=1 O O
i=1 O O
where O O
d O O
is O O
the O O
dimensionality O O
of O O
the O O
input O O
space O O
. O O
we O O
can O O
not O O
simply O O
ﬁnd O O
the O O
mode O O
by O O
setting O O
this O O
gradient O O
to O O
zero O O
, O O
because O O
σn O O
depends O O
nonlinearly O O
on O O
an O O
, O O
and O O
so O O
we O O
resort O O
to O O
an O O
iterative O O
scheme O O
based O O
on O O
the O O
newton-raphson O O
method O O
, O O
which O O
gives O O
rise O O
to O O
an O O
iterative B O
reweighted I I
least I I
squares I I
( O O
irls O O
) O O
algorithm O O
. O O
1.32 O O
( O O
( O O
cid:1 O O
) O O
) O O
consider O O
a O O
vector O O
x O O
of O O
continuous O O
variables O O
with O O
distribution O O
p O O
( O O
x O O
) O O
and O O
corre- O O
sponding O O
entropy B B
h O O
[ O O
x O O
] O O
. O O
an O O
ex- O O
tension O O
to O O
mixtures O O
of O O
conditional B B
gaussian O O
distributions O O
, O O
which O O
permit O O
multimodal O O
conditional B B
distributions O O
, O O
will O O
be O O
discussed O O
in O O
section O O
14.5.1. O O
now O O
consider O O
a O O
data O O
set O O
of O O
inputs O O
x O O
= O O
{ O O
x1 O O
, O O
. O O
this O O
requires O O
that O O
we O O
evaluate O O
the O O
probability B B
distribution O O
over O O
boxes O O
conditioned O O
on O O
the O O
identity O O
of O O
the O O
fruit O O
, O O
whereas O O
the O O
probabilities O O
in O O
( O O
1.16 O O
) O O
– O O
( O O
1.19 O O
) O O
give O O
the O O
probability B B
distribution O O
over O O
the O O
fruit O O
conditioned O O
on O O
the O O
identity O O
of O O
the O O
box O O
. O O
to O O
ﬁnd O O
the O O
precision O O
of O O
this O O
gaussian O O
, O O
we O O
consider O O
the O O
second B O
order I I
terms O I
in O O
( O O
2.102 O O
) O O
, O O
which O O
can O O
be O O
written O O
as O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
t O O
( O O
cid:15 O O
) O O
xt O O
( O O
λ O O
+ O O
atla O O
) O O
x O O
− O O
1 O O
−1 O O
2 O O
2 O O
= O O
−1 O O
−la O O
2 O O
x O O
y O O
ytly O O
+ O O
ytlax O O
+ O O
λ O O
+ O O
atla O O
−atl O O
1 O O
2 O O
l O O
1 O O
xtatly O O
2 O O
= O O
−1 O O
2 O O
ztrz O O
( O O
2.103 O O
) O O
and O O
so O O
the O O
gaussian O O
distribution O O
over O O
z O O
has O O
precision O O
( O O
inverse B B
covariance O O
) O O
matrix O O
given O O
by O O
r O O
= O O
λ O O
+ O O
atla O O
−atl O O
−la O O
. O O
5.3.2 O O
a O O
simple O O
example O O
5.3.3 O O
efﬁciency O O
of O O
backpropagation B B
. O O
if O O
the O O
weight B B
vector I I
starts O O
at O O
the O O
origin O O
and O O
moves O O
ac- O O
cording O O
to O O
the O O
local B B
negative O O
gra- O O
dient O O
direction O O
, O O
then O O
it O O
will O O
follow O O
the O O
path O O
shown O O
by O O
the O O
curve O O
. O O
the O O
logistic B B
sigmoid I I
has O O
been O O
encountered O O
already O O
in O O
earlier O O
chapters O O
and O O
plays O O
an O O
important O O
role O O
in O O
many O O
classiﬁcation B B
algorithms O O
. O O
11.7 O O
( O O
( O O
cid:12 O O
) O O
) O O
suppose O O
that O O
z O O
has O O
a O O
uniform B B
distribution I I
over O O
the O O
interval O O
[ O O
0 O O
, O O
1 O O
] O O
. O O
each O O
forward B B
propagation I I
requires O O
o O O
( O O
w O O
) O O
steps O O
, O O
and O O
5.3. O O
error B B
backpropagation I I
247 O O
figure O O
5.8 O O
illustration O O
of O O
a O O
modular O O
pattern O O
recognition O O
system O O
in O O
which O O
the O O
jacobian O O
matrix O O
can O O
be O O
used O O
to O O
backpropagate O O
error B B
signals O O
from O O
the O O
outputs O O
through O O
to O O
ear- O O
lier O O
modules O O
in O O
the O O
system O O
. O O
this O O
can O O
be O O
done O O
by O O
minimizing O O
an O O
error B B
function I I
that O O
measures O O
the O O
misﬁt O O
between O O
the O O
function O O
y O O
( O O
x O O
, O O
w O O
) O O
, O O
for O O
any O O
given O O
value O O
of O O
w O O
, O O
and O O
the O O
training B B
set I I
data O O
points O O
. O O
if O O
we O O
evaluate O O
the O O
expectations O O
of O O
the O O
maximum B B
likelihood I I
solutions O O
under O O
the O O
exercise O O
2.35 O B
true O O
distribution O O
, O O
we O O
obtain O O
the O O
following O O
results O O
e O O
[ O O
µml O O
] O O
= O O
µ O O
e O O
[ O O
σml O O
] O O
= O O
n O O
− O O
1 O O
this O O
bias B B
by O O
deﬁning O O
a O O
different O O
estimator O O
( O O
cid:4 O O
) O O
σ O O
given O O
by O O
we O O
see O O
that O O
the O O
expectation B B
of O O
the O O
maximum B B
likelihood I I
estimate O O
for O O
the O O
mean B B
is O O
equal O O
to O O
the O O
true O O
mean B B
. O O
to O O
verify O O
this O O
, O O
simply O O
note O O
that O O
there O O
is O O
only O O
one O O
path O O
from O O
node B B
zn−1 O O
to O O
node B B
zn+1 O O
and O O
this O O
is O O
head-to-tail O O
with O O
respect O O
to O O
the O O
observed O O
node O O
zn O O
. O O
given O O
a O O
generative B O
model I I
p O O
( O O
x O O
) O O
we O O
can O O
deﬁne O O
a O O
kernel O O
by O O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
p O O
( O O
x O O
) O O
p O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
. O O
this O O
can O O
be O O
resolved O O
by O O
dividing O O
the O O
input O O
space O O
up O O
into O O
regions O O
and O O
ﬁt O O
a O O
different O O
polynomial O B
in O O
each O O
region O O
, O O
leading O O
to O O
spline B O
functions I O
( O O
hastie O O
et O O
al. O O
, O O
2001 O O
) O O
. O O
( O O
6.74 O O
) O O
unlike O O
the O O
regression B B
case O O
, O O
the O O
covariance B B
matrix I I
no O O
longer O O
includes O O
a O O
noise O O
term O O
because O O
we O O
assume O O
that O O
all O O
of O O
the O O
training B B
data O O
points O O
are O O
correctly O O
labelled O O
. O O
in O O
the O O
case O O
of O O
continuous O O
variables O O
, O O
the O O
required O O
integrations O O
may O O
not O O
have O O
closed-form O O
461 O O
462 O O
10. O O
approximate O O
inference B B
analytical O O
solutions O O
, O O
while O O
the O O
dimensionality O O
of O O
the O O
space O O
and O O
the O O
complexity O O
of O O
the O O
integrand O O
may O O
prohibit O O
numerical O O
integration O O
. O O
the O O
use O O
of O O
improper B B
priors O O
can O O
lead O O
to O O
difﬁculties O O
in O O
selecting O O
regularization B B
coefﬁcients O O
and O O
in O O
model B O
comparison I I
within O O
the O O
bayesian O O
framework O O
, O O
because O O
the O O
corresponding O O
evidence O O
is O O
zero O O
. O O
( O O
c O O
) O O
the O O
result O O
of O O
converting O O
the O O
polytree B O
into O O
a O O
factor B B
graph I I
, O O
which O O
retains O O
the O O
tree B B
structure O O
. O O
1 O O
, O O
x O O
( O O
cid:3 O O
) O O
2 O O
) O O
= O O
( O O
1 O O
2 O O
, O O
1 O O
2 O O
) O O
, O O
and O O
so O O
far O O
, O O
we O O
have O O
considered O O
the O O
problem O O
of O O
maximizing O O
a O O
function O O
subject O O
to O O
an O O
equality B O
constraint I O
of O O
the O O
form O O
g O O
( O O
x O O
) O O
= O O
0. O O
we O O
now O O
consider O O
the O O
problem O O
of O O
maxi- O O
mizing O O
f O O
( O O
x O O
) O O
subject O O
to O O
an O O
inequality B O
constraint I O
of O O
the O O
form O O
g O O
( O O
x O O
) O O
( O O
cid:2 O O
) O O
0 O O
, O O
as O O
illustrated O O
in O O
figure O O
e.3 O O
. O O
, O O
l O O
, O O
where O O
l O O
= O O
100 O O
, O O
and O O
for O O
each O O
data O O
set O O
d O O
( O O
l O O
) O O
we O O
appendix O O
a O O
150 O O
3. O O
linear O O
models O O
for B O
regression I I
t O O
1 O O
0 O O
−1 O O
t O O
1 O O
0 O O
−1 O O
t O O
1 O O
0 O O
−1 O O
0 O O
0 O O
0 O O
ln O O
λ O O
= O O
2.6 O O
t O O
1 O O
0 O O
−1 O O
x O O
1 O O
0 O O
x O O
1 O O
ln O O
λ O O
= O O
−0.31 O O
t O O
1 O O
0 O O
−1 O O
x O O
1 O O
0 O O
x O O
1 O O
ln O O
λ O O
= O O
−2.4 O O
t O O
1 O O
0 O O
−1 O O
x O O
1 O O
0 O O
x O O
1 O O
figure O O
3.5 O O
illustration O O
of O O
the O O
dependence O O
of O O
bias B B
and O O
variance B B
on O O
model O O
complexity O O
, O O
governed O O
by O O
a O O
regulariza- O O
tion O O
parameter O O
λ O O
, O O
using O O
the O O
sinusoidal B B
data I O
set O O
from O O
chapter O O
1. O O
there O O
are O O
l O O
= O O
100 O O
data O O
sets O O
, O O
each O O
having O O
n O O
= O O
25 O O
data O O
points O O
, O O
and O O
there O O
are O O
24 O O
gaussian O O
basis O B
functions O O
in O O
the O O
model O O
so O O
that O O
the O O
total O O
number O O
of O O
parameters O O
is O O
m O O
= O O
25 O O
including O O
the O O
bias B B
parameter I I
. O O
this O O
graph O O
of O O
course O O
describes O O
a O O
much O O
broader O O
class O O
of O O
probability B B
distributions O O
, O O
all O O
of O O
which O O
factorize O O
according O O
to O O
( O O
13.6 O O
) O O
. O O
in O O
fact O O
, O O
we O O
might O O
wonder O O
whether O O
it O O
is O O
a O O
general O O
property O O
of O O
bayesian O O
learning B B
that O O
, O O
as O O
we O O
observe O O
more O O
and O O
more O O
data O O
, O O
the O O
uncertainty O O
represented O O
by O O
the O O
posterior O O
distribution O O
will O O
steadily O O
decrease O O
. O O
in O O
order O O
to O O
obtain O O
the O O
normalization O O
coefﬁcient O O
we O O
note O O
that O O
out O O
of O O
n O O
coin O O
ﬂips O O
, O O
we O O
have O O
to O O
add O O
up O O
all O O
of O O
the O O
possible O O
ways O O
of O O
obtaining O O
m O O
heads O O
, O O
so O O
that O O
the O O
binomial B B
distribution I I
can O O
be O O
written O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
bin O O
( O O
m|n O O
, O O
µ O O
) O O
= O O
µm O O
( O O
1 O O
− O O
µ O O
) O O
n−m O O
where O O
n O O
m O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
n O O
m O O
≡ O O
n O O
! O O
( O O
n O O
− O O
m O O
) O O
! O O
m O O
! O O
( O O
2.9 O O
) O O
( O O
2.10 O O
) O O
exercise O O
2.3 O O
is O O
the O O
number O O
of O O
ways O O
of O O
choosing O O
m O O
objects O O
out O O
of O O
a O O
total O O
of O O
n O O
identical O O
objects O O
. O O
note O O
that O O
the O O
vertical O O
beams O O
are O O
asymmetrically O O
arranged O O
relative B O
to O O
the O O
central O B
axis O O
( O O
shown O O
by O O
the O O
dotted O O
line O O
) O O
. O O
the O O
ideas O O
is O O
that O O
if O O
a O O
narrow O O
beam O O
of O O
gamma O B
rays O O
is O O
passed O O
through O O
the O O
pipe O O
, O O
the O O
attenuation O O
in O O
the O O
intensity O O
of O O
the O O
beam O O
provides O O
information O B
about O O
the O O
density B B
of O O
material O O
along O O
its O O
path O O
. O O
the O O
generalization B B
to O O
multiple O O
variables O O
is O O
straightforward O O
and O O
involves O O
the O O
ja- O O
cobian O O
of O O
the O O
change O O
of O O
variables O O
, O O
so O O
that O O
p O O
( O O
y1 O O
, O O
. O O
4.4. O O
the O O
laplace O O
approximation O O
215 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
0 O O
−2 O O
−1 O O
0 O O
1 O O
2 O O
3 O O
4 O O
40 O O
30 O O
20 O O
10 O O
0 O O
−2 O O
−1 O O
0 O O
1 O O
2 O O
3 O O
4 O O
figure O O
4.14 O O
illustration O O
of O O
the O O
laplace O O
approximation O O
applied O O
to O O
the O O
distribution O O
p O O
( O O
z O O
) O O
∝ O O
exp O O
( O O
−z2/2 O O
) O O
σ O O
( O O
20z O O
+ O O
4 O O
) O O
where O O
σ O O
( O O
z O O
) O O
is O O
the O O
logistic B B
sigmoid I I
function O O
deﬁned O O
by O O
σ O O
( O O
z O O
) O O
= O O
( O O
1 O O
+ O O
e−z O O
) O O
−1 O O
. O O
we O O
therefore O O
need O O
only O O
to O O
ﬁnd O O
its O O
mean B B
and O O
covariance B B
, O O
which O O
are O O
given O O
from O O
( O O
6.50 O O
) O O
by O O
e O O
[ O O
y O O
] O O
= O O
φe O O
[ O O
w O O
] O O
= O O
0 O O
( O O
cid:8 O O
) O O
( O O
cid:9 O O
) O O
( O O
cid:8 O O
) O O
( O O
cid:9 O O
) O O
exercise O O
2.31 O O
( O O
6.52 O O
) O O
( O O
6.53 O O
) O O
( O O
6.54 O O
) O O
cov O O
[ O O
y O O
] O O
= O O
e O O
yyt O O
= O O
φe O O
wwt O O
φt O O
= O O
1 O O
α O O
φφt O O
= O O
k O O
where O O
k O O
is O O
the O O
gram O O
matrix O O
with O O
elements O O
knm O B
= O O
k O O
( O O
xn O O
, O O
xm O O
) O O
= O O
and O O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
is O O
the O O
kernel B O
function I I
. O O
( O O
1.34 O O
) O O
in O O
either O O
case O O
, O O
if O O
we O O
are O O
given O O
a O O
ﬁnite O O
number O O
n O O
of O O
points O O
drawn O O
from O O
the O O
probability B B
distribution O O
or O O
probability B B
density O O
, O O
then O O
the O O
expectation B B
can O O
be O O
approximated O O
as O O
a O O
20 O O
1. O O
introduction O O
ﬁnite O O
sum O O
over O O
these O O
points O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
e O O
[ O O
f O O
] O O
( O O
cid:8 O O
) O O
1 O O
n O O
f O O
( O O
xn O O
) O O
. O O
we O O
say O O
that O O
this O O
graph O O
is O O
fully B O
connected I I
because O O
there O O
is O O
a O O
link B B
between O O
every O O
pair O O
of O O
nodes O O
. O O
the O O
adaboost O O
algorithm O O
is O O
illustrated O O
in O O
figure O O
14.2 O O
, O O
using O O
a O O
subset O O
of O O
30 O O
data O O
points O O
taken O O
from O O
the O O
toy O O
classiﬁcation B B
data O O
set O O
shown O O
in O O
figure O O
a.7 O O
. O O
em O O
al- O O
gorithms O O
for O O
ml O O
factor B O
analysis I I
. O O
section O O
10.2.4 O O
10.2.2 O O
variational B B
lower O O
bound O O
we O O
can O O
also O O
straightforwardly O O
evaluate O O
the O O
lower B B
bound I I
( O O
10.3 O O
) O O
for O O
this O O
model O O
. O O
also O O
µn O O
( O O
cid:2 O O
) O O
0 O O
and O O
( O O
cid:1 O O
) O O
µn O O
( O O
cid:2 O O
) O O
0 O O
together O O
with O O
( O O
7.59 O O
) O O
and O O
( O O
7.60 O O
) O O
, O O
require O O
an O O
( O O
cid:1 O O
) O O
c O O
and O O
( O O
cid:1 O O
) O O
an O O
( O O
cid:1 O O
) O O
c O O
, O O
and O O
so O O
again O O
we O O
have O O
the O O
box B O
constraints I O
φ O O
( O O
x O O
) O O
tφ O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
. O O
( O O
3.79 O O
) O O
we O O
recognize O O
( O O
3.79 O O
) O O
as O O
being O O
equal O O
, O O
up O O
to O O
a O O
constant O O
of O O
proportionality O O
, O O
to O O
the O O
reg- O O
ularized O O
sum-of-squares B O
error I O
function O O
( O O
3.27 O O
) O O
. O O
( O O
6.39 O O
) O O
( O O
cid:6 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
e O O
= O O
1 O O
2 O O
appendix O O
d O O
exercise O O
6.17 O O
using O O
the O O
calculus B B
of I I
variations I I
, O O
we O O
can O O
optimize O O
with O O
respect O O
to O O
the O O
function O O
f O O
( O O
x O O
) O O
to O O
give O O
n O O
( O O
cid:2 O O
) O O
y O O
( O O
xn O O
) O O
= O O
tnh O O
( O O
x O O
− O O
xn O O
) O O
( O O
6.40 O O
) O O
n=1 O O
where O O
the O O
basis O B
functions O O
are O O
given O O
by O O
h O O
( O O
x O O
− O O
xn O O
) O O
= O O
ν O O
( O O
x O O
− O O
xn O O
) O O
ν O O
( O O
x O O
− O O
xn O O
) O O
n O O
( O O
cid:2 O O
) O O
. O O
= O O
l=1 O O
( O O
11.27 O O
) O O
11.1.6 O O
sampling O O
and O O
the O O
em O O
algorithm O O
in O O
addition O O
to O O
providing O O
a O O
mechanism O O
for O O
direct O O
implementation O O
of O O
the O O
bayesian O O
framework O O
, O O
monte O O
carlo O O
methods O O
can O O
also O O
play O O
a O O
role O O
in O O
the O O
frequentist B B
paradigm O O
, O O
for O O
example O O
to O O
ﬁnd O O
maximum B B
likelihood I I
solutions O O
. O O
10.6.3 O O
inference B B
of O O
hyperparameters O O
so O O
far O O
, O O
we O O
have O O
treated O O
the O O
hyperparameter B B
α O O
in O O
the O O
prior B B
distribution O O
as O O
a O O
known O O
constant O O
. O O
there O O
are O O
n O O
ways O O
to O O
choose O O
the O O
ﬁrst O O
object O O
, O O
( O O
n O O
− O O
1 O O
) O O
ways O O
to O O
choose O O
the O O
second O O
object O O
, O O
and O O
so O O
on O O
, O O
leading O O
to O O
a O O
total O O
of O O
n O O
! O O
ways O O
to O O
allocate O O
all O O
n O O
objects O O
to O O
the O O
bins O O
, O O
where O O
n O O
! O O
( O O
pronounced O O
‘ O O
factorial B B
n O O
’ O O
) O O
denotes O O
the O O
product O B
n O O
× O O
( O O
n O O
−1 O O
) O O
×···×2×1 O O
. O O
1 O O
0.5 O O
0 O O
−0.5 O O
−1 O O
0 O O
0.2 O O
0.4 O O
0.6 O O
0.8 O O
1 O O
6.4. O O
gaussian O O
processes O O
311 O O
sian O O
process O O
regression B B
have O O
also O O
been O O
considered O O
, O O
for O O
purposes O O
such O O
as O O
modelling O O
the O O
distribution O O
over O O
low-dimensional O O
manifolds O O
for O O
unsupervised O O
learning B B
( O O
bishop O O
et O O
al. O O
, O O
1998a O O
) O O
and O O
the O O
solution O O
of O O
stochastic B B
differential O O
equations O O
( O O
graepel O O
, O O
2003 O O
) O O
. O O
for O O
any O O
given O O
conﬁguration O O
there O O
are O O
two O O
degrees B B
of I I
freedom I I
corresponding O O
to O O
the O O
fractions O O
of O O
a. O O
data O O
sets O O
681 O O
oil O O
and O O
water O O
, O O
and O O
so O O
for O O
inﬁnite O O
integration O O
time O O
the O O
data O O
will O O
locally O O
live O O
on O O
a O O
two- O O
dimensional O O
manifold B B
. O O
setting O O
the O O
derivative B B
of O O
( O O
14.39 O O
) O O
with O O
respect O O
to O O
wk O O
equal O O
to O O
zero O O
gives O O
n O O
( O O
cid:2 O O
) O O
( O O
cid:10 O O
) O O
0 O O
= O O
γnk O O
n=1 O O
( O O
cid:11 O O
) O O
tn O O
− O O
wt O O
k O O
φn O O
φn O I
( O O
14.40 O O
) O O
which O O
we O O
can O O
write O O
in O O
matrix O O
notation O O
as O O
( O O
14.41 O O
) O O
where O O
rk O O
= O O
diag O O
( O O
γnk O O
) O O
is O O
a O O
diagonal B B
matrix O O
of O O
size O O
n O O
× O O
n. O O
solving O O
for O O
wk O O
, O O
we O O
obtain O O
0 O O
= O O
φtrk O O
( O O
t O O
− O O
φwk O O
) O O
( O O
cid:10 O O
) O O
( O O
14.42 O O
) O O
this O O
represents O O
a O O
set O O
of O O
modiﬁed O O
normal B B
equations I O
corresponding O O
to O O
the O O
weighted B O
least I I
squares I I
problem O O
, O O
of O O
the O O
same O O
form O O
as O O
( O O
4.99 O O
) O O
found O O
in O O
the O O
context O O
of O O
logistic B B
regression I I
. O O
now O O
suppose O O
that O O
the O O
joint O O
distribution O O
of O O
observed O O
and O O
latent O O
variables O O
is O O
a O O
member O O
of O O
the O O
exponential B B
family I I
, O O
parameterized O O
by O O
natural B O
parameters I O
η O O
so O O
that O O
p O O
( O O
x O O
, O O
z|η O O
) O O
= O O
h O O
( O O
xn O O
, O O
zn O O
) O O
g O O
( O O
η O O
) O O
exp O O
ηtu O O
( O O
xn O O
, O O
zn O O
) O O
n O O
( O O
cid:14 O O
) O O
n=1 O O
( O O
cid:26 O O
) O O
( O O
cid:26 O O
) O O
( O O
cid:27 O O
) O O
( O O
cid:27 O O
) O O
. O O
if O O
we O O
assign O O
equal O O
prior B B
probabilities O O
p O O
( O O
m O O
) O O
to O O
the O O
different O O
values O O
of O O
m O O
, O O
then O O
we O O
can O O
interpret O O
l O O
as O O
an O O
approximation O O
to O O
the O O
poste- O O
rior O O
model O O
probability O I
p O O
( O O
m|t O O
) O O
. O O
100 O O
90 O O
80 O O
70 O O
60 O O
50 O O
40 O O
1 O O
2 O O
3 O O
4 O O
5 O O
6 O O
synthetic O O
data O O
throughout O O
the O O
book O O
, O O
we O O
use O O
two O O
simple O O
synthetic B O
data I O
sets I O
to O O
illustrate O O
many O O
of O O
the O O
algorithms O O
. O O
first O O
of O O
all O O
, O O
if O O
a O O
density B B
takes O O
the O O
form O O
p O O
( O O
x|µ O O
) O O
= O O
f O O
( O O
x O O
− O O
µ O O
) O O
( O O
2.232 O O
) O O
then O O
the O O
parameter O O
µ O O
is O O
known O O
as O O
a O O
location B B
parameter I I
. O O
because O O
the O O
latent O B
variables O O
are O O
k-dimensional O O
binary O O
variables O O
, O O
this O O
conditional B B
distribution O O
corresponds O O
to O O
a O O
table O O
of O O
numbers O O
that O O
we O O
denote O O
by O O
a O O
, O O
the O O
elements O O
of O O
which O O
are O O
known O O
as O O
transition O O
probabilities O O
. O O
the O O
marginal B B
likelihood I I
, O O
or O O
evidence O O
, O O
for O O
the O O
hyperparameters O O
is O O
obtained O O
by O O
integrating O O
over O O
the O O
network O O
weights O O
p O O
( O O
d|α O O
, O O
β O O
) O O
= O O
( O O
cid:6 O O
) O O
p O O
( O O
d|w O O
, O O
β O O
) O O
p O O
( O O
w|α O O
) O O
dw O O
. O O
so O O
far O O
, O O
we O O
have O O
considered O O
the O O
rvm O O
for O O
binary O O
classiﬁcation B B
problems O O
. O O
it O O
follows O O
that O O
p O O
( O O
t O O
= O O
−1|y O O
) O O
= O O
1 O O
− O O
σ O O
( O O
y O O
) O O
= O O
σ O O
( O O
−y O O
) O O
, O O
where O O
we O O
have O O
used O O
the O O
properties O O
of O O
the O O
logistic B B
sigmoid I I
function O O
, O O
and O O
so O O
we O O
can O O
write O O
p O O
( O O
t|y O O
) O O
= O O
σ O O
( O O
yt O O
) O O
. O O
3 O O
1.5 O O
0 O O
−1.5 O O
−3 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
3 O O
1.5 O O
0 O O
−1.5 O O
−3 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
6.4.2 O O
gaussian O O
processes O O
for B O
regression I I
in O O
order O O
to O O
apply O O
gaussian O O
process O O
models O O
to O O
the O O
problem O O
of O O
regression B B
, O O
we O O
need O O
to O O
take O O
account O O
of O O
the O O
noise O O
on O O
the O O
observed O O
target O O
values O O
, O O
which O O
are O O
given O O
by O O
tn O O
= O O
yn O O
+ O O
n O O
( O O
6.57 O O
) O O
where O O
yn O O
= O O
y O O
( O O
xn O O
) O O
, O O
and O O
n O O
is O O
a O O
random O O
noise O O
variable O O
whose O O
value O O
is O O
chosen O O
inde- O O
pendently O O
for O O
each O O
observation O O
n. O O
here O O
we O O
shall O O
consider O O
noise O O
processes O O
that O O
have O O
a O O
gaussian O O
distribution O O
, O O
so O O
that O O
p O O
( O O
tn|yn O O
) O O
= O O
n O O
( O O
tn|yn O O
, O O
β O O
−1 O O
) O O
( O O
6.58 O O
) O O
where O O
β O O
is O O
a O O
hyperparameter B O
representing O O
the O O
precision O O
of O O
the O O
noise O O
. O O
because O O
this O O
joint O O
distribution O O
is O O
gaussian O O
, O O
we O O
can O O
apply O O
the O O
results O O
from O O
section O O
2.3.1 O O
to O O
ﬁnd O O
the O O
conditional B B
gaussian O O
distribution O O
. O O
this O O
allows O O
the O O
expectation B B
( O O
11.1 O O
) O O
to O O
be O O
approximated O O
by O O
a O O
ﬁnite O O
sum O O
as O O
long O O
as O O
the O O
samples O O
z O O
( O O
l O O
) O O
are O O
drawn O O
from O O
the O O
distribution O O
p O O
( O O
z O O
) O O
, O O
then O O
e O O
[ O O
( O O
cid:1 O O
) O O
f O O
] O O
= O O
e O O
[ O O
f O O
] O O
and O O
so O O
the O O
estimator O O
( O O
cid:1 O O
) O O
f O O
has O O
the O O
correct O O
mean B B
. O O
+ O O
w2 O O
where O O
( O O
cid:6 O O
) O O
w O O
( O O
cid:6 O O
) O O
2 O O
≡ O O
wtw O O
= O O
w2 O O
m O O
, O O
and O O
the O O
coefﬁcient O O
λ O O
governs O O
the O O
rel- O O
ative O O
importance O O
of O O
the O O
regularization B B
term O O
compared O O
with O O
the O O
sum-of-squares B B
error I I
term O O
. O O
3.5.2 O O
maximizing O O
the O O
evidence B B
function I I
. O O
variables O O
such O O
as O O
zn O O
that O O
appear O O
inside O O
the O O
plate B O
are O O
regarded O O
as O O
latent O B
variables O O
because O O
the O O
number O O
of O O
such O O
variables O O
grows O O
with O O
the O O
size O O
of O O
the O O
data O O
set O O
. O O
we O O
have O O
already O O
seen O O
that O O
the O O
directed B B
chain O O
can O O
be O O
transformed O O
into O O
an O O
equivalent O O
undirected O B
chain O O
. O O
this O O
time O O
, O O
however O O
, O O
we O O
maintain O O
a O O
record O O
of O O
the O O
current O O
state O O
z O O
( O O
τ O O
) O O
, O O
and O O
the O O
proposal B B
distribution I I
q O O
( O O
z|z O O
( O O
τ O O
) O O
) O O
depends O O
on O O
this O O
current O O
state O O
, O O
and O O
so O O
the O O
sequence O O
of O O
samples O O
z O O
( O O
1 O O
) O O
, O O
z O O
( O O
2 O O
) O O
, O O
. O O
a O O
second O O
difﬁculty O O
arises O O
from O O
the O O
transformation O O
behaviour O O
of O O
a O O
probability B B
density O O
under O O
a O O
nonlinear O O
change O O
of O O
variables O O
, O O
given O O
by O O
( O O
1.27 O O
) O O
. O O
ma- O O
chine O O
learning B B
50 O I
, O O
95–126 O O
. O O
however O O
, O O
in O O
practice O O
we O O
have O O
a O O
data O O
set O O
d O O
containing O O
only O O
a O O
ﬁnite O O
number O O
n O O
of O O
data O O
points O O
, O O
and O O
consequently O O
we O O
do O O
not O O
know O O
the O O
regression B B
function I I
h O O
( O O
x O O
) O O
exactly O O
. O O
taking O O
the O O
limit O O
l O O
→ O O
∞ O O
, O O
and O O
assuming O O
suitable O O
regularity O O
of O O
the O O
dis- O O
tributions O O
, O O
we O O
can O O
replace O O
the O O
sums O O
by O O
integrals O O
weighted O O
according O O
to O O
the O O
original O O
sampling O O
distribution O O
q O O
( O O
z O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
i O O
( O O
z O O
( O O
cid:1 O O
) O O
a O O
) O O
{ O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
) O O
/q O O
( O O
z O O
) O O
} O O
q O O
( O O
z O O
) O O
dz O O
( O O
cid:6 O O
) O O
{ O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
) O O
/q O O
( O O
z O O
) O O
} O O
q O O
( O O
z O O
) O O
dz O O
i O O
( O O
z O O
( O O
cid:1 O O
) O O
a O O
) O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
) O O
dz O O
( O O
cid:6 O O
) O O
( O O
cid:4 O O
) O O
p O O
( O O
z O O
) O O
dz O O
p O O
( O O
z O O
( O O
cid:1 O O
) O O
a O O
) O O
= O O
= O O
= O O
i O O
( O O
z O O
( O O
cid:1 O O
) O O
a O O
) O O
p O O
( O O
z O O
) O O
dz O O
( O O
11.26 O O
) O O
which O O
is O O
the O O
cumulative B O
distribution I O
function I O
of O O
p O O
( O O
z O O
) O O
. O O
various O O
meth- O O
ods O O
have O O
therefore O O
been O O
proposed O O
for O O
combining O O
multiple O O
two-class O O
svms O O
in O O
order O O
to O O
build O O
a O O
multiclass B B
classiﬁer O O
. O O
in O O
proceedings O O
of O O
the O O
international O O
conference O O
on O O
independent B B
component I I
analysis I I
and O O
blind O O
signal O O
separa- O O
tion O O
, O O
volume O O
3. O O
hodgson O O
, O O
m. O O
e. O O
( O O
1998 O O
) O O
. O O
this O O
key O O
equation O O
expresses O O
the O O
factorization B B
properties O O
of O O
the O O
joint O O
distribution O O
for O O
a O O
directed B B
graphical O O
model O O
. O O
also O O
shown O O
are O O
three O O
examples O O
of O O
his- O O
togram O O
density B B
estimates O O
corresponding O O
to O O
three O O
different O O
choices O O
for O O
the O O
bin O O
width O O
∆ O O
. O O
by O O
extending O O
the O O
discussion O O
of O O
section O O
5.3.2 O O
, O O
write O O
down O O
the O O
equations O O
for O O
the O O
derivatives O O
of O O
the O O
error B B
function I I
with O O
respect O O
to O O
these O O
additional O O
parameters O O
. O O
as O O
we O O
shall O O
see O O
, O O
in O O
the O O
limit O O
of O O
an O O
inﬁnite O O
number O O
of O O
basis O O
functions O O
, O O
a O O
bayesian O O
neural B O
network I I
with O O
an O O
appropriate O O
prior B B
reduces O O
to O O
a O O
gaussian O O
process O O
, O O
thereby O O
providing O O
a O O
deeper O O
link B O
between O O
neural O O
networks O O
and O O
kernel O O
methods O O
. O O
it O O
is O O
therefore O O
common O O
to O O
run O O
the O O
k-means O O
algo- O O
rithm O O
in O O
order O O
to O O
ﬁnd O O
a O O
suitable O O
initialization O O
for O O
a O O
gaussian O O
mixture B B
model I I
that O O
is O O
subsequently O O
adapted O O
using O O
em O O
. O O
it O O
comprises O O
the O O
product O O
of O O
a O O
gaussian O O
distribution O O
for O O
µ O O
, O O
whose O O
precision O O
is O O
proportional O O
to O O
λ O O
, O O
and O O
a O O
gamma B B
distribution I I
over O O
λ. O O
p O O
( O O
µ O O
, O O
λ|µ0 O O
, O O
β O O
, O O
a O O
, O O
b O O
) O O
= O O
n O O
( O O
cid:10 O O
) O O
µ|µo O O
, O O
( O O
βλ O O
) O O
−1 O O
gam O O
( O O
λ|a O O
, O O
b O O
) O O
. O O
( O O
e.4 O O
) O O
the O O
constrained O O
stationarity O O
condition O O
( O O
e.3 O O
) O O
is O O
obtained O O
by O O
setting O O
∇xl O O
= O O
0. O O
fur- O O
thermore O O
, O O
the O O
condition O O
∂l/∂λ O O
= O O
0 O O
leads O O
to O O
the O O
constraint O O
equation O O
g O O
( O O
x O O
) O O
= O O
0. O O
thus O O
to O O
ﬁnd O O
the O O
maximum O B
of O O
a O O
function O O
f O O
( O O
x O O
) O O
subject O O
to O O
the O O
constraint O O
g O O
( O O
x O O
) O O
= O O
0 O O
, O O
we O O
deﬁne O O
the O O
lagrangian O O
function O O
given O O
by O O
( O O
e.4 O O
) O O
and O O
we O O
then O O
ﬁnd O O
the O O
stationary B B
point O O
of O O
l O O
( O O
x O O
, O O
λ O O
) O O
with O O
respect O O
to O O
both O O
x O O
and O O
λ. O O
for O O
a O O
d-dimensional O O
vector O O
x O O
, O O
this O O
gives O O
d O O
+ O O
1 O O
equations O O
that O O
determine O O
both O O
the O O
stationary B B
point O O
x O O
( O O
cid:3 O O
) O O
and O O
the O O
value O O
of O O
λ. O O
if O O
we O O
are O O
only O O
interested O O
in O O
x O O
( O O
cid:3 O O
) O O
, O O
then O O
we O O
can O O
eliminate O O
λ O O
from O O
the O O
stationarity O O
equa- O O
tions O O
without O O
needing O O
to O O
ﬁnd O O
its O O
value O O
( O O
hence O O
the O O
term O O
‘ O O
undetermined B O
multiplier I O
’ O O
) O O
. O O
in O O
this O O
case O O
, O O
the O O
naive O O
bayes O O
assumption O O
then O O
implies O O
that O O
the O O
covariance B B
matrix I I
for O O
each O O
gaussian O O
is O O
diagonal B B
, O O
and O O
the O O
contours O O
of O O
constant O O
density B B
within O O
each O O
class O O
will O O
be O O
axis-aligned O O
ellipsoids O O
. O O
in O O
proceedings O B
of O O
the O O
sixth O O
annual O O
conference O O
on O O
computational B O
learning I B
theory I I
, O O
pp O O
. O O
in O O
order O O
for O O
this O O
to O O
be O O
valid O O
, O O
we O O
must O O
ensure O O
that O O
the O O
set O O
of O O
variables O O
that O O
appears O O
in O O
each O O
of O O
the O O
conditional B B
distributions O O
is O O
a O O
member O O
of O O
at O O
least O O
one O O
clique B B
of O O
the O O
undirected B B
graph I I
. O O
an O O
example O O
of O O
the O O
ν-svm O O
applied O O
to O O
a O O
synthetic O O
data O O
set O O
is O O
shown O O
in O O
figure O O
7.4. O O
here O O
gaussian O O
kernels O O
of O O
the O O
form O O
exp O O
( O O
−γ O O
( O O
cid:5 O O
) O O
x O O
− O O
x O O
( O O
cid:4 O O
) O O
( O O
cid:5 O O
) O O
2 O O
) O O
have O O
been O O
used O O
, O O
with O O
γ O O
= O O
0.45. O O
although O O
predictions O O
for O O
new O O
inputs O O
are O O
made O O
using O O
only O O
the O O
support O B
vectors O O
, O O
the O O
training B B
phase O O
( O O
i.e. O O
, O O
the O O
determination O O
of O O
the O O
parameters O O
a O O
and O O
b O O
) O O
makes O O
use O O
of O O
the O O
whole O O
data O O
set O O
, O O
and O O
so O O
it O O
is O O
important O O
to O O
have O O
efﬁcient O O
algorithms O O
for O O
solving O O
7.1. O O
maximum B B
margin I I
classiﬁers O O
335 O O
figure O O
7.4 O O
illustration O O
of O O
the O O
ν-svm O O
applied O O
to O O
a O O
nonseparable O O
data O O
set O O
in O O
two O O
dimensions O O
. O O
thus O O
if O O
we O O
use O O
such O O
a O O
model O O
to O O
predict O O
the O O
next O O
observation O O
in O O
a O O
sequence O O
, O O
the O O
distribution O O
of O O
predictions O O
will O O
depend O O
only O O
on O O
the O O
value O O
of O O
the O O
im- O O
mediately O O
preceding O O
observation O O
and O O
will O O
be O O
independent B B
of O O
all O O
earlier O O
observations O O
. O O
13.31 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
starting O O
from O O
the O O
result O O
( O O
13.103 O O
) O O
and O O
by O O
substituting O O
for O O
( O O
cid:1 O O
) O O
α O O
( O O
zn O O
) O O
using O O
( O O
13.84 O O
) O O
, O O
verify O O
the O O
result O O
( O O
13.104 O O
) O O
for O O
the O O
covariance B B
between O O
zn O O
and O O
zn−1 O O
. O O
one O O
technique O O
is O O
to O O
model O O
them O O
directly O O
, O O
for O O
example O O
by O O
representing O O
them O O
as O O
parametric O O
models O O
and O O
then O O
optimizing O O
the O O
parameters O O
using O O
a O O
training B B
set I I
. O O
since O O
the O O
space O O
of O O
hidden O O
variables O O
for O O
this O O
example O O
is O O
only O O
two O O
dimensional O O
, O O
we O O
can O O
illustrate O O
the O O
variational B B
approxima- O O
tion O O
to O O
the O O
posterior O O
distribution O O
by O O
plotting O O
contours O O
of O O
both O O
the O O
true O O
posterior O O
and O O
the O O
factorized O O
approximation O O
, O O
as O O
illustrated O O
in O O
figure O O
10.4 O O
. O O
variational B B
mes- O O
sage O O
passing O O
. O O
one O O
of O O
the O O
signiﬁcant O O
limitations O O
of O O
many O O
such O O
algorithms O O
is O O
that O O
the O O
kernel B O
function I I
k O O
( O O
xn O O
, O O
xm O O
) O O
must O O
be O O
evaluated O O
for O O
all O O
possible O O
pairs O O
xn O O
and O O
xm O O
of O O
training B B
points O O
, O O
which O O
can O O
be O O
computationally O O
infeasible O O
during O O
training B B
and O O
can O O
lead O O
to O O
excessive O O
computation O O
times O O
when O O
making O O
predictions O O
for O O
new O O
data O O
points O O
. O O
3.9 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
repeat O O
the O O
previous O O
exercise O O
but O O
instead O O
of O O
completing B B
the I I
square I I
by O O
hand O O
, O O
make O O
use O O
of O O
the O O
general O O
result O O
for O O
linear-gaussian O O
models O O
given O O
by O O
( O O
2.116 O O
) O O
. O O
the O O
goal O O
in O O
classiﬁcation B B
is O O
to O O
take O O
an O O
input O O
vector O O
x O O
and O O
to O O
assign O O
it O O
to O O
one O O
of O O
k O O
discrete O O
classes O O
ck O O
where O O
k O O
= O O
1 O O
, O O
. O O
, O O
φm−1 O O
) O O
t. O O
in O O
many O O
practical O O
ap- O O
plications O O
of O O
pattern O O
recognition O O
, O O
we O O
will O O
apply O O
some O O
form O O
of O O
ﬁxed O O
pre-processing O O
, O O
3.1. O O
linear O O
basis O O
function O O
models O O
139 O O
or O O
feature B O
extraction I O
, O O
to O O
the O O
original O O
data O O
variables O O
. O O
3.3 O O
( O O
( O O
cid:12 O O
) O O
) O O
consider O O
a O O
data O O
set O O
in O O
which O O
each O O
data O O
point O O
tn O O
is O O
associated O O
with O O
a O O
weighting O O
factor O O
rn O O
> O O
0 O O
, O O
so O O
that O O
the O O
sum-of-squares B B
error I I
function O O
becomes O O
( O O
cid:26 O O
) O O
n O O
( O O
cid:2 O O
) O O
rn O O
n=1 O O
ed O O
( O O
w O O
) O O
= O O
1 O O
2 O O
( O O
cid:27 O O
) O O
2 O O
tn O O
− O O
wtφ O O
( O O
xn O O
) O O
. O O
, O O
xn|zn O O
) O O
p O O
( O O
zn|zn−1 O O
) O O
p O O
( O O
zn−1 O O
) O O
= O O
α O O
( O O
zn−1 O O
) O O
p O O
( O O
xn|zn O O
) O O
p O O
( O O
zn|zn−1 O O
) O O
β O O
( O O
zn O O
) O O
p O O
( O O
x O O
) O O
p O O
( O O
x O O
) O O
( O O
13.43 O O
) O O
p O O
( O O
x O O
) O O
where O O
we O O
have O O
made O O
use O O
of O O
the O O
conditional B B
independence I I
property O O
( O O
13.29 O O
) O O
together O O
with O O
the O O
deﬁnitions O O
of O O
α O O
( O O
zn O O
) O O
and O O
β O O
( O O
zn O O
) O O
given O O
by O O
( O O
13.34 O O
) O O
and O O
( O O
13.35 O O
) O O
. O O
maximum B O
likelihood I I
estimates O O
of O O
linear O O
dynam- O O
ical O O
systems O O
. O O
in O O
the O O
homogeneous B B
phase O O
conﬁguration O O
, O O
the O O
path O O
lengths O O
in O O
oil O O
and O O
water O O
are O O
linearly O B
related O O
to O O
the O O
fractions O O
of O O
oil O O
and O O
water O O
, O O
and O O
so O O
the O O
data O O
points O O
lie O O
close O O
to O O
a O O
linear O O
manifold O O
. O O
( O O
c.20 O O
) O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
the O O
derivative B B
of O O
the O O
inverse B B
of O O
a O O
matrix O O
can O O
be O O
expressed O O
as O O
a−1 O O
∂ O O
∂x O O
= O O
−a−1 O O
∂a O O
∂x O O
a−1 O O
( O O
c.21 O O
) O O
as O O
can O O
be O O
shown O O
by O O
differentiating O O
the O O
equation O O
a−1a O O
= O O
i O O
using O O
( O O
c.20 O O
) O O
and O O
then O O
right O O
multiplying O O
by O O
a−1 O O
. O O
we O O
can O O
view O O
the O O
histogram O O
as O O
a O O
simple O O
way O O
to O O
model O O
a O O
probability B B
distribution O O
given O O
only O O
a O O
ﬁnite O O
number O O
of O O
points O O
drawn O O
from O O
that O O
distribution O O
. O O
for O O
a O O
gaussian O O
, O O
the O O
mode O O
coincides O O
with O O
the O O
mean B B
. O O
the O O
q O O
distribution O O
is O O
set O O
equal O O
to O O
the O O
posterior O O
distribution O O
for O O
the O O
current O O
parameter O O
val- O O
ues O O
θold O O
, O O
causing O O
the O O
lower B B
bound I I
to O O
move O O
up O O
to O O
the O O
same O O
value O O
as O O
the O O
log O O
like- O O
lihood O O
function O O
, O O
with O O
the O O
kl O O
divergence O O
vanishing O O
. O O
to O O
start O O
with O O
, O O
we O O
shall O O
discuss O O
the O O
regression B B
case O O
and O O
then O O
later O O
consider O O
the O O
modiﬁcations O O
needed O O
for O O
solving O O
classiﬁcation B B
tasks O O
. O O
note O O
that O O
each O O
decision B B
region I O
need O O
not O O
be O O
contiguous O O
but O O
could O O
comprise O O
some O O
number O O
of O O
disjoint O O
regions O O
. O O
in O O
the O O
m O O
step O O
, O O
the O O
required O O
sufﬁcient B O
statistics I I
can O O
be O O
updated O O
incrementally O O
. O O
thus O O
the O O
decision O B
rule O I
that O O
minimizes O O
the O O
expected O O
loss O O
is O O
the O O
one O O
that O O
assigns O O
each O O
42 O O
1. O O
introduction O O
figure O O
1.26 O O
illustration O O
of O O
the O O
reject B B
option I I
. O O
from O O
the O O
product B O
rule I I
of I I
probability I I
, O O
we O O
see O O
that O O
this O O
conditional B B
distribution O O
can O O
be O O
x O O
= O O
xa O O
xb O O
( O O
cid:15 O O
) O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
( O O
cid:16 O O
) O O
. O O
in O O
the O O
variational B B
equivalent O O
of O O
the O O
e O O
step O O
, O O
we O O
use O O
the O O
current O O
distributions O O
over O O
the O O
model O O
parameters O O
to O O
evaluate O O
the O O
moments O O
in O O
( O O
10.64 O O
) O O
, O O
( O O
10.65 O O
) O O
, O O
and O O
( O O
10.66 O O
) O O
and O O
hence O O
evaluate O O
e O O
[ O O
znk O O
] O O
= O O
rnk O O
. O O
( O O
3.75 O O
) O O
( O O
cid:6 O O
) O O
166 O O
3. O O
linear O O
models O O
for B O
regression I I
from O O
bayes O O
’ O O
theorem O O
, O O
the O O
posterior O O
distribution O O
for O O
α O O
and O O
β O O
is O O
given O O
by O O
if O O
the O O
prior B B
is O O
relatively O O
ﬂat O O
, O O
then O O
in O O
the O O
evidence O B
framework O O
the O O
values O O
of O O
( O O
cid:1 O O
) O O
α O O
and O O
( O O
cid:1 O O
) O O
β O O
are O O
obtained O O
by O O
maximizing O O
the O O
marginal B B
likelihood I I
function O O
p O O
( O O
t|α O O
, O O
β O O
) O O
. O O
xiv O O
contents O O
2 O O
probability B B
distributions O O
2.1 O O
binary O O
variables O O
. O O
11 O O
'' O O
, O O
,'1 O O
'' O O
'' O O
' O O
, O O
using O O
pea O O
we O O
can O O
make O O
a O O
it O O
> O O
of'e O O
subst.mial O O
nonnalizat O O
; O O
oo O O
of O O
the O O
data O O
to O O
gi\'c O O
it O O
zero O O
mean B B
and O O
unit O O
co'·ariance O O
. O O
685 O O
686 O O
b. O O
probability B B
distributions O O
beta O O
this O O
is O O
a O O
distribution O O
over O O
a O O
continuous O O
variable O O
µ O O
∈ O O
[ O O
0 O O
, O O
1 O O
] O O
, O O
which O O
is O O
often O O
used O O
to O O
represent O O
the O O
probability B B
for O O
some O O
binary O O
event O O
. O O
from O O
( O O
10.129 O O
) O O
we O O
see O O
that O O
the O O
maximizing O O
value O O
of O O
x O O
is O O
given O O
by O O
ξ O O
= O O
− O O
ln O O
( O O
−λ O O
) O O
, O O
and O O
back-substituting O O
we O O
obtain O O
the O O
conjugate B B
function O O
g O O
( O O
λ O O
) O O
in O O
the O O
form O O
g O O
( O O
λ O O
) O O
= O O
λ O O
− O O
λ O O
ln O O
( O O
−λ O O
) O O
( O O
10.131 O O
) O O
as O O
obtained O O
previously O O
. O O
( O O
b O O
) O O
the O O
result O O
of O O
converting O O
the O O
polytree B O
into O O
an O O
undirected B B
graph I I
showing O O
the O O
creation O O
of O O
loops O O
. O O
discuss O O
whether O O
such O O
singularities B O
would O O
arise O O
if O O
the O O
bayesian O O
model O O
were O O
solved O O
using O O
maximum B B
posterior I I
( O O
map O O
) O O
estimation O O
. O O
such O O
ﬁxed O O
basis B B
function I I
models O O
have O O
important O O
limitations O O
, O O
and O O
these O O
will O O
be O O
resolved O O
in O O
later O O
chapters O O
by O O
allowing O O
the O O
basis O B
functions O O
themselves O O
to O O
adapt O O
to O O
the O O
data O O
. O O
when O O
we O O
multiply O O
this O O
likelihood B B
function I I
by O O
the O O
posterior O O
distribution O O
from O O
the O O
second O O
row O O
, O O
we O O
obtain O O
the O O
posterior O O
distribution O O
shown O O
in O O
the O O
middle O O
plot O O
of O O
the O O
third O O
row O O
. O O
( O O
13.1 O O
) O O
n=1 O O
if O O
we O O
now O O
assume O O
that O O
each O O
of O O
the O O
conditional B B
distributions O O
on O O
the O O
right-hand O O
side O O
is O O
independent B B
of O O
all O O
previous O O
observations O O
except O O
the O O
most O O
recent O O
, O O
we O O
obtain O O
the O O
ﬁrst-order O B
markov O O
chain O O
, O O
which O O
is O O
depicted O O
as O O
a O O
graphical B B
model I I
in O O
figure O O
13.3. O O
the O O
608 O O
13. O O
sequential B B
data I I
figure O O
13.3 O O
a O O
ﬁrst-order O B
markov O O
chain O O
of O O
ob- O O
servations O O
{ O O
xn O O
} O O
in O O
which O O
the O O
dis- O O
tribution O O
p O O
( O O
xn|xn−1 O O
) O O
of O O
a O O
particu- O O
lar O O
observation O O
xn O O
is O O
conditioned O O
on O O
the O O
value O O
of O O
the O O
previous O O
ob- O O
servation O O
xn−1 O O
. O O
thus O O
the O O
number O O
of O O
terms O O
in O O
the O O
summation O O
grows O O
616 O O
13. O O
sequential B B
data I I
exponentially O O
with O O
the O O
length O O
of O O
the O O
chain O O
. O O
the O O
inverse B B
of O O
a O O
, O O
denoted O O
a−1 O O
, O O
satisﬁes O O
( O O
ab O O
) O O
t O O
= O O
btat O O
aa−1 O O
= O O
a−1a O O
= O O
i. O O
because O O
abb−1a−1 O O
= O O
i O O
, O O
we O O
have O O
also O O
we O O
have O O
( O O
ab O O
) O O
−1 O O
= O O
b−1a−1 O O
. O O
6.14 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
write O O
down O O
the O O
form O O
of O O
the O O
fisher O O
kernel O O
, O O
deﬁned O O
by O O
( O O
6.33 O O
) O O
, O O
for O O
the O O
case O O
of O O
a O O
distribution O O
p O O
( O O
x|µ O O
) O O
= O O
n O O
( O O
x|µ O O
, O O
s O O
) O O
that O O
is O O
gaussian O O
with O O
mean B B
µ O O
and O O
ﬁxed O O
covariance B B
s. O O
6.15 O O
( O O
( O O
cid:12 O O
) O O
) O O
by O O
considering O O
the O O
determinant O O
of O O
a O O
2 O O
× O O
2 O O
gram O O
matrix O O
, O O
show O O
that O O
a O O
positive- O O
deﬁnite O B
kernel B I
function I O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
satisﬁes O O
the O O
cauchy-schwartz O O
inequality O O
k O O
( O O
x1 O O
, O O
x2 O O
) O O
2 O O
( O O
cid:1 O O
) O O
k O O
( O O
x1 O O
, O O
x1 O O
) O O
k O O
( O O
x2 O O
, O O
x2 O O
) O O
. O O
however O O
, O O
for O O
factor O O
analysis O I
, O O
the O O
analogous O O
property O O
is O O
that O O
if O O
we O O
make O O
a O O
component-wise O O
re-scaling O O
of O O
the O O
data O O
vectors O O
, O O
then O O
this O O
is O O
absorbed O O
into O O
a O O
corresponding O O
re-scaling O O
of O O
the O O
elements O O
of O O
\ O O
) O O
i. O O
exercise O O
12.25 O O
12.3. O O
kernel O O
pea O O
in O O
chapter O O
6 O O
, O O
we O O
saw O O
how O O
the O O
technique O O
of O O
kernel B O
substitution I I
allows O O
us O O
to O O
take O O
an O O
algorithm O O
expressed O O
in O O
terms O O
of O O
scalar O O
products O O
of O O
the O O
form O O
x O O
t O O
x O O
' O O
and O O
generalize O O
that O O
algorithm O O
by O O
replacing O O
the O O
scalar O O
products O O
with O O
a O O
nonlinear O O
kernel O O
. O O
in O O
this O O
case O O
, O O
we O O
must O O
ensure O O
that O O
the O O
function O O
we O O
choose O O
is O O
a O O
valid O O
kernel O O
, O O
in O O
other O O
words O O
that O O
it O O
corresponds O O
to O O
a O O
scalar O O
product O O
in O O
some O O
( O O
perhaps O O
inﬁnite O O
dimensional O O
) O O
feature B O
space I I
. O O
( O O
5.189 O O
) O O
( O O
cid:6 O O
) O O
284 O O
5. O O
neural O O
networks O O
3 O O
2 O O
1 O O
0 O O
−1 O O
−2 O O
3 O O
2 O O
1 O O
0 O O
−1 O O
−2 O O
−2 O O
−1 O O
0 O O
1 O O
2 O O
−2 O O
−1 O O
0 O O
1 O O
2 O O
figure O O
5.23 O O
an O O
illustration O O
of O O
the O O
laplace O O
approximation O O
for O O
a O O
bayesian O O
neural B B
network I I
having O O
8 O O
hidden O O
units O O
with O O
‘ O O
tanh O O
’ O O
activation O B
functions O O
and O O
a O O
single O O
logistic-sigmoid O O
output O O
unit O O
. O O
as O O
in O O
the O O
case O O
of O O
rejection B B
sampling I I
, O O
importance B O
sampling I I
is O O
based O O
on O O
the O O
use O O
of O O
a O O
proposal B B
distribution I I
q O O
( O O
z O O
) O O
from O O
which O O
it O O
is O O
easy O O
to O O
draw O O
samples O O
, O O
as O O
illustrated O O
in O O
figure O O
11.8. O O
we O O
can O O
then O O
express O O
the O O
expectation B B
in O O
the O O
form O O
of O O
a O O
ﬁnite O O
sum O O
over O O
11.1. O O
basic O O
sampling O O
algorithms O O
533 O O
samples O O
{ O O
z O O
( O O
l O O
) O O
} O O
drawn O O
from O O
q O O
( O O
z O O
) O O
e O O
[ O O
f O O
] O O
= O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
= O O
( O O
cid:7 O O
) O O
1 O O
l O O
f O O
( O O
z O O
) O O
p O O
( O O
z O O
) O O
dz O O
f O O
( O O
z O O
) O O
p O O
( O O
z O O
) O O
l O O
( O O
cid:2 O O
) O O
l=1 O O
q O O
( O O
z O O
) O O
q O O
( O O
z O O
) O O
dz O O
p O O
( O O
z O O
( O O
l O O
) O O
) O O
q O O
( O O
z O O
( O O
l O O
) O O
) O O
f O O
( O O
z O O
( O O
l O O
) O O
) O O
. O O
2.57 O O
( O O
( O O
cid:12 O O
) O O
) O O
verify O O
that O O
the O O
multivariate O O
gaussian O O
distribution O O
can O O
be O O
cast O O
in O O
exponential B O
family I I
form O O
( O O
2.194 O O
) O O
and O O
derive O O
expressions O O
for O O
η O O
, O O
u O O
( O O
x O O
) O O
, O O
h O O
( O O
x O O
) O O
and O O
g O O
( O O
η O O
) O O
analogous O O
to O O
( O O
2.220 O O
) O O
– O O
( O O
2.223 O O
) O O
. O O
another O O
problem O O
with O O
the O O
one-versus-the-rest O O
approach O O
is O O
that O O
the O O
training B B
sets O O
are O O
imbalanced O O
. O O
for O O
applications O O
such O O
as O O
visualization B O
and O O
data B O
compression I I
, O O
we O O
can O O
reverse O O
this O O
mapping O O
using O O
bayes O O
' O O
theorem O O
. O O
a O O
modiﬁed O O
method O O
of O O
esti- O O
mation B I
in O O
factor B O
analysis I I
and O O
some O O
large O O
sam- O O
ple O O
results O O
. O O
c O O
in O O
general O O
, O O
this O O
does O O
not O O
factorize O O
into O O
the O O
product O O
p O O
( O O
a O O
) O O
p O O
( O O
b O O
) O O
, O O
and O O
so O O
a O O
( O O
cid:9 O O
) O O
⊥⊥ O O
b O O
| O O
∅ O O
( O O
8.24 O O
) O O
( O O
8.25 O O
) O O
374 O O
8. O O
graphical O O
models O O
figure O O
8.16 O O
as O O
in O O
figure O O
8.15 O O
but O O
where O O
we O O
have O O
conditioned O O
on O O
the O O
value O O
of O O
variable O O
c. O O
c O O
a O O
b O O
where O O
∅ O O
denotes O O
the O O
empty O O
set O O
, O O
and O O
the O O
symbol O O
( O O
cid:9 O O
) O O
⊥⊥ O O
means O O
that O O
the O O
conditional B B
inde- O O
pendence O O
property O O
does O O
not O O
hold O O
in O O
general O O
. O O
also O O
, O O
there O O
is O O
something O O
rather O O
unsatisfying O O
about O O
having O O
to O O
limit O O
the O O
number O O
of O O
parameters O O
in O O
a O O
model O O
according O O
to O O
the O O
size O O
of O O
the O O
available O O
training B B
set I I
. O O
1.2.4 O O
the O O
gaussian O O
distribution O O
we O O
shall O O
devote O O
the O O
whole O O
of O O
chapter O O
2 O O
to O O
a O O
study O O
of O O
various O O
probability B O
dis- O O
tributions O O
and O O
their O O
key O O
properties O O
. O O
to O O
illustrate O O
this O O
, O O
consider O O
again O O
the O O
bayesian O O
mixture O B
of O O
gaussians O O
represented O O
by O O
the O O
directed B B
graph O O
in O O
figure O O
10.5 O O
, O O
in O O
which O O
we O O
are O O
assuming O O
a O O
variational B B
fac- O O
torization O O
given O O
by O O
( O O
10.42 O O
) O O
. O O
13.2.1 O O
maximum B B
likelihood I I
for O O
the O O
hmm O O
if O O
we O O
have O O
observed O O
a O O
data O O
set O O
x O O
= O O
{ O O
x1 O O
, O O
. O O
this O O
is O O
illustrated O O
schematically O O
in O O
figure O O
1.16. O O
figure O O
1.16 O O
schematic O O
illustration O O
of O O
a O O
gaus- O O
sian O O
conditional B B
distribution O O
for O O
t O O
given O O
x O O
given O O
by O O
( O O
1.60 O O
) O O
, O O
in O O
which O O
the O O
mean B B
is O O
given O O
by O O
the O O
polyno- O O
mial O O
function O O
y O O
( O O
x O O
, O O
w O O
) O O
, O O
and O O
the O O
precision O O
is O O
given O O
by O O
the O O
parameter O O
β O O
, O O
which O O
is O O
related O O
to O O
the O O
vari- O O
ance O O
by O O
β−1 O O
= O O
σ2 O O
. O O
we O O
therefore O O
see O O
that O O
the O O
constant O O
k O O
should O O
be O O
as O O
small O O
as O O
possible O O
subject O O
to O O
the O O
thus O O
the O O
fraction O O
of O O
points O O
that O O
are O O
rejected O O
by O O
this O O
method O O
depends O O
on O O
the O O
ratio O O
of O O
( O O
11.14 O O
) O O
1 O O
k O O
( O O
cid:6 O O
) O O
= O O
as O O
an O O
illustration O O
of O O
the O O
use O O
of O O
rejection B B
sampling I I
, O O
consider O O
the O O
task O O
of O O
sampling O O
from O O
the O O
gamma B B
distribution I I
gam O O
( O O
z|a O O
, O O
b O O
) O O
= O O
baza−1 O O
exp O O
( O O
−bz O O
) O O
γ O O
( O O
a O O
) O O
( O O
11.15 O O
) O O
which O O
, O O
for O O
a O O
> O O
1 O O
, O O
has O O
a O O
bell-shaped O O
form O O
, O O
as O O
shown O O
in O O
figure O O
11.5. O O
a O O
suitable O O
proposal B O
distribution I I
is O O
therefore O O
the O O
cauchy O O
( O O
11.8 O O
) O O
because O O
this O O
too O O
is O O
bell-shaped O O
and O O
because O O
we O O
can O O
use O O
the O O
transformation O O
method O O
, O O
discussed O O
earlier O O
, O O
to O O
sample O O
from O O
it O O
. O O
from O O
section O O
4.5.2 O O
, O O
we O O
see O O
that O O
this O O
distribution O O
is O O
gaussian O O
with O O
mean B B
amap O O
≡ O O
a O O
( O O
x O O
, O O
wmap O O
) O O
, O O
and O O
variance B B
( O O
cid:11 O O
) O O
a O O
( O O
x O O
) O O
= O O
bt O O
( O O
x O O
) O O
a−1b O O
( O O
x O O
) O O
. O O
we O O
shall O O
suppose O O
that O O
the O O
conditional B B
distribution O O
p O O
( O O
t|x O O
) O O
is O O
gaussian O O
, O O
with O O
an O O
x-dependent O O
mean B B
given O O
by O O
the O O
output O O
of O O
a O O
neural B B
network I I
model O O
y O O
( O O
x O O
, O O
w O O
) O O
, O O
and O O
with O O
precision O O
( O O
inverse B B
variance O O
) O O
β O O
p O O
( O O
t|x O O
, O O
w O O
, O O
β O O
) O O
= O O
n O O
( O O
t|y O O
( O O
x O O
, O O
w O O
) O O
, O O
β O O
−1 O O
) O O
. O O
there O O
is O O
no O O
need O O
to O O
include O O
a O O
noise O O
distribution O O
because O O
the O O
number O O
of O O
latent O B
variables O O
equals O O
the O O
number O O
of O O
ob O O
( O O
cid:173 O O
) O O
served O O
variables O O
, O O
and O O
therefore O O
the O O
marginal B B
distribution O O
of O O
the O O
observed O O
variables O O
will O O
not O O
in O O
general O O
be O O
singular O O
, O O
so O O
the O O
observed O O
variables O O
are O O
simply O O
deterministic O O
linear O O
combinations O O
of O O
the O O
latent O B
variables O O
. O O
this O O
is O O
an O O
example O O
of O O
ancestral B O
sampling I O
for O O
a O O
directed B B
graphical O O
model O O
. O O
for O O
later O O
comparison O O
with O O
alternative O O
models O O
, O O
we O O
can O O
express O O
the O O
maximum- O O
margin B I
classiﬁer O O
in O O
terms O O
of O O
the O O
minimization O O
of O O
an O O
error B B
function I I
, O O
with O O
a O O
simple O O
quadratic O O
regularizer O O
, O O
in O O
the O O
form O O
e∞ O O
( O O
y O O
( O O
xn O O
) O O
tn O O
− O O
1 O O
) O O
+ O O
λ O O
( O O
cid:5 O O
) O O
w O O
( O O
cid:5 O O
) O O
2 O O
( O O
7.19 O O
) O O
n=1 O O
where O O
e∞ O O
( O O
z O O
) O O
is O O
a O O
function O O
that O O
is O O
zero O O
if O O
z O O
( O O
cid:2 O O
) O O
0 O O
and O O
∞ O O
otherwise O O
and O O
ensures O O
that O O
the O O
constraints O O
( O O
7.5 O O
) O O
are O O
satisﬁed O O
. O O
( O O
11.37 O O
) O O
this O O
of O O
course O O
can O O
be O O
represented O O
as O O
a O O
directed B B
graph O O
in O O
the O O
form O O
of O O
a O O
chain O O
, O O
an O O
ex- O O
ample O O
of O O
which O O
is O O
shown O O
in O O
figure O O
8.38. O O
we O O
can O O
then O O
specify O O
the O O
markov O O
chain O O
by O O
giving O O
the O O
probability B B
distribution O O
for O O
the O O
initial O O
variable O O
p O O
( O O
z O O
( O O
0 O O
) O O
) O O
together O O
with O O
the O O
540 O O
11. O O
sampling B O
methods I I
( O O
cid:2 O O
) O O
conditional B B
probabilities O O
for O O
subsequent O O
variables O O
in O O
the O O
form O O
of O O
transition O O
probabil- O O
ities O O
tm O O
( O O
z O O
( O O
m O O
) O O
, O O
z O O
( O O
m+1 O O
) O O
) O O
≡ O O
p O O
( O O
z O O
( O O
m+1 O O
) O O
|z O O
( O O
m O O
) O O
) O O
. O O
1 O O
− O O
x2 O O
x2 O O
( O O
x O O
( O O
cid:3 O O
) O O
1 O O
, O O
x O O
( O O
cid:3 O O
) O O
2 O O
) O O
x1 O O
g O O
( O O
x1 O O
, O O
x2 O O
) O O
= O O
0 O O
solution O O
of O O
these O O
equations O O
then O O
gives O O
the O O
stationary B B
point O O
as O O
( O O
x O O
( O O
cid:3 O O
) O O
the O O
corresponding O O
value O O
for O O
the O O
lagrange O O
multiplier O O
is O O
λ O O
= O O
1 O O
. O O
as O O
we O O
shall O O
see O O
later O O
, O O
it O O
is O O
closely O O
related O O
to O O
factor B O
analysis I I
( O O
basilevsky O O
, O O
1994 O O
) O O
. O O
one O O
limitation O O
of O O
the O O
parametric O O
approach O O
is O O
that O O
it O O
assumes O O
a O O
speciﬁc O O
functional B B
form O O
for O O
the O O
distribution O O
, O O
which O O
may O O
turn O O
out O O
to O O
be O O
inappropriate O O
for O O
a O O
particular O O
application O O
. O O
if O O
the O O
error B B
function I I
comprises O O
a O O
sum O O
over O O
data O O
points O O
e O O
= O O
n O O
en O O
, O O
then O O
after O O
presen- O O
tation O O
of O O
pattern O O
n O O
, O O
the O O
stochastic B B
gradient I I
descent I I
algorithm O O
updates O O
the O O
parameter O O
vector O O
w O O
using O O
( O O
cid:5 O O
) O O
( O O
3.22 O O
) O O
where O O
τ O O
denotes O O
the O O
iteration O O
number O O
, O O
and O O
η O O
is O O
a O O
learning B B
rate I O
parameter I O
. O O
so O O
far O O
, O O
we O O
have O O
discussed O O
the O O
notion O O
of O O
conditional B B
independence I I
based O O
on O O
sim- O O
ple O O
graph O O
separation O O
and O O
we O O
have O O
proposed O O
a O O
factorization B B
of O O
the O O
joint O O
distribution O O
that O O
is O O
intended O O
to O O
correspond O O
to O O
this O O
conditional B B
independence I I
structure O O
. O O
consider O O
first O O
a O O
multilayer B B
perceptron I I
of O O
the O O
form O O
shown O O
in O O
figure O O
12.18 O O
, O O
hav O O
( O O
cid:173 O O
) O O
ing O O
d O O
inputs O O
, O O
d O O
output O O
units O O
and O O
m O O
hidden O O
units O O
, O O
with O O
m O O
< O O
d. O O
the O O
targets O O
used O O
to O O
train O O
the O O
network O O
are O O
simply O O
the O O
input O O
vectors O O
themselves O O
, O O
so O O
that O O
the O O
network O O
is O O
attempting O O
to O O
map O O
each O O
input O O
vector O O
onto O O
itself O O
. O O
5.2.4 O O
gradient B O
descent I O
optimization O O
. O O
as O O
discussed O O
in O O
section O O
3.1 O O
, O O
the O O
bias B B
parameters O O
in O O
( O O
5.2 O O
) O O
can O O
be O O
absorbed O O
into O O
the O O
set O O
of O O
weight O B
parameters O O
by O O
deﬁning O O
an O O
additional O O
input O O
variable O O
x0 O O
whose O O
value O O
is O O
clamped O O
at O O
x0 O O
= O O
1 O O
, O O
so O O
that O O
( O O
5.2 O O
) O O
takes O O
the O O
form O O
aj O O
= O O
( O O
1 O O
) O O
ji O O
xi O O
. O O
we O O
shall O O
see O O
that O O
this O O
can O O
be O O
achieved O O
using O O
a O O
local B B
message O O
passing O O
scheme O O
in O O
which O O
information O O
is O O
sent O O
alternately O O
forwards O O
and O O
backwards O O
through O O
the O O
network O O
and O O
is O O
known O O
as O O
error B B
backpropagation I I
, O O
or O O
sometimes O O
simply O O
as O O
backprop O O
. O O
( O O
13.67 O O
) O O
µzn→fn+1 O O
( O O
zn O O
) O O
= O O
µfn→zn O O
( O O
zn O O
) O O
µfn+1→zn+1 O O
( O O
zn+1 O O
) O O
= O O
max O O
zn O O
ln O O
fn+1 O O
( O O
zn O O
, O O
zn+1 O O
) O O
+ O O
µzn→fn+1 O O
( O O
zn O O
) O O
( O O
cid:26 O O
) O O
630 O O
13. O O
sequential B B
data I I
figure O O
13.16 O O
a O O
fragment O O
of O O
the O O
hmm O O
lattice O O
showing O O
two O O
possible O O
paths O O
. O O
thus O O
the O O
variance B B
ai O O
in O O
the O O
direction O O
of O O
an O O
eigenvector O O
ui O O
is O O
composed O O
of O O
the O O
sum O O
of O O
a O O
contribution O O
ai O O
( O O
cid:173 O O
) O O
( O O
j'2 O O
from O O
the O O
projection O O
of O O
the O O
unit-variance O O
latent O B
space O O
distribution O O
into O O
data O O
space O O
through O O
the O O
corresponding O O
column O O
of O O
w O O
, O O
plus O O
an O O
isotropic B B
contribution O O
of O O
variance B B
( O O
j'2 O O
which O O
is O O
added O O
in O O
all O O
directions O O
by O O
the O O
noise O O
model O O
. O O
, O O
xn O O
, O O
and O O
so O O
we O O
can O O
simplify O O
the O O
factor B B
graph I I
by O O
absorbing O O
the O O
emission O O
probabilities O O
into O O
the O O
transition B O
probability I I
factors O O
. O O
we O O
see O O
that O O
the O O
relevance O B
vectors O O
tend O O
not O O
to O O
lie O O
in O O
the O O
region O O
of O O
the O O
decision B B
boundary I I
, O O
in O O
contrast O O
to O O
the O O
support B B
vector I I
machine I I
. O O
for O O
independent O B
, O O
identically O O
distributed O O
data O O
, O O
we O O
can O O
use O O
( O O
14.3 O O
) O O
to O O
write O O
the O O
marginal B B
probability I I
of O O
a O O
data O O
set O O
x O O
= O O
{ O O
x1 O O
, O O
. O O
, O O
|t| O O
, O O
with O O
leaf O O
node B B
τ O O
representing O O
a O O
region O O
rτ O O
of O O
input O O
space O O
having O O
nτ O O
data O O
points O O
, O O
and O O
|t| O O
denoting O O
the O O
total O O
number O O
of O O
leaf O O
nodes O O
. O O
furthermore O O
, O O
the O O
existence O O
of O O
these O O
symmetries B O
is O O
not O O
a O O
particular O O
property O O
of O O
the O O
‘ O O
tanh O O
’ O O
function O O
but O O
applies O O
to O O
a O O
wide O O
range O O
of O O
activation O B
functions O O
( O O
k˙urkov´a O O
and O O
kainen O O
, O O
1994 O O
) O O
. O O
10.7. O O
expectation B B
propagation I I
511 O O
figure O O
10.15 O O
illustration O O
of O O
the O O
clutter B B
problem I I
for O O
a O O
data O O
space O O
dimensionality O O
of O O
d O O
= O O
1. O O
training B B
data O O
points O O
, O O
de- O O
noted O O
by O O
the O O
crosses O O
, O O
are O O
drawn O O
from O O
a O O
mixture O B
of O O
two O O
gaussians O O
with O O
components O O
shown O O
in O O
red O O
and O O
green O O
. O O
5.7 O O
( O O
( O O
cid:12 O O
) O O
) O O
show O O
the O O
derivative B B
of O O
the O O
error B B
function I I
( O O
5.24 O O
) O O
with O O
respect O O
to O O
the O O
activation O B
ak O O
for O O
output O O
units O O
having O O
a O O
softmax O O
activation O O
function O I
satisﬁes O O
( O O
5.18 O O
) O O
. O O
11.2 O O
( O O
( O O
cid:12 O O
) O O
) O O
suppose O O
that O O
z O O
is O O
a O O
random O O
variable O O
with O O
uniform B O
distribution I O
over O O
( O O
0 O O
, O O
1 O O
) O O
and O O
−1 O O
( O O
z O O
) O O
where O O
h O O
( O O
y O O
) O O
is O O
given O O
by O O
( O O
11.6 O O
) O O
. O O
2.55 O O
( O O
( O O
cid:12 O O
) O O
) O O
by O O
making O O
use O O
of O O
the O O
result O O
( O O
2.168 O O
) O O
, O O
together O O
with O O
( O O
2.184 O O
) O O
and O O
the O O
trigonometric O O
identity O O
( O O
2.178 O O
) O O
, O O
show O O
that O O
the O O
maximum B B
likelihood I I
solution O O
mml O O
for O O
the O O
concentra- O O
tion O O
of O O
the O O
von O O
mises O O
distribution O O
satisﬁes O O
a O O
( O O
mml O O
) O O
= O O
r O O
where O O
r O O
is O O
the O O
radius O O
of O O
the O O
mean B B
of O O
the O O
observations O O
viewed O O
as O O
unit O O
vectors O O
in O O
the O O
two-dimensional O O
euclidean O O
plane O O
, O O
as O O
illustrated O O
in O O
figure O O
2.17 O O
. O O
in O O
order O O
to O O
ﬁnd O O
the O O
maximum B B
likelihood I I
solution O O
for O O
µ O O
, O O
we O O
need O O
to O O
maximize O O
ln O O
p O O
( O O
d|µ O O
) O O
with O O
respect O O
to O O
µk O O
taking O O
account O O
of O O
the O O
constraint O O
that O O
the O O
µk O O
must O O
sum O O
to O O
one O O
. O O
( O O
c O O
) O O
a O O
different O O
factor B O
graph I I
representing O O
the O O
same O O
distribution O O
, O O
whose O O
factors O O
satisfy O O
fa O O
( O O
x1 O O
, O O
x2 O O
, O O
x3 O O
) O O
fb O O
( O O
x1 O O
, O O
x2 O O
) O O
= O O
ψ O O
( O O
x1 O O
, O O
x2 O O
, O O
x3 O O
) O O
. O O
exercise O O
11.7 O O
530 O O
11. O O
sampling B O
methods I I
figure O O
11.5 O O
plot O O
showing O O
the O O
gamma O B
distribu- O O
tion O O
given O O
by O O
( O O
11.15 O O
) O O
as O O
the O O
green O O
curve O O
, O O
with O O
a O O
scaled O O
cauchy O O
pro- O O
posal O O
distribution O O
shown O O
by O O
the O O
red O O
curve O O
. O O
for O O
instance O O
, O O
if O O
the O O
conditional B B
distributions O O
depend O O
on O O
adjustable O O
parameters O O
( O O
whose O O
values O O
might O O
be O O
inferred O O
from O O
a O O
set O O
of O O
training B B
data O O
) O O
, O O
then O O
all O O
of O O
the O O
condi- O O
tional O O
distributions O O
in O O
the O O
chain O O
will O O
share O O
the O O
same O O
values O O
of O O
those O O
parameters O O
. O O
3.10 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
by O O
making O O
use O O
of O O
the O O
result O O
( O O
2.115 O O
) O O
to O O
evaluate O O
the O O
integral O O
in O O
( O O
3.57 O O
) O O
, O O
verify O O
that O O
the O O
predictive B B
distribution I I
for O O
the O O
bayesian O O
linear B B
regression I I
model O O
is O O
given O O
by O O
( O O
3.58 O O
) O O
in O O
which O O
the O O
input-dependent O O
variance B B
is O O
given O O
by O O
( O O
3.59 O O
) O O
. O O
mixture B O
models O O
: O O
inference B B
and O O
applications O O
to O O
cluster- O O
ing O O
. O O
substituting O O
into O O
the O O
loss B B
function I I
and O O
performing O O
the O O
integral O O
over O O
t O O
, O O
we O O
see O O
that O O
the O O
cross-term O O
vanishes O O
and O O
we O O
obtain O O
an O O
expression O O
for O O
the O O
loss B B
function I I
in O O
the O O
form O O
{ O O
e O O
[ O O
t|x O O
] O O
− O O
t O O
} O O
2p O O
( O O
x O O
) O O
dx O O
. O O
4.23 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
in O O
this O O
exercise O O
, O O
we O O
derive O O
the O O
bic O O
result O O
( O O
4.139 O O
) O O
starting O O
from O O
the O O
laplace O O
approximation O O
to O O
the O O
model B O
evidence I I
given O O
by O O
( O O
4.137 O O
) O O
. O O
maximizing O O
the O O
margin B B
leads O O
to O O
a O O
particular O O
choice O O
of O O
decision B B
boundary I I
, O O
as O O
shown O O
on O O
the O O
right O O
. O O
7.19 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
verify O O
that O O
maximization O O
of O O
the O O
approximate O O
log O O
marginal O O
likelihood B O
function I O
( O O
7.114 O O
) O O
for O O
the O O
classiﬁcation B B
relevance O O
vector O O
machine O I
leads O O
to O O
the O O
result O O
( O O
7.116 O O
) O O
for O O
re-estimation O O
of O O
the O O
hyperparameters O O
. O O
658 O O
14. O O
combining B B
models I O
figure O O
14.1 O O
schematic O O
illustration O O
of O O
the O O
boosting B B
framework O O
. O O
in O O
practice O O
, O O
however O O
, O O
we O O
do O O
not O O
have O O
values O O
for O O
the O O
latent O O
variables O O
so O O
, O O
as O O
discussed O O
earlier O O
, O O
we O O
consider O O
the O O
expectation B B
, O O
with O O
respect O O
to O O
the O O
posterior O O
distribution O O
of O O
the O O
latent O O
variables O O
, O O
of O O
the O O
complete-data O O
log O O
likelihood O O
. O O
such O O
problems O O
can O O
be O O
solved O O
straightforwardly O O
by O O
noting O O
that O O
the O O
exponent O O
in O O
a O O
general O O
gaussian O O
distribution O O
n O O
( O O
x|µ O O
, O O
σ O O
) O O
can O O
be O O
written O O
−1 O O
2 O O
( O O
x O O
− O O
µ O O
) O O
tς O O
−1 O O
( O O
x O O
− O O
µ O O
) O O
= O O
−1 O O
2 O O
xtς O O
−1x O O
+ O O
xtς O O
−1µ O O
+ O O
const O O
( O O
2.71 O O
) O O
−1 O O
and O O
the O O
coefﬁcient O O
of O O
the O O
linear O B
term O O
in O O
x O O
to O O
σ O O
where O O
‘ O O
const O O
’ O O
denotes O O
terms O O
which O O
are O O
independent B B
of O O
x O O
, O O
and O O
we O O
have O O
made O O
use O O
of O O
the O O
symmetry O O
of O O
σ. O O
thus O O
if O O
we O O
take O O
our O O
general O O
quadratic O O
form O O
and O O
express O O
it O O
in O O
the O O
form O O
given O O
by O O
the O O
right-hand O O
side O O
of O O
( O O
2.71 O O
) O O
, O O
then O O
we O O
can O O
immediately O O
equate O O
the O O
matrix O O
of O O
coefﬁcients O O
entering O O
the O O
second B O
order I I
term O O
in O O
x O O
to O O
the O O
inverse B B
covariance O O
−1µ O O
, O O
from O O
which O O
we O O
can O O
matrix O O
σ O O
obtain O O
µ. O O
now O O
let O O
us O O
apply O O
this O O
procedure O O
to O O
the O O
conditional B B
gaussian O O
distribution O O
p O O
( O O
xa|xb O O
) O O
for O O
which O O
the O O
quadratic O O
form O O
in O O
the O O
exponent O O
is O O
given O O
by O O
( O O
2.70 O O
) O O
. O O
however O O
, O O
the O O
maximum B B
likelihood I I
estimate O O
for O O
the O O
covariance B B
has O O
an O O
expectation B B
that O O
is O O
less O O
than O O
the O O
true O O
value O O
, O O
and O O
hence O O
it O O
is O O
biased O O
. O O
10.2. O O
illustration O O
: O O
variational B B
mixture O O
of O O
gaussians O O
we O O
now O O
return O O
to O O
our O O
discussion O O
of O O
the O O
gaussian O O
mixture B B
model I I
and O O
apply O O
the O O
vari- O O
ational O O
inference B B
machinery O O
developed O O
in O O
the O O
previous O O
section O O
. O O
the O O
mean B B
of O O
the O O
projected O O
data O O
is O O
ufx O O
where O O
x O O
is O O
the O O
sample O B
set O O
mean B B
given O O
by O O
and O O
the O O
variance B B
of O O
the O O
projected O O
data O O
is O O
given O O
by O O
( O O
12.1 O O
) O O
( O O
12.2 O O
) O O
where O O
s O O
is O O
the O O
data O O
covariance O O
matrix O I
defined O O
by O O
s O O
= O O
- O O
`` O O
( O O
xn O O
- O O
x O O
) O O
( O O
xn O O
- O O
x O O
) O O
t O O
1 O O
n O O
nlj O O
n=l O O
. O O
more O O
generally O O
, O O
we O O
could O O
model O O
the O O
joint O O
distribution O O
p O O
( O O
t O O
, O O
x O O
) O O
using O O
a O O
gaussian O O
mixture B B
model I I
, O O
trained O O
using O O
techniques O O
discussed O O
in O O
chapter O O
9 O O
( O O
ghahra- O O
mani O O
and O O
jordan O O
, O O
1994 O O
) O O
, O O
and O O
then O O
ﬁnd O O
the O O
corresponding O O
conditional B B
distribution O O
p O O
( O O
t|x O O
) O O
. O O
smooth O O
regression B B
analysis O O
. O O
if O O
we O O
make O O
the O O
bias B B
parameter I I
explicit O O
, O O
then O O
the O O
error B B
function I I
( O O
3.12 O O
) O O
becomes O O
( O O
3.16 O O
) O O
n O O
( O O
cid:2 O O
) O O
{ O O
tn O O
− O O
w0 O O
− O O
m−1 O O
( O O
cid:2 O O
) O O
w0 O O
= O O
t O O
− O O
m−1 O O
( O O
cid:2 O O
) O O
wjφj O O
n=1 O O
j=1 O O
ed O O
( O O
w O O
) O O
= O O
1 O O
2 O O
wjφj O O
( O O
xn O O
) O O
} O O
2 O O
. O O
the O O
issue O O
of O O
model O O
complexity O O
is O O
an O O
important O O
one O O
and O O
will O O
be O O
discussed O O
at O O
length O O
in O O
section O O
1.3. O O
here O O
we O O
simply O O
note O O
that O O
, O O
if O O
we O O
were O O
trying O O
to O O
solve O O
a O O
practical O O
application O O
using O O
this O O
approach O O
of O O
minimizing O O
an O O
error B B
function I I
, O O
we O O
would O O
have O O
to O O
ﬁnd O O
a O O
way O O
to O O
determine O O
a O O
suitable O O
value O O
for O O
the O O
model O O
complexity O O
. O O
for O O
many O O
models O O
, O O
the O O
joint O O
distribution O O
p O O
( O O
z O O
) O O
is O O
conveniently O O
speciﬁed O O
in O O
terms O O
of O O
a O O
graphical B B
model I I
. O O
2.10 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
using O O
the O O
property O O
γ O O
( O O
x O O
+ O O
1 O O
) O O
= O O
xγ O O
( O O
x O O
) O O
of O O
the O O
gamma B B
function I I
, O O
derive O O
the O O
following O O
results O O
for O O
the O O
mean B B
, O O
variance B B
, O O
and O O
covariance B B
of O O
the O O
dirichlet O O
distribution O O
given O O
by O O
( O O
2.38 O O
) O O
e O O
[ O O
µj O O
] O O
= O O
αj O O
α0 O O
var O O
[ O O
µj O O
] O O
= O O
αj O O
( O O
α0 O O
− O O
αj O O
) O O
0 O O
( O O
α0 O O
+ O O
1 O O
) O O
α2 O O
cov O O
[ O O
µjµl O O
] O O
= O O
− O O
αjαl O O
0 O O
( O O
α0 O O
+ O O
1 O O
) O O
, O O
α2 O O
j O O
( O O
cid:9 O O
) O O
= O O
l O O
( O O
2.273 O O
) O O
( O O
2.274 O O
) O O
( O O
2.275 O O
) O O
where O O
α0 O O
is O O
deﬁned O O
by O O
( O O
2.39 O O
) O O
. O O
boltzmann O O
showed O O
that O O
the O O
ther- O O
modynamic O O
entropy B O
s O O
, O O
a O O
macroscopic O O
quantity O O
, O O
could O O
be O O
related O O
to O O
the O O
statistical O O
properties O O
at O O
the O O
micro- O O
scopic O O
level O O
. O O
this O O
is O O
equivalent O O
to O O
the O O
statement O O
that O O
there O O
exists O O
an O O
ordering O O
of O O
the O O
nodes O O
such O O
that O O
there O O
are O O
no O O
links O O
that O O
go O O
from O O
any O O
node B B
to O O
any O O
lower O B
numbered O O
node B B
. O O
predictive B B
distribution I I
. O O
introduction O O
to O O
the O O
calculus B B
of I I
variations I I
. O O
now O O
suppose O O
that O O
, O O
for O O
each O O
observation O O
in O O
x O O
, O O
we O O
were O O
told O O
the O O
corresponding O O
value O O
of O O
the O O
latent B B
variable I I
z. O O
we O O
shall O O
call O O
{ O O
x O O
, O O
z O O
} O O
the O O
complete B O
data I O
set I O
, O O
and O O
we O O
shall O O
refer O O
to O O
the O O
actual O O
observed O O
data O O
x O O
as O O
incomplete O O
, O O
as O O
illustrated O O
in O O
figure O O
9.5. O O
the O O
likelihood B B
function I I
for O O
the O O
complete B O
data I O
set I O
simply O O
takes O O
the O O
form O O
ln O O
p O O
( O O
x O O
, O O
z|θ O O
) O O
, O O
and O O
we O O
shall O O
suppose O O
that O O
maximization O O
of O O
this O O
complete-data O O
log O O
likelihood O O
function O O
is O O
straightforward O O
. O O
the O O
derivative B B
of O O
output O O
k O O
with O O
respect O O
to O O
ξ O O
is O O
given O O
by O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
( O O
cid:7 O O
) O O
d O O
( O O
cid:2 O O
) O O
d O O
( O O
cid:2 O O
) O O
∂yk O O
∂ξ O O
= O O
ξ=0 O O
i=1 O O
∂yk O O
∂xi O O
∂xi O O
∂ξ O O
ξ=0 O O
i=1 O O
= O O
jkiτi O O
( O O
5.126 O O
) O O
where O O
jki O O
is O O
the O O
( O O
k O O
, O O
i O O
) O O
element O O
of O O
the O O
jacobian O O
matrix O O
j O O
, O O
as O O
discussed O O
in O O
section O O
5.3.4. O O
the O O
result O O
( O O
5.126 O O
) O O
can O O
be O O
used O O
to O O
modify O O
the O O
standard O O
error O B
function O I
, O O
so O O
as O O
to O O
encour- O O
age O O
local B B
invariance O O
in O O
the O O
neighbourhood O O
of O O
the O O
data O O
points O O
, O O
by O O
the O O
addition O O
to O O
the O O
original O O
error B B
function I I
e O O
of O O
a O O
regularization B B
function O I
ω O O
to O O
give O O
a O O
total O O
error B B
function I I
of O O
the O O
form O O
( O O
cid:4 O O
) O O
e O O
= O O
e O O
+ O O
λω O O
( O O
cid:23 O O
) O O
2 O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
( O O
5.127 O O
) O O
( O O
cid:22 O O
) O O
d O O
( O O
cid:2 O O
) O O
( O O
cid:23 O O
) O O
2 O O
jnkiτni O O
. O O
the O O
parameter O O
ν O O
is O O
called O O
the O O
number O O
of O O
degrees B O
of I I
freedom I I
of O O
the O O
distribution O O
and O O
is O O
restricted O O
to O O
ν O O
> O O
d O O
− O O
1 O O
to O O
ensure O O
that O O
the O O
gamma B B
function I I
in O O
the O O
normalization O O
factor O B
is O O
well-deﬁned O O
. O O
( O O
2.248 O O
) O O
substituting O O
this O O
expression O O
into O O
( O O
2.246 O O
) O O
then O O
gives O O
the O O
following O O
result O O
for O O
the O O
esti- O O
mated O O
density B B
at O O
x O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
h O O
( O O
cid:17 O O
) O O
( O O
cid:18 O O
) O O
p O O
( O O
x O O
) O O
= O O
1 O O
n O O
1 O O
hd O O
k O O
x O O
− O O
xn O O
h O O
( O O
2.249 O O
) O O
where O O
we O O
have O O
used O O
v O O
= O O
hd O O
for O O
the O O
volume O O
of O O
a O O
hypercube O O
of O O
side O O
h O O
in O O
d O O
di- O O
mensions O O
. O O
as O O
we O O
shall O O
see O O
, O O
it O O
is O O
a O O
very O O
general O O
property O O
that O O
the O O
bayesian O O
and O O
maximum B B
likelihood I I
results O O
will O O
agree O O
in O O
the O O
limit O O
of O O
an O O
inﬁnitely O O
74 O O
2. O O
probability B B
distributions O O
exercise O O
2.7 O O
exercise O O
2.8 O O
large O O
data O O
set O O
. O O
this O O
approach O O
will O O
only O O
yield O O
accurate O O
results O O
if O O
the O O
importance B B
sampling I I
distri- O O
bution O O
pg O O
is O O
closely O O
matched O O
to O O
the O O
distribution O O
pe O O
, O O
so O O
that O O
the O O
ratio O O
pe/pg O O
does O O
not O O
have O O
wide O O
variations O O
. O O
these O O
are O O
speciﬁc O O
examples O O
of O O
parametric O O
distributions O O
, O O
so-called O O
because O O
they O O
are O O
governed O O
by O O
a O O
small O O
number O O
of O O
adaptive O O
parameters O O
, O O
such O O
as O O
the O O
mean B B
and O O
variance B B
in O O
the O O
case O O
of O O
a O O
gaussian O O
for O O
example O O
. O O
( O O
9.14 O O
) O O
n=1 O O
k=1 O O
before O O
discussing O O
how O O
to O O
maximize O O
this O O
function O O
, O O
it O O
is O O
worth O O
emphasizing O O
that O O
there O O
is O O
a O O
signiﬁcant O O
problem O O
associated O O
with O O
the O O
maximum B B
likelihood I I
framework O O
applied O O
to O O
gaussian O O
mixture B B
models O O
, O O
due O O
to O O
the O O
presence O O
of O O
singularities B O
. O O
they O O
are O O
closely O O
related O O
to O O
the O O
mixture B B
density I I
network I I
model O O
discussed O O
in O O
section O O
5.6 O O
. O O
200 O O
4. O O
linear O O
models O O
for O O
classification O O
2.5 O O
2 O O
1.5 O O
1 O O
0.5 O O
0 O O
−0.5 O O
−1 O O
−1.5 O O
−2 O O
−2.5 O O
−2 O O
−1 O O
0 O O
1 O O
2 O O
figure O O
4.11 O O
the O O
left-hand O O
plot O O
shows O O
the O O
class-conditional O O
densities O O
for O O
three O O
classes O O
each O O
having O O
a O O
gaussian O O
distribution O O
, O O
coloured O O
red O O
, O O
green O O
, O O
and O O
blue O O
, O O
in O O
which O O
the O O
red O O
and O O
green O O
classes O O
have O O
the O O
same O O
covariance B B
matrix I I
. O O
we O O
therefore O O
minimize O O
n O O
( O O
cid:2 O O
) O O
c O O
n=1 O O
ξn O O
+ O O
( O O
cid:5 O O
) O O
w O O
( O O
cid:5 O O
) O O
2 O O
1 O O
2 O O
( O O
7.21 O O
) O O
( O O
cid:5 O O
) O O
where O O
the O O
parameter O O
c O O
> O O
0 O O
controls O O
the O O
trade-off O O
between O O
the O O
slack B O
variable I O
penalty O O
and O O
the O O
margin B B
. O O
the O O
data O O
points O O
are O O
generated O O
from O O
a O O
gaussian O O
of O O
mean B B
0.8 O O
and O O
variance B B
0.1 O I
, O O
and O O
the O O
prior B B
is O O
chosen O O
to O O
have O O
mean B B
0. O O
in O O
both O O
the O O
prior B B
and O O
the O O
likelihood B B
function I I
, O O
the O O
variance B B
is O O
set O O
to O O
the O O
true O O
value O O
. O O
illustration O O
: O O
image B O
de-noising I O
. O O
the O O
green O O
curve O O
shows O O
the O O
optimal O O
de- O O
cision O O
boundary O O
, O O
the O O
black O O
curve O O
shows O O
the O O
result O O
of O O
ﬁtting O O
a O O
two-layer O O
network O O
with O O
8 O O
hidden O O
units O O
by O O
maximum O B
likeli- O O
hood O O
, O O
and O O
the O O
red O O
curve O O
shows O O
the O O
re- O O
sult O O
of O O
including O O
a O O
regularizer O O
in O O
which O O
α O O
is O O
optimized O O
using O O
the O O
evidence O O
pro- O O
cedure O O
, O O
starting O O
from O O
the O O
initial O O
value O O
α O O
= O O
0. O O
note O O
that O O
the O O
evidence O O
proce- O O
dure O O
greatly O O
reduces O O
the O O
over-ﬁtting B B
of O O
the O O
network O O
. O O
here O O
the O O
transition O O
matrix O O
was O O
ﬁxed O O
so O O
that O O
in O O
any O O
state O O
there O O
is O O
a O O
5 O O
% O O
probability B B
of O O
making O O
a O O
transition O O
to O O
each O O
of O O
the O O
other O O
states O O
, O O
and O O
consequently O O
a O O
90 O O
% O O
probability B O
of O O
remaining O O
in O O
the O O
same O O
state O O
. O O
( O O
2.107 O O
) O O
( O O
2.108 O O
) O O
µ O O
aµ O O
+ O O
b O O
section O O
2.3 O O
section O O
2.3 O O
next O O
we O O
ﬁnd O O
an O O
expression O O
for O O
the O O
marginal B B
distribution O O
p O O
( O O
y O O
) O O
in O O
which O O
we O O
have O O
marginalized O O
over O O
x. O O
recall O O
that O O
the O O
marginal B B
distribution O O
over O O
a O O
subset O O
of O O
the O O
com- O O
ponents O O
of O O
a O O
gaussian O O
random O O
vector O O
takes O O
a O O
particularly O O
simple O O
form O O
when O O
ex- O O
pressed O O
in O O
terms O O
of O O
the O O
partitioned B B
covariance O O
matrix O O
. O O
( O O
3.89 O O
) O O
multiplying O O
through O O
by O O
2α O O
and O O
rearranging O O
, O O
we O O
obtain O O
3.5. O O
the O O
evidence B O
approximation I I
169 O O
( O O
cid:2 O O
) O O
1 O O
λi O O
+ O O
α O O
i O O
αmt O O
n O O
mn O O
= O O
m O O
− O O
α O O
( O O
cid:2 O O
) O O
γ O O
= O O
λi O O
α O O
+ O O
λi O O
i O O
= O O
γ O O
. O O
now O O
let O O
us O O
examine O O
what O O
happens O O
when O O
we O O
try O O
to O O
use O O
rejection B B
sampling I I
in O O
spaces O O
of O O
high O O
dimensionality O O
. O O
this O O
result O O
is O O
unsurprising O O
, O O
since O O
both O O
principal B O
component I I
analysis I I
and O O
the O O
neural B B
network I I
are O O
using O O
linear O B
dimensionality O I
reduction O O
and O O
are O O
minimizing O O
the O O
same O O
sum-of-squares B B
error I I
function O O
. O O
for O O
example O O
, O O
this O O
allows O O
the O O
rvm O O
to O O
be O O
used O O
to O O
help O O
construct O O
an O O
emission O O
density O O
in O O
a O O
nonlinear O O
extension O O
of O O
the O O
linear B B
dynamical I I
system I I
for O O
tracking O O
faces O O
in O O
video O O
sequences O O
( O O
williams O O
et O O
al. O O
, O O
2005 O O
) O O
. O O
we O O
can O O
extend O O
the O O
laplace O O
method O O
to O O
approximate O O
a O O
distribution O O
p O O
( O O
z O O
) O O
= O O
f O O
( O O
z O O
) O O
/z O O
deﬁned O O
over O O
an O O
m-dimensional O O
space O O
z. O O
at O O
a O O
stationary B B
point O O
z0 O O
the O O
gradient O O
∇f O O
( O O
z O O
) O O
will O O
vanish O O
. O O
the O O
graphical O B
representation O O
of O O
this O O
model O O
is O O
shown O O
in O O
figure O O
8.24. O O
we O O
see O O
that O O
observation O O
of O O
z O O
blocks O O
the O O
path O O
between O O
xi O O
and O O
xj O O
for O O
j O O
( O O
cid:9 O O
) O O
= O O
i O O
( O O
because O O
such O O
paths O O
are O O
tail-to-tail O O
at O O
the O O
node B B
z O O
) O O
and O O
so O O
xi O O
and O O
xj O O
are O O
conditionally O O
independent B B
given O O
z. O O
if O O
, O O
however O O
, O O
we O O
marginalize O O
out O O
z O O
( O O
so O O
that O O
z O O
is O O
unobserved O O
) O O
the O O
tail-to-tail B O
path I O
from O O
xi O O
to O O
xj O O
is O O
no O O
longer O O
blocked O O
. O O
units O O
in O O
a O O
feature B O
map I I
each O O
take O O
inputs O O
only O O
from O O
a O O
small O O
subregion O O
of O O
the O O
image O O
, O O
and O O
all O O
of O O
the O O
units O O
in O O
a O O
feature B O
map I I
are O O
constrained O O
to O O
share O O
the O O
same O O
weight O O
values O O
. O O
this O O
corresponds O O
to O O
choosing O O
the O O
value O O
of O O
w O O
for O O
which O O
the O O
probability B B
of O O
the O O
observed O O
data O O
set O O
is O O
maxi- O O
mized O O
. O O
omitting O O
terms O O
that O O
are O O
independent B B
of O O
ξ O O
, O O
and O O
we O O
also O O
need O O
to O O
optimize O O
the O O
variational B B
parameters O O
ξn O O
, O O
and O O
this O O
is O O
also O O
done O O
by O O
integrating O O
over O O
α O O
, O O
we O O
have O O
( O O
cid:6 O O
) O O
( O O
cid:4 O O
) O O
l O O
( O O
q O O
, O O
ξ O O
) O O
= O O
q O O
( O O
w O O
) O O
ln O O
h O O
( O O
w O O
, O O
ξ O O
) O O
dw O O
+ O O
const O O
. O O
for O O
a O O
localized O O
kernel B O
function I O
, O O
it O O
has O O
the O O
prop- O O
erty O O
of O O
giving O O
more O O
weight O O
to O O
the O O
data O O
points O O
xn O O
that O O
are O O
close O O
to O O
x. O O
note O O
that O O
the O O
kernel O O
( O O
6.46 O O
) O O
satisﬁes O O
the O O
summation O O
constraint O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
k O O
( O O
x O O
, O O
xn O O
) O O
= O O
1 O O
. O O
for O O
a O O
wide O O
range O O
of O O
regression B B
and O O
classiﬁcation B B
tasks O O
, O O
the O O
rvm O O
is O O
found O O
to O O
give O O
models O O
that O O
are O O
typically O O
an O O
order O O
of O O
magnitude O O
more O O
compact O O
than O O
the O O
corresponding O O
support B B
vector I I
machine I I
, O O
resulting O O
in O O
a O O
signiﬁcant O O
improvement O O
in O O
the O O
speed O O
of O O
processing O O
on O O
test O O
data O O
. O O
frequentist B O
evaluation O O
methods O O
offer O O
some O O
protection O O
from O O
such O O
prob- O O
lems O O
, O O
and O O
techniques O O
such O O
as O O
cross-validation B B
remain O O
useful O O
in O O
areas O B
such O O
as O O
model B O
comparison I I
. O O
as O O
we O O
have O O
noted O O
, O O
however O O
, O O
the O O
sum-of-squares B B
error I I
is O O
not O O
robust O O
to O O
outliers B B
, O O
and O O
this O O
section O O
7.1.2 O O
exercise O O
14.8 O O
section O O
4.3.4 O O
exercise O O
14.9 O O
figure O O
14.4 O O
comparison O O
of O O
the O O
squared O O
error B B
( O O
green O O
) O O
with O O
the O O
absolute O O
error B B
( O O
red O O
) O O
showing O O
how O O
the O O
latter O O
places O O
much O O
less O O
emphasis O O
on O O
large O O
errors O O
and O O
hence O O
is O O
more O O
robust O O
to O O
outliers B B
and O O
mislabelled O O
data O O
points O O
. O O
consider O O
the O O
case O O
of O O
n O O
independent B B
data O O
points O O
x1 O O
, O O
. O O
2.3.3 O O
bayes O O
’ O O
theorem O O
for O O
gaussian O O
variables O O
in O O
sections O O
2.3.1 O O
and O O
2.3.2 O O
, O O
we O O
considered O O
a O O
gaussian O O
p O O
( O O
x O O
) O O
in O O
which O O
we O O
parti- O O
tioned O O
the O O
vector O O
x O O
into O O
two O O
subvectors O O
x O O
= O O
( O O
xa O O
, O O
xb O O
) O O
and O O
then O O
found O O
expressions O O
for O O
the O O
conditional B B
distribution O O
p O O
( O O
xa|xb O O
) O O
and O O
the O O
marginal B B
distribution O O
p O O
( O O
xa O O
) O O
. O O
the O O
12- O O
dimensional O O
observation O O
vectors O O
can O O
also O O
be O O
used O O
to O O
test O O
data O O
visualization B O
algo- O O
rithms O O
. O O
for O O
instance O O
, O O
the O O
technique O O
of O O
kernel B O
substitution I I
can O O
be O O
applied O O
to O O
principal B B
component I I
analysis I I
in O O
order O O
to O O
develop O O
a O O
nonlinear O O
variant O O
of O O
pca O O
( O O
sch¨olkopf O O
et O O
al. O O
, O O
1998 O O
) O O
. O O
making O O
use O O
of O O
the O O
result O O
( O O
c.21 O O
) O O
for O O
the O O
derivative B B
of O O
n O O
, O O
together O O
with O O
the O O
result O O
( O O
c.22 O O
) O O
for O O
the O O
derivative B B
of O O
ln|cn| O O
, O O
we O O
obtain O O
−1 O O
c O O
( O O
cid:15 O O
) O O
( O O
cid:16 O O
) O O
−1 O O
n O O
c O O
∂cn O O
∂θi O O
+ O O
1 O O
2 O O
−1 O O
ttc O O
n O O
∂cn O O
∂θi O O
−1 O O
n O O
t. O O
c O O
ln O O
p O O
( O O
t|θ O O
) O O
= O O
−1 O O
∂ O O
∂θi O O
( O O
6.70 O O
) O O
because O O
ln O O
p O O
( O O
t|θ O O
) O O
will O O
in O O
general O O
be O O
a O O
nonconvex O O
function O O
, O O
it O O
can O O
have O O
multiple O O
max- O O
ima O O
. O O
this O O
has O O
the O O
desired O O
effect O O
of O O
giving O O
a O O
lower O B
energy O I
( O O
thus O O
encouraging O O
a O O
higher O O
probability B O
) O O
when O O
xi O O
and O O
yi O O
have O O
the O O
same O O
sign O O
and O O
a O O
higher O O
energy O O
when O O
they O O
have O O
the O O
opposite O O
sign O O
. O O
because O O
the O O
colour O O
is O O
a O O
near-uniform O O
purple O O
, O O
we O O
see O O
that O O
the O O
model O O
assigns O O
a O O
probability B B
of O O
around O O
0.5 O O
to O O
each O O
of O O
the O O
classes O O
over O O
most O O
of O O
input O O
space O O
. O O
, O O
µm O O
) O O
t O O
= O O
µ. O O
corresponding O O
likelihood B B
function I I
takes O O
the O O
form O O
p O O
( O O
d|µ O O
) O O
= O O
µxnk O O
k O O
= O O
p O O
( O O
µ O O
k O O
n O O
xnk O O
) O O
= O O
µmk O O
k O O
. O O
expanding O O
out O O
the O O
square O O
, O O
we O O
see O O
that O O
the O O
variance B B
can O O
also O O
be O O
written O O
in O O
terms O O
of O O
the O O
expectations O O
of O O
f O O
( O O
x O O
) O O
and O O
f O O
( O O
x O O
) O O
2 O O
var O O
[ O O
f O O
] O O
= O O
e O O
[ O O
f O O
( O O
x O O
) O O
2 O O
] O O
− O O
e O O
[ O O
f O O
( O O
x O O
) O O
] O O
2 O O
. O O
for O O
the O O
moment O O
we O O
simply O O
note O O
that O O
it O O
is O O
a O O
nonnegative O O
quantity O O
that O O
would O O
be O O
zero O O
if O O
, O O
and O O
only O O
if O O
, O O
the O O
6 O O
1. O O
introduction O O
figure O O
1.3 O O
the O O
error B B
function I I
( O O
1.2 O O
) O O
corre- O O
sponds O O
to O O
( O O
one O O
half O O
of O O
) O O
the O O
sum O O
of O O
the O O
squares O O
of O O
the O O
displacements O O
( O O
shown O O
by O O
the O O
vertical O O
green O O
bars O O
) O O
of O O
each O O
data O O
point O O
from O O
the O O
function O O
y O O
( O O
x O O
, O O
w O O
) O O
. O O
the O O
parameter O O
c O O
is O O
therefore O O
analogous O O
to O O
( O O
the O O
inverse B B
of O O
) O O
a O O
regularization B B
coefﬁcient O O
because O O
it O O
controls O O
the O O
trade-off O O
between O O
minimizing O O
training B B
errors O O
and O O
controlling O O
model O O
complexity O O
. O O
xn O O
( O O
cid:30 O O
) O O
( O O
cid:30 O O
) O O
as O O
with O O
the O O
calculation O O
of O O
marginals O O
, O O
we O O
see O O
that O O
exchanging O O
the O O
max O O
and O O
product O O
operators O O
results O O
in O O
a O O
much O O
more O O
efﬁcient O O
computation O O
, O O
and O O
one O O
that O O
is O O
easily O O
inter- O O
preted O O
in O O
terms O O
of O O
messages O O
passed O O
from O O
node B B
xn O O
backwards O O
along O O
the O O
chain O O
to O O
node B B
x1 O O
. O O
a O O
minimum O O
that O O
corresponds O O
to O O
the O O
smallest O O
value O O
of O O
the O O
error B B
function I I
for O O
any O O
weight B O
vector I I
is O O
said O O
to O O
be O O
a O O
global B O
minimum I O
. O O
then O O
normalize O O
to O O
ﬁnd O O
the O O
conditional B B
density O O
p O O
( O O
t|x O O
) O O
, O O
and O O
ﬁnally O O
marginalize O O
to O O
ﬁnd O O
the O O
conditional B B
mean O O
given O O
by O O
( O O
1.89 O O
) O O
. O O
this O O
leads O O
us O O
to O O
consider O O
a O O
graphical O B
concept O O
called O O
a O O
clique B B
, O O
which O O
is O O
deﬁned O O
as O O
a O O
subset O O
of O O
the O O
nodes O O
in O O
a O O
graph O O
such O O
that O O
there O O
exists O O
a O O
link B B
between O O
all O O
pairs O O
of O O
nodes O O
in O O
the O O
subset O O
. O O
we O O
can O O
also O O
maximize O O
the O O
log O O
likelihood O O
function O O
( O O
3.11 O O
) O O
with O O
respect O O
to O O
the O O
noise O O
precision B O
parameter I I
β O O
, O O
giving O O
1 O O
βml O O
= O O
1 O O
n O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
{ O O
tn O O
− O O
wt O O
mlφ O O
( O O
xn O O
) O O
} O O
2 O O
( O O
3.21 O O
) O O
3.1. O O
linear O O
basis O O
function O O
models O O
143 O O
figure O O
3.2 O O
geometrical O O
interpretation O O
of O O
the O O
least-squares O O
solution O O
, O O
in O O
an O O
n-dimensional O O
space O O
whose O O
axes O O
are O O
the O O
values O O
of O O
t1 O O
, O O
. O O
show O O
that O O
the O O
conditional B B
density O O
p O O
( O O
xb|xa O O
) O O
is O O
itself O O
a O O
mixture B B
distribution I I
and O O
ﬁnd O O
expressions O O
for O O
the O O
mixing O O
coefﬁcients O O
and O O
for O O
the O O
component O O
densities O O
. O O
, O O
φi O O
( O O
xn O O
) O O
) O O
, O O
in O O
contrast O O
to O O
φn O O
, O O
which O O
denotes O O
the O O
nth O O
row O O
of O O
φ. O O
the O O
matrix O O
c−i O O
represents O O
the O O
matrix O O
c O O
with O O
the O O
contribution O O
from O O
basis B O
function I I
i O O
removed O O
. O O
1.34 O O
( O O
( O O
cid:1 O O
) O O
( O O
cid:1 O O
) O O
) O O
www O O
use O O
the O O
calculus B B
of I I
variations I I
to O O
show O O
that O O
the O O
stationary B B
point O O
of O O
the O O
functional B B
( O O
1.108 O O
) O O
is O O
given O O
by O O
( O O
1.108 O O
) O O
. O O
now O O
suppose O O
we O O
condition O O
on O O
the O O
variable O O
c O O
, O O
as O O
represented O O
by O O
the O O
graph O O
of O O
figure O O
8.16. O O
from O O
( O O
8.23 O O
) O O
, O O
we O O
can O O
easily O O
write O O
down O O
the O O
conditional B B
distribution O O
of O O
a O O
and O O
b O O
, O O
given O O
c O O
, O O
in O O
the O O
form O O
p O O
( O O
a O O
, O O
b|c O O
) O O
= O O
p O O
( O O
a O O
, O O
b O O
, O O
c O O
) O O
p O O
( O O
c O O
) O O
= O O
p O O
( O O
a|c O O
) O O
p O O
( O O
b|c O O
) O O
and O O
so O O
we O O
obtain O O
the O O
conditional B B
independence I I
property O O
a O O
⊥⊥ O O
b O O
| O O
c. O O
we O O
can O O
provide O O
a O O
simple O O
graphical O B
interpretation O O
of O O
this O O
result O O
by O O
considering O O
the O O
path O O
from O O
node B B
a O O
to O O
node B B
b O O
via O O
c. O O
the O O
node B B
c O O
is O O
said O O
to O O
be O O
tail-to-tail O O
with O O
re- O O
spect O O
to O O
this O O
path O O
because O O
the O O
node B B
is O O
connected O O
to O O
the O O
tails O O
of O O
the O O
two O O
arrows O O
, O O
and O O
the O O
presence O O
of O O
such O O
a O O
path O O
connecting O O
nodes O O
a O O
and O O
b O O
causes O O
these O O
nodes O O
to O O
be O O
de- O O
pendent O O
. O O
write O O
down O O
the O O
minimum O O
misclassiﬁcation-rate O O
decision O O
rule O I
assuming O O
the O O
two O O
classes O O
have O O
equal O O
prior B B
probability O O
. O O
for O O
any O O
given O O
value O O
of O O
x O O
, O O
the O O
mixture B B
model I I
provides O O
a O O
general O O
formalism O O
for O O
modelling O O
an O O
arbitrary O O
conditional B B
density O O
function O O
p O O
( O O
t|x O O
) O O
. O O
finally O O
, O O
we O O
note O O
that O O
an O O
important O O
role O O
for O O
the O O
probabilistic O O
pca O O
model O O
is O O
in O O
defining O O
a O O
multivariate O O
gaussian O O
distribution O O
in O O
which O O
the O O
number O O
of O O
degrees O O
of O I
free O O
( O O
cid:173 O O
) O O
dom O O
, O O
in O O
other O O
words O O
the O O
number O O
of O O
independent B B
parameters O O
, O O
can O O
be O O
controlled O O
whilst O O
still O O
allowing O O
the O O
model O O
to O O
capture O O
the O O
dominant O O
correlations O O
in O O
the O O
data O O
. O O
5 O O
0 O O
−1 O O
n O O
= O O
0 O O
n O O
= O O
10 O O
n O O
= O O
2 O O
n O O
= O O
1 O O
0 O O
1 O O
exercise O O
2.40 O O
section O O
2.3.5 O O
we O O
illustrate O O
our O O
analysis O O
of O O
bayesian O O
inference B B
for O O
the O O
mean B B
of O O
a O O
gaussian O O
distribution O O
in O O
figure O O
2.12. O O
the O O
generalization B B
of O O
this O O
result O O
to O O
the O O
case O O
of O O
a O O
d- O O
dimensional O O
gaussian O O
random O O
variable O O
x O O
with O O
known O O
covariance B B
and O O
unknown O O
mean B B
is O O
straightforward O O
. O O
the O O
intuition O O
here O O
is O O
that O O
the O O
identity O O
of O O
the O O
cross O O
should O O
be O O
determined O O
more O O
strongly O O
by O O
nearby O O
points O O
from O O
the O O
training B B
set I I
and O O
less O O
strongly O O
by O O
more O O
distant O O
points O O
. O O
9.2 O O
( O O
( O O
cid:12 O O
) O O
) O O
apply O O
the O O
robbins-monro O O
sequential B O
estimation I O
procedure O O
described O O
in O O
sec- O O
tion O O
2.3.5 O O
to O O
the O O
problem O O
of O O
ﬁnding O O
the O O
roots O O
of O O
the O O
regression B B
function I I
given O O
by O O
the O O
derivatives O O
of O O
j O O
in O O
( O O
9.1 O O
) O O
with O O
respect O O
to O O
µk O O
. O O
8.18 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
show O O
that O O
a O O
distribution O O
represented O O
by O O
a O O
directed B B
tree O O
can O O
trivially O O
be O O
written O O
as O O
an O O
equivalent O O
distribution O I
over O O
the O O
corresponding O O
undirected B B
tree O I
. O O
furthermore O O
, O O
for O O
data O O
sets O O
that O O
are O O
not O O
linearly B B
separable I I
, O O
the O O
perceptron B B
learning O O
algorithm O O
will O O
never O O
converge O O
. O O
the O O
most O O
successful O O
model O O
of O O
this O O
type O O
in O O
the O O
context O O
of O O
pattern O O
recognition O O
is O O
the O O
feed-forward O O
neural B O
network I I
, O O
also O O
known O O
as O O
the O O
multilayer B B
perceptron I I
, O O
discussed O O
in O O
this O O
chapter O O
. O O
( O O
10.225 O O
) O O
( O O
10.226 O O
) O O
note O O
that O O
normalization O O
constants O O
have O O
been O O
omitted O O
, O O
and O O
these O O
can O O
be O O
re-instated O O
at O O
the O O
end O O
by O O
local B B
normalization O O
, O O
as O O
is O O
generally O O
done O O
in O O
belief B B
propagation I I
. O O
give O O
two O O
alternative O O
interpretations O O
of O O
the O O
weighted O O
sum-of-squares O B
error B I
function I O
in O O
terms O O
of O O
( O O
i O O
) O O
data O O
dependent O O
noise O O
variance B B
and O O
( O O
ii O O
) O O
replicated O O
data O O
points O O
. O O
this O O
result O O
is O O
unsurprising O O
because O O
the O O
parameters O O
w O O
deﬁne O O
only O O
the O O
mean B B
of O O
the O O
gaussian O O
noise O O
distribution O O
, O O
and O O
we O O
know O O
from O O
sec- O O
tion O O
2.3.4 O O
that O O
the O O
maximum B B
likelihood I I
solution O O
for O O
the O O
mean B B
of O O
a O O
multivariate O O
gaus- O O
sian O O
is O O
independent B B
of O O
the O O
covariance B B
. O O
we O O
can O O
also O O
examine O O
the O O
bias-variance B O
trade-off I O
quantitatively O O
for O O
this O O
example O O
. O O
we O O
shall O O
sometimes O O
use O O
a O O
shorthand O O
notation O O
for O O
conditional O B
independence O I
( O O
dawid O O
, O O
1979 O O
) O O
in O O
which O O
a O O
⊥⊥ O O
b O O
| O O
c O O
( O O
8.22 O O
) O O
denotes O O
that O O
a O O
is O O
conditionally O O
independent B B
of O O
b O O
given O O
c O O
and O O
is O O
equivalent O O
to O O
( O O
8.20 O O
) O O
. O O
the O O
choice O O
of O O
output O O
unit O O
activation B B
function I I
is O O
discussed O O
in O O
detail O O
in O O
sec- O O
tion O O
5.2. O O
we O O
can O O
combine O O
these O O
various O O
stages O O
to O O
give O O
the O O
overall O O
network O O
function O O
that O O
, O O
for O O
sigmoidal O O
output O O
unit O O
activation O B
functions O O
, O O
takes O O
the O O
form O O
( O O
cid:23 O O
) O O
( O O
cid:23 O O
) O O
( O O
cid:22 O O
) O O
m O O
( O O
cid:2 O O
) O O
( O O
cid:22 O O
) O O
d O O
( O O
cid:2 O O
) O O
yk O O
( O O
x O O
, O O
w O O
) O O
= O O
σ O O
w O O
( O O
2 O O
) O O
kj O O
h O O
( O O
1 O O
) O O
ji O O
xi O O
+ O O
w O O
( O O
1 O O
) O O
j0 O O
w O O
+ O O
w O O
( O O
2 O O
) O O
k0 O O
( O O
5.7 O O
) O O
j=1 O O
i=1 O O
where O O
the O O
set O O
of O O
all O O
weight O O
and O O
bias B B
parameters O O
have O O
been O O
grouped O O
together O O
into O O
a O O
vector O O
w. O O
thus O O
the O O
neural B B
network I I
model O O
is O O
simply O O
a O O
nonlinear O O
function O O
from O O
a O O
set O O
of O O
input O O
variables O O
{ O O
xi O O
} O O
to O O
a O O
set O O
of O O
output O O
variables O O
{ O O
yk O O
} O O
controlled O O
by O O
a O O
vector O O
w O O
of O O
adjustable O O
parameters O O
. O O
in O O
the O O
new O O
coordinate O O
system O O
, O O
whose O O
basis O O
vectors O O
are O O
given O O
by O O
the O O
eigenvectors O O
{ O O
ui O O
} O O
, O O
the O O
contours O O
of O O
constant O O
e O O
are O O
ellipses O O
centred O O
on O O
the O O
origin O O
, O O
as O O
illustrated O O
in O O
figure O O
5.6. O O
for O O
a O O
one-dimensional O O
weight O O
space O O
, O O
a O O
stationary B B
point O O
w O O
( O O
cid:1 O O
) O O
will O O
be O O
a O O
minimum O O
if O O
> O O
0. O O
w O O
( O O
cid:1 O O
) O O
( O O
5.40 O O
) O O
exercise O O
5.12 O O
the O O
corresponding O O
result O O
in O O
d-dimensions O B
is O O
that O O
the O O
hessian O O
matrix O O
, O O
evaluated O O
at O O
w O O
( O O
cid:1 O O
) O O
, O O
should O O
be O O
positive B B
deﬁnite I I
. O O
note O O
that O O
we O O
have O O
already O O
encountered O O
a O O
speciﬁc O O
example O O
of O O
the O O
linear-gaussian O O
relationship O O
when O O
we O O
saw O O
that O O
the O O
conjugate B B
prior I I
for O O
the O O
mean B B
µ O O
of O O
a O O
gaussian O O
variable O O
x O O
is O O
itself O O
a O O
gaussian O O
distribution O O
over O O
µ. O O
the O O
joint O O
distribution O O
over O O
x O O
and O O
µ O O
is O O
therefore O O
gaussian O O
. O O
the O O
model B O
evidence I I
is O O
sometimes O O
also O O
called O O
the O O
marginal B B
likelihood I I
because O O
it O O
can O O
be O O
viewed O O
as O O
a O O
likelihood B B
function I I
over O O
the O O
space O O
of O O
models O O
, O O
in O O
which O O
the O O
parameters O O
have O O
been O O
marginalized O O
out O O
. O O
information O O
from O O
such O O
features O O
can O O
then O O
be O O
merged O O
in O O
later O O
stages O O
of O O
processing O O
in O O
order O O
to O O
detect O O
higher-order O O
features O O
268 O O
5. O O
neural O O
networks O O
input O O
image O O
convolutional O O
layer O O
sub-sampling O O
layer O O
figure O O
5.17 O O
diagram O O
illustrating O O
part O O
of O O
a O O
convolutional B B
neural I I
network I I
, O O
showing O O
a O O
layer O O
of O O
convolu- O O
tional O O
units O O
followed O O
by O O
a O O
layer O O
of O O
subsampling B B
units O O
. O O
the O O
difﬁculty O O
stems O O
from O O
the O O
assumption O O
that O O
the O O
basis O O
functions O O
φj O O
( O O
x O O
) O O
are O O
ﬁxed O O
before O O
the O O
training B B
data O O
set O O
is O O
observed O O
and O O
is O O
a O O
manifestation O O
of O O
the O O
curse O B
of O I
dimen- O O
sionality O O
discussed O O
in O O
section O O
1.4. O O
as O O
a O O
consequence O O
, O O
the O O
number O O
of O O
basis O O
functions O O
needs O O
to O O
grow O O
rapidly O O
, O O
often O O
exponentially O O
, O O
with O O
the O O
dimensionality O O
d O O
of O O
the O O
input O O
space O O
. O O
13.32 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
verify O O
the O O
results O O
( O O
13.110 O O
) O O
and O O
( O O
13.111 O O
) O O
for O O
the O O
m-step O O
equations O O
for O O
µ0 O O
and O O
v0 O O
in O O
the O O
linear B B
dynamical I I
system I I
. O O
n O O
( O O
cid:14 O O
) O O
k O O
( O O
cid:2 O O
) O O
9.10 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
consider O O
a O O
density B B
model O O
given O O
by O O
a O O
mixture B B
distribution I I
p O O
( O O
x O O
) O O
= O O
πkp O O
( O O
x|k O O
) O O
( O O
9.81 O O
) O O
k=1 O O
and O O
suppose O O
that O O
we O O
partition O O
the O O
vector O O
x O O
into O O
two O O
parts O O
so O O
that O O
x O O
= O O
( O O
xa O O
, O O
xb O O
) O O
. O O
another O O
way O O
to O O
see O O
why O O
a O O
gaussian O O
latent B B
variable I I
distribution O O
in O O
a O O
linear O O
model O O
is O O
insufficient O O
to O O
find O O
independent B B
components O O
is O O
to O O
note O O
that O O
the O O
principal O O
compo O O
( O O
cid:173 O O
) O O
nents O O
represent O O
a O O
rotation O O
of O O
the O O
coordinate O O
system O O
in O O
data O O
space O O
such O O
as O O
to O O
diagonal B B
( O O
cid:173 O O
) O O
ize O O
the O O
covariance B B
matrix I I
, O O
so O O
that O O
the O O
data O O
distribution O O
in O O
the O O
new O O
coordinates O O
is O O
then O O
uncorrelated O O
. O O
and O O
thcn O O
we O O
show O O
how O O
pea O O
arises O O
naturally O O
as O O
the O O
maximum B B
likelihood I I
solution O O
10 O O
appendix O O
a O O
section O O
8.1..j O O
section O O
12.1 O O
12.1. O O
principal O B
c01n O O
[ O O
> O O
om O O
'' O O
1 O O
analjsis O O
561 O O
flgu O O
, O O
e12.2 O O
p'if O O
> O O
cipal O O
compooont O B
a O O
'' O O
, O O
,~ O O
'' O O
seeks O O
'' O O
$ O O
pace O O
01 O O
! O O
owe O O
, O O
dimensionality O O
. O O
this O O
property O O
will O O
be O O
important O O
in O O
the O O
hybrid O O
monte O O
carlo O O
algorithm O O
, O O
which O O
is O O
discussed O O
in O O
section O O
11.5.2. O O
one O O
scheme O O
for O O
achieving O O
this O O
is O O
called O O
the O O
leapfrog B O
discretization I O
and O O
involves O O
alternately O O
updat- O O
ing O O
discrete-time O O
approximations O O
( O O
cid:1 O O
) O O
z O O
and O O
( O O
cid:1 O O
) O O
r O O
to O O
the O O
position O O
and O O
momentum O O
variables O O
using O O
( O O
( O O
cid:1 O O
) O O
z O O
( O O
τ O O
) O O
) O O
( O O
cid:1 O O
) O O
ri O O
( O O
τ O O
+ O O
/2 O O
) O O
= O O
( O O
cid:1 O O
) O O
ri O O
( O O
τ O O
) O O
− O O
 O O
( O O
cid:1 O O
) O O
zi O O
( O O
τ O O
+ O O
 O O
) O O
= O O
( O O
cid:1 O O
) O O
zi O O
( O O
τ O O
) O O
+ O O
 O O
( O O
cid:1 O O
) O O
ri O O
( O O
τ O O
+ O O
/2 O O
) O O
( O O
cid:1 O O
) O O
ri O O
( O O
τ O O
+ O O
 O O
) O O
= O O
( O O
cid:1 O O
) O O
ri O O
( O O
τ O O
+ O O
/2 O O
) O O
− O O
 O O
∂e O O
∂zi O O
2 O O
( O O
( O O
cid:1 O O
) O O
z O O
( O O
τ O O
+ O O
 O O
) O O
) O O
. O O
this O O
result O O
holds O O
for O O
a O O
wide O O
range O O
of O O
hidden B O
unit I O
activation O O
functions O O
, O O
but O O
excluding O O
polynomi- O O
als O O
. O O
7.2. O O
relevance B B
vector I I
machines O O
353 O O
3. O O
evaluate O O
σ O O
and O O
m O O
, O O
along O O
with O O
qi O O
and O O
si O O
for O O
all O O
basis O B
functions O O
. O O
even O O
when O O
the O O
data O O
set O O
is O O
linearly B B
separable I I
, O O
there O O
may O O
be O O
many O O
solutions O O
, O O
and O O
which O O
one O O
is O O
found O O
will O O
depend O O
on O O
the O O
initialization O O
of O O
the O O
parameters O O
and O O
on O O
the O O
or- O O
der O O
of O O
presentation O O
of O O
the O O
data O O
points O O
. O O
show O O
that O O
the O O
feature O O
vector O O
φ O O
( O O
x O O
) O O
enters O O
only O O
in O O
the O O
form O O
of O O
the O O
kernel B O
function I I
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
φ O O
( O O
x O O
) O O
tφ O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
. O O
then O O
observe O O
that O O
this O O
represents O O
the O O
convolution O O
of O O
two O O
gaussian O O
distributions O O
, O O
which O O
itself O O
will O O
be O O
gaussian O O
, O O
and O O
ﬁnally O O
make O O
use O O
of O O
the O O
result O O
( O O
1.110 O O
) O O
for O O
the O O
entropy B B
of O O
the O O
univariate O O
gaussian O O
. O O
here O O
we O O
shall O O
adopt O B
a O O
generative O O
4.2. O O
probabilistic O O
generative O O
models O O
197 O O
figure O O
4.9 O O
plot O O
of O O
the O O
logistic B B
sigmoid I I
function O O
σ O O
( O O
a O O
) O O
deﬁned O O
by O O
( O O
4.59 O O
) O O
, O O
shown O O
in O O
red O O
, O O
together O O
with O O
the O O
scaled O O
pro- O O
bit O O
function O O
φ O O
( O O
λa O O
) O O
, O O
for O O
λ2 O O
= O O
π/8 O O
, O O
shown O O
in O O
dashed O O
blue O O
, O O
where O O
φ O O
( O O
a O O
) O O
is O O
deﬁned O O
by O O
( O O
4.114 O O
) O O
. O O
, O O
zk O O
) O O
t O O
is O O
a O O
binary O O
k-dimensional O O
variable O O
having O O
a O O
single O O
component O O
equal O O
to O O
1 O O
, O O
with O O
all O O
other O O
components O O
equal O O
to O O
0. O O
we O O
can O O
then O O
write O O
the O O
conditional B B
distribution O O
of O O
x O O
, O O
given O O
the O O
latent B B
variable I I
, O O
as O O
p O O
( O O
x|z O O
, O O
µ O O
) O O
= O O
( O O
9.52 O O
) O O
k O O
( O O
cid:14 O O
) O O
k=1 O O
p O O
( O O
x|µk O O
) O O
zk O O
k O O
( O O
cid:14 O O
) O O
πzk O O
k O O
. O O
multivari- O O
able O O
functional B B
interpolation O O
and O O
adaptive O O
net- O O
works O O
. O O
5.5.1 O O
consistent B B
gaussian O O
priors O O
. O O
there O O
are O O
now O O
many O O
possible O O
ways O O
to O O
organize O O
the O O
message B B
passing I I
schedule O O
. O O
again O O
a O O
simple O O
inductive O O
argument O O
can O O
be O O
used O O
to O O
verify O O
the O O
validity O O
of O O
this O O
message B O
passing I O
protocol O O
. O O
the O O
training B B
data O O
consists O O
of O O
input O O
vectors O O
{ O O
x1 O O
, O O
. O O
to O O
achieve O O
this O O
, O O
we O O
consider O O
a O O
generalization B B
of O O
this O O
model O O
in O O
which O O
we O O
transform O O
the O O
linear O B
function O I
of O O
w O O
using O O
a O O
nonlinear O O
function O O
f O O
( O O
· O O
) O O
so O O
that O O
( O O
cid:10 O O
) O O
( O O
cid:11 O O
) O O
y O O
( O O
x O O
) O O
= O O
f O O
wtx O O
+ O O
w0 O O
( O O
4.3 O O
) O O
in O O
the O O
machine O O
learning O O
literature O O
f O O
( O O
· O O
) O O
is O O
known O O
as O O
an O O
activation B B
function I I
, O O
whereas O O
its O O
inverse B B
is O O
called O O
a O O
link B B
function I I
in O O
the O O
statistics O O
literature O O
. O O
for O O
instance O O
, O O
we O O
can O O
generalize O O
the O O
gaussian O O
prior B B
to O O
give O O
( O O
cid:29 O O
) O O
( O O
cid:17 O O
) O O
( O O
cid:18 O O
) O O
1/q O O
( O O
cid:30 O O
) O O
m O O
( O O
cid:22 O O
) O O
p O O
( O O
w|α O O
) O O
= O O
q O O
2 O O
α O O
2 O O
1 O O
γ O O
( O O
1/q O O
) O O
exp O O
− O O
α O O
2 O O
|wj|q O O
( O O
3.56 O O
) O O
( O O
cid:23 O O
) O O
m O O
( O O
cid:2 O O
) O O
j=1 O O
in O O
which O O
q O O
= O O
2 O O
corresponds O O
to O O
the O O
gaussian O O
distribution O O
, O O
and O O
only O O
in O O
this O O
case O O
is O O
the O O
prior B B
conjugate O O
to O O
the O O
likelihood B B
function I I
( O O
3.10 O O
) O O
. O O
−1 O O
marginal B B
distribution O O
: O O
p O O
( O O
xa O O
) O O
= O O
n O O
( O O
xa|µa O O
, O O
σaa O O
) O O
. O O
at O O
each O O
cycle O O
of O O
the O O
algorithm O O
, O O
we O O
generate O O
a O O
candidate O O
sample O O
z O O
( O O
cid:1 O O
) O O
from O O
the O O
proposal B B
distribution I I
and O O
then O O
accept O O
the O O
sample O O
according O O
to O O
an O O
appropriate O O
criterion O O
. O O
6.4. O O
gaussian O O
processes O O
309 O O
0 O O
x O O
1 O O
note O O
that O O
the O O
mean B B
( O O
6.66 O O
) O O
of O O
the O O
predictive B B
distribution I I
can O O
be O O
written O O
, O O
as O O
a O O
func- O O
tion O O
of O O
xn O O
+1 O O
, O O
in O O
the O O
form O O
n O O
( O O
cid:2 O O
) O O
m O O
( O O
xn O O
+1 O O
) O O
= O O
ank O O
( O O
xn O O
, O O
xn O O
+1 O O
) O O
( O O
6.68 O O
) O O
exercise O O
6.21 O O
n=1 O O
−1 O O
n O O
t. O O
thus O O
, O O
if O O
the O O
kernel B O
function I I
k O O
( O O
xn O O
, O O
xm O O
) O O
where O O
an O O
is O O
the O O
nth O O
component O O
of O O
c O O
depends O O
only O O
on O O
the O O
distance O O
( O O
cid:5 O O
) O O
xn O O
− O O
xm O O
( O O
cid:5 O O
) O O
, O O
then O O
we O O
obtain O O
an O O
expansion O O
in O O
radial O B
basis O I
functions O O
. O O
9.5 O O
( O O
( O O
cid:12 O O
) O O
) O O
consider O O
the O O
directed B B
graph O O
for O O
a O O
gaussian O O
mixture B B
model I I
shown O O
in O O
figure O O
9.6. O O
by O O
making O O
use O O
of O O
the O O
d-separation B B
criterion O O
discussed O O
in O O
section O O
8.2 O O
, O O
show O O
that O O
the O O
posterior O O
distribution O O
of O O
the O O
latent O O
variables O O
factorizes O O
with O O
respect O O
to O O
the O O
different O O
data O O
points O O
so O O
that O O
p O O
( O O
z|x O O
, O O
µ O O
, O O
σ O O
, O O
π O O
) O O
= O O
p O O
( O O
zn|xn O O
, O O
µ O O
, O O
σ O O
, O O
π O O
) O O
. O O
because O O
the O O
number O O
of O O
instances O O
in O O
column O O
i O O
in O O
figure O O
1.10 O O
is O O
just O O
the O O
sum O O
of O O
the O O
number O O
of O O
instances O O
in O O
each O O
cell O O
of O O
that O O
column O O
, O O
we O O
have O O
ci O O
= O O
j O O
nij O O
and O O
therefore O O
, O O
( O O
cid:5 O O
) O O
( O O
1.6 O O
) O O
14 O O
1. O O
introduction O O
from O O
( O O
1.5 O O
) O O
and O O
( O O
1.6 O O
) O O
, O O
we O O
have O O
p O O
( O O
x O O
= O O
xi O O
) O O
= O O
l O O
( O O
cid:2 O O
) O O
j=1 O O
p O O
( O O
x O O
= O O
xi O O
, O O
y O O
= O O
yj O O
) O O
( O O
1.7 O O
) O O
which O O
is O O
the O O
sum B O
rule I I
of I O
probability I B
. O O
( O O
8.73 O O
) O O
in O O
order O O
to O O
apply O O
the O O
sum-product B B
algorithm I I
to O O
this O O
graph O O
, O O
let O O
us O O
designate O O
node B B
x3 O O
as O O
the O O
root O B
, O O
in O O
which O O
case O O
there O O
are O O
two O O
leaf O O
nodes O O
x1 O O
and O O
x4 O O
. O O
gaussian O O
processes O O
and O O
svm O O
: O O
mean B B
ﬁeld I I
theory I I
and O O
leave-one-out B O
. O O
we O O
ﬁrst O O
run O O
the O O
sum-product B B
algorithm I I
to O O
ﬁnd O O
the O O
corresponding O O
unnormalized O O
marginals O O
( O O
cid:4 O O
) O O
p O O
( O O
xi O O
) O O
. O O
from O O
( O O
8.52 O O
) O O
we O O
see O O
that O O
the O O
expression O O
for O O
the O O
marginal B B
p O O
( O O
xn O O
) O O
decomposes O O
into O O
the O O
product O O
of O O
two O O
factors O O
times O O
the O O
normalization O O
constant O O
p O O
( O O
xn O O
) O O
= O O
µα O O
( O O
xn O O
) O O
µβ O O
( O O
xn O O
) O O
. O O
similarly O O
, O O
for O O
the O O
variational B B
distribution O O
over O O
the O O
parameters O O
, O O
we O O
have O O
ln O O
q O O
( O O
cid:1 O O
) O O
( O O
η O O
) O O
= O O
ln O O
p O O
( O O
η|ν0 O O
, O O
χ0 O O
) O O
+ O O
ez O O
[ O O
ln O O
p O O
( O O
x O O
, O O
z|η O O
) O O
] O O
+ O O
const O O
( O O
10.117 O O
) O O
= O O
ν0 O O
ln O O
g O O
( O O
η O O
) O O
+ O O
ηtχ0 O O
+ O O
ln O O
g O O
( O O
η O O
) O O
+ O O
ηt O O
ezn O O
[ O O
u O O
( O O
xn O O
, O O
zn O O
) O O
] O O
+ O O
const O O
. O O
exact O O
inference O B
in O O
this O O
model O O
is O O
intractable O O
, O O
but O O
vari- O O
ational O O
methods O O
lead O O
to O O
an O O
efﬁcient O O
inference B B
scheme O O
involving O O
forward-backward O O
recursions O O
along O O
each O O
of O O
the O O
continuous O O
and O O
discrete O O
markov O O
chains O O
independently O O
. O O
, O O
xn O O
} O O
, O O
for O O
which O O
the O O
likelihood B B
function I I
is O O
given O O
by O O
now O O
consider O O
a O O
set O O
of O O
independent B B
identically I O
distributed I O
data O O
denoted O O
by O O
x O O
= O O
n O O
( O O
cid:14 O O
) O O
( O O
cid:24 O O
) O O
( O O
cid:25 O O
) O O
( O O
cid:22 O O
) O O
( O O
cid:23 O O
) O O
h O O
( O O
xn O O
) O O
g O O
( O O
η O O
) O O
n O O
exp O O
ηt O O
n=1 O O
n=1 O O
u O O
( O O
xn O O
) O O
. O O
the O O
resulting O O
decision O B
boundaries O O
, O O
corresponding O O
to O O
the O O
minimum O O
misclassiﬁcation O O
rate O O
, O O
will O O
occur O O
when O O
two O O
of O O
the O O
posterior O O
probabilities O O
( O O
the O O
two O O
largest O O
) O O
are O O
equal O O
, O O
and O O
so O O
will O O
be O O
deﬁned O O
by O O
linear O O
functions O O
of O O
x O O
, O O
and O O
so O O
again O O
we O O
have O O
a O O
generalized B B
linear I I
model I I
. O O
552 O O
11. O O
sampling B B
methods I I
11.5.2 O O
hybrid O O
monte O O
carlo O O
as O O
we O O
discussed O O
in O O
the O O
previous O O
section O O
, O O
for O O
a O O
nonzero O O
step O O
size O O
 O O
, O O
the O O
discretiza- O O
tion O O
of O O
the O O
leapfrog O O
algorithm O O
will O O
introduce O O
errors O O
into O O
the O O
integration O O
of O O
the O O
hamil- O O
tonian O O
dynamical O O
equations O O
. O O
such O O
a O O
combined O O
approach O O
was O O
adopted O O
by O O
bishop O O
and O O
svens´en O O
( O O
2003 O O
) O O
in O O
the O O
context O O
of O O
a O O
bayesian O O
treatment O O
of O O
the O O
hierarchical B B
mixture I O
of I I
experts I I
model O O
. O O
4.1.4 O O
fisher O O
’ O O
s O O
linear B B
discriminant I I
one O O
way O O
to O O
view O O
a O O
linear O O
classiﬁcation O B
model O O
is O O
in O O
terms O O
of O O
dimensionality O O
reduction O O
. O O
exercise O O
2.51 O O
where O O
‘ O O
const O O
’ O O
denotes O O
terms O O
independent B B
of O O
θ O O
, O O
and O O
we O O
have O O
made O O
use O O
of O O
the O O
following O O
trigonometrical O O
identities O O
cos2 O O
a O O
+ O O
sin2 O O
a O O
= O O
1 O O
( O O
2.177 O O
) O O
( O O
2.178 O O
) O O
if O O
we O O
now O O
deﬁne O O
m O O
= O O
r0/σ2 O O
, O O
we O O
obtain O O
our O O
ﬁnal O O
expression O O
for O O
the O O
distribution O O
of O O
p O O
( O O
θ O O
) O O
along O O
the O O
unit O O
circle O O
r O O
= O O
1 O O
in O O
the O O
form O O
cos O O
a O O
cos O O
b O O
+ O O
sin O O
a O O
sin O O
b O O
= O O
cos O O
( O O
a O O
− O O
b O O
) O O
. O O
these O O
multiple O O
modes O O
may O O
arise O O
through O O
nonidentiﬁability B O
in O O
the O O
latent O O
space O O
or O O
through O O
complex O O
nonlin- O O
ear O O
dependence O O
on O O
the O O
parameters O O
. O O
next O O
suppose O O
that O O
we O O
also O O
check O O
the O O
state O O
of O O
the O O
battery O O
and O O
ﬁnd O O
that O O
it O O
is O O
ﬂat O O
, O O
i.e. O O
, O O
b O O
= O O
0. O O
we O O
have O O
now O O
observed O O
the O O
states O O
of O O
both O O
the O O
fuel O O
gauge O O
and O O
the O O
battery O O
, O O
as O O
shown O O
by O O
the O O
right-hand O O
graph O O
in O O
figure O O
8.21. O O
the O O
posterior B O
probability I I
that O O
the O O
fuel O O
tank O O
is O O
empty O O
given O O
the O O
observations O O
of O O
both O O
the O O
fuel O O
gauge O O
and O O
the O O
battery O O
state O O
is O O
then O O
given O O
by O O
( O O
cid:5 O O
) O O
p O O
( O O
f O O
= O O
0|g O O
= O O
0 O O
, O O
b O O
= O O
0 O O
) O O
= O O
p O O
( O O
g O O
= O O
0|b O O
= O O
0 O O
, O O
f O O
= O O
0 O O
) O O
p O O
( O O
f O O
= O O
0 O O
) O O
f∈ O O
{ O O
0,1 O O
} O O
p O O
( O O
g O O
= O O
0|b O O
= O O
0 O O
, O O
f O O
) O O
p O O
( O O
f O O
) O O
( O O
cid:7 O O
) O O
0.111 O O
( O O
8.33 O O
) O O
where O O
the O O
prior B B
probability O O
p O O
( O O
b O O
= O O
0 O O
) O O
has O O
cancelled O O
between O O
numerator O O
and O O
denom- O O
inator O O
. O O
as O O
with O O
maximum B B
likelihood I I
, O O
this O O
leads O O
to O O
a O O
two-pass O O
forward-backward O B
recursion O I
to O O
compute O O
posterior O O
probabilities O O
. O O
we O O
have O O
already O O
seen O O
that O O
, O O
in O O
the O O
maximum B B
likelihood I I
approach O O
, O O
the O O
perfor- O O
mance O O
on O O
the O O
training B B
set I I
is O O
not O O
a O O
good O O
indicator O O
of O O
predictive O B
performance O O
on O O
un- O O
seen O O
data O O
due O O
to O O
the O O
problem O O
of O O
over-ﬁtting B B
. O O
appendix O I
e O O
exercise O O
4.4 O O
( O O
cid:5 O O
) O O
i O O
w2 O O
is O O
the O O
mean B B
of O O
the O O
projected O O
data O O
from O O
class O O
ck O O
. O O
over-relaxation B O
method O O
for O O
the O O
monte O O
carlo O O
evaluation O O
of O O
the O O
partition O B
func- O O
tion O O
for O O
multiquadratic O O
actions O O
. O O
for O O
example O O
, O O
we O O
can O O
model O O
the O O
class-conditional O O
densities O O
p O O
( O O
y|ck O O
) O O
using O O
gaussian O O
distributions O O
and O O
then O O
use O O
the O O
techniques O O
of O O
section O O
1.2.4 O O
to O O
ﬁnd O O
the O O
parameters O O
of O O
the O O
gaussian O O
distributions O O
by O O
maximum B B
likelihood I I
. O O
the O O
result O O
( O O
8.66 O O
) O O
says O O
that O O
to O O
evaluate O O
the O O
message O B
sent O O
by O O
a O O
factor O B
node O O
to O O
a O O
vari- O O
able O O
node B O
along O O
the O O
link B B
connecting O O
them O O
, O O
take O O
the O O
product O O
of O O
the O O
incoming O O
messages O O
along O O
all O O
other O O
links O O
coming O O
into O O
the O O
factor O B
node O O
, O O
multiply O O
by O O
the O O
factor O B
associated O O
with O O
that O O
node B B
, O O
and O O
then O O
marginalize O O
over O O
all O O
of O O
the O O
variables O O
associated O O
with O O
the O O
incoming O O
messages O O
. O O
ro- O O
bust O O
bayesian O O
mixture B B
modelling O O
. O O
at O O
each O O
stage O O
the O O
required O O
computations O O
therefore O O
scale O O
like O O
o O O
( O O
m O O
3 O O
) O O
, O O
where O O
m O O
is O O
the O O
number O O
of O O
active O O
basis O O
vectors O O
in O O
the O O
model O O
and O O
is O O
typically O O
much O O
smaller O O
than O O
the O O
number O O
n O O
of O O
training B B
patterns O O
. O O
if O O
we O O
consider O O
an O O
interval O O
a O O
( O O
cid:1 O O
) O O
σ O O
( O O
cid:1 O O
) O O
b O O
, O O
and O O
a O O
scaled O O
interval O O
a/c O O
( O O
cid:1 O O
) O O
σ O O
( O O
cid:1 O O
) O O
b/c O O
, O O
then O O
the O O
prior B B
should O O
assign O O
equal O O
probability B B
mass O O
to O O
these O O
two O O
intervals O O
. O O
for O O
a O O
( O O
cid:2 O O
) O O
1 O O
the O O
density B B
is O O
everywhere O O
ﬁnite O O
, O O
and O O
the O O
special O O
case O O
of O O
a O O
= O O
1 O O
is O O
known O O
as O O
the O O
exponential B B
distribution I I
. O O
the O O
parameter O O
λ O O
is O O
sometimes O O
called O O
the O O
precision O O
of O O
the O O
t-distribution O O
, O O
even O O
though O O
it O O
is O O
not O O
in O O
general O O
equal O O
to O O
the O O
inverse B B
of O O
the O O
variance B B
. O O
section O O
14.4 O O
14.3.1 O O
minimizing O O
exponential O O
error O O
boosting B B
was O O
originally O O
motivated O O
using O O
statistical B B
learning I I
theory I I
, O O
leading O O
to O O
upper O O
bounds O O
on O O
the O O
generalization B B
error O I
. O O
we O O
can O O
view O O
the O O
marginal B B
distribution O O
p O O
( O O
x O O
) O O
as O O
a O O
prior B B
over O O
the O O
latent B B
variable I I
x O O
, O O
and O O
our O O
goal O O
is O O
to O O
infer O O
the O O
corresponding O O
posterior O O
distribution O O
over O O
x. O O
using O O
the O O
sum O O
and O O
product O O
rules O O
of O O
probability B B
we O O
can O O
evaluate O O
( O O
cid:2 O O
) O O
x O O
( O O
cid:1 O O
) O O
p O O
( O O
y O O
) O O
= O O
p O O
( O O
y|x O O
( O O
cid:4 O O
) O O
) O O
p O O
( O O
x O O
( O O
cid:4 O O
) O O
) O O
( O O
8.47 O O
) O O
which O O
can O O
then O O
be O O
used O O
in O O
bayes O O
’ O O
theorem O O
to O O
calculate O O
p O O
( O O
x|y O O
) O O
= O O
p O O
( O O
y|x O O
) O O
p O O
( O O
x O O
) O O
. O O
62 O O
1. O O
introduction O O
1.17 O O
( O O
( O O
cid:1 O O
) O O
( O O
cid:1 O O
) O O
) O O
www O O
the O O
gamma B B
function I I
is O O
deﬁned O O
by O O
ux−1e O O
γ O O
( O O
x O O
) O O
≡ O O
−u O O
du O O
. O O
11.5 O O
( O O
( O O
cid:12 O O
) O O
) O O
www O O
let O O
z O O
be O O
a O O
d-dimensional O O
random O O
variable O O
having O O
a O O
gaussian O O
distribu- O O
tion O O
with O O
zero O O
mean B B
and O O
unit O O
covariance B B
matrix I I
, O O
and O O
suppose O O
that O O
the O O
positive B O
deﬁnite I I
symmetric O O
matrix O O
σ O O
has O O
the O O
cholesky O O
decomposition O O
σ O O
= O O
llt O O
where O O
l O O
is O O
a O O
lower- O O
triangular O O
matrix O O
( O O
i.e. O O
, O O
one O O
with O O
zeros O O
above O O
the O O
leading O O
diagonal B B
) O O
. O O
however O O
, O O
if O O
we O O
only O O
wish O O
to O O
make O O
classiﬁcation B B
decisions O O
, O O
then O O
it O O
can O O
be O O
waste- O O
ful O O
of O O
computational O O
resources O O
, O O
and O O
excessively O O
demanding O O
of O O
data O O
, O O
to O O
ﬁnd O O
the O O
joint O O
distribution O O
p O O
( O O
x O O
, O O
ck O O
) O O
when O O
in O O
fact O O
we O O
only O O
really O O
need O O
the O O
posterior O O
probabilities O O
p O O
( O O
ck|x O O
) O O
, O O
which O O
can O O
be O O
obtained O O
directly O O
through O O
approach O O
( O O
b O O
) O O
. O O
next O O
the O O
graph O O
is O O
triangulated B O
, O O
which O O
involves O O
ﬁnding O O
chord-less O O
cycles O O
containing O O
four O O
or O O
more O O
nodes O O
and O O
adding O O
extra O O
links O O
to O O
eliminate O O
such O O
chord-less O O
cycles O O
. O O
however O O
, O O
if O O
the O O
validation B B
set I I
is O O
small O O
, O O
it O O
will O O
give O O
a O O
relatively O O
noisy O O
estimate O O
of O O
predictive O B
performance O O
. O O
the O O
activations O O
of O O
the O O
hidden O O
units O O
in O O
the O O
ﬁrst O O
hidden O O
layer O O
figure O O
5.10 O O
plot O O
of O O
the O O
sum-of-squares O B
test-set O O
error B B
for O O
the O O
polynomial O B
data O O
set O O
ver- O O
sus O O
the O O
number O O
of O O
hidden O O
units O O
in O O
the O O
network O O
, O O
with O O
30 O O
random O O
starts O O
for O O
each O O
network O O
size O O
, O O
showing O O
the O O
ef- O O
fect O O
of O O
local B B
minima O O
. O O
( O O
8.6 O O
) O O
n=1 O O
this O O
joint O O
distribution O O
can O O
be O O
represented O O
by O O
a O O
graphical B B
model I I
shown O O
in O O
figure O O
8.3. O O
when O O
we O O
start O O
to O O
deal O O
with O O
more O O
complex O O
models O O
later O O
in O O
the O O
book O O
, O O
we O O
shall O O
ﬁnd O O
it O O
inconvenient O O
to O O
have O O
to O O
write O O
out O O
multiple O O
nodes O O
of O O
the O O
form O O
t1 O O
, O O
. O O
note O O
that O O
positive B O
deﬁnite I I
is O O
not O O
the O O
same O O
as O O
all O O
the O O
elements O O
being O O
positive O O
. O O
we O O
therefore O O
consider O O
an O O
alternative O O
error B B
function I I
known O O
as O O
the O O
perceptron B B
cri- O O
terion O O
. O O
80 O O
2. O O
probability B B
distributions O O
functional B B
dependence O O
of O O
the O O
gaussian O O
on O O
x O O
is O O
through O O
the O O
quadratic O O
form O O
∆2 O O
= O O
( O O
x O O
− O O
µ O O
) O O
tς O O
−1 O O
( O O
x O O
− O O
µ O O
) O O
( O O
2.44 O O
) O O
which O O
appears O O
in O O
the O O
exponent O O
. O O
we O O
now O O
examine O O
in O O
more O O
detail O O
350 O O
7. O O
sparse O O
kernel O O
machines O O
t2 O O
c O O
t O O
t1 O O
t2 O O
ϕ O O
c O O
t O O
t1 O O
figure O O
7.10 O O
illustration O O
of O O
the O O
mechanism O O
for O O
sparsity O O
in O O
a O O
bayesian O O
linear B O
regression I I
model O O
, O O
showing O O
a O O
training B B
set I I
vector O O
of O O
target O O
values O O
given O O
by O O
t O O
= O O
( O O
t1 O O
, O O
t2 O O
) O O
t O O
, O O
indicated O O
by O O
the O O
cross O O
, O O
for O O
a O O
model O O
with O O
one O O
basis O B
vector O I
ϕ O O
= O O
( O O
φ O O
( O O
x1 O O
) O O
, O O
φ O O
( O O
x2 O O
) O O
) O O
t O O
, O O
which O O
is O O
poorly O O
aligned O O
with O O
the O O
target O O
data O O
vector O O
t. O O
on O O
the O O
left O O
we O O
see O O
a O O
model O O
having O O
only O O
isotropic B B
noise O O
, O O
so O O
that O O
c O O
= O O
β−1i O O
, O O
corresponding O O
to O O
α O O
= O O
∞ O O
, O O
with O O
β O O
set O O
to O O
its O O
most O O
probable O O
value O O
. O O
( O O
2.240 O O
) O O
section O O
2.3 O O
2.5. O O
nonparametric B O
methods I O
throughout O O
this O O
chapter O O
, O O
we O O
have O O
focussed O O
on O O
the O O
use O O
of O O
probability B B
distributions O O
having O O
speciﬁc O O
functional B B
forms O O
governed O O
by O O
a O O
small O O
number O O
of O O
parameters O O
whose O O
values O O
are O O
to O O
be O O
determined O O
from O O
a O O
data O O
set O O
. O O
the O O
complete-data O O
log O O
likelihood O O
function O O
is O O
then O O
given O O
by O O
ln O O
p O O
( O O
t O O
, O O
w|α O O
, O O
β O O
) O O
= O O
ln O O
p O O
( O O
t|w O O
, O O
β O O
) O O
+ O O
ln O O
p O O
( O O
w|α O O
) O O
( O O
9.61 O O
) O O
9.3. O O
an O O
alternative O O
view O O
of O O
em O O
449 O O
where O O
the O O
likelihood O B
p O O
( O O
t|w O O
, O O
β O O
) O O
and O O
the O O
prior B B
p O O
( O O
w|α O O
) O O
are O O
given O O
by O O
( O O
3.10 O O
) O O
and O O
( O O
3.52 O O
) O O
, O O
respectively O O
, O O
and O O
y O O
( O O
x O O
, O O
w O O
) O O
is O O
given O O
by O O
( O O
3.3 O O
) O O
. O O
the O O
training B B
data O O
set O O
comprises O O
n O O
input O O
vectors O O
x1 O O
, O O
. O O
t O O
1 O O
0 O O
−1 O O
0 O O
x O O
1 O O
7.1.5 O O
computational B B
learning I I
theory I I
historically O O
, O O
support B B
vector I I
machines O O
have O O
largely O O
been O O
motivated O O
and O O
analysed O O
using O O
a O O
theoretical O O
framework O O
known O O
as O O
computational B O
learning I I
theory I I
, O O
also O O
some- O O
times O O
called O O
statistical B O
learning I I
theory I I
( O O
anthony O O
and O O
biggs O O
, O O
1992 O O
; O O
kearns O O
and O O
vazi- O O
rani O O
, O O
1994 O O
; O O
vapnik O O
, O O
1995 O O
; O O
vapnik O O
, O O
1998 O O
) O O
. O O
the O O
resulting O O
formulae O O
will O O
then O O
be O O
illustrated O O
using O O
a O O
simple O O
layered O O
network O O
structure O O
having O O
a O O
single O O
layer O O
of O O
sigmoidal O B
hidden O O
units O O
together O O
with O O
a O O
sum-of-squares B B
error I I
. O O
if O O
we O O
use O O
q O O
( O O
x O O
) O O
to O O
construct O O
a O O
coding O O
scheme O O
for O O
the O O
purpose O O
of O O
transmitting O O
values O O
of O O
x O O
to O O
a O O
receiver O O
, O O
then O O
the O O
average O O
additional O O
amount O O
of O O
information O O
( O O
in O O
nats B O
) O O
required O O
to O O
specify O O
the O O
value O O
of O O
x O O
( O O
assuming O O
we O O
choose O O
an O O
efﬁcient O O
coding O O
scheme O O
) O O
as O O
a O O
result O O
of O O
using O O
q O O
( O O
x O O
) O O
instead O O
of O O
the O O
true O O
distribution O O
p O O
( O O
x O O
) O O
is O O
given O O
by O O
p O O
( O O
x O O
) O O
ln O O
q O O
( O O
x O O
) O O
dx O O
− O O
kl O O
( O O
p O O
( O O
cid:6 O O
) O O
q O O
) O O
= O O
− O O
p O O
( O O
x O O
) O O
ln O O
p O O
( O O
x O O
) O O
dx O O
( O O
cid:16 O O
) O O
( O O
cid:15 O O
) O O
( O O
cid:6 O O
) O O
− O O
( O O
cid:12 O O
) O O
( O O
cid:13 O O
) O O
( O O
cid:6 O O
) O O
( O O
cid:6 O O
) O O
= O O
− O O
p O O
( O O
x O O
) O O
ln O O
q O O
( O O
x O O
) O O
p O O
( O O
x O O
) O O
dx O O
. O O
thus O O
, O O
if O O
{ O O
x1 O O
, O O
x2 O O
, O O
x3 O O
} O O
is O O
a O O
maximal B O
clique I B
and O O
we O O
deﬁne O O
an O O
arbitrary O O
function O O
over O O
this O O
clique B B
, O O
then O O
including O O
another O O
factor O O
deﬁned O O
over O O
a O O
subset O O
of O O
these O O
variables O O
would O O
be O O
redundant O O
. O O
it O O
can O O
be O O
shown O O
that O O
both O O
the O O
k-nearest-neighbour O O
density B B
estimator O O
and O O
the O O
kernel B O
density I I
estimator I O
converge O O
to O O
the O O
true O O
probability B B
density O O
in O O
the O O
limit O O
n O O
→ O O
∞ O O
provided O O
v O O
shrinks O O
suitably O O
with O O
n O O
, O O
and O O
k O O
grows O O
with O O
n O O
( O O
duda O O
and O O
hart O O
, O O
1973 O O
) O O
. O O
similarly O O
, O O
µβ O O
( O O
xn O O
) O O
can O O
be O O
viewed O O
as O O
a O O
message O O
passed O O
backwards O O
8.4. O O
inference B B
in O O
graphical O O
models O O
397 O O
µα O O
( O O
xn−1 O O
) O O
µα O O
( O O
xn O O
) O O
µβ O O
( O O
xn O O
) O O
µβ O O
( O O
xn+1 O O
) O O
x1 O O
xn−1 O O
xn O O
xn+1 O O
xn O O
figure O O
8.38 O O
the O O
marginal B B
distribution O O
p O O
( O O
xn O O
) O O
for O O
a O O
node B B
xn O O
along O O
the O O
chain O O
is O O
ob- O O
tained O O
by O O
multiplying O O
the O O
two O O
messages O O
µα O O
( O O
xn O O
) O O
and O O
µβ O O
( O O
xn O O
) O O
, O O
and O O
then O O
normaliz- O O
ing O O
. O O
similarly O O
, O O
the O O
probability B B
of O O
starting O O
in O O
11.5. O O
the O O
hybrid O O
monte O O
carlo O O
algorithm O O
553 O O
ri O O
( O O
cid:4 O O
) O O
i O O
r O O
zi O O
( O O
cid:4 O O
) O O
i O O
z O O
figure O O
11.14 O O
each O O
step O O
of O O
the O O
leapfrog O O
algorithm O O
( O O
11.64 O O
) O O
– O O
( O O
11.66 O O
) O O
modiﬁes O O
either O O
a O O
position B O
variable I O
zi O O
or O O
a O O
momentum B O
variable I O
ri O O
. O O
finally O O
, O O
show O O
that O O
this O O
implies O O
h O O
( O O
p O O
) O O
must O O
take O O
the O O
form O O
h O O
( O O
p O O
) O O
∝ O O
ln O O
p. O O
1.29 O O
( O O
( O O
cid:1 O O
) O O
) O O
www O O
consider O O
an O O
m-state O O
discrete O O
random O O
variable O O
x O O
, O O
and O O
use O O
jensen O O
’ O O
s O O
in- O O
equality O O
in O O
the O O
form O O
( O O
1.115 O O
) O O
to O O
show O O
that O O
the O O
entropy B B
of O O
its O O
distribution O O
p O O
( O O
x O O
) O O
satisﬁes O O
h O O
[ O O
x O O
] O O
( O O
cid:1 O O
) O O
ln O O
m. O O
1.30 O O
( O O
( O O
cid:1 O O
) O O
( O O
cid:1 O O
) O O
) O O
evaluate O O
the O O
kullback-leibler O O
divergence O O
( O O
1.113 O O
) O O
between O O
two O O
gaussians O O
p O O
( O O
x O O
) O O
= O O
n O O
( O O
x|µ O O
, O O
σ2 O O
) O O
and O O
q O O
( O O
x O O
) O O
= O O
n O O
( O O
x|m O O
, O O
s2 O O
) O O
. O O
before O O
giving O O
a O O
proof O O
, O O
we O O
ﬁrst O O
discuss O O
a O O
generalization B B
, O O
known O O
as O O
the O O
metropolis-hastings O O
algorithm O O
( O O
hast- O O
ings O O
, O O
1970 O O
) O O
, O O
to O O
the O O
case O O
where O O
the O O
proposal B B
distribution I I
is O O
no O O
longer O O
a O O
symmetric O O
function O O
of O O
its O O
arguments O O
. O O
truth O O
and O O
probability B B
. O O
13.14 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
use O O
the O O
deﬁnition O O
( O O
8.67 O O
) O O
of O O
the O O
messages O O
passed O O
from O O
a O O
factor O B
node O I
to O O
a O O
variable O O
node B B
in O O
a O O
factor B B
graph I I
, O O
together O O
with O O
the O O
expression O O
( O O
13.6 O O
) O O
for O O
the O O
joint O O
distribution O O
in O O
a O O
hidden O O
markov O O
model O O
, O O
to O O
show O O
that O O
the O O
deﬁnition O O
( O O
13.52 O O
) O O
of O O
the O O
beta O B
message O O
is O O
the O O
same O O
as O O
the O O
deﬁnition O O
( O O
13.35 O O
) O O
. O O
using O O
the O O
standard O O
result O O
e O O
[ O O
τ O O
] O O
= O O
an O O
/bn O O
for O O
the O O
mean B B
of O O
a O O
gamma B B
distribution I I
, O O
together O O
with O O
( O O
10.29 O O
) O O
and O O
( O O
10.30 O O
) O O
, O O
we O O
have O O
( O O
cid:31 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
1 O O
e O O
[ O O
τ O O
] O O
= O O
e O O
1 O O
n O O
( O O
xn O O
− O O
µ O O
) O O
2 O O
= O O
x2 O O
− O O
2xe O O
[ O O
µ O O
] O O
+ O O
e O O
[ O O
µ2 O O
] O O
. O O
one O O
advantage O O
of O O
approach O O
3 O O
is O O
that O O
it O O
can O O
correctly O O
extrapolate O O
well O O
beyond O O
the O O
range O O
of O O
transformations O O
included O O
in O O
the O O
training B B
set I I
. O O
because O O
the O O
graph O O
for O O
the O O
hidden O O
markov O O
model O O
is O O
a O O
directed B B
tree O I
, O O
this O O
problem O O
can O O
be O O
solved O O
exactly O O
using O O
the O O
max-sum B B
algorithm I I
. O O
following O O
nelder O O
and O O
wedderburn O O
( O O
1972 O O
) O O
, O O
we O O
deﬁne O O
a O O
generalized B B
linear I I
model I I
to O O
be O O
one O O
for O O
which O O
y O O
is O O
a O O
nonlinear O O
function O O
of O O
a O O
linear O O
combination O O
of O O
the O O
input O O
( O O
or O O
feature O O
) O O
variables O O
so O O
that O O
y O O
= O O
f O O
( O O
wtφ O O
) O O
( O O
4.120 O O
) O O
where O O
f O O
( O O
· O O
) O O
is O O
known O O
as O O
the O O
activation B B
function I I
in O O
the O O
machine O O
learning O O
literature O O
, O O
and O O
−1 O O
( O O
· O O
) O O
is O O
known O O
as O O
the O O
link B B
function I I
in O O
statistics O O
. O O
soft B O
weight I I
sharing I I
. O O
in O O
the O O
case O O
of O O
an O O
undirected B B
graph I I
, O O
a O O
tree B B
is O O
deﬁned O O
as O O
a O O
graph O O
in O O
which O O
there O O
is O O
one O O
, O O
and O O
only O O
one O O
, O O
path O O
between O O
any O O
pair O O
of O O
nodes O O
. O O
in O O
a O O
numerical O O
implementation O O
of O O
the O O
variational B B
approach O O
it O O
is O O
important O O
to O O
take O O
account O O
of O O
such O O
additional O O
factorizations O O
. O O
in O O
particular O O
, O O
the O O
solution O O
for O O
q O O
( O O
cid:1 O O
) O O
( O O
µ O O
, O O
λ O O
) O O
is O O
given O O
by O O
the O O
product O O
of O O
an O O
independent B B
distribution O O
q O O
( O O
cid:1 O O
) O O
( O O
µk O O
, O O
λk O O
) O O
over O O
each O O
of O O
the O O
components O O
k O O
of O O
the O O
mixture B B
, O O
whereas O O
the O O
variational B B
posterior O O
distribution O O
q O O
( O O
cid:1 O O
) O O
( O O
z O O
) O O
over O O
the O O
latent O B
variables O O
, O O
given O O
by O O
( O O
10.48 O O
) O O
, O O
factorizes O O
into O O
an O O
independent B B
distribution O O
q O O
( O O
cid:1 O O
) O O
( O O
zn O O
) O O
for O O
each O O
observation O O
n O O
( O O
note O O
that O O
it O O
does O O
not O O
further O O
factorize O O
with O O
respect O O
to O O
k O O
because O O
, O O
for O O
each O O
value O O
of O O
n O O
, O O
the O O
znk O O
are O O
constrained O O
to O O
sum O O
to O O
one O O
over O O
k O O
) O O
. O O
a O O
markov O O
chain O O
is O O
called O O
homogeneous B B
if O O
the O O
transition O O
probabilities O O
are O O
the O O
same O O
for O O
all O O
m. O O
the O O
marginal B B
probability I I
for O O
a O O
particular O O
variable O O
can O O
be O O
expressed O O
in O O
terms O O
of O O
the O O
marginal B B
probability I I
for O O
the O O
previous O O
variable O O
in O O
the O O
chain O O
in O O
the O O
form O O
p O O
( O O
z O O
( O O
m+1 O O
) O O
) O O
= O O
p O O
( O O
z O O
( O O
m+1 O O
) O O
|z O O
( O O
m O O
) O O
) O O
p O O
( O O
z O O
( O O
m O O
) O O
) O O
. O O
the O O
negative O O
of O O
the O O
bias B B
is O O
sometimes O O
called O O
a O O
threshold O O
. O O
consider O O
a O O
simple O O
directed B B
graph O O
over O O
4 O O
nodes O O
shown O O
in O O
figure O O
8.33. O O
the O O
joint O O
distribution O O
for O O
the O O
directed B B
graph O O
takes O O
the O O
form O O
p O O
( O O
x O O
) O O
= O O
p O O
( O O
x1 O O
) O O
p O O
( O O
x2 O O
) O O
p O O
( O O
x3 O O
) O O
p O O
( O O
x4|x1 O O
, O O
x2 O O
, O O
x3 O O
) O O
. O O
in O O
many O O
cases O O
, O O
these O O
symmetries B O
in O O
weight O O
space O O
are O O
of O O
little O O
practi- O O
cal O O
consequence O O
, O O
although O O
in O O
section O O
5.7 O O
we O O
shall O O
encounter O O
a O O
situation O O
in O O
which O O
we O O
need O O
to O O
take O O
them O O
into O O
account O O
. O O
( O O
5.148 O O
) O O
k=1 O O
this O O
is O O
an O O
example O O
of O O
a O O
heteroscedastic B O
model O O
since O O
the O O
noise O O
variance B B
on O O
the O O
data O O
is O O
a O O
function O O
of O O
the O O
input O O
vector O O
x. O O
instead O O
of O O
gaussians O O
, O O
we O O
can O O
use O O
other O O
distribu- O O
tions O O
for O O
the O O
components O O
, O O
such O O
as O O
bernoulli O O
distributions O O
if O O
the O O
target O O
variables O O
are O O
binary O O
rather O O
than O O
continuous O O
. O O
the O O
large B O
margin I I
solution O O
has O O
similar O O
behaviour O O
. O O
furthermore O O
, O O
we O O
see O O
that O O
the O O
parameter O O
ν O O
can O O
be O O
interpreted O O
as O O
a O O
effective O B
number O I
of O I
pseudo-observations O O
in O O
the O O
prior B B
, O O
each O O
of O O
which O O
has O O
a O O
value O O
for O O
the O O
sufﬁcient O B
statistic O O
u O O
( O O
x O O
) O O
given O O
by O O
χ O O
. O O
as O O
we O O
have O O
seen O O
, O O
the O O
conjugate B B
prior I I
distri- O O
bution O O
for O O
µ O O
in O O
this O O
case O O
is O O
a O O
gaussian O O
p O O
( O O
µ|µ0 O O
, O O
σ2 O O
0 O O
) O O
, O O
and O O
we O O
obtain O O
a O O
0 O O
→ O O
∞ O O
. O O
random O O
ﬁeld O O
model O O
whose O O
undirected B B
graph I I
is O O
shown O O
in O O
figure O O
8.31. O O
this O O
graph O O
has O O
two O O
types O O
of O O
cliques O O
, O O
each O O
of O O
which O O
contains O O
two O O
variables O O
. O O
our O O
results O O
for O O
the O O
marginal B B
and O O
conditional B B
distributions O O
of O O
a O O
partitioned B B
gaus- O O
sian O O
are O O
summarized O O
below O O
. O O
the O O
mean B B
of O O
the O O
predictive O B
distribu- O O
tion O O
for O O
the O O
rvm O O
is O O
shown O O
by O O
the O O
red O O
line O O
, O O
and O O
the O O
one O O
standard- O O
deviation O O
predictive B O
distribution I I
is O O
shown O O
by O O
the O O
shaded O O
region O O
. O O
in O O
fact O O
, O O
this O O
would O O
also O O
be O O
the O O
case O O
if O O
, O O
instead O O
of O O
observing O O
the O O
fuel O O
gauge O O
directly O O
, O O
we O O
observed O O
the O O
state O O
of O O
some O O
descendant O O
of O O
g. O O
note O O
that O O
the O O
probability B B
p O O
( O O
f O O
= O O
0|g O O
= O O
0 O O
, O O
b O O
= O O
0 O O
) O O
( O O
cid:7 O O
) O O
0.111 O O
is O O
greater O O
than O O
the O O
prior B B
probability O O
p O O
( O O
f O O
= O O
0 O O
) O O
= O O
0.1 O O
because O O
the O O
observation O O
that O O
the O O
fuel O O
gauge O O
reads O O
zero O O
still O O
provides O O
some O O
evidence O O
in O O
favour O O
of O O
an O O
empty O O
fuel O O
tank O O
. O O
inference B B
in O O
graphical O O
models O O
. O O
here O O
we O O
mention O O
one O O
of O O
particular O O
practical O O
importance O O
called O O
the O O
left-to-right B O
hmm O O
, O O
which O O
is O O
obtained O O
by O O
setting O O
the O O
elements O O
ajk O O
of O O
a O O
to O O
zero O O
if O O
k O O
< O O
j O O
, O O
as O O
illustrated O O
in O O
the O O
section O O
8.1.2 O O
614 O O
13. O O
sequential B B
data I I
figure O O
13.9 O O
example O O
of O O
the O O
state O O
transition O O
diagram O O
for O O
a O O
3-state O O
left-to-right B O
hidden O O
markov O O
model O O
. O O
as O O
with O O
rejection O B
and O O
importance B B
sampling I I
, O O
we O O
again O O
sample O O
from O O
a O O
proposal B B
distribution I O
. O O
thus O O
the O O
conditional B B
independence I I
statement O O
a O O
⊥⊥ O O
b O O
| O O
c O O
does O O
not O O
follow O O
from O O
this O O
graph O O
. O O
wavelets B O
are O O
most O O
applicable O O
when O O
the O O
input O O
values O O
live O O
140 O O
3. O O
linear O O
models O O
for B O
regression I I
1 O O
0.5 O O
0 O O
−0.5 O O
−1 O O
−1 O O
1 O O
0.75 O O
0.5 O O
0.25 O O
0 O O
−1 O O
0 O O
1 O O
1 O O
0.75 O O
0.5 O O
0.25 O O
0 O O
−1 O O
0 O O
1 O O
0 O O
1 O O
figure O O
3.1 O O
examples O O
of O O
basis O O
functions O O
, O O
showing O O
polynomials O O
on O O
the O O
left O O
, O O
gaussians O O
of O O
the O O
form O O
( O O
3.4 O O
) O O
in O O
the O O
centre O O
, O O
and O O
sigmoidal O O
of O O
the O O
form O O
( O O
3.5 O O
) O O
on O O
the O O
right O O
. O O
show O O
that O O
the O O
mean B B
and O O
variance B B
of O O
their O O
sum O O
satisﬁes O O
e O O
[ O O
x O O
+ O O
z O O
] O O
= O O
e O O
[ O O
x O O
] O O
+ O O
e O O
[ O O
z O O
] O O
var O O
[ O O
x O O
+ O O
z O O
] O O
= O O
var O O
[ O O
x O O
] O O
+ O O
var O O
[ O O
z O O
] O O
. O O
the O O
multiplicity B O
w O O
is O O
also O O
known O O
as O O
the O O
weight O B
of O I
the O O
macrostate B O
. O O
two O O
unrelated O O
events O O
will O O
be O O
statistically O O
independent B B
and O O
so O O
p O O
( O O
x O O
, O O
y O O
) O O
= O O
p O O
( O O
x O O
) O O
p O O
( O O
y O O
) O O
. O O
j=1 O O
( O O
9.56 O O
) O O
9.3. O O
an O O
alternative O O
view O O
of O O
em O O
447 O O
if O O
we O O
consider O O
the O O
sum O O
over O O
n O O
in O O
( O O
9.55 O O
) O O
, O O
we O O
see O O
that O O
the O O
responsibilities O O
enter O O
only O O
through O O
two O O
terms O O
, O O
which O O
can O O
be O O
written O O
as O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
1 O O
nk O O
nk O O
= O O
xk O O
= O O
γ O O
( O O
znk O O
) O O
n O O
( O O
cid:2 O O
) O O
γ O O
( O O
znk O O
) O O
xn O O
n=1 O O
( O O
9.57 O O
) O O
( O O
9.58 O O
) O O
where O O
nk O O
is O O
the O O
effective O O
number O I
of O I
data O O
points O O
associated O O
with O O
component O O
k. O O
in O O
the O O
m O O
step O O
, O O
we O O
maximize O O
the O O
expected O O
complete-data O O
log O O
likelihood O O
with O O
respect O O
to O O
the O O
parameters O O
µk O O
and O O
π. O O
if O O
we O O
set O O
the O O
derivative B B
of O O
( O O
9.55 O O
) O O
with O O
respect O O
to O O
µk O O
equal O O
to O O
zero O O
and O O
rearrange O O
the O O
terms O O
, O O
we O O
obtain O O
exercise O O
9.15 O O
exercise O O
9.16 O O
exercise O O
9.17 O O
section O O
9.4 O O
µk O O
= O O
xk O O
. O O
4.2.1 O O
continuous O O
inputs O O
4.2.2 O O
maximum B B
likelihood I I
solution O O
. O O
this O O
approach O O
requires O O
some O O
straightforward O O
sion B O
density O B
is O O
simply O O
modiﬁcations O O
to O O
the O O
em O O
optimization O O
procedure O O
( O O
rabiner O O
, O O
1989 O O
) O O
. O O
if O O
both O O
the O O
latent O O
and O O
the O O
observed O O
variables O O
are O O
gaussian O O
( O O
with O O
a O O
linear-gaussian O O
dependence O O
of O O
the O O
conditional B B
distributions O O
on O O
their O O
parents O O
) O O
, O O
then O O
we O O
obtain O O
the O O
linear B B
dynamical I I
system I I
. O O
78 O O
2. O O
probability B B
distributions O O
figure O O
2.5 O O
plots O O
of O O
the O O
dirichlet O O
distribution O O
over O O
three O O
variables O O
, O O
where O O
the O O
two O O
horizontal O O
axes O O
are O O
coordinates O O
in O O
the O O
plane O B
of O O
the O O
simplex B O
and O O
the O O
vertical O O
axis O O
corresponds O O
to O O
the O O
value O O
of O O
the O O
density B B
. O O
furthermore O O
( O O
unlike O O
the O O
laplace O O
method O O
) O O
, O O
the O O
variational B B
approach O O
is O O
optimizing O O
a O O
well O O
deﬁned O O
objective O O
function O O
given O O
by O O
a O O
rigourous O O
bound O O
on O O
the O O
model B O
evidence I I
. O O
to O O
do O O
this O O
, O O
we O O
must O O
ﬁrst O O
ﬁnd O O
a O O
( O O
local B B
) O O
maximum O O
of O O
the O O
posterior O O
, O O
and O O
this O O
must O O
be O O
done O O
using O O
iterative O O
numerical O O
optimization O O
. O O
some O O
justiﬁcation O O
for O O
the O O
gaussian O O
assumption O O
comes O O
from O O
the O O
central B B
limit I O
theorem I O
by O O
noting O O
that O O
y O O
= O O
wtx O O
is O O
the O O
sum O O
of O O
a O O
set O O
of O O
random O O
variables O O
. O O
( O O
rms O O
) O O
error B B
deﬁned O O
by O O
erms O O
= O O
2e O O
( O O
w O O
( O O
cid:1 O O
) O O
) O O
/n O O
( O O
cid:3 O O
) O O
( O O
1.3 O O
) O O
in O O
which O O
the O O
division O O
by O O
n O O
allows O O
us O O
to O O
compare O O
different O O
sizes O O
of O O
data O O
sets O O
on O O
an O O
equal O O
footing O O
, O O
and O O
the O O
square O O
root O O
ensures O O
that O O
erms O O
is O O
measured O O
on O O
the O O
same O O
scale O O
( O O
and O O
in O O
the O O
same O O
units O O
) O O
as O O
the O O
target O O
variable O I
t. O O
graphs O O
of O O
the O O
training B B
and O O
test B O
set I I
rms O O
errors O O
are O O
shown O O
, O O
for O O
various O O
values O O
of O O
m O O
, O O
in O O
figure O O
1.5. O O
the O O
test B O
set I I
error O O
is O O
a O O
measure O O
of O O
how O O
well O O
we O O
are O O
doing O O
in O O
predicting O O
the O O
values O O
of O O
t O O
for O O
new O O
data O O
observations O O
of O O
x. O O
we O O
note O O
from O O
figure O O
1.5 O O
that O O
small O O
values O O
of O O
m O O
give O O
relatively O O
large O O
values O O
of O O
the O O
test B O
set I I
error O O
, O O
and O O
this O O
can O O
be O O
attributed O O
to O O
the O O
fact O O
that O O
the O O
corresponding O O
polynomials O O
are O O
rather O O
inﬂexible O O
and O O
are O O
incapable O O
of O O
capturing O O
the O O
oscillations O O
in O O
the O O
function O O
sin O O
( O O
2πx O O
) O O
. O O
however O O
, O O
instead O O
of O O
using O O
this O O
optimal O O
boundary O O
, O O
they O O
determine O O
the O O
best O O
hyperplane O O
by O O
minimizing O O
the O O
probability B B
of O O
error B B
relative O O
to O O
the O O
learned O O
density B B
model O O
. O O
2. O O
choose O O
three O O
random O O
numbers O O
f1 O O
, O O
f2 O O
and O O
f3 O O
from O O
the O O
uniform B B
distribution I I
over O O
( O O
0 O O
, O O
1 O O
) O O
and O O
deﬁne O O
foil O O
= O O
f1 O O
f1 O O
+ O O
f2 O O
+ O O
f3 O O
, O O
fwater O O
= O O
f2 O O
f1 O O
+ O O
f2 O O
+ O O
f3 O O
. O O
figure O O
8.51 O O
shows O O
a O O
simple O O
4-node O O
factor O B
figure O O
8.51 O O
a O O
simple O O
factor B B
graph I I
used O O
to O O
illustrate O O
the O O
sum-product B B
algorithm I I
. O O
this O O
is O O
an O O
example O O
of O O
a O O
rather O O
common O O
operation O O
associated O O
with O O
gaussian O O
distributions O O
, O O
sometimes O O
called O O
‘ O O
completing B B
the I I
square I I
’ O O
, O O
in O O
which O O
we O O
are O O
given O O
a O O
quadratic O O
form O O
deﬁning O O
the O O
exponent O O
terms O O
in O O
a O O
gaussian O O
distribution O O
, O O
and O O
we O O
need O O
to O O
determine O O
the O O
corresponding O O
mean B B
and O O
covariance B B
. O O
using O O
the O O
technique O O
of O O
completing B B
the I I
square I I
, O O
we O O
can O O
identify O O
the O O
mean B B
and O O
precision O O
of O O
this O O
gaussian O O
, O O
giving O O
section O O
2.3.1 O O
q O O
( O O
cid:1 O O
) O O
( O O
z1 O O
) O O
= O O
n O O
( O O
z1|m1 O O
, O O
λ−1 O O
11 O O
) O O
where O O
m1 O O
= O O
µ1 O O
− O O
λ−1 O O
11 O O
λ12 O O
( O O
e O O
[ O O
z2 O O
] O O
− O O
µ2 O O
) O O
. O O
1.10 O O
( O O
( O O
cid:1 O O
) O O
) O O
www O O
suppose O O
that O O
the O O
two O O
variables O O
x O O
and O O
z O O
are O O
statistically O O
independent B B
. O O
indeed O O
, O O
the O O
evidence O O
is O O
not O O
deﬁned O O
if O O
the O O
prior B B
is O O
improper B O
, O O
as O O
can O O
be O O
seen O O
by O O
noting O O
that O O
an O O
improper B B
prior I I
has O O
an O O
arbitrary O O
scaling B O
factor I O
( O O
in O O
other O O
words O O
, O O
the O O
normalization O O
coefﬁcient O O
is O O
not O O
deﬁned O O
because O O
the O O
distribution O O
can O O
not O O
be O O
normalized O O
) O O
. O O
write O O
down O O
expressions O O
for O O
the O O
conditional B B
distributions O O
p O O
( O O
µ|x O O
, O O
τ O O
) O O
and O O
p O O
( O O
τ|x O O
, O O
µ O O
) O O
that O O
would O O
be O O
required O O
in O O
order O O
to O O
apply O O
gibbs O O
sampling O O
to O O
the O O
posterior O O
distribution O O
p O O
( O O
µ O O
, O O
τ|x O O
) O O
. O O
by O O
deﬁnition O O
, O O
the O O
required O O
marginal B B
is O O
obtained O O
by O O
summing O O
the O O
joint O O
distribution O O
over O O
all O O
variables O O
except O O
xn O O
, O O
so O O
that O O
( O O
cid:2 O O
) O O
··· O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
( O O
cid:2 O O
) O O
··· O O
p O O
( O O
xn O O
) O O
= O O
p O O
( O O
x O O
) O O
. O O
this O O
will O O
cause O O
the O O
lower B B
bound I I
l O O
to O O
increase O O
( O O
unless O O
it O O
is O O
already O O
at O O
a O O
maximum O B
) O O
, O O
which O O
will O O
necessarily O O
cause O O
the O O
corresponding O O
log O O
likelihood O O
function O O
to O O
increase O O
. O O
consequently O O
they O O
play O O
no O O
role O O
in O O
d-separation B B
. O O
hence O O
, O O
derive O O
a O O
recursive O O
expression O O
analogous O O
to O O
( O O
5.87 O O
) O O
for O O
incrementing O O
the O O
number O O
n O O
of O O
patterns O O
and O O
a O O
similar O O
expres- O O
sion B O
for O O
incrementing O O
the O O
number O O
k O O
of O O
outputs O O
. O O
verify O O
that O O
this O O
reduces O O
to O O
the O O
reject O B
criterion O O
discussed O O
in O O
section O O
1.5.3 O O
when O O
the O O
loss B B
matrix I I
is O O
given O O
by O O
lkj O O
= O O
1 O O
− O O
ikj O O
. O O
furthermore O O
, O O
there O O
will O O
typically O O
be O O
multiple O O
inequivalent O O
stationary B B
points O O
and O O
in O O
particular O O
multiple O O
inequivalent O O
minima O O
. O O
10.36 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
consider O O
the O O
adf O O
approximation O O
scheme O O
discussed O O
in O O
section O O
10.7 O O
, O O
and O O
show O O
that O O
inclusion O O
of O O
the O O
factor O B
fj O O
( O O
θ O O
) O O
leads O O
to O O
an O O
update O O
of O O
the O O
model B O
evidence I I
of O O
the O O
form O O
pj O O
( O O
d O O
) O O
( O O
cid:7 O O
) O O
pj−1 O O
( O O
d O O
) O O
zj O O
( O O
10.242 O O
) O O
522 O O
10. O O
approximate O O
inference B B
where O O
zj O O
is O O
the O O
normalization O O
constant O O
deﬁned O O
by O O
( O O
10.197 O O
) O O
. O O
, O O
wk O O
) O O
= O O
( O O
ynj O O
− O O
tnj O O
) O O
φn O O
( O O
4.109 O O
) O O
n O O
( O O
cid:2 O O
) O O
n=1 O O
exercise O O
4.18 O O
210 O O
4. O O
linear O O
models O O
for O O
classification O O
( O O
cid:5 O O
) O O
k O O
tnk O O
= O O
1. O O
once O O
again O O
, O O
we O O
see O O
the O O
same O O
form O O
arising O O
where O O
we O O
have O O
made O O
use O O
of O O
for O O
the O O
gradient O O
as O O
was O O
found O O
for O O
the O O
sum-of-squares B B
error I I
function O O
with O O
the O O
linear O B
model O O
and O O
the O O
cross-entropy O B
error O I
for O O
the O O
logistic B B
regression I I
model O O
, O O
namely O O
the O O
prod- O O
uct O O
of O O
the O O
error B B
( O O
ynj O O
− O O
tnj O O
) O O
times O O
the O O
basis B B
function I I
φn O O
. O O
10.7.1 O O
example O O
: O O
the O O
clutter B B
problem I I
. O O
6.3 O O
( O O
( O O
cid:12 O O
) O O
) O O
the O O
nearest-neighbour O B
classiﬁer O O
( O O
section O O
2.5.2 O O
) O O
assigns O O
a O O
new O O
input O O
vector O O
x O O
to O O
the O O
same O O
class O O
as O O
that O O
of O O
the O O
nearest O O
input O O
vector O O
xn O O
from O O
the O O
training B B
set I I
, O O
where O O
in O O
the O O
simplest O O
case O O
, O O
the O O
distance O O
is O O
deﬁned O O
by O O
the O O
euclidean O O
metric O O
( O O
cid:5 O O
) O O
x O O
− O O
xn O O
( O O
cid:5 O O
) O O
2. O O
by O O
expressing O O
this O O
rule O O
in O O
terms O O
of O O
scalar O O
products O O
and O O
then O O
making O O
use O O
of O O
kernel O O
sub- O O
stitution O O
, O O
formulate O O
the O O
nearest-neighbour O B
classiﬁer O O
for O O
a O O
general O O
nonlinear O O
kernel O O
. O O
we O O
turn O O
now O O
to O O
a O O
speciﬁc O O
example O O
to O O
illustrate O O
the O O
use O O
of O O
undirected B B
graphs O O
. O O
nearest O O
neighbor O O
pat- O O
tern O O
classiﬁcation B B
. O O
i O O
' O O
and O O
, O O
, O O
' O O
using O O
maximum O B
likelihuo O I
< O O
l O O
, O O
to O O
write O O
`` O O
'' O O
'' O O
'' O O
n O O
lhe O O
likeliltood O O
function O O
, O O
we O O
need O O
an O O
`` O O
'' O O
pression O O
for O O
tl O O
> O O
o O O
marginal B B
distributioo O O
p O O
{ O O
`` O O
) O O
of O O
tl O O
> O O
o O O
~ O O
, O O
,'ed O O
... O O
riahle_ O O
this O O
is O O
exprt__sed O O
. O O
for O O
instance O O
, O O
if O O
we O O
have O O
ten O O
classes O O
each O O
with O O
equal O O
numbers O O
of O O
training B B
data O O
points O O
, O O
then O O
the O O
individual O O
classiﬁers O O
are O O
trained O O
on O O
data O O
sets O O
comprising O O
90 O O
% O O
negative O O
examples O O
and O O
only O O
10 O O
% O O
positive O O
examples O O
, O O
and O O
the O O
symmetry O O
of O O
the O O
original O O
problem O O
is O O
lost O O
. O O
this O O
bound O O
is O O
illustrated O O
in O O
the O O
right-hand O O
plot O O
of O O
figure O O
10.12. O O
we O O
see O O
that O O
the O O
bound O O
has O O
the O O
form O O
of O O
the O O
exponential O B
of O O
a O O
quadratic O O
function O O
of O O
x O O
, O O
which O O
will O O
prove O O
useful O O
when O O
we O O
seek O O
gaussian O O
representations O O
of O O
posterior O O
distributions O O
deﬁned O O
through O O
logistic B B
sigmoid I I
functions O O
. O O
in O O
chapter O O
3 O O
, O O
we O O
considered O O
linear B O
regression I I
models O O
of O O
the O O
form O O
y O O
( O O
x O O
, O O
w O O
) O O
= O O
wtφ O O
( O O
x O O
) O O
in O O
which O O
w O O
is O O
a O O
vector O O
of O O
parameters O O
and O O
φ O O
( O O
x O O
) O O
is O O
a O O
vector O O
of O O
ﬁxed O O
nonlinear O O
basis O O
functions O O
that O O
depend O O
on O O
the O O
input O O
vector O O
x. O O
we O O
showed O O
that O O
a O O
prior B B
distribution O O
over O O
w O O
induced O O
a O O
corresponding O O
prior B B
distribution O O
over O O
functions O O
y O O
( O O
x O O
, O O
w O O
) O O
. O O
the O O
human O O
interpretability O O
of O O
a O O
tree B B
model O O
such O O
as O O
cart O O
is O O
often O O
seen O O
as O O
its O O
major O O
strength O O
. O O
j∈wk O O
( O O
5.123 O O
) O O
( O O
5.124 O O
) O O
5.5. O O
regularization B B
in O O
neural O O
networks O O
259 O O
will O O
remain O O
unchanged O O
under O O
the O O
weight O B
transformations O O
provided O O
the O O
regularization B B
parameters O O
are O O
re-scaled O O
using O O
λ1 O O
→ O O
a1/2λ1 O O
and O O
λ2 O O
→ O O
c O O
( O O
cid:2 O O
) O O
the O O
regularizer O O
( O O
5.121 O O
) O O
corresponds O O
to O O
a O O
prior B B
of O O
the O O
form O O
( O O
cid:2 O O
) O O
−1/2λ2 O O
. O O
although O O
gradient-based O O
techniques O O
are O O
feasible O O
, O O
and O O
indeed O O
will O O
play O O
an O O
important O O
role O O
when O O
we O O
discuss O O
mixture O O
density O B
networks O O
in O O
chapter O O
5 O O
, O O
we O O
now O O
consider O O
an O O
alternative O O
approach O O
known O O
as O O
the O O
em O O
algorithm O O
which O O
has O O
broad O O
applicability O O
and O O
which O O
will O O
lay O O
the O O
foundations O O
for O O
a O O
discussion O O
of O O
variational B B
inference I I
techniques O O
in O O
chapter O O
10 O O
. O O
13 O O
sequential B B
data I I
13.1 O O
markov O O
models O O
. O O
by O O
removing O O
the O O
section O O
8.2 O O
384 O O
8. O O
graphical O O
models O O
figure O O
8.27 O O
an O O
example O O
of O O
an O O
undirected B B
graph I I
in O O
which O O
every O O
path O O
from O O
any O O
node B B
in O O
set O O
a O O
to O O
any O O
node B B
in O O
set O O
b O O
passes O O
through O O
at O O
least O O
one O O
node B B
in O O
set O O
c. O O
conse- O O
quently O O
the O O
conditional B B
independence I I
property O O
a O O
⊥⊥ O O
b O O
| O O
c O O
holds O O
for O O
any O O
probability B B
distribution O O
described O O
by O O
this O O
graph O O
. O O
the O O
rules O O
for O O
the O O
calculus B B
of I I
variations I I
mirror O O
those O O
of O O
standard O O
calculus O O
and O O
are O O
discussed O O
in O O
appendix O O
d. O O
many O O
problems O O
can O O
be O O
expressed O O
in O O
terms O O
of O O
an O O
optimization O O
problem O O
in O O
which O O
the O O
quantity O O
being O O
optimized O O
is O O
a O O
functional B B
. O O
6.4.4 O O
automatic B I
relevance I I
determination I I
in O O
the O O
previous O O
section O O
, O O
we O O
saw O O
how O O
maximum B B
likelihood I I
could O O
be O O
used O O
to O O
de- O O
termine O O
a O O
value O O
for O O
the O O
correlation O O
length-scale O O
parameter O O
in O O
a O O
gaussian O O
process O O
. O O
however O O
, O O
for O O
nodes O O
in O O
the O O
directed B B
graph O O
having O O
more O O
than O O
one O O
parent O O
, O O
this O O
is O O
not O O
sufﬁcient O B
. O O
( O O
10.17 O O
) O O
in O O
this O O
case O O
, O O
we O O
ﬁnd O O
that O O
the O O
optimal O O
solution O O
for O O
qj O O
( O O
zj O O
) O O
is O O
just O O
given O O
by O O
the O O
corre- O O
sponding O O
marginal B B
distribution O O
of O O
p O O
( O O
z O O
) O O
. O O
at O O
time O O
step O O
n O O
, O O
the O O
posterior O O
p O O
( O O
zn|xn O O
) O O
is O O
represented O O
as O O
a O O
mixture B B
distribution I I
, O O
shown O O
schematically O O
as O O
circles O O
whose O O
sizes O O
are O O
proportional O O
to O O
the O O
weights O O
w O O
( O O
l O O
) O O
n O O
. O O
combining B B
models I O
. O O
error B B
backpropagation I O
. O O
next O O
suppose O O
we O O
wish O O
to O O
ﬁnd O O
the O O
marginal B B
distributions O O
p O O
( O O
xs O O
) O O
associated O O
with O O
the O O
sets O O
of O O
variables O O
belonging O O
to O O
each O O
of O O
the O O
factors O O
. O O
5.21 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
extend O O
the O O
expression O O
( O O
5.86 O O
) O O
for O O
the O O
outer B B
product I I
approximation I I
of O O
the O O
hes- O O
sian O O
matrix O O
to O O
the O O
case O O
of O O
k O O
> O O
1 O O
output O O
units O O
. O O
however O O
, O O
for O O
the O O
purpose O O
of O O
solving O O
the O O
inference B B
problem O O
, O O
we O O
shall O O
always O O
be O O
conditioning O O
on O O
the O O
variables O O
x1 O O
, O O
. O O
in O O
the O O
stochastic B B
threshold O I
model O O
, O O
the O O
class O O
label O O
takes O O
the O O
value O O
t O O
= O O
1 O O
if O O
the O O
value O O
of O O
a O O
= O O
wtφ O O
exceeds O O
a O O
threshold O O
, O O
oth- O O
erwise O O
it O O
takes O O
the O O
value O O
t O O
= O O
0. O O
this O O
is O O
equivalent O O
to O O
an O O
activation B B
function I I
given O O
by O O
the O O
cumulative B O
distribution I O
function I O
f O O
( O O
a O O
) O O
. O O
in O O
an O O
undirected B B
graph I I
, O O
the O O
product O O
of O O
two O O
such O O
factors O O
would O O
simply O O
be O O
lumped O O
together O O
into O O
the O O
same O O
clique B B
potential O O
. O O
note O O
that O O
in O O
supervised B O
learning I O
problems O O
such O O
as O O
regres- O O
sion B B
( O O
and O O
classiﬁcation B B
) O O
, O O
we O O
are O O
not O O
seeking O O
to O O
model O O
the O O
distribution O O
of O O
the O O
input O O
variables O O
. O O
569 O O
11 O O
sampling B O
methods I I
11.1 O O
basic O O
sampling O B
algorithms O O
. O O
the O O
simplest O O
example O O
of O O
a O O
kernel B O
function I I
is O O
obtained O O
by O O
considering O O
the O O
identity O O
mapping O O
for O O
the O O
feature B O
space I I
in O O
( O O
6.1 O O
) O O
so O O
that O O
φ O O
( O O
x O O
) O O
= O O
x O O
, O O
in O O
which O O
case O O
k O O
( O O
x O O
, O O
x O O
( O O
cid:4 O O
) O O
) O O
= O O
xtx O O
( O O
cid:4 O O
) O O
. O O
as O O
such O O
, O O
they O O
can O O
never O O
generate O O
exact O O
results O O
, O O
and O O
so O O
their O O
strengths O O
and O O
weaknesses O O
are O O
complementary O O
to O O
those O O
of O O
sampling B O
methods I O
. O O
we O O
turn O O
now O O
to O O
the O O
second O O
ma- O O
jor O O
class O O
of O O
graphical O O
models O O
that O O
are O O
described O O
by O O
undirected B B
graphs O O
and O O
that O O
again O O
specify O O
both O O
a O O
factorization B B
and O O
a O O
set O O
of O O
conditional B B
independence I I
relations O O
. O O
( O O
7.101 O O
) O O
exercise O O
7.16 O O
these O O
two O O
solutions O O
are O O
illustrated O O
in O O
figure O O
7.11. O O
we O O
see O O
that O O
the O O
relative B B
size O O
of O O
the O O
quality O O
and O O
sparsity B B
terms O O
determines O O
whether O O
a O O
particular O O
basis O B
vector O I
will O O
be O O
pruned O O
from O O
the O O
model O O
or O O
not O O
. O O
we O O
group O O
the O O
target O O
variables O O
{ O O
tn O O
} O O
into O O
a O O
column O O
vector O O
that O O
we O O
denote O O
by O O
t O O
where O O
the O O
typeface O O
is O O
chosen O O
to O O
distinguish O O
it O O
from O O
a O O
single O O
observation O O
of O O
a O O
multivariate O O
target O O
, O O
which O O
would O O
be O O
denoted O O
t. O O
making O O
the O O
assumption O O
that O O
these O O
data O O
points O O
are O O
drawn O O
independently O O
from O O
the O O
distribution O O
( O O
3.8 O O
) O O
, O O
we O O
obtain O O
the O O
following O O
expression O O
for O O
the O O
likelihood B B
function I I
, O O
which O O
is O O
a O O
function O O
of O O
the O O
adjustable O O
parameters O O
w O O
and O O
β O O
, O O
in O O
the O O
form O O
p O O
( O O
t|x O O
, O O
w O O
, O O
β O O
) O O
= O O
n O O
( O O
tn|wtφ O O
( O O
xn O O
) O O
, O O
β O O
−1 O O
) O O
( O O
3.10 O O
) O O
where O O
we O O
have O O
used O O
( O O
3.3 O O
) O O
. O O
the O O
entropy B B
of O O
the O O
random O O
variable O O
x O O
is O O
then O O
h O O
[ O O
p O O
] O O
= O O
− O O
p O O
( O O
xi O O
) O O
ln O O
p O O
( O O
xi O O
) O O
. O O
the O O
relationship O O
between O O
early B O
stopping I O
and O O
weight B O
decay I O
can O O
be O O
made O O
quan- O O
titative O O
, O O
thereby O O
showing O O
that O O
the O O
quantity O O
τ O O
η O O
( O O
where O O
τ O O
is O O
the O O
iteration O O
index O O
, O O
and O O
η O O
is O O
the O O
learning B B
rate I O
parameter I O
) O O
plays O O
the O O
role O O
of O O
the O O
reciprocal O O
of O O
the O O
regularization B B
5.5. O O
regularization B B
in O O
neural O O
networks O O
261 O O
0.25 O O
0.2 O O
0.15 O O
0 O O
10 O O
20 O O
30 O O
40 O O
50 O B
0.45 O O
0.4 O O
0.35 O O
0 O O
10 O O
20 O O
30 O O
40 O O
50 O O
figure O O
5.12 O O
an O O
illustration O O
of O O
the O O
behaviour O O
of O O
training B B
set I I
error O O
( O O
left O O
) O O
and O O
validation B O
set I O
error O B
( O O
right O O
) O O
during O O
a O O
typical O O
training B B
session O O
, O O
as O O
a O O
function O O
of O O
the O O
iteration O O
step O O
, O O
for O O
the O O
sinusoidal B O
data I O
set O O
. O O
the O O
maximum O B
with O O
respect O O
to O O
x1 O O
could O O
then O O
be O O
found O O
by O O
differentiation O O
in O O
the O O
usual O O
way O O
, O O
to O O
give O O
the O O
stationary B B
value O O
x O O
( O O
cid:3 O O
) O O
1 O O
, O O
with O O
the O O
corresponding O O
value O O
of O O
x2 O O
given O O
by O O
x O O
( O O
cid:3 O O
) O O
one O O
problem O O
with O O
this O O
approach O O
is O O
that O O
it O O
may O O
be O O
difﬁcult O O
to O O
ﬁnd O O
an O O
analytic O O
solution O O
of O O
the O O
constraint O O
equation O O
that O O
allows O O
x2 O O
to O O
be O O
expressed O O
as O O
an O O
explicit O O
func- O O
tion O O
of O O
x1 O O
. O O
our O O
goal O O
is O O
to O O
ﬁnd O O
the O O
root O B
θ O O
( O O
cid:1 O O
) O O
at O O
which O O
f O O
( O O
θ O O
( O O
cid:1 O O
) O O
) O O
= O O
0. O O
if O O
we O O
had O O
a O O
large O O
data O O
set O O
of O O
observations O O
of O O
z O O
and O O
θ O O
, O O
then O O
we O O
could O O
model O O
the O O
regression B B
function I I
directly O O
and O O
then O O
obtain O O
an O O
estimate O O
of O O
its O O
root O O
. O O
then O O
, O O
if O O
z O O
is O O
a O O
vector O O
valued O O
random O O
variable O O
whose O O
components O O
are O O
independent B B
and O O
gaussian O O
distributed O O
with O O
zero O O
mean B B
and O O
unit O O
vari- O O
ance O O
, O O
then O O
y O O
= O O
µ O O
+ O O
lz O O
will O O
have O O
mean B B
µ O O
and O O
covariance B B
σ. O O
obviously O O
, O O
the O O
transformation O O
technique O O
depends O O
for O O
its O O
success O O
on O O
the O O
ability O O
to O O
calculate O O
and O O
then O O
invert O O
the O O
indeﬁnite O O
integral O O
of O O
the O O
required O O
distribution O O
. O O
10.4.1 O O
variational B B
message O O
passing O O
we O O
have O O
illustrated O B
the O O
application O O
of O O
variational B B
methods O O
by O O
considering O O
a O O
spe- O O
ciﬁc O O
model O O
, O O
the O O
bayesian O O
mixture O B
of O O
gaussians O O
, O O
in O O
some O O
detail O O
. O O
if O O
the O O
pattern O O
is O O
correctly O O
classiﬁed O O
, O O
then O O
the O O
weight B B
vector I I
remains O O
unchanged O O
, O O
whereas O O
if O O
it O O
is O O
incorrectly O O
classiﬁed O O
, O O
then O O
for O O
class O O
c1 O O
we O O
add O O
the O O
vector O O
φ O O
( O O
xn O O
) O O
onto O O
the O O
current O O
estimate O O
of O O
weight B B
vector I I
w O O
while O O
for O O
class O O
c2 O O
we O O
subtract O O
the O O
vector O O
φ O O
( O O
xn O O
) O O
from O O
w. O O
the O O
perceptron B B
learning O O
algorithm O O
is O O
illustrated O O
in O O
figure O O
4.7. O O
if O O
we O O
consider O O
the O O
effect O O
of O O
a O O
single O O
update O O
in O O
the O O
perceptron B B
learning O O
algorithm O O
, O O
we O O
see O O
that O O
the O O
contribution O O
to O O
the O O
error B B
from O O
a O O
misclassiﬁed O O
pattern O O
will O O
be O O
reduced O O
because O O
from O O
( O O
4.55 O O
) O O
we O O
have O O
−w O O
( O O
τ O O
+1 O O
) O O
tφntn O O
= O O
−w O O
( O O
τ O O
) O O
tφntn O O
− O O
( O O
φntn O O
) O O
tφntn O O
< O O
−w O O
( O O
τ O O
) O O
tφntn O O
( O O
4.56 O O
) O O
where O O
we O O
have O O
set O O
η O O
= O O
1 O O
, O O
and O O
made O O
use O O
of O O
( O O
cid:5 O O
) O O
φntn O O
( O O
cid:5 O O
) O O
2 O O
> O O
0. O O
of O O
course O O
, O O
this O O
does O O
not O O
imply O O
that O O
the O O
contribution O O
to O O
the O O
error B B
function I I
from O O
the O O
other O O
misclassiﬁed O O
patterns O O
will O O
have O O
been O O
reduced O O
. O O
consider O O
a O O
metropolis-hastings O O
sampling O O
step O O
involving O O
the O O
variable O O
zk O O
in O O
which O O
the O O
remaining O O
variables O O
z\k O O
remain O O
ﬁxed O O
, O O
and O O
for O O
which O O
the O O
transition B O
probability I I
from O O
z O O
to O O
z O O
( O O
cid:1 O O
) O O
is O O
given O O
by O O
qk O O
( O O
z O O
( O O
cid:1 O O
) O O
|z O O
) O O
= O O
p O O
( O O
z O O
( O O
cid:1 O O
) O O
k|z\k O O
) O O
. O O
the O O
factor O B
of O O
n O O
− O O
1 O O
in O O
the O O
denominator O O
of O O
the O O
bayesian O O
result O O
takes O O
account O O
of O O
the O O
fact O O
that O O
one O O
degree O O
of O O
free- O O
dom O O
has O O
been O O
used O O
in O O
ﬁtting O O
the O O
mean B B
and O O
removes O O
the O O
bias B B
of O O
maximum B B
likelihood I I
. O O
this O O
can O O
also O O
be O O
seen O O
from O O
the O O
result O O
( O O
2.16 O O
) O O
for O O
the O O
variance B B
of O O
the O O
beta B B
distribution I I
, O O
in O O
which O O
we O O
see O O
that O O
the O O
variance B B
goes O O
to O O
zero O O
for O O
a O O
→ O O
∞ O O
or O O
b O O
→ O O
∞ O O
. O O
in O O
section O O
10.6.3 O O
, O O
we O O
shall O O
demonstrate O O
how O O
the O O
variational B B
formalism O O
can O O
be O O
extended B B
to O O
the O O
case O O
where O O
there O O
are O O
unknown O O
hyper- O O
parameters O O
whose O O
values O O
are O O
to O O
be O O
inferred O O
from O O
the O O
data O O
. O O
there O O
will O O
always O O
exist O O
at O O
least O O
one O O
maximal B O
clique I B
that O O
contains O O
all O O
of O O
the O O
variables O O
in O O
the O O
factor O B
as O O
a O O
result O O
of O O
the O O
moralization B O
step O O
. O O
then O O
n O O
= O O
m O O
= O O
3 O O
and O O
µml O O
= O O
1. O O
in O O
this O O
case O O
, O O
the O O
maximum B B
likelihood I I
result O O
would O O
predict O O
that O O
all O O
future O O
observations O O
should O O
give O O
heads O O
. O O
if O O
we O O
interpret O O
the O O
λi O O
as O O
the O O
probability B B
distribution O O
over O O
a O O
discrete O O
variable O O
x O O
taking O O
the O O
values O O
{ O O
xi O O
} O O
, O O
then O O
( O O
1.115 O O
) O O
can O O
be O O
written O O
λif O O
( O O
xi O O
) O O
( O O
1.115 O O
) O O
( O O
cid:1 O O
) O O
i=1 O O
( O O
1.116 O O
) O O
where O O
e O O
[ O O
· O O
] O O
denotes O O
the O O
expectation B B
. O O
lhe O O
proioi O O
) O O
'jl O O
( O O
's O O
are O O
distribuled O O
at O O
random O O
, O O
and O O
during O O
the O O
training B B
process O O
they O O
'selr O O
organize O O
' O O
so O O
as O O
to O O
apl'ro~imalea O O
smoolh O O
manifold B B
. O O
a O O
simple O O
approach O O
to O O
the O O
problem O O
of O O
determining O O
the O O
network O O
parameters O O
is O O
to O O
make O O
an O O
analogy O O
with O O
the O O
discussion O O
of O O
polynomial B B
curve I I
ﬁtting I I
in O O
section O O
1.1 O O
, O O
and O O
therefore O O
to O O
minimize O O
a O O
sum-of-squares B B
error I I
function O O
. O O
( O O
5.22 O O
) O O
k O O
( O O
cid:14 O O
) O O
k=1 O O
exercise O O
5.5 O O
exercise O O
5.6 O O
e O O
( O O
w O O
) O O
= O O
− O O
n O O
( O O
cid:2 O O
) O O
k O O
( O O
cid:2 O O
) O O
taking O O
the O O
negative O O
logarithm O O
of O O
the O O
corresponding O O
likelihood B B
function I I
then O O
gives O O
the O O
following O O
error B O
function I O
{ O O
tnk O O
ln O O
ynk O O
+ O O
( O O
1 O O
− O O
tnk O O
) O O
ln O O
( O O
1 O O
− O O
ynk O O
) O O
} O O
( O O
5.23 O O
) O O
n=1 O O
k=1 O O
where O O
ynk O O
denotes O O
yk O O
( O O
xn O O
, O O
w O O
) O O
. O O
this O O
will O O
lead O O
to O O
more O O
complex O O
analytical O O
and O O
computa- O O
tional O O
properties O O
than O O
for O O
linear O O
regression B I
models O O
. O O
thus O O
we O O
should O O
choose O O
the O O
principal B O
subspace I I
to O O
be O O
aligned O O
with O O
the O O
eigenvector O O
having O O
the O O
larger O O
eigenvalue O O
. O O
13.2.3 O O
the O O
sum-product B B
algorithm I I
for O O
the O O
hmm O O
the O O
directed B B
graph O O
that O O
represents O O
the O O
hidden O O
markov O O
model O O
, O O
shown O O
in O O
fig- O O
ure O O
13.5 O O
, O O
is O O
a O O
tree B B
and O O
so O O
we O O
can O O
solve O O
the O O
problem O O
of O O
ﬁnding O O
local B B
marginals O O
for O O
the O O
hidden O O
variables O O
using O O
the O O
sum-product B B
algorithm I I
. O O
given O O
this O O
deﬁnition O O
of O O
likelihood O B
, O O
we O O
can O O
state O O
bayes O O
’ O O
theorem O O
in O O
words O O
posterior O O
∝ O O
likelihood O B
× O O
prior B B
( O O
1.44 O O
) O O
where O O
all O O
of O O
these O O
quantities O O
are O O
viewed O O
as O O
functions O O
of O O
w. O O
the O O
denominator O O
in O O
( O O
1.43 O O
) O O
is O O
the O O
normalization O O
constant O O
, O O
which O O
ensures O O
that O O
the O O
posterior O O
distribution O O
on O O
the O O
left-hand O O
side O O
is O O
a O O
valid O O
probability B B
density O O
and O O
integrates O O
to O O
one O O
. O O
when O O
one O O
more O O
( O O
variable O O
or O O
factor O O
) O O
node B B
is O O
added O O
, O O
it O O
can O O
be O O
connected O O
only O O
by O O
a O O
single O O
link B B
because O O
the O O
overall O O
graph O O
must O O
remain O O
a O O
tree B B
, O O
and O O
so O O
the O O
new O O
node B B
will O O
be O O
a O O
leaf O O
node B B
. O O
−∞ O O
( O O
1.25 O O
) O O
( O O
1.26 O O
) O O
under O O
a O O
nonlinear O O
change O O
of O O
variable O O
, O O
a O O
probability B B
density O O
transforms O O
differently O O
from O O
a O O
simple O O
function O O
, O O
due O O
to O O
the O O
jacobian O O
factor O O
. O O
in O O
fact O O
, O O
the O O
summation O O
in O O
( O O
13.11 O O
) O O
cor- O O
responds O O
to O O
summing O O
over O O
exponentially O O
many O O
paths O O
through O O
the O O
lattice B O
diagram I I
in O O
figure O O
13.7. O O
we O O
have O O
already O O
encountered O O
a O O
similar O O
difﬁculty O O
when O O
we O O
considered O O
the O O
infer- O O
ence O O
problem O O
for O O
the O O
simple O O
chain O O
of O O
variables O O
in O O
figure O O
8.32. O O
there O O
we O O
were O O
able O O
to O O
make O O
use O O
of O O
the O O
conditional B B
independence I I
properties O O
of O O
the O O
graph O O
to O O
re-order O O
the O O
summations O O
in O O
order O O
to O O
obtain O O
an O O
algorithm O O
whose O O
cost O O
scales O O
linearly O O
, O O
instead O O
of O O
exponentially O O
, O O
with O O
the O O
length O O
of O O
the O O
chain O O
. O O
this O O
allows O O
all O O
available O O
data O O
to O O
be O O
used O O
for O O
training O O
and O O
avoids O O
the O O
multiple O O
training B B
runs O O
for O O
each O O
model O O
associated O O
with O O
cross-validation B O
. O O
mode O O
[ O O
x O O
] O O
= O O
1 O O
if O O
µ O O
( O O
cid:2 O O
) O O
0.5 O O
, O O
0 O O
otherwise O O
the O O
bernoulli O O
is O O
a O O
special O O
case O O
of O O
the O O
binomial B B
distribution I I
for O O
the O O
case O O
of O O
a O O
single O O
observation O O
. O O
3. O O
invariance B B
is O O
built O O
into O O
the O O
pre-processing O O
by O O
extracting O O
features O O
that O O
are O O
invari- O O
ant O O
under O O
the O O
required O O
transformations O O
. O O
476 O O
10. O O
approximate O O
inference B B
we O O
now O O
consider O O
a O O
variational B B
distribution O O
which O O
factorizes O O
between O O
the O O
latent O O
variables O O
and O O
the O O
parameters O O
so O O
that O O
q O O
( O O
z O O
, O O
π O O
, O O
µ O O
, O O
λ O O
) O O
= O O
q O O
( O O
z O O
) O O
q O O
( O O
π O O
, O O
µ O O
, O O
λ O O
) O O
. O O
consider O O
the O O
particular O O
case O O
of O O
a O O
local B B
quadratic O O
approximation O O
around O O
a O O
point O O
w O O
( O O
cid:1 O O
) O O
that O O
is O O
a O O
minimum O O
of O O
the O O
error B B
function I I
. O O
14.10 O O
( O O
( O O
cid:12 O O
) O O
) O O
verify O O
that O O
if O O
we O O
minimize O O
the O O
sum-of-squares B B
error I I
between O O
a O O
set O O
of O O
training B B
values O O
{ O O
tn O O
} O O
and O O
a O O
single O O
predictive O B
value O O
t O O
, O O
then O O
the O O
optimal O O
solution O O
for O O
t O O
is O O
given O O
by O O
the O O
mean B B
of O O
the O O
{ O O
tn O O
} O O
. O O
novelty B O
detection I I
for O O
the O O
identiﬁcation O O
of O O
masses O O
in O O
mamograms O B
. O O
most O O
training B B
algorithms O O
involve O O
an O O
iterative O O
procedure O O
for O O
minimization O O
of O O
an O O
error B B
function I I
, O O
with O O
adjustments O O
to O O
the O O
weights O O
being O O
made O O
in O O
a O O
sequence O O
of O O
steps O O
. O O
by O O
contrast O O
, O O
consider O O
the O O
behaviour O O
of O O
a O O
simple O O
metropolis O O
algorithm O O
with O O
an O O
isotropic B B
gaussian O O
proposal B B
distribution I O
of O O
variance B B
s2 O O
, O O
considered O O
earlier O O
. O O
bayesian O O
theory B B
. O O
( O O
1.42 O O
) O O
if O O
we O O
consider O O
the O O
covariance B B
of O O
the O O
components O O
of O O
a O O
vector O O
x O O
with O O
each O O
other O O
, O O
then O O
we O O
use O O
a O O
slightly O O
simpler O O
notation O O
cov O O
[ O O
x O O
] O O
≡ O O
cov O O
[ O O
x O O
, O O
x O O
] O O
. O O
3.18 O O
( O O
( O O
cid:12 O O
) O O
( O O
cid:12 O O
) O O
) O O
www O O
by O O
completing B B
the I O
square I I
over O O
w O O
, O O
show O O
that O O
the O O
error B B
function I I
( O O
3.79 O O
) O O
in O O
bayesian O O
linear B B
regression I I
can O O
be O O
written O O
in O O
the O O
form O O
( O O
3.80 O O
) O O
. O O
6.3. O O
radial B O
basis I I
function I I
networks O O
301 O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
0 O O
−1 O O
1 O O
0.8 O O
0.6 O O
0.4 O O
0.2 O O
0 O O
−1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
−0.5 O O
0 O O
0.5 O O
1 O O
figure O O
6.2 O O
plot O O
of O O
a O O
set O O
of O O
gaussian O O
basis O B
functions O O
on O O
the O O
left O O
, O O
together O O
with O O
the O O
corresponding O O
normalized O O
basis O O
functions O O
on O O
the O O
right O O
. O O
these O O
are O O
not O O
events O O
that O O
can O O
be O O
repeated O O
numerous O B
times O O
in O O
order O O
to O O
deﬁne O O
a O O
notion O O
of O O
probability B B
as O O
we O O
did O O
earlier O O
in O O
the O O
context O O
of O O
boxes O O
of O O
fruit O O
. O O
5.4.6 O O
fast B O
multiplication I O
by O O
the O O
hessian O O
. O O
one O O
disadvantage O O
of O O
expectation B B
propagation I I
is O O
that O O
there O O
is O O
no O O
guarantee O O
that O O
the O O
iterations O O
will O O
converge O O
. O O
thus O O
we O O
obtain O O
the O O
maximum B B
likelihood I I
solution O O
in O O
the O O
form O O
k O O
= O O
mk O O
µml O O
n O O
( O O
2.33 O O
) O O
which O O
is O O
the O O
fraction O O
of O O
the O O
n O O
observations O O
for O O
which O O
xk O O
= O O
1. O O
we O O
can O O
consider O O
the O O
joint O O
distribution O O
of O O
the O O
quantities O O
m1 O O
, O O
. O O
thus O O
in O O
the O O
case O O
of O O
continuous O O
523 O O
524 O O
11. O O
sampling B O
methods I I
figure O O
11.1 O O
schematic O O
illustration O O
of O O
a O O
function O O
f O O
( O O
z O O
) O O
whose O O
expectation B B
is O O
to O O
be O O
evaluated O O
with O O
respect O O
to O O
a O O
distribution O O
p O O
( O O
z O O
) O O
. O O
now O O
let O O
us O O
apply O O
the O O
newton-raphson O O
update O O
to O O
the O O
cross-entropy B B
error I I
function I I
( O O
4.90 O O
) O O
for O O
the O O
logistic B B
regression I I
model O O
. O O
in O O
our O O
discussion O O
of O O
generative O O
approaches O O
in O O
section O O
4.2 O O
, O O
we O O
saw O O
that O O
under O O
rather O O
general O O
assumptions O O
, O O
the O O
posterior B O
probability I I
of O O
class O O
c1 O O
can O O
be O O
written O O
as O O
a O O
logistic B B
sigmoid I I
acting O O
on O O
a O O
linear O O
function O O
of O O
the O O
feature O O
vector O O
φ O O
so O O
that O O
( O O
4.87 O O
) O O
with O O
p O O
( O O
c2|φ O O
) O O
= O O
1 O O
− O O
p O O
( O O
c1|φ O O
) O O
. O O
once O O
the O O
model O O
is O O
trained O O
it O O
can O O
then O O
de- O O
termine O O
the O O
identity O O
of O O
new O O
digit O O
images O O
, O O
which O O
are O O
said O O
to O O
comprise O O
a O O
test B O
set I I
. O O
at O O
one O O
point O O
, O O
gauss O O
was O O
asked O O
to O O
conduct O O
a O O
geodetic O O
survey O O
of O O
the O O
state O O
of O O
hanover O O
, O O
which O O
led O O
to O O
his O O
for- O O
mulation O O
of O O
the O O
normal B B
distribution I O
, O O
now O O
also O O
known O O
as O O
the O O
gaussian O O
. O O
the O O
inverse B B
of O O
the O O
logistic B B
sigmoid I I
is O O
given O O
by O O
σ O O
( O O
−a O O
) O O
= O O
1 O O
− O O
σ O O
( O O
a O O
) O O
( O O
cid:17 O O
) O O
( O O
cid:18 O O
) O O
a O O
= O O
ln O O
σ O O
1 O O
− O O
σ O O
( O O
4.60 O O
) O O
( O O
4.61 O O
) O O
and O O
is O O
known O O
as O O
the O O
logit B O
function I I
. O O
in O O
other O O
words O O
, O O
the O O
set O O
of O O
nodes O O
in O O
a O O
clique B B
is O O
fully B O
connected I I
. O O
in O O
the O O
second O O
column O O
there O O
are O O
m O O
- O O
2 O O
independent B B
parameters O O
, O O
because O O
the O O
column O O
must O O
be O O
normalized O O
and O O
also O O
must O O
be O O
orthogonal O O
to O O
the O O
previous O O
column O O
, O O
and O O
so O O
on O O
. O O
however O O
, O O
these O O
must O O
be O O
restricted O O
to O O
a O O
feed-forward O O
architecture O O
, O O
in O O
other O O
words O O
to O O
one O O
having O O
no O O
closed O O
directed B B
cycles O O
, O O
to O O
ensure O O
that O O
the O O
outputs O O
are O O
deterministic O O
functions O O
of O O
the O O
inputs O O
. O O
because O O
this O O
grows O O
exponentially O O
with O O
m O O
, O O
it O O
will O O
often O O
render O O
this O O
approach O O
impractical O O
for O O
larger O O
values O O
of O O
m. O O
for O O
continuous O O
variables O O
, O O
we O O
can O O
use O O
linear-gaussian O O
conditional B B
distributions O O
in O O
which O O
each O O
node B B
has O O
a O O
gaussian O O
distribution O O
whose O O
mean B B
is O O
a O O
linear O O
function O O
of O O
its O O
parents O O
. O O
averaged O O
across O O
the O O
three O O
data O O
sets O O
, O O
the O O
mean B B
is O O
correct O O
, O O
but O O
the O O
variance B B
is O O
systematically O O
under-estimated O O
because O O
it O O
is O O
measured O O
relative B O
to O O
the O O
sample B O
mean I I
and O O
not O O
relative B B
to O O
the O O
true O O
mean B B
. O O
if O O
there O O
are O O
d O O
inputs O O
, O O
then O O
a O O
general O O
distribu- O O
tion O O
would O O
correspond O O
to O O
a O O
table O O
of O O
2d O O
numbers O O
for O O
each O O
class O O
, O O
containing O O
2d O O
− O O
1 O O
independent B B
variables I O
( O O
due O O
to O O
the O O
summation O O
constraint O O
) O O
. O O
consider O O
the O O
result O O
( O O
2.121 O O
) O O
for O O
the O O
maximum B B
likelihood I I
estimator O O
of O O
the O O
mean B B
ml O O
when O O
it O O
is O O
based O O
on O O
n O O
observations O O
. O O
the O O
original O O
image O O
shown O O
in O O
figure O O
9.3 O O
has O O
240 O O
× O O
180 O O
= O O
43 O O
, O O
200 O O
pixels O O
and O O
so O O
requires O O
24 O O
× O O
43 O O
, O O
200 O O
= O O
1 O O
, O O
036 O O
, O O
800 O O
bits B B
to O O
transmit O O
directly O O
. O O
identiﬁability B O
will O O
also O O
arise O O
when O O
we O O
discuss O O
models O O
having O O
continuous O O
latent O O
variables O O
in O O
chapter O O
12. O O
however O O
, O O
for O O
the O O
purposes O O
of O O
ﬁnding O O
a O O
good O O
density B B
model O O
, O O
it O O
is O O
irrelevant O O
because O O
any O O
of O O
the O O
equivalent O O
solutions O O
is O O
as O O
good O O
as O O
any O O
other O O
. O O
the O O
use O O
of O O
the O O
expectation B B
may O O
seem O O
somewhat O O
arbitrary O O
. O O
in O O
practice O O
, O O
for O O
anything O O
other O O
than O O
small O O
n O O
, O O
this O O
bias B B
will O O
not O O
prove O O
to O O
be O O
a O O
serious O O
problem O O
. O O
